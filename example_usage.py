#!/usr/bin/env python3
"""
Crawl4AI ä½¿ç”¨ç¤ºä¾‹
åŸºäºå®˜æ–¹æ–‡æ¡£: https://docs.crawl4ai.com/
"""

import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy, CosineStrategy
from crawl4ai.chunking_strategy import RegexChunking

async def basic_crawling_example():
    """åŸºæœ¬çˆ¬å–ç¤ºä¾‹ - æ¥è‡ªå®˜æ–¹æ–‡æ¡£"""
    print("\nğŸš€ åŸºæœ¬çˆ¬å–ç¤ºä¾‹")
    print("=" * 50)
    
    async with AsyncWebCrawler() as crawler:
        # çˆ¬å–å®˜æ–¹ç¤ºä¾‹é¡µé¢
        result = await crawler.arun(url="https://crawl4ai.com")
        
        print(f"âœ… é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
        print(f"ğŸ“„ HTML é•¿åº¦: {len(result.html)}")
        print(f"ğŸ“ Markdown é•¿åº¦: {len(result.markdown)}")
        print(f"ğŸ”— é“¾æ¥æ•°é‡: {len(result.links.get('internal', [])) + len(result.links.get('external', []))}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {len(result.media.get('images', []))}")
        
        # æ˜¾ç¤º Markdown å†…å®¹çš„å‰300å­—ç¬¦
        print(f"\nğŸ“‹ Markdown å†…å®¹é¢„è§ˆ:")
        print("-" * 30)
        print(result.markdown[:300] + "..." if len(result.markdown) > 300 else result.markdown)

async def structured_extraction_example():
    """ç»“æ„åŒ–æå–ç¤ºä¾‹"""
    print("\nğŸ¯ ç»“æ„åŒ–æå–ç¤ºä¾‹")
    print("=" * 50)
    
    async with AsyncWebCrawler() as crawler:
        # ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–ç‰¹å®šå†…å®¹
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            css_selector="tr.athing"  # æå– Hacker News çš„æ–‡ç« æ¡ç›®
        )
        
        print(f"âœ… æˆåŠŸæå–ç»“æ„åŒ–å†…å®¹")
        print(f"ğŸ“Š æå–çš„å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
        print(f"ğŸ” å†…å®¹é¢„è§ˆ:")
        print("-" * 30)
        print(result.cleaned_html[:400] + "..." if len(result.cleaned_html) > 400 else result.cleaned_html)

async def markdown_generation_example():
    """Markdown ç”Ÿæˆç¤ºä¾‹"""
    print("\nğŸ“ Markdown ç”Ÿæˆç¤ºä¾‹")
    print("=" * 50)
    
    async with AsyncWebCrawler() as crawler:
        # ç”Ÿæˆæ¸…æ´çš„ Markdownï¼Œé€‚åˆ RAG æµæ°´çº¿
        result = await crawler.arun(
            url="https://python.org",
            word_count_threshold=10,  # è¿‡æ»¤æ‰å°‘äº10ä¸ªè¯çš„å†…å®¹å—
            only_text=True  # åªè¿”å›æ–‡æœ¬å†…å®¹
        )
        
        print(f"âœ… ç”Ÿæˆ Markdown æˆåŠŸ")
        print(f"ğŸ“ Markdown é•¿åº¦: {len(result.markdown)}")
        print(f"ğŸ§¹ æ¸…æ´æ–‡æœ¬é•¿åº¦: {len(result.cleaned_html)}")
        
        # ä¿å­˜ Markdown åˆ°æ–‡ä»¶
        with open('/Users/lawgenesis-q6lr/Desktop/crawl4ai/python_org_content.md', 'w', encoding='utf-8') as f:
            f.write(result.markdown)
        print(f"ğŸ’¾ Markdown å·²ä¿å­˜åˆ°: python_org_content.md")

async def advanced_crawling_example():
    """é«˜çº§çˆ¬å–ç¤ºä¾‹ - ä½¿ç”¨è‡ªå®šä¹‰é…ç½®"""
    print("\nâš™ï¸ é«˜çº§çˆ¬å–ç¤ºä¾‹")
    print("=" * 50)
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # ä½¿ç”¨é«˜çº§å‚æ•°è¿›è¡Œçˆ¬å–
        result = await crawler.arun(
            url="https://example.com",
            word_count_threshold=5,
            exclude_external_links=True,  # æ’é™¤å¤–éƒ¨é“¾æ¥
            exclude_social_media_links=True,  # æ’é™¤ç¤¾äº¤åª’ä½“é“¾æ¥
            bypass_cache=True,  # ç»•è¿‡ç¼“å­˜
            process_iframes=True,  # å¤„ç† iframe
            remove_overlay_elements=True  # ç§»é™¤è¦†ç›–å…ƒç´ 
        )
        
        print(f"âœ… é«˜çº§çˆ¬å–å®Œæˆ")
        print(f"ğŸ“Š å¤„ç†åçš„å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
        print(f"ğŸ”— å†…éƒ¨é“¾æ¥: {len(result.links.get('internal', []))}")
        print(f"ğŸŒ å¤–éƒ¨é“¾æ¥: {len(result.links.get('external', []))}")
        print(f"âœ… é¡µé¢æˆåŠŸå¤„ç†: {result.success}")

async def content_filtering_example():
    """å†…å®¹è¿‡æ»¤ç¤ºä¾‹"""
    print("\nğŸ” å†…å®¹è¿‡æ»¤ç¤ºä¾‹")
    print("=" * 50)
    
    async with AsyncWebCrawler() as crawler:
        # ä½¿ç”¨å†…å®¹è¿‡æ»¤
        result = await crawler.arun(
            url="https://httpbin.org/html",
            excluded_tags=['script', 'style', 'nav', 'footer'],  # æ’é™¤ç‰¹å®šæ ‡ç­¾
            word_count_threshold=10,  # æœ€å°è¯æ•°é˜ˆå€¼
            only_text=False  # ä¿ç•™HTMLç»“æ„
        )
        
        print(f"âœ… å†…å®¹è¿‡æ»¤å®Œæˆ")
        print(f"ğŸ“„ è¿‡æ»¤åHTMLé•¿åº¦: {len(result.html)}")
        print(f"ğŸ“ æ¸…æ´å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
        print(f"ğŸ¯ Markdowné•¿åº¦: {len(result.markdown)}")

async def multiple_urls_example():
    """å¤šURLçˆ¬å–ç¤ºä¾‹"""
    print("\nğŸŒ å¤šURLçˆ¬å–ç¤ºä¾‹")
    print("=" * 50)
    
    urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/user-agent"
    ]
    
    async with AsyncWebCrawler() as crawler:
        # æ‰¹é‡çˆ¬å–å¤šä¸ªURL
        results = await crawler.arun_many(
            urls=urls,
            word_count_threshold=5
        )
        
        print(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆ")
        print(f"ğŸ“Š çˆ¬å–URLæ•°é‡: {len(results)}")
        
        for i, result in enumerate(results):
            if result.success:
                print(f"  {i+1}. {urls[i]} - æˆåŠŸ (é•¿åº¦: {len(result.markdown)})")
            else:
                print(f"  {i+1}. {urls[i]} - å¤±è´¥: {result.error_message}")

async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¯ Crawl4AI å®˜æ–¹æ–‡æ¡£ç¤ºä¾‹æµ‹è¯•")
    print("åŸºäºå®˜æ–¹æ–‡æ¡£: https://docs.crawl4ai.com/")
    print("=" * 60)
    
    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        await basic_crawling_example()
        await structured_extraction_example()
        await markdown_generation_example()
        await advanced_crawling_example()
        await content_filtering_example()
        await multiple_urls_example()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ ä½ ç°åœ¨å¯ä»¥æ ¹æ®è¿™äº›ç¤ºä¾‹å¼€å§‹ä½¿ç”¨ Crawl4AI äº†")
        print("ğŸ“š æ›´å¤šä¿¡æ¯è¯·è®¿é—®: https://docs.crawl4ai.com/")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
