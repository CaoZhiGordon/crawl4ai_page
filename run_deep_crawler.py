#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦ç½‘ç«™çˆ¬å–å™¨ - ç”¨æˆ·å‹å¥½çš„è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªç®€å•çš„ç•Œé¢æ¥é…ç½®å’Œè¿è¡Œæ·±åº¦ç½‘ç«™çˆ¬å–å™¨ã€‚
æ‚¨å¯ä»¥é€šè¿‡ä¿®æ”¹ä¸‹é¢çš„é…ç½®æ¥è‡ªå®šä¹‰çˆ¬å–è¡Œä¸ºã€‚

ä½¿ç”¨æ–¹æ³•:
1. ä¿®æ”¹ä¸‹æ–¹çš„ CRAWL_CONFIGS é…ç½®
2. è¿è¡Œ: python run_deep_crawler.py
"""

import asyncio
import sys
from pathlib import Path

# å¯¼å…¥æˆ‘ä»¬çš„æ·±åº¦çˆ¬å–å™¨
try:
    from deep_website_crawler import DeepWebsiteCrawler, CrawlConfig
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ deep_website_crawlerï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

# ğŸ¯ çˆ¬å–é…ç½® - åœ¨è¿™é‡Œè‡ªå®šä¹‰æ‚¨çš„çˆ¬å–è®¾ç½®
CRAWL_CONFIGS = {
    # PyOD å®˜æ–¹æ–‡æ¡£ç½‘ç«™
    "pyod_docs": CrawlConfig(
        start_url="https://pyod.readthedocs.io/en/latest/index.html",
        max_depth=3,              # çˆ¬å–æ·±åº¦ï¼š3å±‚
        max_pages=100,            # æœ€å¤§é¡µé¢æ•°ï¼š100é¡µ
        concurrent_limit=3,       # å¹¶å‘æ•°ï¼š3ä¸ªåŒæ—¶è¿›è¡Œ
        delay=1.5,                # æ¯æ¬¡è¯·æ±‚é—´éš”ï¼š1.5ç§’
        same_domain_only=True,    # åªçˆ¬å–åŒåŸŸåé¡µé¢
        output_dir="pyod_website_crawl", # è¾“å‡ºç›®å½•
        merge_markdown=True,      # ç”Ÿæˆåˆå¹¶çš„markdownæ–‡æ¡£
        word_count_threshold=10,  # æœ€å°è¯æ•°é˜ˆå€¼
        # é¢å¤–çš„æ’é™¤æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
        exclude_patterns=[
            r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$', 
            r'/search\?', r'/genindex', r'/modindex',
            r'/_static/', r'/_sources/'
        ]
    ),
    
    # å…¶ä»–ç½‘ç«™ç¤ºä¾‹é…ç½®
    "example_blog": CrawlConfig(
        start_url="https://example-blog.com",
        max_depth=2,
        max_pages=50,
        concurrent_limit=2,
        delay=2.0,
        same_domain_only=True,
        output_dir="example_blog_crawl",
        merge_markdown=True,
        include_patterns=[r'/blog/', r'/post/']  # åªåŒ…å«åšå®¢æ–‡ç« 
    ),
    
    "custom_site": CrawlConfig(
        start_url="https://your-target-site.com",
        max_depth=2,
        max_pages=30,
        concurrent_limit=2,
        delay=1.0,
        same_domain_only=True,
        output_dir="custom_site_crawl",
        merge_markdown=True
    )
}

def print_banner():
    """æ˜¾ç¤ºç¨‹åºæ¨ªå¹…"""
    print("ğŸŒ" + "=" * 58 + "ğŸŒ")
    print("          æ·±åº¦ç½‘ç«™çˆ¬å–å™¨ - Deep Website Crawler")
    print("                   åŸºäº crawl4ai æ„å»º")
    print("ğŸŒ" + "=" * 58 + "ğŸŒ")

def print_config_info(config_name: str, config: CrawlConfig):
    """æ˜¾ç¤ºé…ç½®ä¿¡æ¯"""
    print(f"\nğŸ“‹ çˆ¬å–é…ç½®: {config_name}")
    print("-" * 50)
    print(f"ğŸ¯ èµ·å§‹URL: {config.start_url}")
    print(f"ğŸ—ï¸  æœ€å¤§æ·±åº¦: {config.max_depth} å±‚")
    print(f"ğŸ“„ æœ€å¤§é¡µé¢: {config.max_pages} é¡µ")
    print(f"âš¡ å¹¶å‘æ•°: {config.concurrent_limit} ä¸ª")
    print(f"â±ï¸  è¯·æ±‚å»¶æ—¶: {config.delay} ç§’")
    print(f"ğŸŒ åŒåŸŸé™åˆ¶: {'æ˜¯' if config.same_domain_only else 'å¦'}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"ğŸ“ åˆå¹¶æ–‡æ¡£: {'æ˜¯' if config.merge_markdown else 'å¦'}")

def select_config() -> tuple[str, CrawlConfig]:
    """è®©ç”¨æˆ·é€‰æ‹©é…ç½®"""
    print("\nğŸ¯ å¯ç”¨çš„çˆ¬å–é…ç½®:")
    print("-" * 30)
    
    configs_list = list(CRAWL_CONFIGS.items())
    for i, (name, config) in enumerate(configs_list, 1):
        domain = config.start_url.split('/')[2] if '//' in config.start_url else config.start_url
        print(f"{i}. {name} - {domain}")
    
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©é…ç½® (1-{len(configs_list)}) æˆ–æŒ‰ Enter ä½¿ç”¨é»˜è®¤é…ç½® [1]: ").strip()
            
            if not choice:  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
                choice = 1
            else:
                choice = int(choice)
            
            if 1 <= choice <= len(configs_list):
                selected = configs_list[choice - 1]
                return selected[0], selected[1]
            else:
                print(f"âŒ è¯·è¾“å…¥ 1-{len(configs_list)} ä¹‹é—´çš„æ•°å­—")
                
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            sys.exit(0)

def confirm_crawl(config_name: str, config: CrawlConfig) -> bool:
    """ç¡®è®¤å¼€å§‹çˆ¬å–"""
    print_config_info(config_name, config)
    
    print(f"\nâš ï¸  æ³¨æ„äº‹é¡¹:")
    print(f"â€¢ è¯·éµå®ˆç½‘ç«™çš„robots.txtå’Œä½¿ç”¨æ¡æ¬¾")
    print(f"â€¢ çˆ¬å–å¯èƒ½éœ€è¦ {config.max_pages * config.delay / 60:.1f} åˆ†é’Ÿæˆ–æ›´é•¿æ—¶é—´")
    print(f"â€¢ è¯·ç¡®ä¿æ‚¨æœ‰æƒé™çˆ¬å–ç›®æ ‡ç½‘ç«™")
    
    while True:
        try:
            confirm = input("\nâ“ ç¡®å®šå¼€å§‹çˆ¬å–ï¼Ÿ(y/N): ").strip().lower()
            if confirm in ['y', 'yes', 'æ˜¯', 'ok']:
                return True
            elif confirm in ['n', 'no', 'å¦', ''] or not confirm:
                return False
            else:
                print("è¯·è¾“å…¥ y æˆ– n")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            sys.exit(0)

async def run_crawler(config_name: str, config: CrawlConfig):
    """è¿è¡Œçˆ¬å–å™¨"""
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–: {config_name}")
    print("=" * 60)
    
    try:
        crawler = DeepWebsiteCrawler(config)
        
        # å¼€å§‹çˆ¬å–
        results = await crawler.crawl_website()
        
        # ç”Ÿæˆåˆå¹¶æ–‡æ¡£
        if config.merge_markdown and results:
            await crawler.generate_merged_markdown()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\n" + "ğŸ‰" + "=" * 58 + "ğŸ‰")
        print(f"           çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        print(f"âœ… æˆåŠŸçˆ¬å–: {len(results)} ä¸ªé¡µé¢")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.output_dir}/")
        print(f"ğŸ“ ä¸»è¦æ–‡ä»¶:")
        print(f"   â€¢ {config.output_dir}/merged_website_content.md (åˆå¹¶æ–‡æ¡£)")
        print(f"   â€¢ {config.output_dir}/html/ (HTMLæ–‡ä»¶)")
        print(f"   â€¢ {config.output_dir}/markdown/ (å•é¡µé¢MDæ–‡ä»¶)")
        print(f"   â€¢ {config.output_dir}/metadata/ (å…ƒæ•°æ®)")
        print("ğŸ‰" + "=" * 58 + "ğŸ‰")
        
        return True
        
    except KeyboardInterrupt:
        print("\nâ›” çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("-" * 20)
    print("1. çˆ¬å–PyODå®˜æ–¹æ–‡æ¡£ï¼š")
    print("   é€‰æ‹©é…ç½® 1 (pyod_docs)")
    print("   å°†ç”Ÿæˆå®Œæ•´çš„PyODæ–‡æ¡£markdownç‰ˆæœ¬")
    
    print("\n2. è‡ªå®šä¹‰ç½‘ç«™çˆ¬å–ï¼š")
    print("   ä¿®æ”¹ CRAWL_CONFIGS ä¸­çš„ 'custom_site' é…ç½®")
    print("   è®¾ç½®æ‚¨æƒ³çˆ¬å–çš„ç½‘ç«™URLå’Œå‚æ•°")
    
    print("\n3. æ·»åŠ æ–°é…ç½®ï¼š")
    print("   åœ¨ CRAWL_CONFIGS ä¸­æ·»åŠ æ–°çš„é…ç½®é¡¹")
    print("   è®¾ç½®URLã€æ·±åº¦ã€é¡µé¢æ•°ç­‰å‚æ•°")

async def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import crawl4ai
        import aiohttp
        from bs4 import BeautifulSoup
    except ImportError as e:
        print(f"\nâŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…æ‰€éœ€ä¾èµ–:")
        print("pip install crawl4ai aiohttp beautifulsoup4")
        sys.exit(1)
    
    show_usage_examples()
    
    try:
        # è®©ç”¨æˆ·é€‰æ‹©é…ç½®
        config_name, config = select_config()
        
        # ç¡®è®¤çˆ¬å–
        if confirm_crawl(config_name, config):
            # å¼€å§‹çˆ¬å–
            success = await run_crawler(config_name, config)
            
            if success:
                print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆï¼æŸ¥çœ‹ {config.output_dir}/ ç›®å½•è·å–ç»“æœ")
            else:
                print(f"\nğŸ˜ ä»»åŠ¡æœªèƒ½å®Œæˆ")
        else:
            print("\nğŸ‘‹ ä»»åŠ¡å·²å–æ¶ˆï¼Œå†è§ï¼")
    
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())

