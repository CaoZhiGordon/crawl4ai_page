# 🌐 深度网站爬取器使用指南

基于 crawl4ai 开发的智能网站深度爬取工具，可以递归爬取任意网站的所有子页面，并生成完整的markdown文档。

## 📋 功能特性

- ✅ **递归深度爬取**: 自动发现并爬取网站的所有子页面
- ✅ **智能链接过滤**: 自动过滤无效链接、文件下载链接等
- ✅ **并发爬取**: 支持多线程并发，提高爬取效率
- ✅ **防封机制**: 支持请求延时、域名限制等防封措施
- ✅ **结果管理**: 自动保存HTML、Markdown、元数据等
- ✅ **文档合并**: 将所有页面合并为一个完整的markdown项目
- ✅ **进度显示**: 实时显示爬取进度和统计信息
- ✅ **配置灵活**: 支持深度限制、页面数限制、内容过滤等

## 🚀 快速开始

### 1. 环境准备

确保已安装所需依赖：

```bash
pip install crawl4ai aiohttp beautifulsoup4
```

### 2. 运行爬取器

最简单的方式是使用预配置的运行脚本：

```bash
python run_deep_crawler.py
```

程序会显示可用的配置选项，选择一个即可开始爬取。

### 3. 默认配置示例

#### PyOD官方文档爬取
```python
config = CrawlConfig(
    start_url="https://pyod.readthedocs.io/en/latest/index.html",
    max_depth=3,              # 爬取深度：3层
    max_pages=100,            # 最大页面数：100页
    concurrent_limit=3,       # 并发数：3个
    delay=1.5,                # 请求间隔：1.5秒
    same_domain_only=True,    # 只爬取同域名
    output_dir="pyod_website_crawl",
    merge_markdown=True       # 生成合并文档
)
```

## ⚙️ 配置说明

### CrawlConfig 参数详解

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `start_url` | str | 必需 | 起始爬取URL |
| `max_depth` | int | 3 | 最大爬取深度（层数） |
| `max_pages` | int | 100 | 最大爬取页面数 |
| `concurrent_limit` | int | 5 | 并发爬取线程数 |
| `delay` | float | 1.0 | 每次请求间隔（秒） |
| `same_domain_only` | bool | True | 是否只爬取同域名页面 |
| `output_dir` | str | "deep_crawl_results" | 结果输出目录 |
| `merge_markdown` | bool | True | 是否生成合并的markdown文档 |
| `word_count_threshold` | int | 10 | 最小词数阈值（过滤短内容） |
| `include_patterns` | List[str] | [] | 包含URL模式（正则表达式） |
| `exclude_patterns` | List[str] | [预设] | 排除URL模式（正则表达式） |

### 默认排除模式

程序自动排除以下类型的链接：
- 文件下载：`.pdf`, `.jpg`, `.png`, `.gif`, `.zip` 等
- 脚本文件：`.css`, `.js`, `.xml`, `.json`
- 功能页面：搜索、登录、注册、管理页面
- 锚点链接：`#` 开头的页面内跳转

## 📁 输出结果

爬取完成后，会在指定目录生成以下文件结构：

```
output_dir/
├── merged_website_content.md    # 🔥 合并的完整文档
├── html/                        # 原始HTML文件
│   ├── 20250101_120000_page1.html
│   └── 20250101_120001_page2.html
├── markdown/                    # 单页面Markdown文件
│   ├── 20250101_120000_page1.md
│   └── 20250101_120001_page2.md
├── metadata/                    # 页面元数据
│   ├── 20250101_120000_page1_metadata.json
│   └── 20250101_120001_page2_metadata.json
└── pages/                      # 其他临时文件
```

### 主要输出文件

1. **`merged_website_content.md`** - 🔥 **最重要的文件**
   - 包含所有页面的完整内容
   - 自动生成目录结构
   - 按深度和URL排序
   - 包含爬取统计信息

2. **`html/` 目录** - 原始HTML文件
   - 保留完整的原始页面结构
   - 可用于后续分析或备份

3. **`markdown/` 目录** - 单页面Markdown文件
   - 每个页面对应一个MD文件
   - 包含页面标题、URL、爬取时间等元信息

4. **`metadata/` 目录** - 页面元数据
   - JSON格式的详细元数据
   - 包含链接统计、文件路径等信息

## 🎯 使用场景示例

### 1. 爬取技术文档网站

```python
# 适合：官方文档、API文档、技术博客
config = CrawlConfig(
    start_url="https://docs.example.com",
    max_depth=4,
    max_pages=200,
    concurrent_limit=3,
    delay=1.0,
    include_patterns=[r'/docs/', r'/api/', r'/guide/']
)
```

### 2. 爬取新闻博客网站

```python
# 适合：新闻网站、博客、内容网站
config = CrawlConfig(
    start_url="https://blog.example.com",
    max_depth=3,
    max_pages=100,
    delay=2.0,  # 更保守的延时
    include_patterns=[r'/post/', r'/article/', r'/blog/']
)
```

### 3. 爬取企业网站

```python
# 适合：公司网站、产品介绍
config = CrawlConfig(
    start_url="https://company.example.com",
    max_depth=2,
    max_pages=50,
    exclude_patterns=[
        r'/careers/', r'/jobs/', r'/admin/',  # 排除招聘和管理页面
        r'\.pdf$', r'/download/'  # 排除下载页面
    ]
)
```

## ⚠️ 注意事项和最佳实践

### 1. 法律和道德规范
- ✅ **始终遵守** 目标网站的 `robots.txt` 文件
- ✅ **尊重** 网站的服务条款和使用政策
- ✅ **合理设置** 请求频率，避免对服务器造成压力
- ✅ **仅用于** 合法的学习、研究或个人用途

### 2. 技术最佳实践

#### 设置合适的延时
```python
# 小型网站
delay=1.0  # 1秒间隔

# 大型网站或敏感网站
delay=2.0  # 2-3秒间隔，更安全

# 非常保守的设置
delay=5.0  # 5秒间隔，适用于严格的网站
```

#### 合理设置并发数
```python
# 保守设置（推荐）
concurrent_limit=2

# 中等设置
concurrent_limit=3

# 激进设置（谨慎使用）
concurrent_limit=5
```

#### 控制爬取规模
```python
# 测试爬取
max_pages=10, max_depth=1

# 小规模爬取
max_pages=50, max_depth=2

# 大规模爬取
max_pages=200, max_depth=3
```

### 3. 故障排除

#### 常见问题及解决方案

**Q: 爬取速度太慢？**
```python
# 解决方案：适度增加并发，减少延时
concurrent_limit=4  # 增加并发
delay=0.5  # 减少延时（谨慎）
```

**Q: 经常被网站阻止？**
```python
# 解决方案：更保守的设置
concurrent_limit=1  # 单线程
delay=3.0  # 增加延时
# 考虑使用代理或更换User-Agent
```

**Q: 爬取到太多无用页面？**
```python
# 解决方案：完善过滤规则
exclude_patterns=[
    r'/search\?', r'/filter\?',  # 排除搜索和过滤页面
    r'/tag/', r'/category/',     # 排除标签和分类页面
    r'\.pdf$', r'\.doc$'         # 排除文档文件
]
```

**Q: 内存使用过高？**
```python
# 解决方案：限制页面数量和并发
max_pages=50        # 减少页面数
concurrent_limit=2  # 减少并发
```

### 4. 性能优化建议

1. **分批爬取**: 对于大型网站，考虑分批多次爬取
2. **监控资源**: 注意CPU和内存使用情况
3. **网络稳定**: 确保网络连接稳定
4. **存储空间**: 确保有足够的磁盘空间存储结果

## 🔧 自定义开发

### 添加新的配置

在 `run_deep_crawler.py` 中的 `CRAWL_CONFIGS` 添加新配置：

```python
CRAWL_CONFIGS = {
    "my_custom_site": CrawlConfig(
        start_url="https://my-target-site.com",
        max_depth=2,
        max_pages=30,
        concurrent_limit=2,
        delay=1.5,
        output_dir="my_site_crawl",
        # 自定义过滤规则
        include_patterns=[r'/articles/', r'/posts/'],
        exclude_patterns=[r'/ads/', r'/tracking/']
    )
}
```

### 扩展功能

如需添加新功能，可以修改 `deep_website_crawler.py`：

1. **添加新的提取策略**
2. **实现内容后处理**
3. **增加新的输出格式**
4. **集成数据库存储**

## 📊 示例输出

爬取完成后的统计信息示例：

```
🎉 网站爬取完成!
📊 总计爬取: 45 个页面
⏱️ 总耗时: 125.30 秒
💾 结果保存在: pyod_website_crawl

✅ 合并文档已生成: pyod_website_crawl/merged_website_content.md
📄 文档大小: 1,234,567 字符

📊 爬取统计:
- 总页面数: 45
- 总字符数: 1,234,567
- 各深度页面分布:
  - 深度 0: 1 页面
  - 深度 1: 12 页面
  - 深度 2: 23 页面
  - 深度 3: 9 页面
```

## 🆘 支持和反馈

如果您遇到问题或有改进建议：

1. **检查配置**: 确认所有参数设置正确
2. **查看日志**: 注意控制台输出的错误信息
3. **调整参数**: 尝试更保守的爬取设置
4. **检查网络**: 确认目标网站可正常访问

---

**祝您使用愉快！🎉**

> **免责声明**: 本工具仅供学习和研究使用。使用者需自行承担使用本工具的法律责任，并确保遵守相关法律法规和网站使用条款。

