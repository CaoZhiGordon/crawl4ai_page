#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Crawl4AI å®‰è£…å’ŒåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•å„ç§åŸºæœ¬åŠŸèƒ½ä»¥éªŒè¯åº“æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import sys
from crawl4ai import AsyncWebCrawler

async def test_basic_crawl():
    """æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½...")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # æµ‹è¯•æŠ“å–ä¸€ä¸ªç®€å•çš„ç½‘é¡µ
        result = await crawler.arun(url="https://httpbin.org/html")
        
        print(f"âœ… æˆåŠŸæŠ“å–é¡µé¢")
        print(f"ğŸ“Š HTML é•¿åº¦: {len(result.html)}")
        print(f"ğŸ“‹ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
        print(f"â±ï¸  æŠ“å–æ—¶é—´: {result.metadata.get('processing_time', 'N/A')} ç§’")
        
        return result

async def test_text_extraction():
    """æµ‹è¯•æ–‡æœ¬æå–åŠŸèƒ½"""
    print("\nğŸ”¤ å¼€å§‹æµ‹è¯•æ–‡æœ¬æå–åŠŸèƒ½...")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://httpbin.org/html",
            word_count_threshold=10
        )
        
        print(f"âœ… æˆåŠŸæå–æ–‡æœ¬")
        print(f"ğŸ“ æå–çš„æ–‡æœ¬é•¿åº¦: {len(result.cleaned_html)}")
        print(f"ğŸ” æ–‡æœ¬å†…å®¹é¢„è§ˆ: {result.cleaned_html[:200]}...")
        
        return result

async def test_with_custom_config():
    """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
    print("\nâš™ï¸  å¼€å§‹æµ‹è¯•è‡ªå®šä¹‰é…ç½®...")
    
    # ä½¿ç”¨ç®€å•çš„é…ç½®æµ‹è¯•
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://httpbin.org/user-agent",
            word_count_threshold=5
        )
        
        print(f"âœ… æˆåŠŸä½¿ç”¨è‡ªå®šä¹‰é…ç½®")
        print(f"ğŸ¤– ç”¨æˆ·ä»£ç†æµ‹è¯•: {len(result.html) > 0}")
        print(f"ğŸ“„ å“åº”å†…å®¹é•¿åº¦: {len(result.html)}")
        print(f"ğŸ” å“åº”å†…å®¹é¢„è§ˆ: {result.cleaned_html[:100] if result.cleaned_html else 'N/A'}...")
        
        return result

def test_import():
    """æµ‹è¯•åº“å¯¼å…¥"""
    print("ğŸ“¦ æµ‹è¯•åº“å¯¼å…¥...")
    
    try:
        import crawl4ai
        print(f"âœ… crawl4ai ç‰ˆæœ¬: {crawl4ai.__version__ if hasattr(crawl4ai, '__version__') else 'æœªçŸ¥'}")
        
        from crawl4ai import AsyncWebCrawler
        print("âœ… AsyncWebCrawler å¯¼å…¥æˆåŠŸ")
        
        from crawl4ai.extraction_strategy import LLMExtractionStrategy, CosineStrategy
        print("âœ… æå–ç­–ç•¥å¯¼å…¥æˆåŠŸ")
        
        from crawl4ai.chunking_strategy import RegexChunking, NlpSentenceChunking
        print("âœ… åˆ†å—ç­–ç•¥å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Crawl4AI å®‰è£…æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # æµ‹è¯•å¯¼å…¥
    import_success = test_import()
    
    if not import_success:
        print("\nâŒ åº“å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        sys.exit(1)
    
    try:
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        await test_basic_crawl()
        
        # æ–‡æœ¬æå–æµ‹è¯•  
        await test_text_extraction()
        
        # è‡ªå®šä¹‰é…ç½®æµ‹è¯•
        await test_with_custom_config()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Crawl4AI å®‰è£…æˆåŠŸä¸”å·¥ä½œæ­£å¸¸")
        print("ğŸ’¡ ä½ ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨ Crawl4AI è¿›è¡Œç½‘é¡µçˆ¬å–äº†")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"ğŸ”§ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())
