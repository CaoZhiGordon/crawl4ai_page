#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦ç½‘ç«™çˆ¬å–è„šæœ¬
ä½¿ç”¨ crawl4ai å®ç°å¯¹ä»»æ„ç½‘ç«™çš„æ·±åº¦çˆ¬å–ï¼Œå¹¶å°†ç»“æœåˆå¹¶ä¸ºå®Œæ•´çš„markdowné¡¹ç›®

åŠŸèƒ½ç‰¹æ€§ï¼š
- é€’å½’çˆ¬å–ç½‘ç«™çš„æ‰€æœ‰å­é¡µé¢
- æ™ºèƒ½é“¾æ¥å‘ç°å’Œè¿‡æ»¤
- é¿å…é‡å¤çˆ¬å–
- å¹¶å‘çˆ¬å–æ”¯æŒ
- ç”Ÿæˆå®Œæ•´çš„markdownæ–‡æ¡£
- æ”¯æŒæ·±åº¦é™åˆ¶å’ŒåŸŸåè¿‡æ»¤
- è¿›åº¦å®æ—¶æ˜¾ç¤º

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0
åŸºäº: crawl4ai
"""

import asyncio
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Set, List, Dict, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse, urlunparse
from dataclasses import dataclass, field

try:
    from crawl4ai import AsyncWebCrawler
    import aiohttp
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…æ‰€éœ€ä¾èµ–: pip install crawl4ai aiohttp beautifulsoup4")
    sys.exit(1)

@dataclass
class CrawlConfig:
    """çˆ¬å–é…ç½®ç±»"""
    start_url: str
    max_depth: int = 3
    max_pages: int = 100
    concurrent_limit: int = 5
    delay: float = 1.0
    same_domain_only: bool = True
    include_patterns: List[str] = field(default_factory=list)
    exclude_patterns: List[str] = field(default_factory=list)
    output_dir: str = "deep_crawl_results"
    merge_markdown: bool = True
    word_count_threshold: int = 10
    
    def __post_init__(self):
        # é»˜è®¤æ’é™¤æ¨¡å¼ï¼šå¸¸è§çš„éå†…å®¹é“¾æ¥
        if not self.exclude_patterns:
            self.exclude_patterns = [
                r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$', r'\.svg$',
                r'\.css$', r'\.js$', r'\.xml$', r'\.json$',
                r'/search\?', r'/login', r'/logout', r'/register',
                r'/admin', r'/api/', r'#', r'javascript:',
                r'\.zip$', r'\.rar$', r'\.exe$', r'\.dmg$'
            ]

@dataclass
class PageResult:
    """å•ä¸ªé¡µé¢çˆ¬å–ç»“æœ"""
    url: str
    title: str
    markdown: str
    html: str
    links: List[str]
    metadata: Dict[str, Any]
    depth: int
    timestamp: datetime
    success: bool
    error: Optional[str] = None

class DeepWebsiteCrawler:
    """æ·±åº¦ç½‘ç«™çˆ¬å–å™¨"""
    
    def __init__(self, config: CrawlConfig):
        self.config = config
        self.visited_urls: Set[str] = set()
        self.pending_urls: Set[str] = set()
        self.results: Dict[str, PageResult] = {}
        self.semaphore = asyncio.Semaphore(config.concurrent_limit)
        self.session = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.setup_output_directory()
        
        # è·å–ç›®æ ‡åŸŸå
        self.target_domain = urlparse(config.start_url).netloc
        
        print(f"ğŸš€ åˆå§‹åŒ–æ·±åº¦çˆ¬å–å™¨")
        print(f"ğŸ“ èµ·å§‹URL: {config.start_url}")
        print(f"ğŸŒ ç›®æ ‡åŸŸå: {self.target_domain}")
        print(f"ğŸ“Š é…ç½®: æœ€å¤§æ·±åº¦={config.max_depth}, æœ€å¤§é¡µé¢={config.max_pages}")
    
    def setup_output_directory(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        try:
            # åˆ›å»ºä¸»ç›®å½•å’Œå­ç›®å½•
            dirs = [
                self.config.output_dir,
                f"{self.config.output_dir}/pages",
                f"{self.config.output_dir}/html",
                f"{self.config.output_dir}/markdown", 
                f"{self.config.output_dir}/metadata"
            ]
            
            for dir_path in dirs:
                os.makedirs(dir_path, exist_ok=True)
            
            print(f"âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡: {self.config.output_dir}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
            sys.exit(1)
    
    def normalize_url(self, url: str, base_url: str) -> Optional[str]:
        """è§„èŒƒåŒ–URL"""
        try:
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            url = urljoin(base_url, url)
            
            # è§£æURL
            parsed = urlparse(url)
            
            # ç§»é™¤fragmentï¼ˆé”šç‚¹ï¼‰
            parsed = parsed._replace(fragment='')
            
            # é‡æ–°ç»„åˆURL
            normalized = urlunparse(parsed)
            
            return normalized
        except Exception:
            return None
    
    def is_valid_url(self, url: str, depth: int) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è¢«çˆ¬å–"""
        try:
            # åŸºæœ¬æ£€æŸ¥
            if not url or url in self.visited_urls:
                return False
            
            # æ·±åº¦æ£€æŸ¥
            if depth > self.config.max_depth:
                return False
            
            # é¡µé¢æ•°é‡é™åˆ¶
            if len(self.visited_urls) >= self.config.max_pages:
                return False
            
            parsed = urlparse(url)
            
            # åŸŸåæ£€æŸ¥
            if self.config.same_domain_only and parsed.netloc != self.target_domain:
                return False
            
            # åè®®æ£€æŸ¥
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # æ’é™¤æ¨¡å¼æ£€æŸ¥
            for pattern in self.config.exclude_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return False
            
            # åŒ…å«æ¨¡å¼æ£€æŸ¥ï¼ˆå¦‚æœæœ‰æŒ‡å®šï¼‰
            if self.config.include_patterns:
                for pattern in self.config.include_patterns:
                    if re.search(pattern, url, re.IGNORECASE):
                        break
                else:
                    return False
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ URLéªŒè¯é”™è¯¯ {url}: {e}")
            return False
    
    def extract_links_from_content(self, html: str, base_url: str) -> List[str]:
        """ä»HTMLå†…å®¹ä¸­æå–é“¾æ¥"""
        links = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ‰€æœ‰é“¾æ¥
            for link_tag in soup.find_all('a', href=True):
                href = link_tag['href'].strip()
                if href:
                    normalized = self.normalize_url(href, base_url)
                    if normalized and normalized not in links:
                        links.append(normalized)
            
            print(f"ğŸ”— ä»é¡µé¢æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
            return links
            
        except Exception as e:
            print(f"âš ï¸ æå–é“¾æ¥å¤±è´¥: {e}")
            return []
    
    async def crawl_single_page(self, url: str, depth: int) -> Optional[PageResult]:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        async with self.semaphore:
            try:
                print(f"ğŸŒ æ­£åœ¨çˆ¬å– (æ·±åº¦ {depth}): {url}")
                
                async with AsyncWebCrawler(verbose=False) as crawler:
                    result = await crawler.arun(
                        url=url,
                        word_count_threshold=self.config.word_count_threshold,
                        exclude_external_links=False,
                        process_iframes=True,
                        remove_overlay_elements=True
                    )
                
                if not result.success:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {url} - {result.error_message}")
                    return PageResult(
                        url=url, title="", markdown="", html="", links=[],
                        metadata={}, depth=depth, timestamp=datetime.now(),
                        success=False, error=result.error_message
                    )
                
                # æå–é“¾æ¥
                links = self.extract_links_from_content(result.html, url)
                valid_links = [link for link in links if self.is_valid_url(link, depth + 1)]
                
                # æ·»åŠ æ–°å‘ç°çš„é“¾æ¥åˆ°å¾…å¤„ç†é˜Ÿåˆ—
                for link in valid_links:
                    if link not in self.visited_urls:
                        self.pending_urls.add(link)
                
                # åˆ›å»ºé¡µé¢ç»“æœ
                page_result = PageResult(
                    url=url,
                    title=result.metadata.get('title', 'æ— æ ‡é¢˜'),
                    markdown=result.markdown,
                    html=result.html,
                    links=valid_links,
                    metadata=result.metadata,
                    depth=depth,
                    timestamp=datetime.now(),
                    success=True
                )
                
                # ä¿å­˜å•é¡µç»“æœ
                await self.save_page_result(page_result)
                
                print(f"âœ… çˆ¬å–å®Œæˆ: {url} (æ ‡é¢˜: {page_result.title[:50]}...)")
                print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦, å‘ç°é“¾æ¥: {len(valid_links)} ä¸ª")
                
                # æ·»åŠ å»¶æ—¶
                if self.config.delay > 0:
                    await asyncio.sleep(self.config.delay)
                
                return page_result
                
            except Exception as e:
                print(f"âŒ çˆ¬å–é¡µé¢æ—¶å‘ç”Ÿé”™è¯¯ {url}: {e}")
                return PageResult(
                    url=url, title="", markdown="", html="", links=[],
                    metadata={}, depth=depth, timestamp=datetime.now(),
                    success=False, error=str(e)
                )
    
    async def save_page_result(self, result: PageResult):
        """ä¿å­˜å•ä¸ªé¡µé¢çš„ç»“æœ"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = result.timestamp.strftime("%Y%m%d_%H%M%S")
            domain = urlparse(result.url).netloc.replace('.', '_')
            safe_path = re.sub(r'[^\w\-_.]', '_', urlparse(result.url).path)[:50]
            base_name = f"{timestamp}_{domain}_{safe_path}"
            
            # ä¿å­˜HTML
            html_path = f"{self.config.output_dir}/html/{base_name}.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(result.html)
            
            # ä¿å­˜Markdown
            md_path = f"{self.config.output_dir}/markdown/{base_name}.md"
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(f"# {result.title}\n\n")
                f.write(f"**URL:** {result.url}\n\n")
                f.write(f"**çˆ¬å–æ—¶é—´:** {result.timestamp}\n\n")
                f.write(f"**æ·±åº¦:** {result.depth}\n\n")
                f.write("---\n\n")
                f.write(result.markdown)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = f"{self.config.output_dir}/metadata/{base_name}_metadata.json"
            metadata = {
                'url': result.url,
                'title': result.title,
                'depth': result.depth,
                'timestamp': result.timestamp.isoformat(),
                'success': result.success,
                'error': result.error,
                'links_count': len(result.links),
                'markdown_length': len(result.markdown),
                'html_length': len(result.html),
                'metadata': result.metadata,
                'files': {
                    'html': html_path,
                    'markdown': md_path,
                    'metadata': metadata_path
                }
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é¡µé¢ç»“æœå¤±è´¥ {result.url}: {e}")
    
    async def crawl_website(self) -> Dict[str, PageResult]:
        """å¼€å§‹æ·±åº¦çˆ¬å–ç½‘ç«™"""
        print(f"\nğŸš€ å¼€å§‹æ·±åº¦çˆ¬å–ç½‘ç«™")
        print("=" * 60)
        
        start_time = time.time()
        
        # æ·»åŠ èµ·å§‹URL
        self.pending_urls.add(self.config.start_url)
        
        current_depth = 0
        
        while self.pending_urls and current_depth <= self.config.max_depth:
            # è·å–å½“å‰æ·±åº¦çš„URL
            current_batch = []
            for url in list(self.pending_urls):
                if len(self.visited_urls) >= self.config.max_pages:
                    break
                if url not in self.visited_urls:
                    current_batch.append((url, current_depth))
                    self.visited_urls.add(url)
                    self.pending_urls.discard(url)
            
            if not current_batch:
                current_depth += 1
                continue
            
            print(f"\nğŸ“Š çˆ¬å–æ·±åº¦ {current_depth}: {len(current_batch)} ä¸ªé¡µé¢")
            print("-" * 40)
            
            # å¹¶å‘çˆ¬å–å½“å‰æ‰¹æ¬¡
            tasks = [
                self.crawl_single_page(url, depth) 
                for url, depth in current_batch
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if isinstance(result, PageResult) and result.success:
                    self.results[result.url] = result
                elif isinstance(result, Exception):
                    print(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
            
            # æ˜¾ç¤ºè¿›åº¦
            success_count = len([r for r in results if isinstance(r, PageResult) and r.success])
            print(f"âœ… æ·±åº¦ {current_depth} å®Œæˆ: {success_count}/{len(current_batch)} é¡µé¢æˆåŠŸ")
            
            current_depth += 1
        
        elapsed_time = time.time() - start_time
        
        print(f"\nğŸ‰ ç½‘ç«™çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡çˆ¬å–: {len(self.results)} ä¸ªé¡µé¢")
        print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {self.config.output_dir}")
        
        return self.results
    
    async def generate_merged_markdown(self) -> str:
        """ç”Ÿæˆåˆå¹¶çš„markdownæ–‡æ¡£"""
        if not self.config.merge_markdown or not self.results:
            return ""
        
        print(f"\nğŸ“ ç”Ÿæˆåˆå¹¶çš„markdownæ–‡æ¡£...")
        
        # æŒ‰æ·±åº¦å’ŒURLæ’åº
        sorted_results = sorted(
            self.results.values(), 
            key=lambda x: (x.depth, x.url)
        )
        
        # æ„å»ºmarkdownå†…å®¹
        merged_content = []
        
        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
        merged_content.append(f"# æ·±åº¦çˆ¬å–é¡¹ç›®: {self.target_domain}\n")
        merged_content.append(f"**èµ·å§‹URL:** {self.config.start_url}\n")
        merged_content.append(f"**çˆ¬å–æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        merged_content.append(f"**æ€»é¡µé¢æ•°:** {len(self.results)}\n")
        merged_content.append(f"**æœ€å¤§æ·±åº¦:** {self.config.max_depth}\n\n")
        
        # ç”Ÿæˆç›®å½•
        merged_content.append("## ğŸ“‹ ç›®å½•\n")
        for i, result in enumerate(sorted_results, 1):
            indent = "  " * result.depth
            title = result.title[:80] + "..." if len(result.title) > 80 else result.title
            merged_content.append(f"{indent}{i}. [{title}](#{i}-{result.depth})\n")
        merged_content.append("\n---\n\n")
        
        # æ·»åŠ æ¯ä¸ªé¡µé¢çš„å†…å®¹
        for i, result in enumerate(sorted_results, 1):
            merged_content.append(f"## {i}. {result.title} {{#{i}-{result.depth}}}\n\n")
            merged_content.append(f"**URL:** {result.url}\n")
            merged_content.append(f"**æ·±åº¦:** {result.depth}\n")
            merged_content.append(f"**çˆ¬å–æ—¶é—´:** {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if result.markdown:
                # è°ƒæ•´markdownæ ‡é¢˜çº§åˆ«ï¼Œé¿å…å†²çª
                adjusted_markdown = result.markdown
                # å°†åŸæœ‰çš„h1-h6æ ‡é¢˜éƒ½å¢åŠ 2çº§
                for level in range(6, 0, -1):
                    old_header = '#' * level + ' '
                    new_header = '#' * (level + 2) + ' '
                    adjusted_markdown = adjusted_markdown.replace(old_header, new_header)
                
                merged_content.append(adjusted_markdown)
            else:
                merged_content.append("*ï¼ˆæ­¤é¡µé¢æ— å†…å®¹ï¼‰*\n")
            
            merged_content.append("\n---\n\n")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        merged_content.append("## ğŸ“Š çˆ¬å–ç»Ÿè®¡\n\n")
        total_chars = sum(len(r.markdown) for r in self.results.values())
        by_depth = {}
        for result in self.results.values():
            by_depth[result.depth] = by_depth.get(result.depth, 0) + 1
        
        merged_content.append(f"- æ€»é¡µé¢æ•°: {len(self.results)}\n")
        merged_content.append(f"- æ€»å­—ç¬¦æ•°: {total_chars:,}\n")
        merged_content.append("- å„æ·±åº¦é¡µé¢åˆ†å¸ƒ:\n")
        for depth in sorted(by_depth.keys()):
            merged_content.append(f"  - æ·±åº¦ {depth}: {by_depth[depth]} é¡µé¢\n")
        
        final_content = ''.join(merged_content)
        
        # ä¿å­˜åˆå¹¶çš„markdown
        merged_path = f"{self.config.output_dir}/merged_website_content.md"
        with open(merged_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        print(f"âœ… åˆå¹¶æ–‡æ¡£å·²ç”Ÿæˆ: {merged_path}")
        print(f"ğŸ“„ æ–‡æ¡£å¤§å°: {len(final_content):,} å­—ç¬¦")
        
        return final_content

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ æ·±åº¦ç½‘ç«™çˆ¬å–å·¥å…·")
    print("=" * 40)
    
    # è¿™é‡Œå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶è®¾ç½®
    # ç¤ºä¾‹é…ç½®ï¼šçˆ¬å– PyOD å®˜ç½‘
    config = CrawlConfig(
        start_url="https://pyod.readthedocs.io/en/latest/index.html",
        max_depth=3,
        max_pages=50,
        concurrent_limit=3,
        delay=2.0,  # å¢åŠ å»¶æ—¶ä»¥é¿å…è¢«å°
        same_domain_only=True,
        output_dir="pyod_website_crawl",
        merge_markdown=True,
        word_count_threshold=10
    )
    
    crawler = DeepWebsiteCrawler(config)
    
    try:
        # å¼€å§‹çˆ¬å–
        results = await crawler.crawl_website()
        
        # ç”Ÿæˆåˆå¹¶æ–‡æ¡£
        if config.merge_markdown:
            await crawler.generate_merged_markdown()
        
        print(f"\nğŸ‰ æ·±åº¦çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        print(f"âœ… æˆåŠŸçˆ¬å– {len(results)} ä¸ªé¡µé¢")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config.output_dir}")
        
    except KeyboardInterrupt:
        print("\nâ›” ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())

