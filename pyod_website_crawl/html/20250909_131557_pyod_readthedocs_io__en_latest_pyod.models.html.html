<!DOCTYPE html><html class="" lang="en" data-content_root="./" data-readthedocs-tool="sphinx" data-readthedocs-tool-theme="furo"><head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1">
<style type="text/css">:root, :host {
  --fa-font-solid: normal 900 1em/1 "Font Awesome 7 Free";
  --fa-font-regular: normal 400 1em/1 "Font Awesome 7 Free";
  --fa-font-light: normal 300 1em/1 "Font Awesome 7 Pro";
  --fa-font-thin: normal 100 1em/1 "Font Awesome 7 Pro";
  --fa-font-duotone: normal 900 1em/1 "Font Awesome 7 Duotone";
  --fa-font-duotone-regular: normal 400 1em/1 "Font Awesome 7 Duotone";
  --fa-font-duotone-light: normal 300 1em/1 "Font Awesome 7 Duotone";
  --fa-font-duotone-thin: normal 100 1em/1 "Font Awesome 7 Duotone";
  --fa-font-brands: normal 400 1em/1 "Font Awesome 7 Brands";
  --fa-font-sharp-solid: normal 900 1em/1 "Font Awesome 7 Sharp";
  --fa-font-sharp-regular: normal 400 1em/1 "Font Awesome 7 Sharp";
  --fa-font-sharp-light: normal 300 1em/1 "Font Awesome 7 Sharp";
  --fa-font-sharp-thin: normal 100 1em/1 "Font Awesome 7 Sharp";
  --fa-font-sharp-duotone-solid: normal 900 1em/1 "Font Awesome 7 Sharp Duotone";
  --fa-font-sharp-duotone-regular: normal 400 1em/1 "Font Awesome 7 Sharp Duotone";
  --fa-font-sharp-duotone-light: normal 300 1em/1 "Font Awesome 7 Sharp Duotone";
  --fa-font-sharp-duotone-thin: normal 100 1em/1 "Font Awesome 7 Sharp Duotone";
  --fa-font-slab-regular: normal 400 1em/1 "Font Awesome 7 Slab";
  --fa-font-slab-press-regular: normal 400 1em/1 "Font Awesome 7 Slab Press";
  --fa-font-whiteboard-semibold: normal 600 1em/1 "Font Awesome 7 Whiteboard";
  --fa-font-thumbprint-light: normal 300 1em/1 "Font Awesome 7 Thumbprint";
  --fa-font-notdog-solid: normal 900 1em/1 "Font Awesome 7 Notdog";
  --fa-font-notdog-duo-solid: normal 900 1em/1 "Font Awesome 7 Notdog Duo";
  --fa-font-etch-solid: normal 900 1em/1 "Font Awesome 7 Etch";
  --fa-font-jelly-regular: normal 400 1em/1 "Font Awesome 7 Jelly";
  --fa-font-jelly-fill-regular: normal 400 1em/1 "Font Awesome 7 Jelly Fill";
  --fa-font-jelly-duo-regular: normal 400 1em/1 "Font Awesome 7 Jelly Duo";
  --fa-font-chisel-regular: normal 400 1em/1 "Font Awesome 7 Chisel";
}

.svg-inline--fa {
  box-sizing: content-box;
  display: var(--fa-display, inline-block);
  height: 1em;
  overflow: visible;
  vertical-align: -0.125em;
  width: var(--fa-width, 1.25em);
}
.svg-inline--fa.fa-2xs {
  vertical-align: 0.1em;
}
.svg-inline--fa.fa-xs {
  vertical-align: 0em;
}
.svg-inline--fa.fa-sm {
  vertical-align: -0.0714285714em;
}
.svg-inline--fa.fa-lg {
  vertical-align: -0.2em;
}
.svg-inline--fa.fa-xl {
  vertical-align: -0.25em;
}
.svg-inline--fa.fa-2xl {
  vertical-align: -0.3125em;
}
.svg-inline--fa.fa-pull-left,
.svg-inline--fa .fa-pull-start {
  float: inline-start;
  margin-inline-end: var(--fa-pull-margin, 0.3em);
}
.svg-inline--fa.fa-pull-right,
.svg-inline--fa .fa-pull-end {
  float: inline-end;
  margin-inline-start: var(--fa-pull-margin, 0.3em);
}
.svg-inline--fa.fa-li {
  width: var(--fa-li-width, 2em);
  inset-inline-start: calc(-1 * var(--fa-li-width, 2em));
  inset-block-start: 0.25em; /* syncing vertical alignment with Web Font rendering */
}

.fa-layers-counter, .fa-layers-text {
  display: inline-block;
  position: absolute;
  text-align: center;
}

.fa-layers {
  display: inline-block;
  height: 1em;
  position: relative;
  text-align: center;
  vertical-align: -0.125em;
  width: var(--fa-width, 1.25em);
}
.fa-layers .svg-inline--fa {
  inset: 0;
  margin: auto;
  position: absolute;
  transform-origin: center center;
}

.fa-layers-text {
  left: 50%;
  top: 50%;
  transform: translate(-50%, -50%);
  transform-origin: center center;
}

.fa-layers-counter {
  background-color: var(--fa-counter-background-color, #ff253a);
  border-radius: var(--fa-counter-border-radius, 1em);
  box-sizing: border-box;
  color: var(--fa-inverse, #fff);
  line-height: var(--fa-counter-line-height, 1);
  max-width: var(--fa-counter-max-width, 5em);
  min-width: var(--fa-counter-min-width, 1.5em);
  overflow: hidden;
  padding: var(--fa-counter-padding, 0.25em 0.5em);
  right: var(--fa-right, 0);
  text-overflow: ellipsis;
  top: var(--fa-top, 0);
  transform: scale(var(--fa-counter-scale, 0.25));
  transform-origin: top right;
}

.fa-layers-bottom-right {
  bottom: var(--fa-bottom, 0);
  right: var(--fa-right, 0);
  top: auto;
  transform: scale(var(--fa-layers-scale, 0.25));
  transform-origin: bottom right;
}

.fa-layers-bottom-left {
  bottom: var(--fa-bottom, 0);
  left: var(--fa-left, 0);
  right: auto;
  top: auto;
  transform: scale(var(--fa-layers-scale, 0.25));
  transform-origin: bottom left;
}

.fa-layers-top-right {
  top: var(--fa-top, 0);
  right: var(--fa-right, 0);
  transform: scale(var(--fa-layers-scale, 0.25));
  transform-origin: top right;
}

.fa-layers-top-left {
  left: var(--fa-left, 0);
  right: auto;
  top: var(--fa-top, 0);
  transform: scale(var(--fa-layers-scale, 0.25));
  transform-origin: top left;
}

.fa-1x {
  font-size: 1em;
}

.fa-2x {
  font-size: 2em;
}

.fa-3x {
  font-size: 3em;
}

.fa-4x {
  font-size: 4em;
}

.fa-5x {
  font-size: 5em;
}

.fa-6x {
  font-size: 6em;
}

.fa-7x {
  font-size: 7em;
}

.fa-8x {
  font-size: 8em;
}

.fa-9x {
  font-size: 9em;
}

.fa-10x {
  font-size: 10em;
}

.fa-2xs {
  font-size: calc(10 / 16 * 1em); /* converts a 10px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 10 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 10 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-xs {
  font-size: calc(12 / 16 * 1em); /* converts a 12px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 12 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 12 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-sm {
  font-size: calc(14 / 16 * 1em); /* converts a 14px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 14 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 14 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-lg {
  font-size: calc(20 / 16 * 1em); /* converts a 20px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 20 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 20 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-xl {
  font-size: calc(24 / 16 * 1em); /* converts a 24px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 24 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 24 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-2xl {
  font-size: calc(32 / 16 * 1em); /* converts a 32px size into an em-based value that's relative to the scale's 16px base */
  line-height: calc(1 / 32 * 1em); /* sets the line-height of the icon back to that of it's parent */
  vertical-align: calc((6 / 32 - 0.375) * 1em); /* vertically centers the icon taking into account the surrounding text's descender */
}

.fa-width-auto {
  --fa-width: auto;
}

.fa-fw,
.fa-width-fixed {
  --fa-width: 1.25em;
}

.fa-ul {
  list-style-type: none;
  margin-inline-start: var(--fa-li-margin, 2.5em);
  padding-inline-start: 0;
}
.fa-ul > li {
  position: relative;
}

.fa-li {
  inset-inline-start: calc(-1 * var(--fa-li-width, 2em));
  position: absolute;
  text-align: center;
  width: var(--fa-li-width, 2em);
  line-height: inherit;
}

/* Heads Up: Bordered Icons will not be supported in the future!
  - This feature will be deprecated in the next major release of Font Awesome (v8)!
  - You may continue to use it in this version *v7), but it will not be supported in Font Awesome v8.
*/
/* Notes:
* --@{v.$css-prefix}-border-width = 1/16 by default (to render as ~1px based on a 16px default font-size)
* --@{v.$css-prefix}-border-padding =
  ** 3/16 for vertical padding (to give ~2px of vertical whitespace around an icon considering it's vertical alignment)
  ** 4/16 for horizontal padding (to give ~4px of horizontal whitespace around an icon)
*/
.fa-border {
  border-color: var(--fa-border-color, #eee);
  border-radius: var(--fa-border-radius, 0.1em);
  border-style: var(--fa-border-style, solid);
  border-width: var(--fa-border-width, 0.0625em);
  box-sizing: var(--fa-border-box-sizing, content-box);
  padding: var(--fa-border-padding, 0.1875em 0.25em);
}

.fa-pull-left,
.fa-pull-start {
  float: inline-start;
  margin-inline-end: var(--fa-pull-margin, 0.3em);
}

.fa-pull-right,
.fa-pull-end {
  float: inline-end;
  margin-inline-start: var(--fa-pull-margin, 0.3em);
}

.fa-beat {
  animation-name: fa-beat;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, ease-in-out);
}

.fa-bounce {
  animation-name: fa-bounce;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, cubic-bezier(0.28, 0.84, 0.42, 1));
}

.fa-fade {
  animation-name: fa-fade;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, cubic-bezier(0.4, 0, 0.6, 1));
}

.fa-beat-fade {
  animation-name: fa-beat-fade;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, cubic-bezier(0.4, 0, 0.6, 1));
}

.fa-flip {
  animation-name: fa-flip;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, ease-in-out);
}

.fa-shake {
  animation-name: fa-shake;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, linear);
}

.fa-spin {
  animation-name: fa-spin;
  animation-delay: var(--fa-animation-delay, 0s);
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 2s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, linear);
}

.fa-spin-reverse {
  --fa-animation-direction: reverse;
}

.fa-pulse,
.fa-spin-pulse {
  animation-name: fa-spin;
  animation-direction: var(--fa-animation-direction, normal);
  animation-duration: var(--fa-animation-duration, 1s);
  animation-iteration-count: var(--fa-animation-iteration-count, infinite);
  animation-timing-function: var(--fa-animation-timing, steps(8));
}

@media (prefers-reduced-motion: reduce) {
  .fa-beat,
  .fa-bounce,
  .fa-fade,
  .fa-beat-fade,
  .fa-flip,
  .fa-pulse,
  .fa-shake,
  .fa-spin,
  .fa-spin-pulse {
    animation: none !important;
    transition: none !important;
  }
}
@keyframes fa-beat {
  0%, 90% {
    transform: scale(1);
  }
  45% {
    transform: scale(var(--fa-beat-scale, 1.25));
  }
}
@keyframes fa-bounce {
  0% {
    transform: scale(1, 1) translateY(0);
  }
  10% {
    transform: scale(var(--fa-bounce-start-scale-x, 1.1), var(--fa-bounce-start-scale-y, 0.9)) translateY(0);
  }
  30% {
    transform: scale(var(--fa-bounce-jump-scale-x, 0.9), var(--fa-bounce-jump-scale-y, 1.1)) translateY(var(--fa-bounce-height, -0.5em));
  }
  50% {
    transform: scale(var(--fa-bounce-land-scale-x, 1.05), var(--fa-bounce-land-scale-y, 0.95)) translateY(0);
  }
  57% {
    transform: scale(1, 1) translateY(var(--fa-bounce-rebound, -0.125em));
  }
  64% {
    transform: scale(1, 1) translateY(0);
  }
  100% {
    transform: scale(1, 1) translateY(0);
  }
}
@keyframes fa-fade {
  50% {
    opacity: var(--fa-fade-opacity, 0.4);
  }
}
@keyframes fa-beat-fade {
  0%, 100% {
    opacity: var(--fa-beat-fade-opacity, 0.4);
    transform: scale(1);
  }
  50% {
    opacity: 1;
    transform: scale(var(--fa-beat-fade-scale, 1.125));
  }
}
@keyframes fa-flip {
  50% {
    transform: rotate3d(var(--fa-flip-x, 0), var(--fa-flip-y, 1), var(--fa-flip-z, 0), var(--fa-flip-angle, -180deg));
  }
}
@keyframes fa-shake {
  0% {
    transform: rotate(-15deg);
  }
  4% {
    transform: rotate(15deg);
  }
  8%, 24% {
    transform: rotate(-18deg);
  }
  12%, 28% {
    transform: rotate(18deg);
  }
  16% {
    transform: rotate(-22deg);
  }
  20% {
    transform: rotate(22deg);
  }
  32% {
    transform: rotate(-12deg);
  }
  36% {
    transform: rotate(12deg);
  }
  40%, 100% {
    transform: rotate(0deg);
  }
}
@keyframes fa-spin {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}
.fa-rotate-90 {
  transform: rotate(90deg);
}

.fa-rotate-180 {
  transform: rotate(180deg);
}

.fa-rotate-270 {
  transform: rotate(270deg);
}

.fa-flip-horizontal {
  transform: scale(-1, 1);
}

.fa-flip-vertical {
  transform: scale(1, -1);
}

.fa-flip-both,
.fa-flip-horizontal.fa-flip-vertical {
  transform: scale(-1, -1);
}

.fa-rotate-by {
  transform: rotate(var(--fa-rotate-angle, 0));
}

.svg-inline--fa .fa-primary {
  fill: var(--fa-primary-color, currentColor);
  opacity: var(--fa-primary-opacity, 1);
}

.svg-inline--fa .fa-secondary {
  fill: var(--fa-secondary-color, currentColor);
  opacity: var(--fa-secondary-opacity, 0.4);
}

.svg-inline--fa.fa-swap-opacity .fa-primary {
  opacity: var(--fa-secondary-opacity, 0.4);
}

.svg-inline--fa.fa-swap-opacity .fa-secondary {
  opacity: var(--fa-primary-opacity, 1);
}

.svg-inline--fa mask .fa-primary,
.svg-inline--fa mask .fa-secondary {
  fill: black;
}

.svg-inline--fa.fa-inverse {
  fill: var(--fa-inverse, #fff);
}

.fa-stack {
  display: inline-block;
  height: 2em;
  line-height: 2em;
  position: relative;
  vertical-align: middle;
  width: 2.5em;
}

.fa-inverse {
  color: var(--fa-inverse, #fff);
}

.svg-inline--fa.fa-stack-1x {
  height: 1em;
  width: 1.25em;
}
.svg-inline--fa.fa-stack-2x {
  height: 2em;
  width: 2.5em;
}

.fa-stack-1x,
.fa-stack-2x {
  bottom: 0;
  left: 0;
  margin: auto;
  position: absolute;
  right: 0;
  top: 0;
  z-index: var(--fa-stack-z-index, auto);
}</style><link rel="author" title="About these documents" href="about.html"><link rel="index" title="Index" href="genindex.html"><link rel="search" title="Search" href="search.html"><link rel="next" title="Utility Functions" href="pyod.utils.html"><link rel="prev" title="API Reference" href="pyod.html">

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.07.19 -->
        <title>All Models - pyod 2.0.5 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079">
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?v=25af2a20">
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?v=8dab3a3b">
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style><script async="" type="text/javascript" src="/_/static/javascript/readthedocs-addons.js"></script><meta name="readthedocs-project-slug" content="pyod"><meta name="readthedocs-version-slug" content="latest"><meta name="readthedocs-resolver-filename" content="/pyod.models.html"><meta name="readthedocs-http-status" content="200"><script id="ethicaladsjs" type="text/javascript" async="true" src="https://media.ethicalads.io/media/client/ethicalads.min.js"></script><style>@layer defaults{:root{--ea-container-md: 720px;--ea-container-lg: 960px;--ea-container-xl: 1040px;--ea-image-width: 120px;--ea-image-width-xs: 44px;--ea-image-placement-width: 180px;--ea-image-placement-width-horizontal: 320px;--ea-fixedheader-height: 50px;--ea-font-size: 14px;--ea-font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji;--ea-bgcolor: rgba(0, 0, 0, 0.03);--ea-bgcolor-dark: rgba(255, 255, 255, 0.05);--ea-stylefixed-bgcolor: rgb(220, 220, 220);--ea-stylefixed-bgcolor-dark: rgb(80, 80, 80);--ea-color-link: rgb(80, 80, 80);--ea-color-link-dark: rgb(220, 220, 220);--ea-color-link-active: rgb(54.5, 54.5, 54.5);--ea-color-link-dark-active: rgb(245.5, 245.5, 245.5);--ea-color-link-callout: rgb(105.5, 105.5, 105.5);--ea-color-link-callout-dark: rgb(194.5, 194.5, 194.5);--ea-color-link-callout-active: #505050;--ea-color-link-callout-dark-active: gainsboro;--ea-color-link-bold: #088cdb;--ea-color-link-bold-dark: rgb(80.3788546256, 185.6299559471, 248.6211453744);--ea-color-domain: rgb(156.5, 156.5, 156.5);--ea-color-domain-dark: rgb(143.5, 143.5, 143.5)}}[data-ea-publisher].loaded{font-size:var(--ea-font-size);font-family:var(--ea-font-family);font-weight:normal;font-style:normal;letter-spacing:0px;vertical-align:baseline;line-height:1.3em}[data-ea-publisher].loaded a{text-decoration:none}[data-ea-publisher].loaded .ea-pixel{display:none}[data-ea-publisher].loaded .ea-content{margin:1em 1em .5em 1em;padding:1em;background:var(--ea-bgcolor);color:var(--ea-color-link);color:var(--ea-color-link)}[data-ea-publisher].loaded .ea-content a:link{color:var(--ea-color-link)}[data-ea-publisher].loaded .ea-content a:visited{color:var(--ea-color-link)}[data-ea-publisher].loaded .ea-content a:hover{color:var(--ea-color-link-active)}[data-ea-publisher].loaded .ea-content a:active{color:var(--ea-color-link-active)}[data-ea-publisher].loaded .ea-content a strong,[data-ea-publisher].loaded .ea-content a b{color:var(--ea-color-link-bold)}[data-ea-publisher].loaded .ea-callout{color:var(--ea-color-link-callout)}[data-ea-publisher].loaded .ea-callout a{font-size:.8em}[data-ea-publisher].loaded .ea-callout a:link{color:var(--ea-color-link-callout)}[data-ea-publisher].loaded .ea-callout a:visited{color:var(--ea-color-link-callout)}[data-ea-publisher].loaded .ea-callout a:hover{color:var(--ea-color-link-callout-active)}[data-ea-publisher].loaded .ea-callout a:active{color:var(--ea-color-link-callout-active)}[data-ea-publisher].loaded .ea-callout a strong,[data-ea-publisher].loaded .ea-callout a b{color:var(--ea-color-link-callout)}[data-ea-publisher].loaded .ea-domain{margin-top:.75em;font-size:.8em;text-align:center;color:var(--ea-color-domain)}[data-ea-publisher].loaded.dark{--ea-bgcolor: var(--ea-bgcolor-dark);--ea-stylefixed-bgcolor: var(--ea-stylefixed-bgcolor-dark);--ea-color-link: var(--ea-color-link-dark);--ea-color-link-active: var(--ea-color-link-dark-active);--ea-color-link-callout: var(--ea-color-link-callout-dark);--ea-color-link-callout-active: var(--ea-color-link-callout-dark-active);--ea-color-link-bold: var(--ea-color-link-bold-dark);--ea-color-domain: var(--ea-color-domain-dark)}@media(prefers-color-scheme: dark){[data-ea-publisher].loaded.adaptive{--ea-bgcolor: var(--ea-bgcolor-dark);--ea-stylefixed-bgcolor: var(--ea-stylefixed-bgcolor-dark);--ea-color-link: var(--ea-color-link-dark);--ea-color-link-active: var(--ea-color-link-dark-active);--ea-color-link-callout: var(--ea-color-link-callout-dark);--ea-color-link-callout-active: var(--ea-color-link-callout-dark-active);--ea-color-link-bold: var(--ea-color-link-bold-dark);--ea-color-domain: var(--ea-color-domain-dark)}}html.dark [data-ea-publisher].loaded.adaptive-css,body.dark [data-ea-publisher].loaded.adaptive-css,html[data-theme=dark] [data-ea-publisher].loaded.adaptive-css,body[data-theme=dark] [data-ea-publisher].loaded.adaptive-css,html[data-bs-theme=dark] [data-ea-publisher].loaded.adaptive-css,body[data-bs-theme=dark] [data-ea-publisher].loaded.adaptive-css{--ea-bgcolor: var(--ea-bgcolor-dark);--ea-stylefixed-bgcolor: var(--ea-stylefixed-bgcolor-dark);--ea-color-link: var(--ea-color-link-dark);--ea-color-link-active: var(--ea-color-link-dark-active);--ea-color-link-callout: var(--ea-color-link-callout-dark);--ea-color-link-callout-active: var(--ea-color-link-callout-dark-active);--ea-color-link-bold: var(--ea-color-link-bold-dark);--ea-color-domain: var(--ea-color-domain-dark)}@media(prefers-color-scheme: dark){html.auto [data-ea-publisher].loaded.adaptive-css,body.auto [data-ea-publisher].loaded.adaptive-css,html.system [data-ea-publisher].loaded.adaptive-css,body.system [data-ea-publisher].loaded.adaptive-css,html[data-theme=auto] [data-ea-publisher].loaded.adaptive-css,body[data-theme=auto] [data-ea-publisher].loaded.adaptive-css,html[data-bs-theme=auto] [data-ea-publisher].loaded.adaptive-css,body[data-bs-theme=auto] [data-ea-publisher].loaded.adaptive-css{--ea-bgcolor: var(--ea-bgcolor-dark);--ea-stylefixed-bgcolor: var(--ea-stylefixed-bgcolor-dark);--ea-color-link: var(--ea-color-link-dark);--ea-color-link-active: var(--ea-color-link-dark-active);--ea-color-link-callout: var(--ea-color-link-callout-dark);--ea-color-link-callout-active: var(--ea-color-link-callout-dark-active);--ea-color-link-bold: var(--ea-color-link-bold-dark);--ea-color-domain: var(--ea-color-domain-dark)}}[data-ea-publisher].loaded .ea-content{border:0px;border-radius:3px;box-shadow:0px 2px 3px rgba(0,0,0,.15)}[data-ea-publisher].loaded.raised .ea-content{border:0px;border-radius:3px;box-shadow:0px 2px 3px rgba(0,0,0,.15)}[data-ea-publisher].loaded.bordered .ea-content{border:1px solid rgba(0,0,0,.04);border-radius:3px;box-shadow:none}[data-ea-publisher].loaded.bordered.dark .ea-content{border:1px solid hsla(0,0%,100%,.07)}@media(prefers-color-scheme: dark){[data-ea-publisher].loaded.bordered.adaptive .ea-content{border:1px solid hsla(0,0%,100%,.07)}}[data-ea-publisher].loaded.flat .ea-content{border:0px;border-radius:3px;box-shadow:none}[data-ea-type=image].loaded,[data-ea-publisher]:not([data-ea-type]).loaded,.ea-type-image{display:inline-block}[data-ea-type=image].loaded .ea-content,[data-ea-publisher]:not([data-ea-type]).loaded .ea-content,.ea-type-image .ea-content{max-width:var(--ea-image-placement-width);overflow:auto;text-align:center}[data-ea-type=image].loaded .ea-content>a>img,[data-ea-publisher]:not([data-ea-type]).loaded .ea-content>a>img,.ea-type-image .ea-content>a>img{width:var(--ea-image-width);height:auto;display:inline-block}[data-ea-type=image].loaded .ea-content>.ea-text,[data-ea-publisher]:not([data-ea-type]).loaded .ea-content>.ea-text,.ea-type-image .ea-content>.ea-text{margin-top:1em;font-size:1em;text-align:center}[data-ea-type=image].loaded .ea-callout,[data-ea-publisher]:not([data-ea-type]).loaded .ea-callout,.ea-type-image .ea-callout{max-width:var(--ea-image-placement-width);margin:0em 1em 1em 1em;padding-left:1em;padding-right:1em;font-style:italic;text-align:right}[data-ea-type=image].loaded.horizontal .ea-content,[data-ea-publisher]:not([data-ea-type]).loaded.horizontal .ea-content,.ea-type-image.horizontal .ea-content{max-width:var(--ea-image-placement-width-horizontal)}[data-ea-type=image].loaded.horizontal .ea-content>a>img,[data-ea-publisher]:not([data-ea-type]).loaded.horizontal .ea-content>a>img,.ea-type-image.horizontal .ea-content>a>img{float:left;margin-right:1em}[data-ea-type=image].loaded.horizontal .ea-content .ea-text,[data-ea-publisher]:not([data-ea-type]).loaded.horizontal .ea-content .ea-text,.ea-type-image.horizontal .ea-content .ea-text{margin-top:0em;text-align:left;overflow:auto}[data-ea-type=image].loaded.horizontal .ea-callout,[data-ea-publisher]:not([data-ea-type]).loaded.horizontal .ea-callout,.ea-type-image.horizontal .ea-callout{max-width:var(--ea-image-placement-width-horizontal);text-align:right}[data-ea-type=text].loaded,.ea-type-text{font-size:var(--ea-font-size)}[data-ea-type=text].loaded .ea-content,.ea-type-text .ea-content{text-align:left}[data-ea-type=text].loaded .ea-callout,.ea-type-text .ea-callout{margin:.5em 1em 1em 1em;padding-left:1em;padding-right:1em;text-align:right;font-style:italic}[data-ea-style=stickybox].loaded{position:fixed;bottom:20px;right:20px;z-index:100}[data-ea-style=stickybox].loaded .ea-type-image .ea-stickybox-hide{cursor:pointer;position:absolute;top:.75em;right:.75em;background-color:#fefefe;border:1px solid #088cdb;border-radius:50%;color:#088cdb;font-size:1em;text-align:center;height:1.5em;width:1.5em;line-height:1.4}[data-ea-style=stickybox].loaded .ea-type-text{display:none !important}@media(max-width: 1300px){[data-ea-style=stickybox].loaded{position:static;bottom:0;right:0;margin:auto;text-align:center}[data-ea-style=stickybox].loaded .ea-stickybox-hide{display:none}}@media(min-width: 1301px){[data-ea-style=stickybox].loaded .ea-type-image .ea-content{background:var(--ea-stylefixed-bgcolor)}}[data-ea-style=fixedfooter].loaded{position:fixed;bottom:0;left:0;z-index:200;width:100%;max-width:100%}[data-ea-style=fixedfooter].loaded .ea-type-text{width:100%;max-width:100%;display:flex;z-index:200;background:var(--ea-stylefixed-bgcolor)}[data-ea-style=fixedfooter].loaded .ea-type-text .ea-content{border:0px;border-radius:3px;box-shadow:none}[data-ea-style=fixedfooter].loaded .ea-type-text .ea-content{background-color:inherit;max-width:100%;margin:0;padding:1em;flex:auto}[data-ea-style=fixedfooter].loaded .ea-type-text .ea-callout{max-width:100%;margin:0;padding:1em;flex:initial}@media(max-width: 576px){[data-ea-style=fixedfooter].loaded .ea-type-text .ea-callout{display:none}}[data-ea-style=fixedfooter].loaded .ea-type-text .ea-fixedfooter-hide{cursor:pointer;color:var(--ea-color-link);padding:1em;flex:initial;margin:auto 0}[data-ea-style=fixedfooter].loaded .ea-type-text .ea-fixedfooter-hide span{padding:.25em;font-size:.8em;font-weight:bold;border:.15em solid var(--ea-color-link);border-radius:.5em;white-space:nowrap}[data-ea-style=fixedfooter].loaded .ea-type-image{display:none !important}[data-ea-style=fixedheader]{height:var(--ea-fixedheader-height);width:100%;max-width:100%;background:var(--ea-stylefixed-bgcolor);border-bottom:1px solid var(--ea-background-color)}@media(max-width: 768px){[data-ea-style=fixedheader]{display:none !important}}[data-ea-style=fixedheader].loaded .ea-type-image,[data-ea-style=fixedheader].loaded .ea-type-text{width:var(--ea-container-xl);margin:0 auto;display:flex}@media(max-width: 992px){[data-ea-style=fixedheader].loaded .ea-type-image,[data-ea-style=fixedheader].loaded .ea-type-text{width:var(--ea-container-md)}}@media(max-width: 1200px){[data-ea-style=fixedheader].loaded .ea-type-image,[data-ea-style=fixedheader].loaded .ea-type-text{width:var(--ea-container-lg)}}[data-ea-style=fixedheader].loaded .ea-type-image .ea-content,[data-ea-style=fixedheader].loaded .ea-type-text .ea-content{border:0px;border-radius:3px;box-shadow:none}[data-ea-style=fixedheader].loaded .ea-type-image .ea-content,[data-ea-style=fixedheader].loaded .ea-type-text .ea-content{background-color:inherit;max-width:100%;margin:0;padding:0;flex:auto;display:flex}[data-ea-style=fixedheader].loaded .ea-type-image .ea-content .ea-text,[data-ea-style=fixedheader].loaded .ea-type-text .ea-content .ea-text{margin-top:0;padding:1em;flex:auto;text-align:left}[data-ea-style=fixedheader].loaded .ea-type-image .ea-callout,[data-ea-style=fixedheader].loaded .ea-type-text .ea-callout{max-width:100%;margin:0;padding:1em;flex:initial}@media(max-width: 576px){[data-ea-style=fixedheader].loaded .ea-type-image .ea-callout,[data-ea-style=fixedheader].loaded .ea-type-text .ea-callout{display:none}}[data-ea-style=fixedheader].loaded .ea-type-image img{width:var(--ea-image-width-xs) !important;margin:.6em}[data-ea-style=fixedheader].loaded .ea-type-image .ea-domain{display:none}</style><script src="https://server.ethicalads.io/api/v1/decision/?publisher=readthedocs&amp;ad_types=text-v1&amp;div_ids=readthedocs-ea-text-nostyle-sphinx&amp;callback=ad_1757394955691_442836&amp;keywords=anomaly-detection%7Coutlier-detection%7Coutlier-ensembles%7Cpython%7Creadthedocs-project-219652%7Creadthedocs-project-pyod%7Capi%7Cclustering%7Cnumpy&amp;campaign_types=community%7Chouse%7Cpaid&amp;format=jsonp&amp;client_version=2.1.0&amp;placement_index=0&amp;url=https%3A%2F%2Fpyod.readthedocs.io%2Fen%2Flatest%2Fpyod.models.html" type="text/javascript" async=""></script></head>
  <body data-theme="auto">
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"></path>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"></path>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"></line>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"></line>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"></line>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"></line>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"></line>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"></line>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"></line>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"></line>
      <circle cx="14.5" cy="9.55" r="3.6"></circle>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"></path>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"></line>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"></line>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"></line>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"></line>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"></line>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"></line>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"></line>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"></line>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"></circle>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4"></path>
      <path d="M13.5 6.5l4 4"></path>
      <path d="M20 21l2 -2l-2 -2"></path>
      <path d="M17 17l-2 2l2 2"></path>
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0"></path>
      <path d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008"></path>
      <path d="M20 21l2 -2l-2 -2"></path>
      <path d="M17 17l-2 2l2 2"></path>
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">pyod 2.0.5 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  <span class="sidebar-brand-text">pyod 2.0.5 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_persistence.html">Model Save &amp; Load</a></li>
<li class="toctree-l1"><a class="reference internal" href="fast_train.html">Fast Train with SUOD</a></li>
<li class="toctree-l1"><a class="reference internal" href="example.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_cc.html">API CheatSheet</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="pyod.html">API Reference</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of API Reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">All Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pyod.utils.html">Utility Functions</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="issues.html">Known Issues &amp; Warnings</a></li>
<li class="toctree-l1"><a class="reference internal" href="relevant_knowledge.html">Outlier Detection 101</a></li>
<li class="toctree-l1"><a class="reference internal" href="pubs.html">Citations &amp; Achievements</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About us</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="_sources/pyod.models.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="all-models">
<h1>All Models<a class="headerlink" href="#all-models" title="Link to this heading">¶</a></h1>
<section id="module-pyod.models.abod">
<span id="pyod-models-abod-module"></span><h2>pyod.models.abod module<a class="headerlink" href="#module-pyod.models.abod" title="Link to this heading">¶</a></h2>
<p>Angle-based Outlier Detector (ABOD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.abod.</span></span><span class="sig-name descname"><span class="pre">ABOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fast'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/abod.html#ABOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.abod.ABOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>ABOD class for Angle-base Outlier Detection.
For an observation, the variance of its weighted cosine scores to all
neighbors could be viewed as the outlying score.
See <span id="id1">[<a class="reference internal" href="#id1118" title="Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444–452. ACM, 2008.">BKZ+08</a>]</span> for details.</p>
<p>Two version of ABOD are supported:</p>
<ul class="simple">
<li><p>Fast ABOD: use k nearest neighbors to approximate.</p></li>
<li><p>Original ABOD: consider all training points with high time complexity at
O(n^3).</p></li>
</ul>
<section id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_neighbors<span class="classifier">int, optional (default=10)</span></dt><dd><p>Number of neighbors to use by default for k neighbors queries.</p>
</dd>
<dt>method: str, optional (default=’fast’)</dt><dd><p>Valid values for metric are:</p>
<ul class="simple">
<li><p>‘fast’: fast ABOD. Only consider n_neighbors of training points</p></li>
<li><p>‘default’: original ABOD with all training points, which could be
slow</p></li>
</ul>
</dd>
</dl>
</section>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1173"><span class="problematic" id="id1174">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1175"><span class="problematic" id="id1176">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1177"><span class="problematic" id="id1178">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id2">
<h4>Parameters<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="returns">
<h4>Returns<a class="headerlink" href="#returns" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/abod.html#ABOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.abod.ABOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id3">
<h4>Parameters<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id4">
<h4>Returns<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/abod.html#ABOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.abod.ABOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id5">
<h4>Parameters<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id6">
<h4>Returns<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id7">
<h4>Parameters<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id8">
<h4>Returns<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id9">
<h4>Parameters<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id10">
<h4>Returns<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id11">
<h4>Parameters<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id12">
<h4>Returns<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id13">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id14">
<h4>Parameters<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id15">
<h4>Returns<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id16">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id17">
<h4>Parameters<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id18">
<h4>Returns<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.abod.ABOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.abod.ABOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id19">
<h4>Parameters<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id20">
<h4>Returns<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.ae1svm">
<span id="pyod-models-ae1svm-module"></span><h2>pyod.models.ae1svm module<a class="headerlink" href="#module-pyod.models.ae1svm" title="Link to this heading">¶</a></h2>
<p>Using AE-1SVM with Outlier Detection (PyTorch)
Source: <a class="reference external" href="https://arxiv.org/pdf/1804.04888">https://arxiv.org/pdf/1804.04888</a>
There is another implementation of this model by Minh Nghia: <a class="reference external" href="https://github.com/minh-nghia/AE-1SVM">https://github.com/minh-nghia/AE-1SVM</a> (Tensorflow)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.ae1svm.</span></span><span class="sig-name descname"><span class="pre">AE1SVM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_neurons</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_approx_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ae1svm.html#AE1SVM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Auto Encoder with One-class SVM for anomaly detection.</p>
<p>Note: self.device is needed or all tensors may not be on the same device
(if device w/ GPU running)</p>
<section id="id21">
<h3>Parameters<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>hidden_neurons<span class="classifier">list, optional (default=[64, 32])</span></dt><dd><p>Number of neurons in each hidden layer.</p>
</dd>
<dt>hidden_activation<span class="classifier">str, optional (default=’relu’)</span></dt><dd><p>Activation function for the hidden layers.</p>
</dd>
<dt>batch_norm<span class="classifier">bool, optional (default=True)</span></dt><dd><p>Whether to use batch normalization.</p>
</dd>
<dt>learning_rate<span class="classifier">float, optional (default=1e-3)</span></dt><dd><p>Learning rate for training the model.</p>
</dd>
<dt>epochs<span class="classifier">int, optional (default=50)</span></dt><dd><p>Number of training epochs.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>Size of each training batch.</p>
</dd>
<dt>dropout_rate<span class="classifier">float, optional (default=0.2)</span></dt><dd><p>Dropout rate for regularization.</p>
</dd>
<dt>weight_decay<span class="classifier">float, optional (default=1e-5)</span></dt><dd><p>Weight decay (L2 penalty) for the optimizer.</p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>Whether to apply standard scaling to the input data.</p>
</dd>
<dt>loss_fn<span class="classifier">callable, optional (default=torch.nn.MSELoss)</span></dt><dd><p>Loss function to use for reconstruction loss.</p>
</dd>
<dt>contamination<span class="classifier">float, optional (default=0.1)</span></dt><dd><p>Proportion of outliers in the data.</p>
</dd>
<dt>alpha<span class="classifier">float, optional (default=1.0)</span></dt><dd><p>Weight for the reconstruction loss in the final loss computation.</p>
</dd>
<dt>sigma<span class="classifier">float, optional (default=1.0)</span></dt><dd><p>Scaling factor for the random Fourier features.</p>
</dd>
<dt>nu<span class="classifier">float, optional (default=0.1)</span></dt><dd><p>Parameter for the SVM loss.</p>
</dd>
<dt>kernel_approx_features<span class="classifier">int, optional (default=1000)</span></dt><dd><p>Number of random Fourier features to approximate the kernel.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id22">
<h4>Parameters<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id23">
<h4>Returns<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ae1svm.html#AE1SVM.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<section id="id24">
<h4>Parameters<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy.ndarray</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id25">
<h4>Returns<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>numpy.ndarray</dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ae1svm.html#AE1SVM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit the model to the data.</p>
<section id="id26">
<h4>Parameters<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy.ndarray</span></dt><dd><p>Input data.</p>
</dd>
<dt>y<span class="classifier">None</span></dt><dd><p>Ignored, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id27">
<h4>Returns<a class="headerlink" href="#id27" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id28">
<h4>Parameters<a class="headerlink" href="#id28" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id29">
<h4>Returns<a class="headerlink" href="#id29" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id30">
<h4>Parameters<a class="headerlink" href="#id30" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id31">
<h4>Returns<a class="headerlink" href="#id31" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id32">
<h4>Parameters<a class="headerlink" href="#id32" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id33">
<h4>Returns<a class="headerlink" href="#id33" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id34">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id35">
<h4>Parameters<a class="headerlink" href="#id35" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id36">
<h4>Returns<a class="headerlink" href="#id36" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id37">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id38">
<h4>Parameters<a class="headerlink" href="#id38" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id39">
<h4>Returns<a class="headerlink" href="#id39" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ae1svm.AE1SVM.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ae1svm.AE1SVM.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id40">
<h4>Parameters<a class="headerlink" href="#id40" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id41">
<h4>Returns<a class="headerlink" href="#id41" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.alad">
<span id="pyod-models-alad-module"></span><h2>pyod.models.alad module<a class="headerlink" href="#module-pyod.models.alad" title="Link to this heading">¶</a></h2>
<p>Using Adversarially Learned Anomaly Detection</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.alad.</span></span><span class="sig-name descname"><span class="pre">ALAD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation_hidden_gen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_hidden_disc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dec_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[5,</span> <span class="pre">10,</span> <span class="pre">25]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enc_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[25,</span> <span class="pre">10,</span> <span class="pre">5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_xx_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[25,</span> <span class="pre">10,</span> <span class="pre">5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_zz_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[25,</span> <span class="pre">10,</span> <span class="pre">5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_xz_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[25,</span> <span class="pre">10,</span> <span class="pre">5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_gen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_disc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_recon_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_recon_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_disc_zz_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_normalization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/alad.html#ALAD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.alad.ALAD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Adversarially Learned Anomaly Detection (ALAD). 
Paper: <a class="reference external" href="https://arxiv.org/pdf/1812.02288.pdf">https://arxiv.org/pdf/1812.02288.pdf</a></p>
<p>See <span id="id42">[<a class="reference internal" href="#id1163" title="Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining (ICDM), 727–736. IEEE, 2018.">BZRF+18</a>]</span> for details.</p>
<section id="id43">
<h3>Parameters<a class="headerlink" href="#id43" title="Link to this heading">¶</a></h3>
<dl>
<dt>output_activation<span class="classifier">str, optional (default=None)</span></dt><dd><p>Activation function to use for output layers for encoder and dector.</p>
</dd>
<dt>activation_hidden_disc<span class="classifier">str, optional (default=’tanh’)</span></dt><dd><p>Activation function to use for hidden layers in discrimators.</p>
</dd>
<dt>activation_hidden_gen<span class="classifier">str, optional (default=’tanh’)</span></dt><dd><p>Activation function to use for hidden layers in encoder and decoder
(i.e. generator).</p>
</dd>
<dt>epochs<span class="classifier">int, optional (default=500)</span></dt><dd><p>Number of epochs to train the model.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>Number of samples per gradient update.</p>
</dd>
<dt>dropout_rate<span class="classifier">float in (0., 1), optional (default=0.2)</span></dt><dd><p>The dropout to be used across all layers.</p>
</dd>
<dt>dec_layers<span class="classifier">list, optional (default=[5,10,25])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for the d
ecoder network.
Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>enc_layers<span class="classifier">list, optional (default=[25,10,5])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for the
encoder network.
Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>disc_xx_layers<span class="classifier">list, optional (default=[25,10,5])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for
discriminator_xx.
Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>disc_zz_layers<span class="classifier">list, optional (default=[25,10,5])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for
discriminator_zz.
Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>disc_xz_layers<span class="classifier">list, optional (default=[25,10,5])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for
discriminator_xz.
Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>learning_rate_gen: float in (0., 1), optional (default=0.001)</dt><dd><p>learning rate of training the encoder and decoder</p>
</dd>
<dt>learning_rate_disc: float in (0., 1), optional (default=0.001)</dt><dd><p>learning rate of training the discriminators</p>
</dd>
<dt>add_recon_loss: bool optional (default=False)</dt><dd><p>add an extra loss for encoder and decoder based on the reconstruction
error</p>
</dd>
<dt>lambda_recon_loss: float in (0., 1), optional (default=0.1)</dt><dd><p>if <code class="docutils literal notranslate"><span class="pre">add_recon_loss=</span> <span class="pre">True</span></code>, the reconstruction loss gets multiplied
by <code class="docutils literal notranslate"><span class="pre">lambda_recon_loss</span></code> and added to the total loss for the generator</p>
<blockquote>
<div><p>(i.e. encoder and decoder).</p>
</div></blockquote>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply standardization on the data.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=1)</span></dt><dd><p>Verbosity mode.
- 0 = silent
- 1 = progress bar</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. When fitting this is used
to define the threshold on the decision function.</p>
</dd>
<dt>device<span class="classifier">str or None, optional (default=None)</span></dt><dd><p>The device to use for computation. If None, the default device will be used.
Possible values include ‘cpu’ or ‘gpu’. This parameter allows the user
to specify the preferred device for running the model.</p>
</dd>
</dl>
</section>
<section id="id44">
<h3>Attributes<a class="headerlink" href="#id44" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1179"><span class="problematic" id="id1180">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data [0,1].
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1181"><span class="problematic" id="id1182">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1183"><span class="problematic" id="id1184">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id45">
<h4>Parameters<a class="headerlink" href="#id45" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id46">
<h4>Returns<a class="headerlink" href="#id46" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/alad.html#ALAD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.alad.ALAD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</div></blockquote>
<section id="id47">
<h4>Returns<a class="headerlink" href="#id47" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/alad.html#ALAD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.alad.ALAD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The input samples.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
<section id="id48">
<h4>Returns<a class="headerlink" href="#id48" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id49">
<h4>Parameters<a class="headerlink" href="#id49" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id50">
<h4>Returns<a class="headerlink" href="#id50" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id51">
<h4>Parameters<a class="headerlink" href="#id51" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id52">
<h4>Returns<a class="headerlink" href="#id52" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.plot_learning_curves">
<span class="sig-name descname"><span class="pre">plot_learning_curves</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_ind</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_smoothening</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/alad.html#ALAD.plot_learning_curves"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.alad.ALAD.plot_learning_curves" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id53">
<h4>Parameters<a class="headerlink" href="#id53" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id54">
<h4>Returns<a class="headerlink" href="#id54" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id55">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id56">
<h4>Parameters<a class="headerlink" href="#id56" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id57">
<h4>Returns<a class="headerlink" href="#id57" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id58">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id59">
<h4>Parameters<a class="headerlink" href="#id59" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id60">
<h4>Returns<a class="headerlink" href="#id60" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.alad.ALAD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.alad.ALAD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id61">
<h4>Parameters<a class="headerlink" href="#id61" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id62">
<h4>Returns<a class="headerlink" href="#id62" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.anogan">
<span id="pyod-models-anogan-module"></span><h2>pyod.models.anogan module<a class="headerlink" href="#module-pyod.models.anogan" title="Link to this heading">¶</a></h2>
<p>Anomaly Detection with Generative Adversarial Networks  (AnoGAN)
Paper: <a class="reference external" href="https://arxiv.org/pdf/1703.05921.pdf">https://arxiv.org/pdf/1703.05921.pdf</a>
Note, that this is another implementation of AnoGAN as the one from <a class="reference external" href="https://github.com/fuchami/ANOGAN">https://github.com/fuchami/ANOGAN</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.anogan.</span></span><span class="sig-name descname"><span class="pre">AnoGAN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation_hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim_G</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">G_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[20,</span> <span class="pre">10,</span> <span class="pre">3,</span> <span class="pre">10,</span> <span class="pre">20]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">D_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[20,</span> <span class="pre">10,</span> <span class="pre">5]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_D_layer_for_recon_error</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_query</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs_query</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/anogan.html#AnoGAN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.anogan.AnoGAN" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Anomaly Detection with Generative Adversarial Networks  (AnoGAN).
See the original paper “Unsupervised anomaly detection with generative
adversarial networks to guide marker discovery”.</p>
<p>See <span id="id63">[<a class="reference internal" href="#id1159" title="Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, 146–157. Springer, 2017.">BSSeebockW+17</a>]</span> for details.</p>
<section id="id64">
<h3>Parameters<a class="headerlink" href="#id64" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>output_activation<span class="classifier">str, optional (default=None)</span></dt><dd><p>Activation function to use for output layer.</p>
</dd>
<dt>activation_hidden<span class="classifier">str, optional (default=’tanh’)</span></dt><dd><p>Activation function to use for output layer.</p>
</dd>
<dt>epochs<span class="classifier">int, optional (default=500)</span></dt><dd><p>Number of epochs to train the model.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>Number of samples per gradient update.</p>
</dd>
<dt>dropout_rate<span class="classifier">float in (0., 1), optional (default=0.2)</span></dt><dd><p>The dropout to be used across all layers.</p>
</dd>
<dt>G_layers<span class="classifier">list, optional (default=[20,10,3,10,20])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for the
generator. Thus, [10,10] indicates 2 hidden layers having each 10 nodes.</p>
</dd>
<dt>D_layers<span class="classifier">list, optional (default=[20,10,5])</span></dt><dd><p>List that indicates the number of nodes per hidden layer for the
discriminator. Thus, [10,10] indicates 2 hidden layers having each 10
nodes.</p>
</dd>
<dt>learning_rate: float in (0., 1), optional (default=0.001)</dt><dd><p>learning rate of training the network</p>
</dd>
<dt>index_D_layer_for_recon_error: int, optional (default = 1)</dt><dd><p>This is the index of the hidden layer in the discriminator for which
the reconstruction error will be determined between query sample and
the sample created from the latent space.</p>
</dd>
<dt>learning_rate_query: float in (0., 1), optional (default=0.001)</dt><dd><p>learning rate for the backpropagation steps needed to find a point in
the latent space of the generator that approximate the query sample</p>
</dd>
<dt>epochs_query: int, optional (default=20) </dt><dd><p>Number of epochs to approximate the query sample in the latent space
of the generator</p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply standardization on the data.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=1)</span></dt><dd><p>Verbosity mode.
- 0 = silent
- 1 = progress bar</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. When fitting this is used
to define the threshold on the decision function.</p>
</dd>
</dl>
</section>
<section id="id65">
<h3>Attributes<a class="headerlink" href="#id65" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1185"><span class="problematic" id="id1186">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data [0,1].
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1187"><span class="problematic" id="id1188">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1189"><span class="problematic" id="id1190">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id66">
<h4>Parameters<a class="headerlink" href="#id66" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id67">
<h4>Returns<a class="headerlink" href="#id67" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/anogan.html#AnoGAN.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.anogan.AnoGAN.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id68">
<h4>Parameters<a class="headerlink" href="#id68" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id69">
<h4>Returns<a class="headerlink" href="#id69" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/anogan.html#AnoGAN.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.anogan.AnoGAN.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id70">
<h4>Parameters<a class="headerlink" href="#id70" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id71">
<h4>Returns<a class="headerlink" href="#id71" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id72">
<h4>Parameters<a class="headerlink" href="#id72" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id73">
<h4>Returns<a class="headerlink" href="#id73" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id74">
<h4>Parameters<a class="headerlink" href="#id74" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id75">
<h4>Returns<a class="headerlink" href="#id75" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id76">
<h4>Parameters<a class="headerlink" href="#id76" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id77">
<h4>Returns<a class="headerlink" href="#id77" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.plot_learning_curves">
<span class="sig-name descname"><span class="pre">plot_learning_curves</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_ind</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_smoothening</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/anogan.html#AnoGAN.plot_learning_curves"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.anogan.AnoGAN.plot_learning_curves" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id78">
<h4>Parameters<a class="headerlink" href="#id78" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id79">
<h4>Returns<a class="headerlink" href="#id79" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id80">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id81">
<h4>Parameters<a class="headerlink" href="#id81" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id82">
<h4>Returns<a class="headerlink" href="#id82" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id83">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id84">
<h4>Parameters<a class="headerlink" href="#id84" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id85">
<h4>Returns<a class="headerlink" href="#id85" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id86">
<h4>Parameters<a class="headerlink" href="#id86" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id87">
<h4>Returns<a class="headerlink" href="#id87" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.anogan.AnoGAN.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.anogan.AnoGAN.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id88">
<h4>Returns<a class="headerlink" href="#id88" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.auto_encoder">
<span id="pyod-models-auto-encoder-module"></span><h2>pyod.models.auto_encoder module<a class="headerlink" href="#module-pyod.models.auto_encoder" title="Link to this heading">¶</a></h2>
<p>Using AutoEncoder with Outlier Detection</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.auto_encoder.</span></span><span class="sig-name descname"><span class="pre">AutoEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_compile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">{'weight_decay':</span> <span class="pre">1e-05}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_neuron_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[64,</span> <span class="pre">32]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_activation_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/auto_encoder.html#AutoEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDeepLearningDetector</span></code></p>
<p>Auto Encoder (AE) is a type of neural networks for learning useful data
representations in an unsupervised manner. Similar to PCA, AE could be used
to detect outlying objects in the data by calculating the reconstruction
errors. See <span id="id89">[<a class="reference internal" href="#id1122" title="Charu C Aggarwal. Outlier analysis. In Data mining, 75–79. Springer, 2015.">BAgg15</a>]</span> Chapter 3 for details.</p>
<section id="id90">
<h3>Parameters<a class="headerlink" href="#id90" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, 
i.e. the proportion of outliers in the data set. 
Used when fitting to define the threshold on the decision function.</p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply the preprocessing procedure before training models.</p>
</dd>
<dt>lr<span class="classifier">float, optional (default=1e-3)</span></dt><dd><p>The initial learning rate for the optimizer.</p>
</dd>
<dt>epoch_num<span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of epochs for training.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>The batch size for training.</p>
</dd>
<dt>optimizer_name<span class="classifier">str, optional (default=’adam’)</span></dt><dd><p>The name of theoptimizer used to train the model.</p>
</dd>
<dt>device<span class="classifier">str, optional (default=None)</span></dt><dd><p>The device to use for the model. If None, it will be decided
automatically. If you want to use MPS, set it to ‘mps’.</p>
</dd>
<dt>random_state<span class="classifier">int, optional (default=42)</span></dt><dd><p>The random seed for reproducibility.</p>
</dd>
<dt>use_compile<span class="classifier">bool, optional (default=False)</span></dt><dd><p>Whether to compile the model.
If True, the model will be compiled before training.
This is only available for
PyTorch version &gt;= 2.0.0. and Python &lt; 3.12.</p>
</dd>
<dt>compile_mode<span class="classifier">str, optional (default=’default’)</span></dt><dd><p>The mode to compile the model.
Can be either “default”, “reduce-overhead”,
“max-autotune” or “max-autotune-no-cudagraphs”.
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile">https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile</a> for details.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=1)</span></dt><dd><p>Verbosity mode.
- 0 = silent
- 1 = progress bar
- 2 = one line per epoch.</p>
</dd>
<dt>optimizer_params<span class="classifier">dict, optional (default={‘weight_decay’: 1e-5})</span></dt><dd><p>Additional parameters for the optimizer.
For example, <cite>optimizer_params={‘weight_decay’: 1e-5}</cite>.</p>
</dd>
<dt>hidden_neuron_list<span class="classifier">list, optional (default=[64, 32])</span></dt><dd><p>The number of neurons per hidden layers. 
So the network has the structure as [feature_size, 64, 32, 32, 64, feature_size].</p>
</dd>
<dt>hidden_activation_name<span class="classifier">str, optional (default=’relu’)</span></dt><dd><p>The activation function used in hidden layers.</p>
</dd>
<dt>batch_norm<span class="classifier">boolean, optional (default=True)</span></dt><dd><p>Whether to apply Batch Normalization,
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html</a></p>
</dd>
<dt>dropout_rate<span class="classifier">float in (0., 1), optional (default=0.2)</span></dt><dd><p>The dropout to be used across all layers.</p>
</dd>
</dl>
</section>
<section id="id91">
<h3>Attributes<a class="headerlink" href="#id91" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">torch.nn.Module</span></dt><dd><p>The underlying AutoEncoder model.</p>
</dd>
<dt>optimizer<span class="classifier">torch.optim</span></dt><dd><p>The optimizer used to train the model.</p>
</dd>
<dt>criterion<span class="classifier">torch.nn.modules</span></dt><dd><p>The loss function used to train the model.</p>
</dd>
<dt><a href="#id1191"><span class="problematic" id="id1192">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1193"><span class="problematic" id="id1194">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1195"><span class="problematic" id="id1196">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.build_model">
<span class="sig-name descname"><span class="pre">build_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/auto_encoder.html#AutoEncoder.build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.build_model" title="Link to this definition">¶</a></dt>
<dd><p>Need to define model in this method.
self.feature_size is the number of features in the input data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id92">
<h4>Parameters<a class="headerlink" href="#id92" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id93">
<h4>Returns<a class="headerlink" href="#id93" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</div></blockquote>
<dl class="simple">
<dt>batch_size<span class="classifier">int, optional (default=None)</span></dt><dd><p>The batch size for processing the input samples.
If not specified, the default batch size is used.</p>
</dd>
</dl>
<section id="id94">
<h4>Returns<a class="headerlink" href="#id94" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_loader</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.evaluate" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate the deep learning model.</p>
<section id="id95">
<h4>Parameters<a class="headerlink" href="#id95" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>data_loader<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>The data loader for evaluating the model.</p>
</dd>
</dl>
</section>
<section id="id96">
<h4>Returns<a class="headerlink" href="#id96" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id97">
<h4>Parameters<a class="headerlink" href="#id97" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">numpy array of shape (n_samples,), optional (default=None)</span></dt><dd><p>The ground truth of input samples. Not used in unsupervised methods.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id98">
<h4>Parameters<a class="headerlink" href="#id98" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id99">
<h4>Returns<a class="headerlink" href="#id99" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id100">
<h4>Parameters<a class="headerlink" href="#id100" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id101">
<h4>Returns<a class="headerlink" href="#id101" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id102">
<h4>Parameters<a class="headerlink" href="#id102" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id103">
<h4>Returns<a class="headerlink" href="#id103" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id104">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id105">
<h4>Parameters<a class="headerlink" href="#id105" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id106">
<h4>Returns<a class="headerlink" href="#id106" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id107">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id108">
<h4>Parameters<a class="headerlink" href="#id108" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id109">
<h4>Returns<a class="headerlink" href="#id109" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id110">
<h4>Parameters<a class="headerlink" href="#id110" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id111">
<h4>Returns<a class="headerlink" href="#id111" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.save" title="Link to this definition">¶</a></dt>
<dd><p>Save the model to the specified path.</p>
<section id="id112">
<h4>Parameters<a class="headerlink" href="#id112" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>path<span class="classifier">str</span></dt><dd><p>The path to save the model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_loader</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.train" title="Link to this definition">¶</a></dt>
<dd><p>Train the deep learning model.</p>
<section id="id113">
<h4>Parameters<a class="headerlink" href="#id113" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>train_loader<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>The data loader for training the model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.training_forward">
<span class="sig-name descname"><span class="pre">training_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/auto_encoder.html#AutoEncoder.training_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.training_forward" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass for training the model.
Abstract method to be implemented.</p>
<section id="id114">
<h4>Parameters<a class="headerlink" href="#id114" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>batch_data<span class="classifier">tuple</span></dt><dd><p>The batch data for training the model.</p>
</dd>
</dl>
</section>
<section id="id115">
<h4>Returns<a class="headerlink" href="#id115" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>loss<span class="classifier">float or tuple of float</span></dt><dd><p>The loss.item of the model, or a tuple of loss.item 
if there are multiple losses.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.auto_encoder.AutoEncoder.training_prepare">
<span class="sig-name descname"><span class="pre">training_prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.auto_encoder.AutoEncoder.training_prepare" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</dd></dl>

</section>
<section id="pyod-models-auto-encoder-torch-module">
<h2>pyod.models.auto_encoder_torch module<a class="headerlink" href="#pyod-models-auto-encoder-torch-module" title="Link to this heading">¶</a></h2>
</section>
<section id="module-pyod.models.cblof">
<span id="pyod-models-cblof-module"></span><h2>pyod.models.cblof module<a class="headerlink" href="#module-pyod.models.cblof" title="Link to this heading">¶</a></h2>
<p>Clustering Based Local Outlier Factor (CBLOF)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.cblof.</span></span><span class="sig-name descname"><span class="pre">CBLOF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_clusters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clustering_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cblof.html#CBLOF"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cblof.CBLOF" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>The CBLOF operator calculates the outlier score based on cluster-based
local outlier factor.</p>
<p>CBLOF takes as an input the data set and the cluster model that was
generated by a clustering algorithm. It classifies the clusters into small
clusters and large clusters using the parameters alpha and beta.
The anomaly score is then calculated based on the size of the cluster the
point belongs to as well as the distance to the nearest large cluster.</p>
<p>Use weighting for outlier factor based on the sizes of the clusters as
proposed in the original publication. Since this might lead to unexpected
behavior (outliers close to small clusters are not found), it is disabled
by default.Outliers scores are solely computed based on their distance to
the closest large cluster center.</p>
<p>By default, kMeans is used for clustering algorithm instead of
Squeezer algorithm mentioned in the original paper for multiple reasons.</p>
<p>See <span id="id116">[<a class="reference internal" href="#id1127" title="Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. Pattern Recognition Letters, 24(9-10):1641–1650, 2003.">BHXD03</a>]</span> for details.</p>
<section id="id117">
<h3>Parameters<a class="headerlink" href="#id117" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_clusters<span class="classifier">int, optional (default=8)</span></dt><dd><p>The number of clusters to form as well as the number of
centroids to generate.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>clustering_estimator<span class="classifier">Estimator, optional (default=None)</span></dt><dd><p>The base clustering algorithm for performing data clustering.
A valid clustering algorithm should be passed in. The estimator should
have standard sklearn APIs, fit() and predict(). The estimator should
have attributes <code class="docutils literal notranslate"><span class="pre">labels_</span></code> and <code class="docutils literal notranslate"><span class="pre">cluster_centers_</span></code>.
If <code class="docutils literal notranslate"><span class="pre">cluster_centers_</span></code> is not in the attributes once the model is fit,
it is calculated as the mean of the samples in a cluster.</p>
<p>If not set, CBLOF uses KMeans for scalability. See
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a></p>
</dd>
<dt>alpha<span class="classifier">float in (0.5, 1), optional (default=0.9)</span></dt><dd><p>Coefficient for deciding small and large clusters. The ratio
of the number of samples in large clusters to the number of samples in
small clusters.</p>
</dd>
<dt>beta<span class="classifier">int or float in (1,), optional (default=5).</span></dt><dd><p>Coefficient for deciding small and large clusters. For a list
sorted clusters by size <cite>|C1|, |C2|, …, |Cn|, beta = |Ck|/|Ck-1|</cite></p>
</dd>
<dt>use_weights<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If set to True, the size of clusters are used as weights in
outlier score calculation.</p>
</dd>
<dt>check_estimator<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If set to True, check whether the base estimator is consistent with
sklearn standard.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>check_estimator may throw errors with scikit-learn 0.20 above.</p>
</div>
</dd>
<dt>random_state<span class="classifier">int, RandomState or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id118">
<h3>Attributes<a class="headerlink" href="#id118" title="Link to this heading">¶</a></h3>
<dl>
<dt><a href="#id1197"><span class="problematic" id="id1198">clustering_estimator_</span></a><span class="classifier">Estimator, sklearn instance</span></dt><dd><p>Base estimator for clustering.</p>
</dd>
<dt><a href="#id1199"><span class="problematic" id="id1200">cluster_labels_</span></a><span class="classifier">list of shape (n_samples,)</span></dt><dd><p>Cluster assignment for the training samples.</p>
</dd>
<dt><a href="#id1201"><span class="problematic" id="id1202">n_clusters_</span></a><span class="classifier">int</span></dt><dd><p>Actual number of clusters (possibly different from n_clusters).</p>
</dd>
<dt><a href="#id1203"><span class="problematic" id="id1204">cluster_sizes_</span></a><span class="classifier">list of shape (<a href="#id1205"><span class="problematic" id="id1206">n_clusters_</span></a>,)</span></dt><dd><p>The size of each cluster once fitted with the training data.</p>
</dd>
<dt><a href="#id1207"><span class="problematic" id="id1208">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher scores.
This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1209"><span class="problematic" id="id1210">cluster_centers_</span></a><span class="classifier">numpy array of shape (<a href="#id1211"><span class="problematic" id="id1212">n_clusters_</span></a>, n_features)</span></dt><dd><p>The center of each cluster.</p>
</dd>
<dt><a href="#id1213"><span class="problematic" id="id1214">small_cluster_labels_</span></a><span class="classifier">list of clusters numbers</span></dt><dd><p>The cluster assignments belonging to small clusters.</p>
</dd>
<dt><a href="#id1215"><span class="problematic" id="id1216">large_cluster_labels_</span></a><span class="classifier">list of clusters numbers</span></dt><dd><p>The cluster assignments belonging to large clusters.</p>
</dd>
<dt><a href="#id1217"><span class="problematic" id="id1218">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1219"><span class="problematic" id="id1220">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id119">
<h4>Parameters<a class="headerlink" href="#id119" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id120">
<h4>Returns<a class="headerlink" href="#id120" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cblof.html#CBLOF.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cblof.CBLOF.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id121">
<h4>Parameters<a class="headerlink" href="#id121" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id122">
<h4>Returns<a class="headerlink" href="#id122" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cblof.html#CBLOF.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cblof.CBLOF.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id123">
<h4>Parameters<a class="headerlink" href="#id123" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id124">
<h4>Returns<a class="headerlink" href="#id124" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id125">
<h4>Parameters<a class="headerlink" href="#id125" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id126">
<h4>Returns<a class="headerlink" href="#id126" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id127">
<h4>Parameters<a class="headerlink" href="#id127" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id128">
<h4>Returns<a class="headerlink" href="#id128" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id129">
<h4>Parameters<a class="headerlink" href="#id129" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id130">
<h4>Returns<a class="headerlink" href="#id130" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id131">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id132">
<h4>Parameters<a class="headerlink" href="#id132" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id133">
<h4>Returns<a class="headerlink" href="#id133" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id134">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id135">
<h4>Parameters<a class="headerlink" href="#id135" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id136">
<h4>Returns<a class="headerlink" href="#id136" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cblof.CBLOF.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cblof.CBLOF.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id137">
<h4>Parameters<a class="headerlink" href="#id137" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id138">
<h4>Returns<a class="headerlink" href="#id138" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.cof">
<span id="pyod-models-cof-module"></span><h2>pyod.models.cof module<a class="headerlink" href="#module-pyod.models.cof" title="Link to this heading">¶</a></h2>
<p>Connectivity-Based Outlier Factor (COF) Algorithm</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.cof.COF">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.cof.</span></span><span class="sig-name descname"><span class="pre">COF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fast'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cof.html#COF"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cof.COF" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Connectivity-Based Outlier Factor (COF) COF uses the ratio of average
chaining distance of data point and the average of average chaining
distance of k nearest neighbor of the data point, as the outlier score
for observations.</p>
<p>See <span id="id139">[<a class="reference internal" href="#id1137" title="Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535–548. Springer, 2002.">BTCFC02</a>]</span> for details.</p>
<p>Two version of COF are supported:</p>
<ul class="simple">
<li><p>Fast COF: computes the entire pairwise distance matrix at the cost of a
O(n^2) memory requirement.</p></li>
<li><p>Memory efficient COF: calculates pairwise distances incrementally.
Use this implementation when it is not feasible to fit the n-by-n 
distance in memory. This leads to a linear overhead because many 
distances will have to be recalculated.</p></li>
</ul>
<section id="id140">
<h3>Parameters<a class="headerlink" href="#id140" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_neighbors<span class="classifier">int, optional (default=20)</span></dt><dd><p>Number of neighbors to use by default for k neighbors queries.
Note that n_neighbors should be less than the number of samples.
If n_neighbors is larger than the number of samples provided,
all samples will be used.</p>
</dd>
<dt>method<span class="classifier">string, optional (default=’fast’)</span></dt><dd><p>Valid values for method are:</p>
<ul class="simple">
<li><p>‘fast’ Fast COF, computes the full pairwise distance matrix up front.</p></li>
<li><p>‘memory’ Memory-efficient COF, computes pairwise distances only when
needed at the cost of computational speed.</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id141">
<h3>Attributes<a class="headerlink" href="#id141" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1221"><span class="problematic" id="id1222">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1223"><span class="problematic" id="id1224">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1225"><span class="problematic" id="id1226">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
<dt><a href="#id1227"><span class="problematic" id="id1228">n_neighbors_</span></a>: int</dt><dd><p>Number of neighbors to use by default for k neighbors queries.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id142">
<h4>Parameters<a class="headerlink" href="#id142" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id143">
<h4>Returns<a class="headerlink" href="#id143" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cof.html#COF.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cof.COF.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id144">
<h4>Parameters<a class="headerlink" href="#id144" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id145">
<h4>Returns<a class="headerlink" href="#id145" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cof.html#COF.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cof.COF.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id146">
<h4>Parameters<a class="headerlink" href="#id146" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id147">
<h4>Returns<a class="headerlink" href="#id147" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id148">
<h4>Parameters<a class="headerlink" href="#id148" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id149">
<h4>Returns<a class="headerlink" href="#id149" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id150">
<h4>Parameters<a class="headerlink" href="#id150" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id151">
<h4>Returns<a class="headerlink" href="#id151" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id152">
<h4>Parameters<a class="headerlink" href="#id152" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id153">
<h4>Returns<a class="headerlink" href="#id153" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id154">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id155">
<h4>Parameters<a class="headerlink" href="#id155" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id156">
<h4>Returns<a class="headerlink" href="#id156" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id157">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id158">
<h4>Parameters<a class="headerlink" href="#id158" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id159">
<h4>Returns<a class="headerlink" href="#id159" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cof.COF.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cof.COF.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id160">
<h4>Parameters<a class="headerlink" href="#id160" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id161">
<h4>Returns<a class="headerlink" href="#id161" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.combination">
<span id="pyod-models-combination-module"></span><h2>pyod.models.combination module<a class="headerlink" href="#module-pyod.models.combination" title="Link to this heading">¶</a></h2>
<p>A collection of model combination functionalities.</p>
<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.aom">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">aom</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#aom"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.aom" title="Link to this definition">¶</a></dt>
<dd><p>Average of Maximum - An ensemble method for combining multiple
estimators. See <span id="id162">[<a class="reference internal" href="#id1115" title="Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17(1):24–47, 2015.">BAS15</a>]</span> for details.</p>
<p>First dividing estimators into subgroups, take the maximum score as the
subgroup score. Finally, take the average of all subgroup outlier scores.</p>
<section id="id163">
<h3>Parameters<a class="headerlink" href="#id163" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>The score matrix outputted from various estimators</p>
</dd>
<dt>n_buckets<span class="classifier">int, optional (default=5)</span></dt><dd><p>The number of subgroups to build</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’static’)</span></dt><dd><p>{‘static’, ‘dynamic’}, if ‘dynamic’, build subgroups
randomly with dynamic bucket size.</p>
</dd>
<dt>bootstrap_estimators<span class="classifier">bool, optional (default=False)</span></dt><dd><p>Whether estimators are drawn with replacement.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the
random number generator; If RandomState instance, random_state is
the random number generator; If None, the random number generator
is the RandomState instance used by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id164">
<h3>Returns<a class="headerlink" href="#id164" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">Numpy array of shape (n_samples,)</span></dt><dd><p>The combined outlier scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.average">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">average</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#average"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.average" title="Link to this definition">¶</a></dt>
<dd><p>Combination method to merge the outlier scores from multiple estimators
by taking the average.</p>
<section id="id165">
<h3>Parameters<a class="headerlink" href="#id165" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>Score matrix from multiple estimators on the same samples.</p>
</dd>
<dt>estimator_weights<span class="classifier">list of shape (1, n_estimators)</span></dt><dd><p>If specified, using weighted average</p>
</dd>
</dl>
</section>
<section id="id166">
<h3>Returns<a class="headerlink" href="#id166" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">numpy array of shape (n_samples, )</span></dt><dd><p>The combined outlier scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.majority_vote">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">majority_vote</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#majority_vote"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.majority_vote" title="Link to this definition">¶</a></dt>
<dd><p>Combination method to merge the scores from multiple estimators
by majority vote.</p>
<section id="id167">
<h3>Parameters<a class="headerlink" href="#id167" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>Score matrix from multiple estimators on the same samples.</p>
</dd>
<dt>weights<span class="classifier">numpy array of shape (1, n_estimators)</span></dt><dd><p>If specified, using weighted majority weight.</p>
</dd>
</dl>
</section>
<section id="id168">
<h3>Returns<a class="headerlink" href="#id168" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">numpy array of shape (n_samples, )</span></dt><dd><p>The combined scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.maximization">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">maximization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#maximization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.maximization" title="Link to this definition">¶</a></dt>
<dd><p>Combination method to merge the outlier scores from multiple estimators
by taking the maximum.</p>
<section id="id169">
<h3>Parameters<a class="headerlink" href="#id169" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>Score matrix from multiple estimators on the same samples.</p>
</dd>
</dl>
</section>
<section id="id170">
<h3>Returns<a class="headerlink" href="#id170" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">numpy array of shape (n_samples, )</span></dt><dd><p>The combined outlier scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.median">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">median</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#median"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.median" title="Link to this definition">¶</a></dt>
<dd><p>Combination method to merge the scores from multiple estimators
by taking the median.</p>
<section id="id171">
<h3>Parameters<a class="headerlink" href="#id171" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>Score matrix from multiple estimators on the same samples.</p>
</dd>
</dl>
</section>
<section id="id172">
<h3>Returns<a class="headerlink" href="#id172" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">numpy array of shape (n_samples, )</span></dt><dd><p>The combined scores.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.combination.moa">
<span class="sig-prename descclassname"><span class="pre">pyod.models.combination.</span></span><span class="sig-name descname"><span class="pre">moa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_buckets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/combination.html#moa"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.combination.moa" title="Link to this definition">¶</a></dt>
<dd><p>Maximization of Average - An ensemble method for combining multiple
estimators. See <span id="id173">[<a class="reference internal" href="#id1115" title="Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17(1):24–47, 2015.">BAS15</a>]</span> for details.</p>
<p>First dividing estimators into subgroups, take the average score as the
subgroup score. Finally, take the maximization of all subgroup outlier
scores.</p>
<section id="id174">
<h3>Parameters<a class="headerlink" href="#id174" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>scores<span class="classifier">numpy array of shape (n_samples, n_estimators)</span></dt><dd><p>The score matrix outputted from various estimators</p>
</dd>
<dt>n_buckets<span class="classifier">int, optional (default=5)</span></dt><dd><p>The number of subgroups to build</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’static’)</span></dt><dd><p>{‘static’, ‘dynamic’}, if ‘dynamic’, build subgroups
randomly with dynamic bucket size.</p>
</dd>
<dt>bootstrap_estimators<span class="classifier">bool, optional (default=False)</span></dt><dd><p>Whether estimators are drawn with replacement.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the
random number generator; If RandomState instance, random_state is
the random number generator; If None, the random number generator
is the RandomState instance used by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id175">
<h3>Returns<a class="headerlink" href="#id175" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>combined_scores<span class="classifier">Numpy array of shape (n_samples,)</span></dt><dd><p>The combined outlier scores.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
<section id="module-pyod.models.cd">
<span id="pyod-models-cd-module"></span><h2>pyod.models.cd module<a class="headerlink" href="#module-pyod.models.cd" title="Link to this heading">¶</a></h2>
<p>Cook’s distance outlier detection (CD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.cd.CD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.cd.</span></span><span class="sig-name descname"><span class="pre">CD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">LinearRegression()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cd.html#CD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cd.CD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<dl class="simple">
<dt>Cook’s distance can be used to identify points that negatively</dt><dd><p>affect a regression model. A combination of each observation’s
leverage and residual values are used in the measurement. Higher
leverage and residuals relate to  higher Cook’s distances. Note
that this method is unsupervised and requires at least two 
features for X with which to calculate the mean Cook’s distance
for each datapoint. Read more in the <span id="id176">[<a class="reference internal" href="#id1155" title="R Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19(1):15–18, 1977.">BCoo77</a>]</span>.</p>
</dd>
</dl>
<section id="id177">
<h3>Parameters<a class="headerlink" href="#id177" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>model<span class="classifier">object, optional (default=LinearRegression())</span></dt><dd><p>Regression model used to calculate the Cook’s distance</p>
</dd>
</dl>
</section>
<section id="id178">
<h3>Attributes<a class="headerlink" href="#id178" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1229"><span class="problematic" id="id1230">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1231"><span class="problematic" id="id1232">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The modified z-score to use as a threshold. Observations with
a modified z-score (based on the median absolute deviation) greater
than this value will be classified as outliers.</p>
</dd>
<dt><a href="#id1233"><span class="problematic" id="id1234">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id179">
<h4>Parameters<a class="headerlink" href="#id179" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id180">
<h4>Returns<a class="headerlink" href="#id180" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cd.html#CD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cd.CD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
For consistency, outliers are assigned with larger anomaly scores.</p>
<section id="id181">
<h4>Parameters<a class="headerlink" href="#id181" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id182">
<h4>Returns<a class="headerlink" href="#id182" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/cd.html#CD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.cd.CD.fit" title="Link to this definition">¶</a></dt>
<dd><p>“Fit detector. y is ignored in unsupervised methods.</p>
<section id="id183">
<h4>Parameters<a class="headerlink" href="#id183" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id184">
<h4>Returns<a class="headerlink" href="#id184" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id185">
<h4>Parameters<a class="headerlink" href="#id185" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id186">
<h4>Returns<a class="headerlink" href="#id186" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id187">
<h4>Parameters<a class="headerlink" href="#id187" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id188">
<h4>Returns<a class="headerlink" href="#id188" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id189">
<h4>Parameters<a class="headerlink" href="#id189" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id190">
<h4>Returns<a class="headerlink" href="#id190" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id191">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id192">
<h4>Parameters<a class="headerlink" href="#id192" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id193">
<h4>Returns<a class="headerlink" href="#id193" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id194">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id195">
<h4>Parameters<a class="headerlink" href="#id195" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id196">
<h4>Returns<a class="headerlink" href="#id196" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.cd.CD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.cd.CD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id197">
<h4>Parameters<a class="headerlink" href="#id197" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id198">
<h4>Returns<a class="headerlink" href="#id198" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.copod">
<span id="pyod-models-copod-module"></span><h2>pyod.models.copod module<a class="headerlink" href="#module-pyod.models.copod" title="Link to this heading">¶</a></h2>
<p>Copula Based Outlier Detector (COPOD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.copod.</span></span><span class="sig-name descname"><span class="pre">COPOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/copod.html#COPOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.copod.COPOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>COPOD class for Copula Based Outlier Detector.
COPOD is a parameter-free, highly interpretable outlier detection algorithm
based on empirical copula models.
See <span id="id199">[<a class="reference internal" href="#id1148" title="Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In IEEE International Conference on Data Mining (ICDM). IEEE, 2020.">BLZB+20</a>]</span> for details.</p>
<section id="id200">
<h3>Parameters<a class="headerlink" href="#id200" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_jobs<span class="classifier">optional (default=1)</span></dt><dd><p>The number of jobs to run in parallel for both <cite>fit</cite> and
<cite>predict</cite>. If -1, then the number of jobs is set to the
number of cores.</p>
</dd>
</dl>
</section>
<section id="id201">
<h3>Attributes<a class="headerlink" href="#id201" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1235"><span class="problematic" id="id1236">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1237"><span class="problematic" id="id1238">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1239"><span class="problematic" id="id1240">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id202">
<h4>Parameters<a class="headerlink" href="#id202" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id203">
<h4>Returns<a class="headerlink" href="#id203" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/copod.html#COPOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.copod.COPOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict raw anomaly score of X using the fitted detector.</dt><dd><p>For consistency, outliers are assigned with larger anomaly scores.</p>
</dd>
</dl>
<section id="id204">
<h4>Parameters<a class="headerlink" href="#id204" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id205">
<h4>Returns<a class="headerlink" href="#id205" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.explain_outlier">
<span class="sig-name descname"><span class="pre">explain_outlier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ind</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">columns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoffs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/copod.html#COPOD.explain_outlier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.copod.COPOD.explain_outlier" title="Link to this definition">¶</a></dt>
<dd><p>Plot dimensional outlier graph for a given data point within
the dataset.</p>
<section id="id206">
<h4>Parameters<a class="headerlink" href="#id206" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>ind<span class="classifier">int</span></dt><dd><p>The index of the data point one wishes to obtain
a dimensional outlier graph for.</p>
</dd>
<dt>columns<span class="classifier">list</span></dt><dd><p>Specify a list of features/dimensions for plotting. If not 
specified, use all features.</p>
</dd>
<dt>cutoffs<span class="classifier">list of floats in (0., 1), optional (default=[0.95, 0.99])</span></dt><dd><p>The significance cutoff bands of the dimensional outlier graph.</p>
</dd>
<dt>feature_names<span class="classifier">list of strings</span></dt><dd><p>The display names of all columns of the dataset,
to show on the x-axis of the plot.</p>
</dd>
<dt>file_name<span class="classifier">string</span></dt><dd><p>The name to save the figure</p>
</dd>
<dt>file_type<span class="classifier">string</span></dt><dd><p>The file type to save the figure</p>
</dd>
</dl>
</section>
<section id="id207">
<h4>Returns<a class="headerlink" href="#id207" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>Plot<span class="classifier">matplotlib plot</span></dt><dd><p>The dimensional outlier graph for data point with index ind.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/copod.html#COPOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.copod.COPOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The input samples.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
<section id="id208">
<h4>Returns<a class="headerlink" href="#id208" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id209">
<h4>Parameters<a class="headerlink" href="#id209" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id210">
<h4>Returns<a class="headerlink" href="#id210" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id211">
<h4>Parameters<a class="headerlink" href="#id211" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id212">
<h4>Returns<a class="headerlink" href="#id212" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id213">
<h4>Parameters<a class="headerlink" href="#id213" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id214">
<h4>Returns<a class="headerlink" href="#id214" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id215">
<h4>Parameters<a class="headerlink" href="#id215" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id216">
<h4>Returns<a class="headerlink" href="#id216" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id217">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id218">
<h4>Parameters<a class="headerlink" href="#id218" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id219">
<h4>Returns<a class="headerlink" href="#id219" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id220">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id221">
<h4>Parameters<a class="headerlink" href="#id221" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id222">
<h4>Returns<a class="headerlink" href="#id222" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id223">
<h4>Parameters<a class="headerlink" href="#id223" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id224">
<h4>Returns<a class="headerlink" href="#id224" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.copod.COPOD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.copod.COPOD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id225">
<h4>Returns<a class="headerlink" href="#id225" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.copod.skew">
<span class="sig-prename descclassname"><span class="pre">pyod.models.copod.</span></span><span class="sig-name descname"><span class="pre">skew</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/copod.html#skew"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.copod.skew" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-pyod.models.deep_svdd">
<span id="pyod-models-deep-svdd-module"></span><h2>pyod.models.deep_svdd module<a class="headerlink" href="#module-pyod.models.deep_svdd" title="Link to this heading">¶</a></h2>
<p>Deep One-Class Classification for outlier detection</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.deep_svdd.</span></span><span class="sig-name descname"><span class="pre">DeepSVDD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_ae</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_neurons</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sigmoid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_regularizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/deep_svdd.html#DeepSVDD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Deep One-Class Classifier with AutoEncoder (AE) is a type of neural
networks for learning useful data representations in an unsupervised way.
DeepSVDD trains a neural network while minimizing the volume of a
hypersphere that encloses the network representations of the data,
forcing the network to extract the common factors of variation.
Similar to PCA, DeepSVDD could be used to detect outlying objects in the
data by calculating the distance from center
See <span id="id226">[<a class="reference internal" href="#id1151" title="Lukas Ruff, Robert Vandermeulen, Nico Görnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. International conference on machine learning, 2018.">BRVG+18</a>]</span> for details.</p>
<section id="id227">
<h3>Parameters<a class="headerlink" href="#id227" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>n_features: int, </dt><dd><p>Number of features in the input data.</p>
</dd>
<dt>c: float, optional (default=’forwad_nn_pass’)</dt><dd><p>Deep SVDD center, the default will be calculated based on network
initialization first forward pass. To get repeated results set
random_state if c is set to None.</p>
</dd>
<dt>use_ae: bool, optional (default=False)</dt><dd><p>The AutoEncoder type of DeepSVDD it reverse neurons from hidden_neurons
if set to True.</p>
</dd>
<dt>hidden_neurons<span class="classifier">list, optional (default=[64, 32])</span></dt><dd><p>The number of neurons per hidden layers. if use_ae is True, neurons
will be reversed eg. [64, 32] -&gt; [64, 32, 32, 64, n_features]</p>
</dd>
<dt>hidden_activation<span class="classifier">str, optional (default=’relu’)</span></dt><dd><p>Activation function to use for hidden layers.
All hidden layers are forced to use the same type of activation.
See <a class="reference external" href="https://keras.io/activations/">https://keras.io/activations/</a></p>
</dd>
<dt>output_activation<span class="classifier">str, optional (default=’sigmoid’)</span></dt><dd><p>Activation function to use for output layer.
See <a class="reference external" href="https://keras.io/activations/">https://keras.io/activations/</a></p>
</dd>
<dt>optimizer<span class="classifier">str, optional (default=’adam’)</span></dt><dd><p>String (name of optimizer) or optimizer instance.
See <a class="reference external" href="https://keras.io/optimizers/">https://keras.io/optimizers/</a></p>
</dd>
<dt>epochs<span class="classifier">int, optional (default=100)</span></dt><dd><p>Number of epochs to train the model.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>Number of samples per gradient update.</p>
</dd>
<dt>dropout_rate<span class="classifier">float in (0., 1), optional (default=0.2)</span></dt><dd><p>The dropout to be used across all layers.</p>
</dd>
<dt>l2_regularizer<span class="classifier">float in (0., 1), optional (default=0.1)</span></dt><dd><p>The regularization strength of activity_regularizer
applied on each layer. By default, l2 regularizer is used. See
<a class="reference external" href="https://keras.io/regularizers/">https://keras.io/regularizers/</a></p>
</dd>
<dt>validation_size<span class="classifier">float in (0., 1), optional (default=0.1)</span></dt><dd><p>The percentage of data to be used for validation.</p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply standardization on the data.</p>
</dd>
<dt>random_state<span class="classifier">random_state: int, RandomState instance or None, optional</span></dt><dd><p>(default=None)
If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. When fitting this is used
to define the threshold on the decision function.</p>
</dd>
</dl>
</section>
<section id="id228">
<h3>Attributes<a class="headerlink" href="#id228" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1241"><span class="problematic" id="id1242">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1243"><span class="problematic" id="id1244">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1245"><span class="problematic" id="id1246">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id229">
<h4>Parameters<a class="headerlink" href="#id229" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id230">
<h4>Returns<a class="headerlink" href="#id230" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/deep_svdd.html#DeepSVDD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id231">
<h4>Parameters<a class="headerlink" href="#id231" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id232">
<h4>Returns<a class="headerlink" href="#id232" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/deep_svdd.html#DeepSVDD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id233">
<h4>Parameters<a class="headerlink" href="#id233" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id234">
<h4>Returns<a class="headerlink" href="#id234" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id235">
<h4>Parameters<a class="headerlink" href="#id235" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id236">
<h4>Returns<a class="headerlink" href="#id236" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id237">
<h4>Parameters<a class="headerlink" href="#id237" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id238">
<h4>Returns<a class="headerlink" href="#id238" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id239">
<h4>Parameters<a class="headerlink" href="#id239" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id240">
<h4>Returns<a class="headerlink" href="#id240" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id241">
<h4>Parameters<a class="headerlink" href="#id241" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id242">
<h4>Returns<a class="headerlink" href="#id242" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id243">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id244">
<h4>Parameters<a class="headerlink" href="#id244" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id245">
<h4>Returns<a class="headerlink" href="#id245" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id246">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id247">
<h4>Parameters<a class="headerlink" href="#id247" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id248">
<h4>Returns<a class="headerlink" href="#id248" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id249">
<h4>Parameters<a class="headerlink" href="#id249" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id250">
<h4>Returns<a class="headerlink" href="#id250" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.deep_svdd.DeepSVDD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.deep_svdd.DeepSVDD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id251">
<h4>Returns<a class="headerlink" href="#id251" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.devnet">
<span id="pyod-models-devnet-module"></span><h2>pyod.models.devnet module<a class="headerlink" href="#module-pyod.models.devnet" title="Link to this heading">¶</a></h2>
<p>Deep anomaly detection with deviation networks
Part of the codes are adapted from
<a class="reference external" href="https://github.com/GuansongPang/deviation-network">https://github.com/GuansongPang/deviation-network</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.devnet.</span></span><span class="sig-name descname"><span class="pre">DevNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_batch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">known_outliers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cont_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.02</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/devnet.html#DevNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.devnet.DevNet" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id252">
<h3>Parameters<a class="headerlink" href="#id252" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id253">
<h3>Returns<a class="headerlink" href="#id253" title="Link to this heading">¶</a></h3>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/devnet.html#DevNet.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.devnet.DevNet.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly scores of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on the fitted
detector. For consistency, outliers are assigned with
higher anomaly scores.</p>
<section id="id254">
<h3>Parameters<a class="headerlink" href="#id254" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id255">
<h3>Returns<a class="headerlink" href="#id255" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/devnet.html#DevNet.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.devnet.DevNet.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id256">
<h3>Parameters<a class="headerlink" href="#id256" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id257">
<h3>Returns<a class="headerlink" href="#id257" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id258">
<h3>Parameters<a class="headerlink" href="#id258" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id259">
<h3>Returns<a class="headerlink" href="#id259" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/devnet.html#DevNet.fit_predict_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.devnet.DevNet.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector with labels, predict on samples, and evaluate the model by predefined metrics.</p>
<section id="id260">
<h3>Parameters<a class="headerlink" href="#id260" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The labels or target values corresponding to X.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:
- ‘roc_auc_score’: ROC score
- ‘prc_n_score’: Precision @ rank n score</p>
</dd>
</dl>
</section>
<section id="id261">
<h3>Returns<a class="headerlink" href="#id261" title="Link to this heading">¶</a></h3>
<p>score : float</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id262">
<h3>Parameters<a class="headerlink" href="#id262" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id263">
<h3>Returns<a class="headerlink" href="#id263" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id264">
<h3>Parameters<a class="headerlink" href="#id264" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id265">
<h3>Returns<a class="headerlink" href="#id265" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id266">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id267">
<h3>Parameters<a class="headerlink" href="#id267" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id268">
<h3>Returns<a class="headerlink" href="#id268" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id269">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id270">
<h3>Parameters<a class="headerlink" href="#id270" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id271">
<h3>Returns<a class="headerlink" href="#id271" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id272">
<h3>Parameters<a class="headerlink" href="#id272" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id273">
<h3>Returns<a class="headerlink" href="#id273" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.devnet.DevNet.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.devnet.DevNet.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id274">
<h3>Returns<a class="headerlink" href="#id274" title="Link to this heading">¶</a></h3>
<p>self : object</p>
</section>
</dd></dl>

</dd></dl>

</section>
<section id="module-pyod.models.dif">
<span id="pyod-models-dif-module"></span><h2>pyod.models.dif module<a class="headerlink" href="#module-pyod.models.dif" title="Link to this heading">¶</a></h2>
<p>Deep Isolation Forest for Anomaly Detection (DIF)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.dif.DIF">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.dif.</span></span><span class="sig-name descname"><span class="pre">DIF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">representation_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_neurons</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_connection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ensemble</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/dif.html#DIF"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.dif.DIF" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Deep Isolation Forest (DIF) is an extension of iForest. It uses deep
representation ensemble to achieve non-linear isolation on original data
space. See <span id="id275">[<a class="reference internal" href="#id1166" title="Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, ():1-14, 2023. doi:10.1109/TKDE.2023.3270293.">BXPWW23</a>]</span> for details.</p>
<section id="id276">
<h3>Parameters<a class="headerlink" href="#id276" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>batch_size<span class="classifier">int, optional (default=1000)</span></dt><dd><p>Number of samples per gradient update.</p>
</dd>
<dt>representation_dim, int, optional (default=20)</dt><dd><p>Dimensionality of the representation space.</p>
</dd>
<dt>hidden_neurons, list, optional (default=[64, 32])</dt><dd><p>The number of neurons per hidden layers. So the network has the
structure as [n_features, hidden_neurons[0], hidden_neurons[1], …, representation_dim]</p>
</dd>
<dt>hidden_activation, str, optional (default=’tanh’)</dt><dd><p>Activation function to use for hidden layers.
All hidden layers are forced to use the same type of activation.
See <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</a> for details.
Currently only
‘relu’: nn.ReLU()
‘sigmoid’: nn.Sigmoid()
‘tanh’: nn.Tanh()
are supported. See pyod/utils/torch_utility.py for details.</p>
</dd>
<dt>skip_connection, boolean, optional (default=False)</dt><dd><p>If True, apply skip-connection in the neural network structure.</p>
</dd>
<dt>n_ensemble, int, optional (default=50)</dt><dd><p>The number of deep representation ensemble members.</p>
</dd>
<dt>n_estimators, int, optional (default=6)</dt><dd><p>The number of isolation forest of each representation.</p>
</dd>
<dt>max_samples, int, optional (default=256)</dt><dd><p>The number of samples to draw from X to train each base isolation tree.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>random_state<span class="classifier">int or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random
number generator;
If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</p>
</dd>
<dt>device, ‘cuda’, ‘cpu’, or None, optional (default=None)</dt><dd><p>if ‘cuda’, use GPU acceleration in torch
if ‘cpu’, use cpu in torch
if None, automatically determine whether GPU is available</p>
</dd>
</dl>
</section>
<section id="id277">
<h3>Attributes<a class="headerlink" href="#id277" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>net_lst<span class="classifier">list of torch.Module</span></dt><dd><p>The list of representation neural networks.</p>
</dd>
<dt>iForest_lst<span class="classifier">list of iForest</span></dt><dd><p>The list of instantiated iForest model.</p>
</dd>
<dt>x_reduced_lst: list of numpy array</dt><dd><p>The list of training data representations</p>
</dd>
<dt><a href="#id1247"><span class="problematic" id="id1248">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1249"><span class="problematic" id="id1250">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1251"><span class="problematic" id="id1252">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id278">
<h4>Parameters<a class="headerlink" href="#id278" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id279">
<h4>Returns<a class="headerlink" href="#id279" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/dif.html#DIF.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.dif.DIF.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id280">
<h4>Parameters<a class="headerlink" href="#id280" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id281">
<h4>Returns<a class="headerlink" href="#id281" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/dif.html#DIF.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.dif.DIF.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id282">
<h4>Parameters<a class="headerlink" href="#id282" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id283">
<h4>Returns<a class="headerlink" href="#id283" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id284">
<h4>Parameters<a class="headerlink" href="#id284" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id285">
<h4>Returns<a class="headerlink" href="#id285" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id286">
<h4>Parameters<a class="headerlink" href="#id286" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id287">
<h4>Returns<a class="headerlink" href="#id287" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id288">
<h4>Parameters<a class="headerlink" href="#id288" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id289">
<h4>Returns<a class="headerlink" href="#id289" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id290">
<h4>Parameters<a class="headerlink" href="#id290" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id291">
<h4>Returns<a class="headerlink" href="#id291" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id292">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id293">
<h4>Parameters<a class="headerlink" href="#id293" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id294">
<h4>Returns<a class="headerlink" href="#id294" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id295">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id296">
<h4>Parameters<a class="headerlink" href="#id296" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id297">
<h4>Returns<a class="headerlink" href="#id297" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id298">
<h4>Parameters<a class="headerlink" href="#id298" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id299">
<h4>Returns<a class="headerlink" href="#id299" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.dif.DIF.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.dif.DIF.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id300">
<h4>Returns<a class="headerlink" href="#id300" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.ecod">
<span id="pyod-models-ecod-module"></span><h2>pyod.models.ecod module<a class="headerlink" href="#module-pyod.models.ecod" title="Link to this heading">¶</a></h2>
<p>Unsupervised Outlier Detection Using
Empirical Cumulative Distribution Functions (ECOD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.ecod.</span></span><span class="sig-name descname"><span class="pre">ECOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ecod.html#ECOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ecod.ECOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>ECOD class for Unsupervised Outlier Detection Using Empirical
Cumulative Distribution Functions (ECOD)
ECOD is a parameter-free, highly interpretable outlier detection algorithm
based on empirical CDF functions.
See <span id="id301">[<a class="reference internal" href="#id1154" title="Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 2022.">BLZH+22</a>]</span> for details.</p>
<section id="id302">
<h3>Parameters<a class="headerlink" href="#id302" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_jobs<span class="classifier">optional (default=1)</span></dt><dd><p>The number of jobs to run in parallel for both <cite>fit</cite> and
<cite>predict</cite>. If -1, then the number of jobs is set to the
number of cores.</p>
</dd>
</dl>
</section>
<section id="id303">
<h3>Attributes<a class="headerlink" href="#id303" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1253"><span class="problematic" id="id1254">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1255"><span class="problematic" id="id1256">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1257"><span class="problematic" id="id1258">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id304">
<h4>Parameters<a class="headerlink" href="#id304" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id305">
<h4>Returns<a class="headerlink" href="#id305" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ecod.html#ECOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ecod.ECOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict raw anomaly score of X using the fitted detector.</dt><dd><p>For consistency, outliers are assigned with larger anomaly scores.</p>
</dd>
</dl>
<section id="id306">
<h4>Parameters<a class="headerlink" href="#id306" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id307">
<h4>Returns<a class="headerlink" href="#id307" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.explain_outlier">
<span class="sig-name descname"><span class="pre">explain_outlier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ind</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">columns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoffs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ecod.html#ECOD.explain_outlier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ecod.ECOD.explain_outlier" title="Link to this definition">¶</a></dt>
<dd><p>Plot dimensional outlier graph for a given data point within
the dataset.</p>
<section id="id308">
<h4>Parameters<a class="headerlink" href="#id308" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>ind<span class="classifier">int</span></dt><dd><p>The index of the data point one wishes to obtain
a dimensional outlier graph for.</p>
</dd>
<dt>columns<span class="classifier">list</span></dt><dd><p>Specify a list of features/dimensions for plotting. If not
specified, use all features.</p>
</dd>
<dt>cutoffs<span class="classifier">list of floats in (0., 1), optional (default=[0.95, 0.99])</span></dt><dd><p>The significance cutoff bands of the dimensional outlier graph.</p>
</dd>
<dt>feature_names<span class="classifier">list of strings</span></dt><dd><p>The display names of all columns of the dataset,
to show on the x-axis of the plot.</p>
</dd>
<dt>file_name<span class="classifier">string</span></dt><dd><p>The name to save the figure</p>
</dd>
<dt>file_type<span class="classifier">string</span></dt><dd><p>The file type to save the figure</p>
</dd>
</dl>
</section>
<section id="id309">
<h4>Returns<a class="headerlink" href="#id309" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>Plot<span class="classifier">matplotlib plot</span></dt><dd><p>The dimensional outlier graph for data point with index ind.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ecod.html#ECOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ecod.ECOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The input samples.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
<section id="id310">
<h4>Returns<a class="headerlink" href="#id310" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id311">
<h4>Parameters<a class="headerlink" href="#id311" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id312">
<h4>Returns<a class="headerlink" href="#id312" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id313">
<h4>Parameters<a class="headerlink" href="#id313" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id314">
<h4>Returns<a class="headerlink" href="#id314" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id315">
<h4>Parameters<a class="headerlink" href="#id315" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id316">
<h4>Returns<a class="headerlink" href="#id316" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id317">
<h4>Parameters<a class="headerlink" href="#id317" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id318">
<h4>Returns<a class="headerlink" href="#id318" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id319">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id320">
<h4>Parameters<a class="headerlink" href="#id320" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id321">
<h4>Returns<a class="headerlink" href="#id321" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id322">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id323">
<h4>Parameters<a class="headerlink" href="#id323" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id324">
<h4>Returns<a class="headerlink" href="#id324" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id325">
<h4>Parameters<a class="headerlink" href="#id325" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id326">
<h4>Returns<a class="headerlink" href="#id326" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ecod.ECOD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ecod.ECOD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id327">
<h4>Returns<a class="headerlink" href="#id327" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.ecod.skew">
<span class="sig-prename descclassname"><span class="pre">pyod.models.ecod.</span></span><span class="sig-name descname"><span class="pre">skew</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ecod.html#skew"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ecod.skew" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-pyod.models.feature_bagging">
<span id="pyod-models-feature-bagging-module"></span><h2>pyod.models.feature_bagging module<a class="headerlink" href="#module-pyod.models.feature_bagging" title="Link to this heading">¶</a></h2>
<p>Feature bagging detector</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.feature_bagging.</span></span><span class="sig-name descname"><span class="pre">FeatureBagging</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_detector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'average'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/feature_bagging.html#FeatureBagging"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>A feature bagging detector is a meta estimator that fits a number of
base detectors on various sub-samples of the dataset and use averaging
or other combination methods to improve the predictive accuracy and
control over-fitting.</p>
<p>The sub-sample size is always the same as the original input sample size
but the features are randomly sampled from half of the features to all
features.</p>
<p>By default, LOF is used as the base estimator. However, any estimator
could be used as the base estimator, such as kNN and ABOD.</p>
<p>Feature bagging first construct n subsamples by random selecting a subset
of features, which induces the diversity of base estimators.</p>
<p>Finally, the prediction score is generated by averaging/taking the maximum
of all base detectors. See <span id="id328">[<a class="reference internal" href="#id1119" title="Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157–166. ACM, 2005.">BLK05</a>]</span> for details.</p>
<section id="id329">
<h3>Parameters<a class="headerlink" href="#id329" title="Link to this heading">¶</a></h3>
<dl>
<dt>base_estimator<span class="classifier">object or None, optional (default=None)</span></dt><dd><p>The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a LOF detector.</p>
</dd>
<dt>n_estimators<span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of base estimators in the ensemble.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>max_features<span class="classifier">int or float, optional (default=1.0)</span></dt><dd><p>The number of features to draw from X to train each base estimator.</p>
<ul class="simple">
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</dd>
<dt>bootstrap_features<span class="classifier">bool, optional (default=False)</span></dt><dd><p>Whether features are drawn with replacement.</p>
</dd>
<dt>check_detector<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If set to True, check whether the base estimator is consistent with
pyod standard.</p>
</dd>
<dt>check_estimator<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If set to True, check whether the base estimator is consistent with
sklearn standard.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>check_estimator</cite> will be removed in pyod 0.8.0.; it will be
replaced by <cite>check_detector</cite>.</p>
</div>
</dd>
<dt>n_jobs<span class="classifier">optional (default=1)</span></dt><dd><p>The number of jobs to run in parallel for both <cite>fit</cite> and
<cite>predict</cite>. If -1, then the number of jobs is set to the
number of cores.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</p>
</dd>
<dt>combination<span class="classifier">str, optional (default=’average’)</span></dt><dd><p>The method of combination:</p>
<ul class="simple">
<li><p>if ‘average’: take the average of all detectors</p></li>
<li><p>if ‘max’: take the maximum scores of all detectors</p></li>
</ul>
</dd>
<dt>verbose<span class="classifier">int, optional (default=0)</span></dt><dd><p>Controls the verbosity of the building process.</p>
</dd>
<dt>estimator_params<span class="classifier">dict, optional (default=None)</span></dt><dd><p>The list of attributes to use as parameters
when instantiating a new base estimator. If none are given,
default parameters are used.</p>
</dd>
</dl>
</section>
<section id="id330">
<h3>Attributes<a class="headerlink" href="#id330" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1259"><span class="problematic" id="id1260">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1261"><span class="problematic" id="id1262">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1263"><span class="problematic" id="id1264">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id331">
<h4>Parameters<a class="headerlink" href="#id331" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id332">
<h4>Returns<a class="headerlink" href="#id332" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/feature_bagging.html#FeatureBagging.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id333">
<h4>Parameters<a class="headerlink" href="#id333" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id334">
<h4>Returns<a class="headerlink" href="#id334" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/feature_bagging.html#FeatureBagging.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id335">
<h4>Parameters<a class="headerlink" href="#id335" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id336">
<h4>Returns<a class="headerlink" href="#id336" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id337">
<h4>Parameters<a class="headerlink" href="#id337" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id338">
<h4>Returns<a class="headerlink" href="#id338" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id339">
<h4>Parameters<a class="headerlink" href="#id339" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id340">
<h4>Returns<a class="headerlink" href="#id340" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id341">
<h4>Parameters<a class="headerlink" href="#id341" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id342">
<h4>Returns<a class="headerlink" href="#id342" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id343">
<h4>Parameters<a class="headerlink" href="#id343" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id344">
<h4>Returns<a class="headerlink" href="#id344" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id345">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id346">
<h4>Parameters<a class="headerlink" href="#id346" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id347">
<h4>Returns<a class="headerlink" href="#id347" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id348">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id349">
<h4>Parameters<a class="headerlink" href="#id349" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id350">
<h4>Returns<a class="headerlink" href="#id350" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id351">
<h4>Parameters<a class="headerlink" href="#id351" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id352">
<h4>Returns<a class="headerlink" href="#id352" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.feature_bagging.FeatureBagging.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.feature_bagging.FeatureBagging.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id353">
<h4>Returns<a class="headerlink" href="#id353" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.gmm">
<span id="pyod-models-gmm-module"></span><h2>pyod.models.gmm module<a class="headerlink" href="#module-pyod.models.gmm" title="Link to this heading">¶</a></h2>
<p>Outlier detection based on Gaussian Mixture Model (GMM).</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.gmm.</span></span><span class="sig-name descname"><span class="pre">GMM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariance_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'full'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reg_covar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">means_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precisions_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/gmm.html#GMM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.gmm.GMM" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Wrapper of scikit-learn Gaussian Mixture Model with more functionalities.
Unsupervised Outlier Detection.</p>
<p>See <span id="id354">[<a class="reference internal" href="#id1122" title="Charu C Aggarwal. Outlier analysis. In Data mining, 75–79. Springer, 2015.">BAgg15</a>]</span> Chapter 2 for details.</p>
<section id="id355">
<h3>Parameters<a class="headerlink" href="#id355" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>n_components<span class="classifier">int, default=1</span></dt><dd><p>The number of mixture components.</p>
</dd>
<dt>covariance_type<span class="classifier">{‘full’, ‘tied’, ‘diag’, ‘spherical’}, default=’full’</span></dt><dd><p>String describing the type of covariance parameters to use.</p>
</dd>
<dt>tol<span class="classifier">float, default=1e-3</span></dt><dd><p>The convergence threshold. EM iterations will stop when the
lower bound average gain is below this threshold.</p>
</dd>
<dt>reg_covar<span class="classifier">float, default=1e-6</span></dt><dd><p>Non-negative regularization added to the diagonal of covariance.
Allows to assure that the covariance matrices are all positive.</p>
</dd>
<dt>max_iter<span class="classifier">int, default=100</span></dt><dd><p>The number of EM iterations to perform.</p>
</dd>
<dt>n_init<span class="classifier">int, default=1</span></dt><dd><p>The number of initializations to perform. The best results are kept.</p>
</dd>
<dt>init_params<span class="classifier">{‘kmeans’, ‘random’}, default=’kmeans’</span></dt><dd><p>The method used to initialize the weights, the means and the
precisions.</p>
</dd>
<dt>weights_init<span class="classifier">array-like of shape (n_components, ), default=None</span></dt><dd><p>The user-provided initial weights.
If it is None, weights are initialized using the <cite>init_params</cite> method.</p>
</dd>
<dt>means_init<span class="classifier">array-like of shape (n_components, n_features), default=None</span></dt><dd><p>The user-provided initial means,
If it is None, means are initialized using the <cite>init_params</cite> method.</p>
</dd>
<dt>precisions_init<span class="classifier">array-like, default=None</span></dt><dd><p>The user-provided initial precisions (inverse of the covariance
matrices).
If it is None, precisions are initialized using the ‘init_params’
method.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, default=None</span></dt><dd><p>Controls the random seed given to the method chosen to initialize the
parameters.</p>
</dd>
<dt>warm_start<span class="classifier">bool, default=False</span></dt><dd><p>If ‘warm_start’ is True, the solution of the last fitting is used as
initialization for the next call of fit().</p>
</dd>
<dt>verbose<span class="classifier">int, default=0</span></dt><dd><p>Enable verbose output.</p>
</dd>
<dt>verbose_interval<span class="classifier">int, default=10</span></dt><dd><p>Number of iteration done before the next print.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set.</p>
</dd>
</dl>
</section>
<section id="id356">
<h3>Attributes<a class="headerlink" href="#id356" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1265"><span class="problematic" id="id1266">weights_</span></a><span class="classifier">array-like of shape (n_components,)</span></dt><dd><p>The weights of each mixture components.</p>
</dd>
<dt><a href="#id1267"><span class="problematic" id="id1268">means_</span></a><span class="classifier">array-like of shape (n_components, n_features)</span></dt><dd><p>The mean of each mixture component.</p>
</dd>
<dt><a href="#id1269"><span class="problematic" id="id1270">covariances_</span></a><span class="classifier">array-like</span></dt><dd><p>The covariance of each mixture component.</p>
</dd>
<dt><a href="#id1271"><span class="problematic" id="id1272">precisions_</span></a><span class="classifier">array-like</span></dt><dd><p>The precision matrices for each component in the mixture.</p>
</dd>
<dt><a href="#id1273"><span class="problematic" id="id1274">precisions_cholesky_</span></a><span class="classifier">array-like</span></dt><dd><p>The cholesky decomposition of the precision matrices of each mixture
component.</p>
</dd>
<dt><a href="#id1275"><span class="problematic" id="id1276">converged_</span></a><span class="classifier">bool</span></dt><dd><p>True when convergence was reached in fit(), False otherwise.</p>
</dd>
<dt><a href="#id1277"><span class="problematic" id="id1278">n_iter_</span></a><span class="classifier">int</span></dt><dd><p>Number of step used by the best fit of EM to reach the convergence.</p>
</dd>
<dt><a href="#id1279"><span class="problematic" id="id1280">lower_bound_</span></a><span class="classifier">float</span></dt><dd><p>Lower bound value on the log-likelihood (of the training data with
respect to the model) of the best fit of EM.</p>
</dd>
<dt><a href="#id1281"><span class="problematic" id="id1282">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.</p>
</dd>
<dt><a href="#id1283"><span class="problematic" id="id1284">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1285"><span class="problematic" id="id1286">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id357">
<h4>Parameters<a class="headerlink" href="#id357" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id358">
<h4>Returns<a class="headerlink" href="#id358" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/gmm.html#GMM.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.gmm.GMM.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id359">
<h4>Parameters<a class="headerlink" href="#id359" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id360">
<h4>Returns<a class="headerlink" href="#id360" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/gmm.html#GMM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.gmm.GMM.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id361">
<h4>Parameters<a class="headerlink" href="#id361" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like, shape (n_samples,)</span></dt><dd><p>Per-sample weights. Rescale C per sample. Higher weights
force the classifier to put more emphasis on these points.</p>
</dd>
</dl>
</section>
<section id="id362">
<h4>Returns<a class="headerlink" href="#id362" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id363">
<h4>Parameters<a class="headerlink" href="#id363" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id364">
<h4>Returns<a class="headerlink" href="#id364" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id365">
<h4>Parameters<a class="headerlink" href="#id365" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id366">
<h4>Returns<a class="headerlink" href="#id366" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.precisions_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">precisions_</span></span><a class="headerlink" href="#pyod.models.gmm.GMM.precisions_" title="Link to this definition">¶</a></dt>
<dd><p>The precision matrices for each component in the mixture.
Decorator for scikit-learn Gaussian Mixture Model attributes.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.precisions_cholesky_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">precisions_cholesky_</span></span><a class="headerlink" href="#pyod.models.gmm.GMM.precisions_cholesky_" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>The cholesky decomposition of the precision matrices</dt><dd><p>of each mixture component.</p>
</dd>
</dl>
<p>Decorator for scikit-learn Gaussian Mixture Model attributes.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id367">
<h4>Parameters<a class="headerlink" href="#id367" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id368">
<h4>Returns<a class="headerlink" href="#id368" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id369">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id370">
<h4>Parameters<a class="headerlink" href="#id370" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id371">
<h4>Returns<a class="headerlink" href="#id371" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id372">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id373">
<h4>Parameters<a class="headerlink" href="#id373" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id374">
<h4>Returns<a class="headerlink" href="#id374" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.gmm.GMM.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.gmm.GMM.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id375">
<h4>Parameters<a class="headerlink" href="#id375" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id376">
<h4>Returns<a class="headerlink" href="#id376" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.hbos">
<span id="pyod-models-hbos-module"></span><h2>pyod.models.hbos module<a class="headerlink" href="#module-pyod.models.hbos" title="Link to this heading">¶</a></h2>
<p>Histogram-based Outlier Detection (HBOS)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.hbos.</span></span><span class="sig-name descname"><span class="pre">HBOS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/hbos.html#HBOS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.hbos.HBOS" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Histogram- based outlier detection (HBOS) is an efficient unsupervised
method. It assumes the feature independence and calculates the degree
of outlyingness by building histograms. See <span id="id377">[<a class="reference internal" href="#id1120" title="Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. KI-2012: Poster and Demo Track, pages 59–63, 2012.">BGD12</a>]</span>
for details.</p>
<p>Two versions of HBOS are supported:        
- Static number of bins: uses a static number of bins for all features.
- Automatic number of bins: every feature uses a number of bins deemed to</p>
<blockquote>
<div><p>be optimal according to the Birge-Rozenblac method
(<span id="id378">[<a class="reference internal" href="#id1152" title="Lucien Birgé and Yves Rozenholc. How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10:24–45, 2006.">BBirgeR06</a>]</span>).</p>
</div></blockquote>
<section id="id379">
<h3>Parameters<a class="headerlink" href="#id379" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>n_bins<span class="classifier">int or string, optional (default=10)</span></dt><dd><p>The number of bins. “auto” uses the birge-rozenblac method for
automatic selection of the optimal number of bins for each feature.</p>
</dd>
<dt>alpha<span class="classifier">float in (0, 1), optional (default=0.1)</span></dt><dd><p>The regularizer for preventing overflow.</p>
</dd>
<dt>tol<span class="classifier">float in (0, 1), optional (default=0.5)</span></dt><dd><p>The parameter to decide the flexibility while dealing
the samples falling outside the bins.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
</dl>
</section>
<section id="id380">
<h3>Attributes<a class="headerlink" href="#id380" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1287"><span class="problematic" id="id1288">bin_edges_</span></a><span class="classifier">numpy array of shape (n_bins + 1, n_features )</span></dt><dd><p>The edges of the bins.</p>
</dd>
<dt><a href="#id1289"><span class="problematic" id="id1290">hist_</span></a><span class="classifier">numpy array of shape (n_bins, n_features)</span></dt><dd><p>The density of each histogram.</p>
</dd>
<dt><a href="#id1291"><span class="problematic" id="id1292">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1293"><span class="problematic" id="id1294">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1295"><span class="problematic" id="id1296">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id381">
<h4>Parameters<a class="headerlink" href="#id381" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id382">
<h4>Returns<a class="headerlink" href="#id382" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/hbos.html#HBOS.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.hbos.HBOS.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id383">
<h4>Parameters<a class="headerlink" href="#id383" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id384">
<h4>Returns<a class="headerlink" href="#id384" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/hbos.html#HBOS.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.hbos.HBOS.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id385">
<h4>Parameters<a class="headerlink" href="#id385" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id386">
<h4>Returns<a class="headerlink" href="#id386" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id387">
<h4>Parameters<a class="headerlink" href="#id387" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id388">
<h4>Returns<a class="headerlink" href="#id388" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id389">
<h4>Parameters<a class="headerlink" href="#id389" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id390">
<h4>Returns<a class="headerlink" href="#id390" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id391">
<h4>Parameters<a class="headerlink" href="#id391" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id392">
<h4>Returns<a class="headerlink" href="#id392" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id393">
<h4>Parameters<a class="headerlink" href="#id393" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id394">
<h4>Returns<a class="headerlink" href="#id394" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id395">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id396">
<h4>Parameters<a class="headerlink" href="#id396" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id397">
<h4>Returns<a class="headerlink" href="#id397" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id398">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id399">
<h4>Parameters<a class="headerlink" href="#id399" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id400">
<h4>Returns<a class="headerlink" href="#id400" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id401">
<h4>Parameters<a class="headerlink" href="#id401" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id402">
<h4>Returns<a class="headerlink" href="#id402" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.hbos.HBOS.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.hbos.HBOS.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id403">
<h4>Returns<a class="headerlink" href="#id403" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.iforest">
<span id="pyod-models-iforest-module"></span><h2>pyod.models.iforest module<a class="headerlink" href="#module-pyod.models.iforest" title="Link to this heading">¶</a></h2>
<p>IsolationForest Outlier Detector. Implemented on scikit-learn library.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.iforest.</span></span><span class="sig-name descname"><span class="pre">IForest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">behaviour</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'old'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/iforest.html#IForest"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.iforest.IForest" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Wrapper of scikit-learn Isolation Forest with more functionalities.</p>
<p>The IsolationForest ‘isolates’ observations by randomly selecting a
feature and then randomly selecting a split value between the maximum and
minimum values of the selected feature.
See <span id="id404">[<a class="reference internal" href="#id1112" title="Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, 413–422. IEEE, 2008.">BLTZ08</a>, <a class="reference internal" href="#id1113" title="Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(1):3, 2012.">BLTZ12</a>]</span> for details.</p>
<p>Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.</p>
<p>This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.</p>
<p>Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.</p>
<section id="id405">
<h3>Parameters<a class="headerlink" href="#id405" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_estimators<span class="classifier">int, optional (default=100)</span></dt><dd><p>The number of base estimators in the ensemble.</p>
</dd>
<dt>max_samples<span class="classifier">int or float, optional (default=”auto”)</span></dt><dd><p>The number of samples to draw from X to train each base estimator.</p>
<blockquote>
<div><ul class="simple">
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
<li><p>If “auto”, then <cite>max_samples=min(256, n_samples)</cite>.</p></li>
</ul>
</div></blockquote>
<p>If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the decision function.</p>
</dd>
<dt>max_features<span class="classifier">int or float, optional (default=1.0)</span></dt><dd><p>The number of features to draw from X to train each base estimator.</p>
<blockquote>
<div><ul class="simple">
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>bootstrap<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.</p>
</dd>
<dt>n_jobs<span class="classifier">integer, optional (default=1)</span></dt><dd><p>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</p>
</dd>
<dt>behaviour<span class="classifier">str, default=’old’</span></dt><dd><p>Behaviour of the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> which can be either ‘old’ or
‘new’. Passing <code class="docutils literal notranslate"><span class="pre">behaviour='new'</span></code> makes the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code>
change to match other anomaly detection algorithm API which will be
the default behaviour in the future. As explained in details in the
<code class="docutils literal notranslate"><span class="pre">offset_</span></code> attribute documentation, the <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> becomes
dependent on the contamination parameter, in such a way that 0 becomes
its natural threshold to detect outliers.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.7.0: </span><code class="docutils literal notranslate"><span class="pre">behaviour</span></code> is added in 0.7.0 for back-compatibility purpose.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.20: </span><code class="docutils literal notranslate"><span class="pre">behaviour='old'</span></code> is deprecated in sklearn 0.20 and will not be
possible in 0.22.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.22: </span><code class="docutils literal notranslate"><span class="pre">behaviour</span></code> parameter will be deprecated in sklearn 0.22 and
removed in 0.24.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Only applicable for sklearn 0.20 above.</p>
</div>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=0)</span></dt><dd><p>Controls the verbosity of the tree building process.</p>
</dd>
</dl>
</section>
<section id="id406">
<h3>Attributes<a class="headerlink" href="#id406" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1297"><span class="problematic" id="id1298">estimators_</span></a><span class="classifier">list of DecisionTreeClassifier</span></dt><dd><p>The collection of fitted sub-estimators.</p>
</dd>
<dt><a href="#id1299"><span class="problematic" id="id1300">estimators_samples_</span></a><span class="classifier">list of arrays</span></dt><dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</p>
</dd>
<dt><a href="#id1301"><span class="problematic" id="id1302">max_samples_</span></a><span class="classifier">integer</span></dt><dd><p>The actual number of samples</p>
</dd>
<dt><a href="#id1303"><span class="problematic" id="id1304">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1305"><span class="problematic" id="id1306">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1307"><span class="problematic" id="id1308">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id407">
<h4>Parameters<a class="headerlink" href="#id407" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id408">
<h4>Returns<a class="headerlink" href="#id408" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/iforest.html#IForest.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.iforest.IForest.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id409">
<h4>Parameters<a class="headerlink" href="#id409" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id410">
<h4>Returns<a class="headerlink" href="#id410" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.feature_importances_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#pyod.models.iforest.IForest.feature_importances_" title="Link to this definition">¶</a></dt>
<dd><p>The impurity-based feature importance. The higher, the more
important the feature. The importance of a feature is computed as the
(normalized) total reduction of the criterion brought by that feature.
It is also known as the Gini importance.</p>
<p>impurity-based feature importance can be misleading for
high cardinality features (many unique values). See
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html">https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html</a>
as an alternative.</p>
<section id="id411">
<h4>Returns<a class="headerlink" href="#id411" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt><a href="#id1309"><span class="problematic" id="id1310">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/iforest.html#IForest.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.iforest.IForest.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id412">
<h4>Parameters<a class="headerlink" href="#id412" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id413">
<h4>Returns<a class="headerlink" href="#id413" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id414">
<h4>Parameters<a class="headerlink" href="#id414" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id415">
<h4>Returns<a class="headerlink" href="#id415" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id416">
<h4>Parameters<a class="headerlink" href="#id416" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id417">
<h4>Returns<a class="headerlink" href="#id417" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.max_samples_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_samples_</span></span><a class="headerlink" href="#pyod.models.iforest.IForest.max_samples_" title="Link to this definition">¶</a></dt>
<dd><p>The actual number of samples.
Decorator for scikit-learn Isolation Forest attributes.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.n_features_in_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_features_in_</span></span><a class="headerlink" href="#pyod.models.iforest.IForest.n_features_in_" title="Link to this definition">¶</a></dt>
<dd><p>The number of features seen during the fit.
Decorator for scikit-learn Isolation Forest attributes.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.offset_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#pyod.models.iforest.IForest.offset_" title="Link to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
Decorator for scikit-learn Isolation Forest attributes.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id418">
<h4>Parameters<a class="headerlink" href="#id418" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id419">
<h4>Returns<a class="headerlink" href="#id419" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id420">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id421">
<h4>Parameters<a class="headerlink" href="#id421" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id422">
<h4>Returns<a class="headerlink" href="#id422" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id423">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id424">
<h4>Parameters<a class="headerlink" href="#id424" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id425">
<h4>Returns<a class="headerlink" href="#id425" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.iforest.IForest.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.iforest.IForest.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id426">
<h4>Parameters<a class="headerlink" href="#id426" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id427">
<h4>Returns<a class="headerlink" href="#id427" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.inne">
<span id="pyod-models-inne-module"></span><h2>pyod.models.inne module<a class="headerlink" href="#module-pyod.models.inne" title="Link to this heading">¶</a></h2>
<p>Isolation-based anomaly detection using nearest-neighbor ensembles.
Part of the codes are adapted from <a class="reference external" href="https://github.com/xhan97/inne">https://github.com/xhan97/inne</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.inne.INNE">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.inne.</span></span><span class="sig-name descname"><span class="pre">INNE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/inne.html#INNE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.inne.INNE" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Isolation-based anomaly detection using nearest-neighbor ensembles.</p>
<p>The INNE algorithm uses the nearest neighbour ensemble to isolate
anomalies. It partitions the data space into regions using a subsample and
determines an isolation score for each region. As each region adapts to
local distribution, the calculated isolation score is a local measure that
is relative to the local neighbourhood, enabling it to detect both global
and local anomalies. INNE has linear time complexity to efficiently handle
large and high-dimensional datasets with complex distributions.</p>
<p>See <span id="id428">[<a class="reference internal" href="#id1158" title="Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. Computational Intelligence, 34(4):968–998, 2018.">BBTA+18</a>]</span> for details.</p>
<section id="id429">
<h3>Parameters<a class="headerlink" href="#id429" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_estimators<span class="classifier">int, default=200</span></dt><dd><p>The number of base estimators in the ensemble.</p>
</dd>
<dt>max_samples<span class="classifier">int or float, optional (default=”auto”)</span></dt><dd><p>The number of samples to draw from X to train each base estimator.</p>
<blockquote>
<div><ul class="simple">
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples</cite> * X.shape[0]` samples.</p></li>
<li><p>If “auto”, then <cite>max_samples=min(8, n_samples)</cite>.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the decision function.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id430">
<h3>Attributes<a class="headerlink" href="#id430" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1311"><span class="problematic" id="id1312">max_samples_</span></a><span class="classifier">integer</span></dt><dd><p>The actual number of samples</p>
</dd>
<dt><a href="#id1313"><span class="problematic" id="id1314">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1315"><span class="problematic" id="id1316">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1317"><span class="problematic" id="id1318">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id431">
<h4>Parameters<a class="headerlink" href="#id431" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id432">
<h4>Returns<a class="headerlink" href="#id432" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/inne.html#INNE.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.inne.INNE.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id433">
<h4>Parameters<a class="headerlink" href="#id433" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples.</p>
</dd>
</dl>
</section>
<section id="id434">
<h4>Returns<a class="headerlink" href="#id434" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/inne.html#INNE.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.inne.INNE.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id435">
<h4>Parameters<a class="headerlink" href="#id435" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id436">
<h4>Returns<a class="headerlink" href="#id436" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id437">
<h4>Parameters<a class="headerlink" href="#id437" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id438">
<h4>Returns<a class="headerlink" href="#id438" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id439">
<h4>Parameters<a class="headerlink" href="#id439" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id440">
<h4>Returns<a class="headerlink" href="#id440" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id441">
<h4>Parameters<a class="headerlink" href="#id441" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id442">
<h4>Returns<a class="headerlink" href="#id442" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id443">
<h4>Parameters<a class="headerlink" href="#id443" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id444">
<h4>Returns<a class="headerlink" href="#id444" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id445">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id446">
<h4>Parameters<a class="headerlink" href="#id446" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id447">
<h4>Returns<a class="headerlink" href="#id447" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id448">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id449">
<h4>Parameters<a class="headerlink" href="#id449" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id450">
<h4>Returns<a class="headerlink" href="#id450" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id451">
<h4>Parameters<a class="headerlink" href="#id451" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id452">
<h4>Returns<a class="headerlink" href="#id452" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.inne.INNE.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.inne.INNE.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id453">
<h4>Returns<a class="headerlink" href="#id453" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.kde">
<span id="pyod-models-kde-module"></span><h2>pyod.models.kde module<a class="headerlink" href="#module-pyod.models.kde" title="Link to this heading">¶</a></h2>
<p>Kernel Density Estimation (KDE) for Unsupervised Outlier Detection.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.kde.KDE">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.kde.</span></span><span class="sig-name descname"><span class="pre">KDE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bandwidth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kde.html#KDE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kde.KDE" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>KDE class for outlier detection.</p>
<p>For an observation, its negative log probability density could be viewed
as the outlying score.</p>
<p>See <span id="id454">[<a class="reference internal" href="#id1156" title="Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition, 61–75. Springer, 2007.">BLLP07</a>]</span> for details.</p>
<section id="id455">
<h3>Parameters<a class="headerlink" href="#id455" title="Link to this heading">¶</a></h3>
<dl>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>bandwidth<span class="classifier">float, optional (default=1.0)</span></dt><dd><p>The bandwidth of the kernel.</p>
</dd>
<dt>algorithm<span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’}, optional</span></dt><dd><p>Algorithm used to compute the kernel density estimator:</p>
<ul class="simple">
<li><p>‘ball_tree’ will use BallTree</p></li>
<li><p>‘kd_tree’ will use KDTree</p></li>
<li><p>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#pyod.models.kde.KDE.fit" title="pyod.models.kde.KDE.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
</dd>
<dt>leaf_size<span class="classifier">int, optional (default = 30)</span></dt><dd><p>Leaf size passed to BallTree. This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p>
</dd>
<dt>metric<span class="classifier">string or callable, default ‘minkowski’</span></dt><dd><p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’,
‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’,
‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</dd>
<dt>metric_params<span class="classifier">dict, optional (default = None)</span></dt><dd><p>Additional keyword arguments for the metric function.</p>
</dd>
</dl>
</section>
<section id="id456">
<h3>Attributes<a class="headerlink" href="#id456" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1319"><span class="problematic" id="id1320">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1321"><span class="problematic" id="id1322">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1323"><span class="problematic" id="id1324">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id457">
<h4>Parameters<a class="headerlink" href="#id457" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id458">
<h4>Returns<a class="headerlink" href="#id458" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kde.html#KDE.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kde.KDE.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id459">
<h4>Parameters<a class="headerlink" href="#id459" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id460">
<h4>Returns<a class="headerlink" href="#id460" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kde.html#KDE.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kde.KDE.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id461">
<h4>Parameters<a class="headerlink" href="#id461" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id462">
<h4>Returns<a class="headerlink" href="#id462" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id463">
<h4>Parameters<a class="headerlink" href="#id463" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id464">
<h4>Returns<a class="headerlink" href="#id464" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id465">
<h4>Parameters<a class="headerlink" href="#id465" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id466">
<h4>Returns<a class="headerlink" href="#id466" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id467">
<h4>Parameters<a class="headerlink" href="#id467" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id468">
<h4>Returns<a class="headerlink" href="#id468" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id469">
<h4>Parameters<a class="headerlink" href="#id469" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id470">
<h4>Returns<a class="headerlink" href="#id470" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id471">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id472">
<h4>Parameters<a class="headerlink" href="#id472" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id473">
<h4>Returns<a class="headerlink" href="#id473" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id474">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id475">
<h4>Parameters<a class="headerlink" href="#id475" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id476">
<h4>Returns<a class="headerlink" href="#id476" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id477">
<h4>Parameters<a class="headerlink" href="#id477" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id478">
<h4>Returns<a class="headerlink" href="#id478" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kde.KDE.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kde.KDE.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id479">
<h4>Returns<a class="headerlink" href="#id479" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.knn">
<span id="pyod-models-knn-module"></span><h2>pyod.models.knn module<a class="headerlink" href="#module-pyod.models.knn" title="Link to this heading">¶</a></h2>
<p>k-Nearest Neighbors Detector (kNN)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.knn.KNN">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.knn.</span></span><span class="sig-name descname"><span class="pre">KNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'largest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/knn.html#KNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.knn.KNN" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>kNN class for outlier detection.
For an observation, its distance to its kth nearest neighbor could be
viewed as the outlying score. It could be viewed as a way to measure
the density. See <span id="id480">[<a class="reference internal" href="#id1117" title="Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15–27. Springer, 2002.">BAP02</a>, <a class="reference internal" href="#id1116" title="Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427–438. ACM, 2000.">BRRS00</a>]</span> for
details.</p>
<p>Three kNN detectors are supported:
largest: use the distance to the kth neighbor as the outlier score
mean: use the average of all k neighbors as the outlier score
median: use the median of the distance to k neighbors as the outlier score</p>
<section id="id481">
<h3>Parameters<a class="headerlink" href="#id481" title="Link to this heading">¶</a></h3>
<dl>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_neighbors<span class="classifier">int, optional (default = 5)</span></dt><dd><p>Number of neighbors to use by default for k neighbors queries.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’largest’)</span></dt><dd><p>{‘largest’, ‘mean’, ‘median’}</p>
<ul class="simple">
<li><p>‘largest’: use the distance to the kth neighbor as the outlier score</p></li>
<li><p>‘mean’: use the average of all k neighbors as the outlier score</p></li>
<li><p>‘median’: use the median of the distance to k neighbors as the
outlier score</p></li>
</ul>
</dd>
<dt>radius<span class="classifier">float, optional (default = 1.0)</span></dt><dd><p>Range of parameter space to use by default for <cite>radius_neighbors</cite>
queries.</p>
</dd>
<dt>algorithm<span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt><dd><p>Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li><p>‘ball_tree’ will use BallTree</p></li>
<li><p>‘kd_tree’ will use KDTree</p></li>
<li><p>‘brute’ will use a brute-force search.</p></li>
<li><p>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#pyod.models.knn.KNN.fit" title="pyod.models.knn.KNN.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.74: </span><code class="docutils literal notranslate"><span class="pre">algorithm</span></code> is deprecated in PyOD 0.7.4 and will not be
possible in 0.7.6. It has to use BallTree for consistency.</p>
</div>
</dd>
<dt>leaf_size<span class="classifier">int, optional (default = 30)</span></dt><dd><p>Leaf size passed to BallTree. This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p>
</dd>
<dt>metric<span class="classifier">string or callable, default ‘minkowski’</span></dt><dd><p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>from scikit-learn: [‘cityblock’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’,
‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’,
‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</dd>
<dt>p<span class="classifier">integer, optional (default = 2)</span></dt><dd><p>Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances</a></p>
</dd>
<dt>metric_params<span class="classifier">dict, optional (default = None)</span></dt><dd><p>Additional keyword arguments for the metric function.</p>
</dd>
<dt>n_jobs<span class="classifier">int, optional (default = 1)</span></dt><dd><p>The number of parallel jobs to run for neighbors search.
If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.
Affects only kneighbors and kneighbors_graph methods.</p>
</dd>
</dl>
</section>
<section id="id482">
<h3>Attributes<a class="headerlink" href="#id482" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1325"><span class="problematic" id="id1326">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1327"><span class="problematic" id="id1328">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1329"><span class="problematic" id="id1330">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id483">
<h4>Parameters<a class="headerlink" href="#id483" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id484">
<h4>Returns<a class="headerlink" href="#id484" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/knn.html#KNN.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.knn.KNN.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id485">
<h4>Parameters<a class="headerlink" href="#id485" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id486">
<h4>Returns<a class="headerlink" href="#id486" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/knn.html#KNN.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.knn.KNN.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id487">
<h4>Parameters<a class="headerlink" href="#id487" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id488">
<h4>Returns<a class="headerlink" href="#id488" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id489">
<h4>Parameters<a class="headerlink" href="#id489" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id490">
<h4>Returns<a class="headerlink" href="#id490" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id491">
<h4>Parameters<a class="headerlink" href="#id491" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id492">
<h4>Returns<a class="headerlink" href="#id492" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id493">
<h4>Parameters<a class="headerlink" href="#id493" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id494">
<h4>Returns<a class="headerlink" href="#id494" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id495">
<h4>Parameters<a class="headerlink" href="#id495" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id496">
<h4>Returns<a class="headerlink" href="#id496" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id497">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id498">
<h4>Parameters<a class="headerlink" href="#id498" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id499">
<h4>Returns<a class="headerlink" href="#id499" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id500">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id501">
<h4>Parameters<a class="headerlink" href="#id501" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id502">
<h4>Returns<a class="headerlink" href="#id502" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id503">
<h4>Parameters<a class="headerlink" href="#id503" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id504">
<h4>Returns<a class="headerlink" href="#id504" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.knn.KNN.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.knn.KNN.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id505">
<h4>Returns<a class="headerlink" href="#id505" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.kpca">
<span id="pyod-models-kpca-module"></span><h2>pyod.models.kpca module<a class="headerlink" href="#module-pyod.models.kpca" title="Link to this heading">¶</a></h2>
<p>Kernel Principal Component Analysis (KPCA) Outlier Detector</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.kpca.</span></span><span class="sig-name descname"><span class="pre">KPCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_selected_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eigen_solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_zero_eig</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subset_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kpca.html#KPCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kpca.KPCA" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>KPCA class for outlier detection.</p>
<p>PCA is performed on the feature space uniquely determined by the kernel,
and the reconstruction error on the feature space is used as the anomaly score.</p>
<p>See <span id="id506">[<a class="reference internal" href="#id1164" title="Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40(3):863–874, 2007.">BHof07</a>]</span>
Heiko Hoffmann, “Kernel PCA for novelty detection,”
Pattern Recognition, vol.40, no.3, pp. 863-874, 2007.
<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0031320306003414">https://www.sciencedirect.com/science/article/pii/S0031320306003414</a>
for details.</p>
<section id="id507">
<h3>Parameters<a class="headerlink" href="#id507" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_components<span class="classifier">int, optional (default=None)</span></dt><dd><p>Number of components. If None, all non-zero components are kept.</p>
</dd>
<dt>n_selected_components<span class="classifier">int, optional (default=None)</span></dt><dd><p>Number of selected principal components
for calculating the outlier scores. It is not necessarily equal to
the total number of the principal components. If not set, use
all principal components.</p>
</dd>
<dt>kernel<span class="classifier">string {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’,</span></dt><dd><blockquote>
<div><p>‘cosine’, ‘precomputed’}, optional (default=’rbf’)</p>
</div></blockquote>
<p>Kernel used for PCA.</p>
</dd>
<dt>gamma<span class="classifier">float, optional (default=None)</span></dt><dd><p>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
kernels. If <code class="docutils literal notranslate"><span class="pre">gamma</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, then it is set to <code class="docutils literal notranslate"><span class="pre">1/n_features</span></code>.</p>
</dd>
<dt>degree<span class="classifier">int, optional (default=3)</span></dt><dd><p>Degree for poly kernels. Ignored by other kernels.</p>
</dd>
<dt>coef0<span class="classifier">float, optional (default=1)</span></dt><dd><p>Independent term in poly and sigmoid kernels.
Ignored by other kernels.</p>
</dd>
<dt>kernel_params<span class="classifier">dict, optional (default=None)</span></dt><dd><p>Parameters (keyword arguments) and
values for kernel passed as callable object.
Ignored by other kernels.</p>
</dd>
<dt>alpha<span class="classifier">float, optional (default=1.0)</span></dt><dd><p>Hyperparameter of the ridge regression that learns the
inverse transform (when inverse_transform=True).</p>
</dd>
<dt>eigen_solver<span class="classifier">string, {‘auto’, ‘dense’, ‘arpack’, ‘randomized’},             default=’auto’</span></dt><dd><p>Select eigensolver to use. If <cite>n_components</cite> is much
less than the number of training samples, randomized (or arpack to a
smaller extend) may be more efficient than the dense eigensolver.
Randomized SVD is performed according to the method of Halko et al.</p>
<dl class="simple">
<dt>auto :</dt><dd><p>the solver is selected by a default policy based on n_samples
(the number of training samples) and <cite>n_components</cite>:
if the number of components to extract is less than 10 (strict) and
the number of samples is more than 200 (strict), the ‘arpack’
method is enabled. Otherwise the exact full eigenvalue
decomposition is computed and optionally truncated afterwards
(‘dense’ method).</p>
</dd>
<dt>dense :</dt><dd><p>run exact full eigenvalue decomposition calling the standard
LAPACK solver via <cite>scipy.linalg.eigh</cite>, and select the components
by postprocessing.</p>
</dd>
<dt>arpack :</dt><dd><p>run SVD truncated to n_components calling ARPACK solver using
<cite>scipy.sparse.linalg.eigsh</cite>. It requires strictly
0 &lt; n_components &lt; n_samples</p>
</dd>
<dt>randomized :</dt><dd><p>run randomized SVD.
implementation selects eigenvalues based on their module; therefore
using this method can lead to unexpected results if the kernel is
not positive semi-definite.</p>
</dd>
</dl>
</dd>
<dt>tol<span class="classifier">float, optional (default=0)</span></dt><dd><p>Convergence tolerance for arpack.
If 0, optimal value will be chosen by arpack.</p>
</dd>
<dt>max_iter<span class="classifier">int, optional (default=None)</span></dt><dd><p>Maximum number of iterations for arpack.
If None, optimal value will be chosen by arpack.</p>
</dd>
<dt>remove_zero_eig<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If True, then all components with zero eigenvalues are removed, so
that the number of components in the output may be &lt; n_components
(and sometimes even zero due to numerical instability).
When n_components is None, this parameter is ignored and components
with zero eigenvalues are removed regardless.</p>
</dd>
<dt>copy_X<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, input X is copied and stored by the model in the <cite>X_fit_</cite>
attribute. If no further changes will be done to X, setting
<cite>copy_X=False</cite> saves memory by storing a reference.</p>
</dd>
<dt>n_jobs<span class="classifier">int, optional (default=None)</span></dt><dd><p>The number of parallel jobs to run.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors.</p>
</dd>
<dt>sampling<span class="classifier">bool, optional (default=False)</span></dt><dd><p>If True, sampling subset from the dataset is performed only once,
in order to reduce time complexity while keeping detection performance.</p>
</dd>
<dt>subset_size<span class="classifier">float in (0., 1.0) or int (0, n_samples), optional (default=20)</span></dt><dd><p>If sampling is True, the size of subset is specified.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance
used by np.random.</p>
</dd>
</dl>
</section>
<section id="id508">
<h3>Attributes<a class="headerlink" href="#id508" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1331"><span class="problematic" id="id1332">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1333"><span class="problematic" id="id1334">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1335"><span class="problematic" id="id1336">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id509">
<h4>Parameters<a class="headerlink" href="#id509" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id510">
<h4>Returns<a class="headerlink" href="#id510" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kpca.html#KPCA.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kpca.KPCA.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id511">
<h4>Parameters<a class="headerlink" href="#id511" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id512">
<h4>Returns<a class="headerlink" href="#id512" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/kpca.html#KPCA.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.kpca.KPCA.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id513">
<h4>Parameters<a class="headerlink" href="#id513" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id514">
<h4>Returns<a class="headerlink" href="#id514" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id515">
<h4>Parameters<a class="headerlink" href="#id515" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id516">
<h4>Returns<a class="headerlink" href="#id516" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id517">
<h4>Parameters<a class="headerlink" href="#id517" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id518">
<h4>Returns<a class="headerlink" href="#id518" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id519">
<h4>Parameters<a class="headerlink" href="#id519" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id520">
<h4>Returns<a class="headerlink" href="#id520" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id521">
<h4>Parameters<a class="headerlink" href="#id521" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id522">
<h4>Returns<a class="headerlink" href="#id522" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id523">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id524">
<h4>Parameters<a class="headerlink" href="#id524" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id525">
<h4>Returns<a class="headerlink" href="#id525" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id526">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id527">
<h4>Parameters<a class="headerlink" href="#id527" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id528">
<h4>Returns<a class="headerlink" href="#id528" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id529">
<h4>Parameters<a class="headerlink" href="#id529" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id530">
<h4>Returns<a class="headerlink" href="#id530" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.kpca.KPCA.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.kpca.KPCA.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id531">
<h4>Returns<a class="headerlink" href="#id531" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.lmdd">
<span id="pyod-models-lmdd-module"></span><h2>pyod.models.lmdd module<a class="headerlink" href="#module-pyod.models.lmdd" title="Link to this heading">¶</a></h2>
<p>Linear Model Deviation-base outlier detection (LMDD).</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.lmdd.</span></span><span class="sig-name descname"><span class="pre">LMDD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dis_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'aad'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lmdd.html#LMDD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lmdd.LMDD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Linear Method for Deviation-based Outlier Detection.</p>
<p>LMDD employs the concept of the smoothing factor which
indicates how much the dissimilarity can be reduced by
removing a subset of elements from the data-set.
Read more in the <span id="id532">[<a class="reference internal" href="#id1143" title="Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In KDD, volume 1141, 972–981. 1996.">BAAR96</a>]</span>.</p>
<p>Note: this implementation has minor modification to make it output scores
instead of labels.</p>
<section id="id533">
<h3>Parameters<a class="headerlink" href="#id533" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_iter<span class="classifier">int, optional (default=50)</span></dt><dd><p>Number of iterations where in each iteration,
the process is repeated after randomizing the order of the input.
Note that n_iter is a very important factor that affects the accuracy.
The higher the better the accuracy and the longer the execution.</p>
</dd>
<dt>dis_measure: str, optional (default=’aad’)</dt><dd><p>Dissimilarity measure to be used in calculating the smoothing factor
for points, options available:</p>
<ul class="simple">
<li><p>‘aad’: Average Absolute Deviation</p></li>
<li><p>‘var’: Variance</p></li>
<li><p>‘iqr’: Interquartile Range</p></li>
</ul>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id534">
<h3>Attributes<a class="headerlink" href="#id534" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1337"><span class="problematic" id="id1338">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1339"><span class="problematic" id="id1340">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1341"><span class="problematic" id="id1342">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id535">
<h4>Parameters<a class="headerlink" href="#id535" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id536">
<h4>Returns<a class="headerlink" href="#id536" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lmdd.html#LMDD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lmdd.LMDD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id537">
<h4>Parameters<a class="headerlink" href="#id537" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id538">
<h4>Returns<a class="headerlink" href="#id538" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lmdd.html#LMDD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lmdd.LMDD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id539">
<h4>Parameters<a class="headerlink" href="#id539" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id540">
<h4>Returns<a class="headerlink" href="#id540" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id541">
<h4>Parameters<a class="headerlink" href="#id541" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id542">
<h4>Returns<a class="headerlink" href="#id542" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id543">
<h4>Parameters<a class="headerlink" href="#id543" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id544">
<h4>Returns<a class="headerlink" href="#id544" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id545">
<h4>Parameters<a class="headerlink" href="#id545" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id546">
<h4>Returns<a class="headerlink" href="#id546" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id547">
<h4>Parameters<a class="headerlink" href="#id547" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id548">
<h4>Returns<a class="headerlink" href="#id548" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id549">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id550">
<h4>Parameters<a class="headerlink" href="#id550" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id551">
<h4>Returns<a class="headerlink" href="#id551" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id552">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id553">
<h4>Parameters<a class="headerlink" href="#id553" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id554">
<h4>Returns<a class="headerlink" href="#id554" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id555">
<h4>Parameters<a class="headerlink" href="#id555" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id556">
<h4>Returns<a class="headerlink" href="#id556" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lmdd.LMDD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lmdd.LMDD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id557">
<h4>Returns<a class="headerlink" href="#id557" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.loda">
<span id="pyod-models-loda-module"></span><h2>pyod.models.loda module<a class="headerlink" href="#module-pyod.models.loda" title="Link to this heading">¶</a></h2>
<p>Loda: Lightweight on-line detector of anomalies
Adapted from tilitools (<a class="reference external" href="https://github.com/nicococo/tilitools">https://github.com/nicococo/tilitools</a>) by</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.loda.LODA">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.loda.</span></span><span class="sig-name descname"><span class="pre">LODA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_random_cuts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loda.html#LODA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loda.LODA" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Loda: Lightweight on-line detector of anomalies. See
<span id="id558">[<a class="reference internal" href="#id1145" title="Tomáš Pevn\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102(2):275–304, 2016.">BPevny16</a>]</span> for more information.</p>
<p>Two versions of LODA are supported:        
- Static number of bins: uses a static number of bins for all random cuts.
- Automatic number of bins: every random cut uses a number of bins deemed</p>
<blockquote>
<div><p>to be optimal according to the Birge-Rozenblac method
(<span id="id559">[<a class="reference internal" href="#id1152" title="Lucien Birgé and Yves Rozenholc. How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10:24–45, 2006.">BBirgeR06</a>]</span>).</p>
</div></blockquote>
<section id="id560">
<h3>Parameters<a class="headerlink" href="#id560" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_bins<span class="classifier">int or string, optional (default = 10)</span></dt><dd><p>The number of bins for the histogram. If set to “auto”, the 
Birge-Rozenblac method will be used to automatically determine the 
optimal number of bins.</p>
</dd>
<dt>n_random_cuts<span class="classifier">int, optional (default = 100)</span></dt><dd><p>The number of random cuts.</p>
</dd>
</dl>
</section>
<section id="id561">
<h3>Attributes<a class="headerlink" href="#id561" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1343"><span class="problematic" id="id1344">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1345"><span class="problematic" id="id1346">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1347"><span class="problematic" id="id1348">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id562">
<h4>Parameters<a class="headerlink" href="#id562" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id563">
<h4>Returns<a class="headerlink" href="#id563" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loda.html#LODA.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loda.LODA.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id564">
<h4>Parameters<a class="headerlink" href="#id564" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id565">
<h4>Returns<a class="headerlink" href="#id565" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loda.html#LODA.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loda.LODA.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id566">
<h4>Parameters<a class="headerlink" href="#id566" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id567">
<h4>Returns<a class="headerlink" href="#id567" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id568">
<h4>Parameters<a class="headerlink" href="#id568" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id569">
<h4>Returns<a class="headerlink" href="#id569" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id570">
<h4>Parameters<a class="headerlink" href="#id570" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id571">
<h4>Returns<a class="headerlink" href="#id571" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id572">
<h4>Parameters<a class="headerlink" href="#id572" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id573">
<h4>Returns<a class="headerlink" href="#id573" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id574">
<h4>Parameters<a class="headerlink" href="#id574" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id575">
<h4>Returns<a class="headerlink" href="#id575" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id576">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id577">
<h4>Parameters<a class="headerlink" href="#id577" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id578">
<h4>Returns<a class="headerlink" href="#id578" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id579">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id580">
<h4>Parameters<a class="headerlink" href="#id580" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id581">
<h4>Returns<a class="headerlink" href="#id581" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id582">
<h4>Parameters<a class="headerlink" href="#id582" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id583">
<h4>Returns<a class="headerlink" href="#id583" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loda.LODA.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loda.LODA.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id584">
<h4>Returns<a class="headerlink" href="#id584" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.lof">
<span id="pyod-models-lof-module"></span><h2>pyod.models.lof module<a class="headerlink" href="#module-pyod.models.lof" title="Link to this heading">¶</a></h2>
<p>Local Outlier Factor (LOF). Implemented on scikit-learn library.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.lof.LOF">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.lof.</span></span><span class="sig-name descname"><span class="pre">LOF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">novelty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lof.html#LOF"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lof.LOF" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Wrapper of scikit-learn LOF Class with more functionalities.
Unsupervised Outlier Detection using Local Outlier Factor (LOF).</p>
<p>The anomaly score of each sample is called Local Outlier Factor.
It measures the local deviation of density of a given sample with
respect to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of
its neighbors, one can identify samples that have a substantially lower
density than their neighbors. These are considered outliers.
See <span id="id585">[<a class="reference internal" href="#id1123" title="Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based local outliers. In ACM sigmod record, volume 29, 93–104. ACM, 2000.">BBKNS00</a>]</span> for details.</p>
<section id="id586">
<h3>Parameters<a class="headerlink" href="#id586" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_neighbors<span class="classifier">int, optional (default=20)</span></dt><dd><p>Number of neighbors to use by default for <cite>kneighbors</cite> queries.
If n_neighbors is larger than the number of samples provided,
all samples will be used.</p>
</dd>
<dt>algorithm<span class="classifier">{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional</span></dt><dd><p>Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li><p>‘ball_tree’ will use BallTree</p></li>
<li><p>‘kd_tree’ will use KDTree</p></li>
<li><p>‘brute’ will use a brute-force search.</p></li>
<li><p>‘auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#pyod.models.lof.LOF.fit" title="pyod.models.lof.LOF.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size<span class="classifier">int, optional (default=30)</span></dt><dd><p>Leaf size passed to <cite>BallTree</cite> or <cite>KDTree</cite>. This can
affect the speed of the construction and query, as well as the memory
required to store the tree. The optimal value depends on the
nature of the problem.</p>
</dd>
<dt>metric<span class="classifier">string or callable, default ‘minkowski’</span></dt><dd><p>metric used for the distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If ‘precomputed’, the training input X is expected to be a distance
matrix.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’,
‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’,
‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics:
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/spatial.distance.html">http://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a></p>
</dd>
<dt>p<span class="classifier">integer, optional (default = 2)</span></dt><dd><p>Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances</a></p>
</dd>
<dt>metric_params<span class="classifier">dict, optional (default = None)</span></dt><dd><p>Additional keyword arguments for the metric function.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. When fitting this is used to define the
threshold on the decision function.</p>
</dd>
<dt>n_jobs<span class="classifier">int, optional (default = 1)</span></dt><dd><p>The number of parallel jobs to run for neighbors search.
If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.
Affects only kneighbors and kneighbors_graph methods.</p>
</dd>
<dt>novelty<span class="classifier">bool (default=False)</span></dt><dd><p>By default, LocalOutlierFactor is only meant to be used for outlier
detection (novelty=False). Set novelty to True if you want to use
LocalOutlierFactor for novelty detection. In this case be aware that
that you should only use predict, decision_function and score_samples
on new unseen data and not on the training set.</p>
</dd>
</dl>
</section>
<section id="id587">
<h3>Attributes<a class="headerlink" href="#id587" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1349"><span class="problematic" id="id1350">n_neighbors_</span></a><span class="classifier">int</span></dt><dd><p>The actual number of neighbors used for <cite>kneighbors</cite> queries.</p>
</dd>
<dt><a href="#id1351"><span class="problematic" id="id1352">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1353"><span class="problematic" id="id1354">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1355"><span class="problematic" id="id1356">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id588">
<h4>Parameters<a class="headerlink" href="#id588" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id589">
<h4>Returns<a class="headerlink" href="#id589" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lof.html#LOF.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lof.LOF.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id590">
<h4>Parameters<a class="headerlink" href="#id590" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id591">
<h4>Returns<a class="headerlink" href="#id591" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lof.html#LOF.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lof.LOF.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id592">
<h4>Parameters<a class="headerlink" href="#id592" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id593">
<h4>Returns<a class="headerlink" href="#id593" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id594">
<h4>Parameters<a class="headerlink" href="#id594" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id595">
<h4>Returns<a class="headerlink" href="#id595" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id596">
<h4>Parameters<a class="headerlink" href="#id596" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id597">
<h4>Returns<a class="headerlink" href="#id597" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id598">
<h4>Parameters<a class="headerlink" href="#id598" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id599">
<h4>Returns<a class="headerlink" href="#id599" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id600">
<h4>Parameters<a class="headerlink" href="#id600" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id601">
<h4>Returns<a class="headerlink" href="#id601" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id602">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id603">
<h4>Parameters<a class="headerlink" href="#id603" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id604">
<h4>Returns<a class="headerlink" href="#id604" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id605">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id606">
<h4>Parameters<a class="headerlink" href="#id606" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id607">
<h4>Returns<a class="headerlink" href="#id607" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id608">
<h4>Parameters<a class="headerlink" href="#id608" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id609">
<h4>Returns<a class="headerlink" href="#id609" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lof.LOF.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lof.LOF.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id610">
<h4>Returns<a class="headerlink" href="#id610" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.loci">
<span id="pyod-models-loci-module"></span><h2>pyod.models.loci module<a class="headerlink" href="#module-pyod.models.loci" title="Link to this heading">¶</a></h2>
<p>Local Correlation Integral (LOCI).
Part of the codes are adapted from <a class="reference external" href="https://github.com/Cloudy10/loci">https://github.com/Cloudy10/loci</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.loci.</span></span><span class="sig-name descname"><span class="pre">LOCI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loci.html#LOCI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loci.LOCI" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Local Correlation Integral.</p>
<p>LOCI is highly effective for detecting outliers and groups of 
outliers ( a.k.a.micro-clusters), which offers the following advantages 
and novelties: (a) It provides an automatic, data-dictated cut-off to 
determine whether a point is an outlier—in contrast, previous methods 
force users to pick cut-offs, without any hints as to what cut-off value 
is best for a given dataset. (b) It can provide a LOCI plot for each 
point; this plot summarizes a wealth of information about the data in 
the vicinity of the point, determining clusters, micro-clusters, their 
diameters and their inter-cluster distances. None of the existing 
outlier-detection methods can match this feature, because they output 
only a single number for each point: its outlierness score.(c) It can 
be computed as quickly as the best previous methods
Read more in the <span id="id611">[<a class="reference internal" href="#id1130" title="Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In Data Engineering, 2003. Proceedings. 19th International Conference on, 315–326. IEEE, 2003.">BPKGF03</a>]</span>.</p>
<section id="id612">
<h3>Parameters<a class="headerlink" href="#id612" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1) </span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>alpha<span class="classifier">int, default = 0.5</span></dt><dd><p>The neighbourhood parameter measures how large of a neighbourhood
should be considered “local”.</p>
</dd>
<dt>k: int, default = 3</dt><dd><p>An outlier cutoff threshold for determine whether or not a point 
should be considered an outlier.</p>
</dd>
</dl>
</section>
<section id="id613">
<h3>Attributes<a class="headerlink" href="#id613" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1357"><span class="problematic" id="id1358">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1359"><span class="problematic" id="id1360">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1361"><span class="problematic" id="id1362">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.models.loci</span><span class="w"> </span><span class="kn">import</span> <span class="n">LOCI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">50</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contamination</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_train</span><span class="o">=</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="n">n_test</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">contamination</span><span class="o">=</span><span class="n">contamination</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LOCI</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="go">LOCI(alpha=0.5, contamination=0.1, k=None)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id614">
<h4>Parameters<a class="headerlink" href="#id614" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id615">
<h4>Returns<a class="headerlink" href="#id615" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loci.html#LOCI.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loci.LOCI.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly scores of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on the fitted
detector. For consistency, outliers are assigned with
higher anomaly scores.</p>
<section id="id616">
<h4>Parameters<a class="headerlink" href="#id616" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id617">
<h4>Returns<a class="headerlink" href="#id617" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/loci.html#LOCI.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.loci.LOCI.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit the model using X as training data.</p>
<section id="id618">
<h4>Parameters<a class="headerlink" href="#id618" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">array, shape (n_samples, n_features)</span></dt><dd><p>Training data.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id619">
<h4>Returns<a class="headerlink" href="#id619" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id620">
<h4>Parameters<a class="headerlink" href="#id620" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id621">
<h4>Returns<a class="headerlink" href="#id621" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id622">
<h4>Parameters<a class="headerlink" href="#id622" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id623">
<h4>Returns<a class="headerlink" href="#id623" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id624">
<h4>Parameters<a class="headerlink" href="#id624" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id625">
<h4>Returns<a class="headerlink" href="#id625" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id626">
<h4>Parameters<a class="headerlink" href="#id626" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id627">
<h4>Returns<a class="headerlink" href="#id627" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id628">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id629">
<h4>Parameters<a class="headerlink" href="#id629" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id630">
<h4>Returns<a class="headerlink" href="#id630" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id631">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id632">
<h4>Parameters<a class="headerlink" href="#id632" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id633">
<h4>Returns<a class="headerlink" href="#id633" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id634">
<h4>Parameters<a class="headerlink" href="#id634" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id635">
<h4>Returns<a class="headerlink" href="#id635" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.loci.LOCI.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.loci.LOCI.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id636">
<h4>Returns<a class="headerlink" href="#id636" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.lunar">
<span id="pyod-models-lunar-module"></span><h2>pyod.models.lunar module<a class="headerlink" href="#module-pyod.models.lunar" title="Link to this heading">¶</a></h2>
<p>LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.lunar.</span></span><span class="sig-name descname"><span class="pre">LUNAR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'WEIGHT'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbours</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negative_sampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'MIXED'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">MinMaxScaler()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proportion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lunar.html#LUNAR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lunar.LUNAR" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>LUNAR class for outlier detection. See <a class="reference external" href="https://www.aaai.org/AAAI22Papers/AAAI-51.GoodgeA.pdf">https://www.aaai.org/AAAI22Papers/AAAI-51.GoodgeA.pdf</a> for details.
For an observation, its ordered list of distances to its k nearest neighbours is input to a neural network, 
with one of the following outputs:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>SCORE_MODEL: network directly outputs the anomaly score.</p></li>
<li><dl class="simple">
<dt>WEIGHT_MODEL: network outputs a set of weights for the k distances, the anomaly score is then the</dt><dd><p>sum of weighted distances.</p>
</dd>
</dl>
</li>
</ol>
</div></blockquote>
<p>See <span id="id637">[<a class="reference internal" href="#id1160" title="Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 6737–6745. 2022.">BGHNN22</a>]</span> for details.</p>
<section id="id638">
<h3>Parameters<a class="headerlink" href="#id638" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model_type: str in [‘WEIGHT’, ‘SCORE’], optional (default = ‘WEIGHT’)</dt><dd><p>Whether to use WEIGHT_MODEL or SCORE_MODEL for anomaly scoring.</p>
</dd>
<dt>n_neighbors: int, optional (default = 5)</dt><dd><p>Number of neighbors to use by default for k neighbors queries.</p>
</dd>
<dt>negative_sampling: str in [‘UNIFORM’, ‘SUBSPACE’, MIXED’], optional (default = ‘MIXED)</dt><dd><p>Type of negative samples to use between:</p>
<ul class="simple">
<li><p>‘UNIFORM’: uniformly distributed samples</p></li>
<li><p>‘SUBSPACE’: subspace perturbation (additive random noise in a subset of features)</p></li>
<li><p>‘MIXED’: a combination of both types of samples</p></li>
</ul>
</dd>
<dt>val_size: float in [0,1], optional (default = 0.1)</dt><dd><p>Proportion of samples to be used for model validation</p>
</dd>
<dt>scaler: object in {StandardScaler(), MinMaxScaler(), optional (default = MinMaxScaler())</dt><dd><p>Method of data normalization</p>
</dd>
<dt>epsilon: float, optional (default = 0.1)</dt><dd><p>Hyper-parameter for the generation of negative samples. 
A smaller epsilon results in negative samples more similar to normal samples.</p>
</dd>
<dt>proportion: float, optional (default = 1.0)</dt><dd><p>Hyper-parameter for the proprotion of negative samples to use relative to the 
number of normal training samples.</p>
</dd>
<dt>n_epochs: int, optional (default = 200)</dt><dd><p>Number of epochs to train neural network.</p>
</dd>
<dt>lr: float, optional (default = 0.001)</dt><dd><p>Learning rate.</p>
</dd>
<dt>wd: float, optional (default = 0.1)</dt><dd><p>Weight decay.</p>
</dd>
<dt>verbose: int in {0,1}, optional (default = 0):</dt><dd><p>To view or hide training progress</p>
</dd>
</dl>
</section>
<section id="id639">
<h3>Attributes<a class="headerlink" href="#id639" title="Link to this heading">¶</a></h3>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id640">
<h4>Parameters<a class="headerlink" href="#id640" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id641">
<h4>Returns<a class="headerlink" href="#id641" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lunar.html#LUNAR.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lunar.LUNAR.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
For consistency, outliers are assigned with larger anomaly scores.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The training input samples.</p>
</div></blockquote>
<section id="id642">
<h4>Returns<a class="headerlink" href="#id642" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lunar.html#LUNAR.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lunar.LUNAR.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is assumed to be 0 for all training samples.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The input samples.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Overwritten with 0 for all training samples (assumed to be normal).</p>
</dd>
</dl>
<section id="id643">
<h4>Returns<a class="headerlink" href="#id643" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id644">
<h4>Parameters<a class="headerlink" href="#id644" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id645">
<h4>Returns<a class="headerlink" href="#id645" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id646">
<h4>Parameters<a class="headerlink" href="#id646" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id647">
<h4>Returns<a class="headerlink" href="#id647" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id648">
<h4>Parameters<a class="headerlink" href="#id648" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id649">
<h4>Returns<a class="headerlink" href="#id649" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id650">
<h4>Parameters<a class="headerlink" href="#id650" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id651">
<h4>Returns<a class="headerlink" href="#id651" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id652">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id653">
<h4>Parameters<a class="headerlink" href="#id653" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id654">
<h4>Returns<a class="headerlink" href="#id654" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id655">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id656">
<h4>Parameters<a class="headerlink" href="#id656" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id657">
<h4>Returns<a class="headerlink" href="#id657" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id658">
<h4>Parameters<a class="headerlink" href="#id658" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id659">
<h4>Returns<a class="headerlink" href="#id659" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lunar.LUNAR.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lunar.LUNAR.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id660">
<h4>Returns<a class="headerlink" href="#id660" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.lscp">
<span id="pyod-models-lscp-module"></span><h2>pyod.models.lscp module<a class="headerlink" href="#module-pyod.models.lscp" title="Link to this heading">¶</a></h2>
<p>Locally Selective Combination of Parallel Outlier Ensembles (LSCP).
Adapted from the original implementation.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.lscp.</span></span><span class="sig-name descname"><span class="pre">LSCP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">detector_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_region_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lscp.html#LSCP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lscp.LSCP" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Locally Selection Combination in Parallel Outlier Ensembles</p>
<p>LSCP is an unsupervised parallel outlier detection ensemble which selects
competent detectors in the local region of a test instance. This
implementation uses an Average of Maximum strategy. First, a heterogeneous
list of base detectors is fit to the training data and then generates a
pseudo ground truth for each train instance is generated by
taking the maximum outlier score.</p>
<p>For each test instance:
1) The local region is defined to be the set of nearest training points in
randomly sampled feature subspaces which occur more frequently than
a defined threshold over multiple iterations.</p>
<p>2) Using the local region, a local pseudo ground truth is defined and the
pearson correlation is calculated between each base detector’s training
outlier scores and the pseudo ground truth.</p>
<p>3) A histogram is built out of pearson correlation scores; detectors in
the largest bin are selected as competent base detectors for the given
test instance.</p>
<p>4) The average outlier score of the selected competent detectors is taken
to be the final score.</p>
<p>See <span id="id661">[<a class="reference internal" href="#id1131" title="Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585–593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.">BZNHL19</a>]</span> for details.</p>
<section id="id662">
<h3>Parameters<a class="headerlink" href="#id662" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>detector_list<span class="classifier">List, length must be greater than 1</span></dt><dd><p>Base unsupervised outlier detectors from PyOD. (Note: requires fit and
decision_function methods)</p>
</dd>
<dt>local_region_size<span class="classifier">int, optional (default=30)</span></dt><dd><p>Number of training points to consider in each iteration of the local
region generation process (30 by default).</p>
</dd>
<dt>local_max_features<span class="classifier">float in (0.5, 1.), optional (default=1.0)</span></dt><dd><p>Maximum proportion of number of features to consider when defining the
local region (1.0 by default).</p>
</dd>
<dt>n_bins<span class="classifier">int, optional (default=10)</span></dt><dd><p>Number of bins to use when selecting the local region</p>
</dd>
<dt>random_state<span class="classifier">RandomState, optional (default=None)</span></dt><dd><p>A random number generator instance to define the state of the random
permutations generator.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function (0.1 by default).</p>
</dd>
</dl>
</section>
<section id="id663">
<h3>Attributes<a class="headerlink" href="#id663" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1363"><span class="problematic" id="id1364">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1365"><span class="problematic" id="id1366">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1367"><span class="problematic" id="id1368">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
</section>
<section id="id664">
<h3>Examples<a class="headerlink" href="#id664" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.utils.utility</span><span class="w"> </span><span class="kn">import</span> <span class="n">standardizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.models.lscp</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSCP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.models.lof</span><span class="w"> </span><span class="kn">import</span> <span class="n">LOF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_train</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">contamination</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">standardizer</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">LOF</span><span class="p">(),</span> <span class="n">LOF</span><span class="p">()]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LSCP</span><span class="p">(</span><span class="n">detector_list</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="go">LSCP(...)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id665">
<h4>Parameters<a class="headerlink" href="#id665" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id666">
<h4>Returns<a class="headerlink" href="#id666" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lscp.html#LSCP.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lscp.LSCP.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id667">
<h4>Parameters<a class="headerlink" href="#id667" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id668">
<h4>Returns<a class="headerlink" href="#id668" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/lscp.html#LSCP.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.lscp.LSCP.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id669">
<h4>Parameters<a class="headerlink" href="#id669" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id670">
<h4>Returns<a class="headerlink" href="#id670" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id671">
<h4>Parameters<a class="headerlink" href="#id671" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id672">
<h4>Returns<a class="headerlink" href="#id672" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id673">
<h4>Parameters<a class="headerlink" href="#id673" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id674">
<h4>Returns<a class="headerlink" href="#id674" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id675">
<h4>Parameters<a class="headerlink" href="#id675" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id676">
<h4>Returns<a class="headerlink" href="#id676" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id677">
<h4>Parameters<a class="headerlink" href="#id677" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id678">
<h4>Returns<a class="headerlink" href="#id678" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id679">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id680">
<h4>Parameters<a class="headerlink" href="#id680" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id681">
<h4>Returns<a class="headerlink" href="#id681" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id682">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id683">
<h4>Parameters<a class="headerlink" href="#id683" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id684">
<h4>Returns<a class="headerlink" href="#id684" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id685">
<h4>Parameters<a class="headerlink" href="#id685" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id686">
<h4>Returns<a class="headerlink" href="#id686" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.lscp.LSCP.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.lscp.LSCP.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id687">
<h4>Returns<a class="headerlink" href="#id687" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.mad">
<span id="pyod-models-mad-module"></span><h2>pyod.models.mad module<a class="headerlink" href="#module-pyod.models.mad" title="Link to this heading">¶</a></h2>
<p>Median Absolute deviation (MAD) Algorithm.
Strictly for Univariate Data.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.mad.MAD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.mad.</span></span><span class="sig-name descname"><span class="pre">MAD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mad.html#MAD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mad.MAD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Median Absolute Deviation: for measuring the distances between
data points and the median in terms of median distance.
See <span id="id688">[<a class="reference internal" href="#id1147" title="Boris Iglewicz and David Caster Hoaglin. How to detect and handle outliers. Volume 16. Asq Press, 1993.">BIH93</a>]</span> for details.</p>
<section id="id689">
<h3>Parameters<a class="headerlink" href="#id689" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>threshold<span class="classifier">float, optional (default=3.5)</span></dt><dd><p>The modified z-score to use as a threshold. Observations with
a modified z-score (based on the median absolute deviation) greater
than this value will be classified as outliers.</p>
</dd>
</dl>
</section>
<section id="id690">
<h3>Attributes<a class="headerlink" href="#id690" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1369"><span class="problematic" id="id1370">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1371"><span class="problematic" id="id1372">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The modified z-score to use as a threshold. Observations with
a modified z-score (based on the median absolute deviation) greater
than this value will be classified as outliers.</p>
</dd>
<dt><a href="#id1373"><span class="problematic" id="id1374">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id691">
<h4>Parameters<a class="headerlink" href="#id691" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id692">
<h4>Returns<a class="headerlink" href="#id692" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mad.html#MAD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mad.MAD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id693">
<h4>Parameters<a class="headerlink" href="#id693" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.
Note that <cite>n_features</cite> must equal 1.</p>
</dd>
</dl>
</section>
<section id="id694">
<h4>Returns<a class="headerlink" href="#id694" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mad.html#MAD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mad.MAD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id695">
<h4>Parameters<a class="headerlink" href="#id695" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Note that <cite>n_features</cite> must equal 1.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id696">
<h4>Returns<a class="headerlink" href="#id696" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id697">
<h4>Parameters<a class="headerlink" href="#id697" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id698">
<h4>Returns<a class="headerlink" href="#id698" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id699">
<h4>Parameters<a class="headerlink" href="#id699" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id700">
<h4>Returns<a class="headerlink" href="#id700" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id701">
<h4>Parameters<a class="headerlink" href="#id701" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id702">
<h4>Returns<a class="headerlink" href="#id702" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id703">
<h4>Parameters<a class="headerlink" href="#id703" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id704">
<h4>Returns<a class="headerlink" href="#id704" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id705">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id706">
<h4>Parameters<a class="headerlink" href="#id706" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id707">
<h4>Returns<a class="headerlink" href="#id707" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id708">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id709">
<h4>Parameters<a class="headerlink" href="#id709" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id710">
<h4>Returns<a class="headerlink" href="#id710" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id711">
<h4>Parameters<a class="headerlink" href="#id711" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id712">
<h4>Returns<a class="headerlink" href="#id712" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mad.MAD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mad.MAD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id713">
<h4>Returns<a class="headerlink" href="#id713" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.mcd">
<span id="pyod-models-mcd-module"></span><h2>pyod.models.mcd module<a class="headerlink" href="#module-pyod.models.mcd" title="Link to this heading">¶</a></h2>
<p>Outlier Detection with Minimum Covariance Determinant (MCD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.mcd.</span></span><span class="sig-name descname"><span class="pre">MCD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assume_centered</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">support_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mcd.html#MCD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mcd.MCD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Detecting outliers in a Gaussian distributed dataset using
Minimum Covariance Determinant (MCD): robust estimator of covariance.</p>
<p>The Minimum Covariance Determinant covariance estimator is to be applied
on Gaussian-distributed data, but could still be relevant on data
drawn from a unimodal, symmetric distribution. It is not meant to be used
with multi-modal data (the algorithm used to fit a MinCovDet object is
likely to fail in such a case).
One should consider projection pursuit methods to deal with multi-modal
datasets.</p>
<p>First fit a minimum covariance determinant model and then compute the
Mahalanobis distance as the outlier degree of the data</p>
<p>See <span id="id714">[<a class="reference internal" href="#id1126" title="Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. Computational Statistics &amp; Data Analysis, 44(4):625–638, 2004.">BHR04</a>, <a class="reference internal" href="#id1125" title="Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. Technometrics, 41(3):212–223, 1999.">BRD99</a>]</span> for details.</p>
<section id="id715">
<h3>Parameters<a class="headerlink" href="#id715" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>store_precision<span class="classifier">bool</span></dt><dd><p>Specify if the estimated precision is stored.</p>
</dd>
<dt>assume_centered<span class="classifier">bool</span></dt><dd><p>If True, the support of the robust location and the covariance
estimates is computed, and a covariance estimate is recomputed from
it, without centering the data.
Useful to work with data whose mean is significantly equal to
zero but is not exactly zero.
If False, the robust location and covariance are directly computed
with the FastMCD algorithm without additional treatment.</p>
</dd>
<dt>support_fraction<span class="classifier">float, 0 &lt; support_fraction &lt; 1</span></dt><dd><p>The proportion of points to be included in the support of the raw
MCD estimate. Default is None, which implies that the minimum
value of support_fraction will be used within the algorithm:
[n_sample + n_features + 1] / 2</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id716">
<h3>Attributes<a class="headerlink" href="#id716" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1375"><span class="problematic" id="id1376">raw_location_</span></a><span class="classifier">array-like, shape (n_features,)</span></dt><dd><p>The raw robust estimated location before correction and re-weighting.</p>
</dd>
<dt><a href="#id1377"><span class="problematic" id="id1378">raw_covariance_</span></a><span class="classifier">array-like, shape (n_features, n_features)</span></dt><dd><p>The raw robust estimated covariance before correction and re-weighting.</p>
</dd>
<dt><a href="#id1379"><span class="problematic" id="id1380">raw_support_</span></a><span class="classifier">array-like, shape (n_samples,)</span></dt><dd><p>A mask of the observations that have been used to compute
the raw robust estimates of location and shape, before correction
and re-weighting.</p>
</dd>
<dt><a href="#id1381"><span class="problematic" id="id1382">location_</span></a><span class="classifier">array-like, shape (n_features,)</span></dt><dd><p>Estimated robust location</p>
</dd>
<dt><a href="#id1383"><span class="problematic" id="id1384">covariance_</span></a><span class="classifier">array-like, shape (n_features, n_features)</span></dt><dd><p>Estimated robust covariance matrix</p>
</dd>
<dt><a href="#id1385"><span class="problematic" id="id1386">precision_</span></a><span class="classifier">array-like, shape (n_features, n_features)</span></dt><dd><p>Estimated pseudo inverse matrix.
(stored only if store_precision is True)</p>
</dd>
<dt><a href="#id1387"><span class="problematic" id="id1388">support_</span></a><span class="classifier">array-like, shape (n_samples,)</span></dt><dd><p>A mask of the observations that have been used to compute
the robust estimates of location and shape.</p>
</dd>
<dt><a href="#id1389"><span class="problematic" id="id1390">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted. Mahalanobis distances of the training set (on which
<cite>:meth:`fit</cite> is called) observations.</p>
</dd>
<dt><a href="#id1391"><span class="problematic" id="id1392">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1393"><span class="problematic" id="id1394">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id717">
<h4>Parameters<a class="headerlink" href="#id717" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id718">
<h4>Returns<a class="headerlink" href="#id718" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mcd.html#MCD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mcd.MCD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id719">
<h4>Parameters<a class="headerlink" href="#id719" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id720">
<h4>Returns<a class="headerlink" href="#id720" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mcd.html#MCD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mcd.MCD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id721">
<h4>Parameters<a class="headerlink" href="#id721" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id722">
<h4>Returns<a class="headerlink" href="#id722" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id723">
<h4>Parameters<a class="headerlink" href="#id723" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id724">
<h4>Returns<a class="headerlink" href="#id724" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id725">
<h4>Parameters<a class="headerlink" href="#id725" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id726">
<h4>Returns<a class="headerlink" href="#id726" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id727">
<h4>Parameters<a class="headerlink" href="#id727" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id728">
<h4>Returns<a class="headerlink" href="#id728" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id729">
<h4>Parameters<a class="headerlink" href="#id729" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id730">
<h4>Returns<a class="headerlink" href="#id730" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id731">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id732">
<h4>Parameters<a class="headerlink" href="#id732" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id733">
<h4>Returns<a class="headerlink" href="#id733" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id734">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id735">
<h4>Parameters<a class="headerlink" href="#id735" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id736">
<h4>Returns<a class="headerlink" href="#id736" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id737">
<h4>Parameters<a class="headerlink" href="#id737" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id738">
<h4>Returns<a class="headerlink" href="#id738" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mcd.MCD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mcd.MCD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id739">
<h4>Returns<a class="headerlink" href="#id739" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.mo_gaal">
<span id="pyod-models-mo-gaal-module"></span><h2>pyod.models.mo_gaal module<a class="headerlink" href="#module-pyod.models.mo_gaal" title="Link to this heading">¶</a></h2>
<p>Multiple-Objective Generative Adversarial Active Learning.
Part of the codes are adapted from
<a class="reference external" href="https://github.com/leibinghe/GAAL-based-outlier-detection">https://github.com/leibinghe/GAAL-based-outlier-detection</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.mo_gaal.</span></span><span class="sig-name descname"><span class="pre">MO_GAAL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_g</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mo_gaal.html#MO_GAAL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Multi-Objective Generative Adversarial Active Learning.</p>
<p>MO_GAAL directly generates informative potential outliers to assist the
classifier in describing a boundary that can separate outliers from normal
data effectively. Moreover, to prevent the generator from falling into the
mode collapsing problem, the network structure of SO-GAAL is expanded from
a single generator (SO-GAAL) to multiple generators with different
objectives (MO-GAAL) to generate a reasonable reference distribution for
the whole dataset.
Read more in the <span id="id740">[<a class="reference internal" href="#id1132" title="Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.">BLLZ+19</a>]</span>.</p>
<section id="id741">
<h3>Parameters<a class="headerlink" href="#id741" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>k<span class="classifier">int, optional (default=10)</span></dt><dd><p>The number of sub generators.</p>
</dd>
<dt>stop_epochs<span class="classifier">int, optional (default=20)</span></dt><dd><p>The number of epochs of training. The number of total epochs equals to three times of stop_epochs.</p>
</dd>
<dt>lr_d<span class="classifier">float, optional (default=0.01)</span></dt><dd><p>The learn rate of the discriminator.</p>
</dd>
<dt>lr_g<span class="classifier">float, optional (default=0.0001)</span></dt><dd><p>The learn rate of the generator.</p>
</dd>
<dt>momentum<span class="classifier">float, optional (default=0.9)</span></dt><dd><p>The momentum parameter for SGD.</p>
</dd>
</dl>
</section>
<section id="id742">
<h3>Attributes<a class="headerlink" href="#id742" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1395"><span class="problematic" id="id1396">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1397"><span class="problematic" id="id1398">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1399"><span class="problematic" id="id1400">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id743">
<h4>Parameters<a class="headerlink" href="#id743" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id744">
<h4>Returns<a class="headerlink" href="#id744" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mo_gaal.html#MO_GAAL.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id745">
<h4>Parameters<a class="headerlink" href="#id745" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id746">
<h4>Returns<a class="headerlink" href="#id746" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/mo_gaal.html#MO_GAAL.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id747">
<h4>Parameters<a class="headerlink" href="#id747" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id748">
<h4>Returns<a class="headerlink" href="#id748" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id749">
<h4>Parameters<a class="headerlink" href="#id749" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id750">
<h4>Returns<a class="headerlink" href="#id750" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id751">
<h4>Parameters<a class="headerlink" href="#id751" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id752">
<h4>Returns<a class="headerlink" href="#id752" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id753">
<h4>Parameters<a class="headerlink" href="#id753" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id754">
<h4>Returns<a class="headerlink" href="#id754" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id755">
<h4>Parameters<a class="headerlink" href="#id755" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id756">
<h4>Returns<a class="headerlink" href="#id756" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id757">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id758">
<h4>Parameters<a class="headerlink" href="#id758" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id759">
<h4>Returns<a class="headerlink" href="#id759" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id760">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id761">
<h4>Parameters<a class="headerlink" href="#id761" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id762">
<h4>Returns<a class="headerlink" href="#id762" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id763">
<h4>Parameters<a class="headerlink" href="#id763" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id764">
<h4>Returns<a class="headerlink" href="#id764" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.mo_gaal.MO_GAAL.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.mo_gaal.MO_GAAL.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id765">
<h4>Returns<a class="headerlink" href="#id765" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.ocsvm">
<span id="pyod-models-ocsvm-module"></span><h2>pyod.models.ocsvm module<a class="headerlink" href="#module-pyod.models.ocsvm" title="Link to this heading">¶</a></h2>
<p>One-class SVM detector. Implemented on scikit-learn library.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.ocsvm.</span></span><span class="sig-name descname"><span class="pre">OCSVM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ocsvm.html#OCSVM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ocsvm.OCSVM" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Wrapper of scikit-learn one-class SVM Class with more functionalities.
Unsupervised Outlier Detection.</p>
<p>Estimate the support of a high-dimensional distribution.</p>
<p>The implementation is based on libsvm.
See <a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection">http://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection</a>
and <span id="id766">[<a class="reference internal" href="#id1136" title="Bernhard Schölkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443–1471, 2001.">BScholkopfPST+01</a>]</span>.</p>
<section id="id767">
<h3>Parameters<a class="headerlink" href="#id767" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>kernel<span class="classifier">string, optional (default=’rbf’)</span></dt><dd><p>Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p>
</dd>
<dt>nu<span class="classifier">float, optional</span></dt><dd><p>An upper bound on the fraction of training
errors and a lower bound of the fraction of support
vectors. Should be in the interval (0, 1]. By default 0.5
will be taken.</p>
</dd>
<dt>degree<span class="classifier">int, optional (default=3)</span></dt><dd><p>Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p>
</dd>
<dt>gamma<span class="classifier">float, optional (default=’auto’)</span></dt><dd><p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.
If gamma is ‘auto’ then 1/n_features will be used instead.</p>
</dd>
<dt>coef0<span class="classifier">float, optional (default=0.0)</span></dt><dd><p>Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p>
</dd>
<dt>tol<span class="classifier">float, optional</span></dt><dd><p>Tolerance for stopping criterion.</p>
</dd>
<dt>shrinking<span class="classifier">bool, optional</span></dt><dd><p>Whether to use the shrinking heuristic.</p>
</dd>
<dt>cache_size<span class="classifier">float, optional</span></dt><dd><p>Specify the size of the kernel cache (in MB).</p>
</dd>
<dt>verbose<span class="classifier">bool, default: False</span></dt><dd><p>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p>
</dd>
<dt>max_iter<span class="classifier">int, optional (default=-1)</span></dt><dd><p>Hard limit on iterations within solver, or -1 for no limit.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
</dl>
</section>
<section id="id768">
<h3>Attributes<a class="headerlink" href="#id768" title="Link to this heading">¶</a></h3>
<dl>
<dt><a href="#id1401"><span class="problematic" id="id1402">support_</span></a><span class="classifier">array-like, shape = [n_SV]</span></dt><dd><p>Indices of support vectors.</p>
</dd>
<dt><a href="#id1403"><span class="problematic" id="id1404">support_vectors_</span></a><span class="classifier">array-like, shape = [nSV, n_features]</span></dt><dd><p>Support vectors.</p>
</dd>
<dt><a href="#id1405"><span class="problematic" id="id1406">dual_coef_</span></a><span class="classifier">array, shape = [1, n_SV]</span></dt><dd><p>Coefficients of the support vectors in the decision function.</p>
</dd>
<dt><a href="#id1407"><span class="problematic" id="id1408">coef_</span></a><span class="classifier">array, shape = [1, n_features]</span></dt><dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
</dd>
<dt><a href="#id1409"><span class="problematic" id="id1410">intercept_</span></a><span class="classifier">array, shape = [1,]</span></dt><dd><p>Constant in the decision function.</p>
</dd>
<dt><a href="#id1411"><span class="problematic" id="id1412">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1413"><span class="problematic" id="id1414">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1415"><span class="problematic" id="id1416">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id769">
<h4>Parameters<a class="headerlink" href="#id769" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id770">
<h4>Returns<a class="headerlink" href="#id770" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ocsvm.html#OCSVM.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id771">
<h4>Parameters<a class="headerlink" href="#id771" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id772">
<h4>Returns<a class="headerlink" href="#id772" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/ocsvm.html#OCSVM.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id773">
<h4>Parameters<a class="headerlink" href="#id773" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like, shape (n_samples,)</span></dt><dd><p>Per-sample weights. Rescale C per sample. Higher weights
force the classifier to put more emphasis on these points.</p>
</dd>
</dl>
</section>
<section id="id774">
<h4>Returns<a class="headerlink" href="#id774" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id775">
<h4>Parameters<a class="headerlink" href="#id775" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id776">
<h4>Returns<a class="headerlink" href="#id776" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id777">
<h4>Parameters<a class="headerlink" href="#id777" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id778">
<h4>Returns<a class="headerlink" href="#id778" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id779">
<h4>Parameters<a class="headerlink" href="#id779" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id780">
<h4>Returns<a class="headerlink" href="#id780" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id781">
<h4>Parameters<a class="headerlink" href="#id781" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id782">
<h4>Returns<a class="headerlink" href="#id782" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id783">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id784">
<h4>Parameters<a class="headerlink" href="#id784" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id785">
<h4>Returns<a class="headerlink" href="#id785" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id786">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id787">
<h4>Parameters<a class="headerlink" href="#id787" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id788">
<h4>Returns<a class="headerlink" href="#id788" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id789">
<h4>Parameters<a class="headerlink" href="#id789" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id790">
<h4>Returns<a class="headerlink" href="#id790" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.ocsvm.OCSVM.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.ocsvm.OCSVM.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id791">
<h4>Returns<a class="headerlink" href="#id791" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.pca">
<span id="pyod-models-pca-module"></span><h2>pyod.models.pca module<a class="headerlink" href="#module-pyod.models.pca" title="Link to this heading">¶</a></h2>
<p>Principal Component Analysis (PCA) Outlier Detector</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.pca.PCA">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.pca.</span></span><span class="sig-name descname"><span class="pre">PCA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_selected_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">whiten</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">svd_solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterated_power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weighted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">standardization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/pca.html#PCA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.pca.PCA" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Principal component analysis (PCA) can be used in detecting outliers.
PCA is a linear dimensionality reduction using Singular Value Decomposition
of the data to project it to a lower dimensional space.</p>
<p>In this procedure, covariance matrix of the data can be decomposed to
orthogonal vectors, called eigenvectors, associated with eigenvalues. The
eigenvectors with high eigenvalues capture most of the variance in the
data.</p>
<p>Therefore, a low dimensional hyperplane constructed by k eigenvectors can
capture most of the variance in the data. However, outliers are different
from normal data points, which is more obvious on the hyperplane
constructed by the eigenvectors with small eigenvalues.</p>
<p>Therefore, outlier scores can be obtained as the sum of the projected
distance of a sample on all eigenvectors.
See <span id="id792">[<a class="reference internal" href="#id1122" title="Charu C Aggarwal. Outlier analysis. In Data mining, 75–79. Springer, 2015.">BAgg15</a>, <a class="reference internal" href="#id1121" title="Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.">BSCSC03</a>]</span> for details.</p>
<p>Score(X) = Sum of weighted euclidean distance between each sample to the
hyperplane constructed by the selected eigenvectors</p>
<section id="id793">
<h3>Parameters<a class="headerlink" href="#id793" title="Link to this heading">¶</a></h3>
<dl>
<dt>n_components<span class="classifier">int, float, None or string</span></dt><dd><p>Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p>if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used
to guess the dimension
if <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> and svd_solver == ‘full’, select the number
of components such that the amount of variance that needs to be
explained is greater than the percentage specified by n_components
n_components cannot be equal to n_features for svd_solver == ‘arpack’.</p>
</dd>
<dt>n_selected_components<span class="classifier">int, optional (default=None)</span></dt><dd><p>Number of selected principal components
for calculating the outlier scores. It is not necessarily equal to
the total number of the principal components. If not set, use
all principal components.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>copy<span class="classifier">bool (default True)</span></dt><dd><p>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</p>
</dd>
<dt>whiten<span class="classifier">bool, optional (default False)</span></dt><dd><p>When True (False by default) the <cite>components_</cite> vectors are multiplied
by the square root of n_samples and then divided by the singular values
to ensure uncorrelated outputs with unit component-wise variances.</p>
<p>Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt>svd_solver<span class="classifier">string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}</span></dt><dd><dl class="simple">
<dt>auto :</dt><dd><p>the solver is selected by a default policy based on <cite>X.shape</cite> and
<cite>n_components</cite>: if the input data is larger than 500x500 and the
number of components to extract is lower than 80% of the smallest
dimension of the data, then the more efficient ‘randomized’
method is enabled. Otherwise the exact full SVD is computed and
optionally truncated afterwards.</p>
</dd>
<dt>full :</dt><dd><p>run exact full SVD calling the standard LAPACK solver via
<cite>scipy.linalg.svd</cite> and select the components by postprocessing</p>
</dd>
<dt>arpack :</dt><dd><p>run SVD truncated to n_components calling ARPACK solver via
<cite>scipy.sparse.linalg.svds</cite>. It requires strictly
0 &lt; n_components &lt; X.shape[1]</p>
</dd>
<dt>randomized :</dt><dd><p>run randomized SVD by the method of Halko et al.</p>
</dd>
</dl>
</dd>
<dt>tol<span class="classifier">float &gt;= 0, optional (default .0)</span></dt><dd><p>Tolerance for singular values computed by svd_solver == ‘arpack’.</p>
</dd>
<dt>iterated_power<span class="classifier">int &gt;= 0, or ‘auto’, (default ‘auto’)</span></dt><dd><p>Number of iterations for the power method computed by
svd_solver == ‘randomized’.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">svd_solver</span></code> == ‘arpack’ or ‘randomized’.</p>
</dd>
<dt>weighted<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, the eigenvalues are used in score computation.
The eigenvectors with small eigenvalues comes with more importance
in outlier score calculation.</p>
</dd>
<dt>standardization<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, perform standardization first to convert
data to zero mean and unit variance.
See <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html">http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html</a></p>
</dd>
</dl>
</section>
<section id="id794">
<h3>Attributes<a class="headerlink" href="#id794" title="Link to this heading">¶</a></h3>
<dl>
<dt><a href="#id1417"><span class="problematic" id="id1418">components_</span></a><span class="classifier">array, shape (n_components, n_features)</span></dt><dd><p>Principal axes in feature space, representing the directions of
maximum variance in the data. The components are sorted by
<code class="docutils literal notranslate"><span class="pre">explained_variance_</span></code>.</p>
</dd>
<dt><a href="#id1419"><span class="problematic" id="id1420">explained_variance_</span></a><span class="classifier">array, shape (n_components,)</span></dt><dd><p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
</dd>
<dt><a href="#id1421"><span class="problematic" id="id1422">explained_variance_ratio_</span></a><span class="classifier">array, shape (n_components,)</span></dt><dd><p>Percentage of variance explained by each of the selected components.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0.</p>
</dd>
<dt><a href="#id1423"><span class="problematic" id="id1424">singular_values_</span></a><span class="classifier">array, shape (n_components,)</span></dt><dd><p>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code class="docutils literal notranslate"><span class="pre">n_components</span></code>
variables in the lower-dimensional space.</p>
</dd>
<dt><a href="#id1425"><span class="problematic" id="id1426">mean_</span></a><span class="classifier">array, shape (n_features,)</span></dt><dd><p>Per-feature empirical mean, estimated from the training set.</p>
<p>Equal to <cite>X.mean(axis=0)</cite>.</p>
</dd>
<dt><a href="#id1427"><span class="problematic" id="id1428">n_components_</span></a><span class="classifier">int</span></dt><dd><p>The estimated number of components. When n_components is set
to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this
number is estimated from input data. Otherwise it equals the parameter
n_components, or n_features if n_components is None.</p>
</dd>
<dt><a href="#id1429"><span class="problematic" id="id1430">noise_variance_</span></a><span class="classifier">float</span></dt><dd><p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
computed the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
</dd>
<dt><a href="#id1431"><span class="problematic" id="id1432">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1433"><span class="problematic" id="id1434">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1435"><span class="problematic" id="id1436">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id795">
<h4>Parameters<a class="headerlink" href="#id795" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id796">
<h4>Returns<a class="headerlink" href="#id796" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/pca.html#PCA.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.pca.PCA.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id797">
<h4>Parameters<a class="headerlink" href="#id797" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id798">
<h4>Returns<a class="headerlink" href="#id798" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.explained_variance_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">explained_variance_</span></span><a class="headerlink" href="#pyod.models.pca.PCA.explained_variance_" title="Link to this definition">¶</a></dt>
<dd><p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/pca.html#PCA.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.pca.PCA.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id799">
<h4>Parameters<a class="headerlink" href="#id799" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id800">
<h4>Returns<a class="headerlink" href="#id800" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id801">
<h4>Parameters<a class="headerlink" href="#id801" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id802">
<h4>Returns<a class="headerlink" href="#id802" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id803">
<h4>Parameters<a class="headerlink" href="#id803" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id804">
<h4>Returns<a class="headerlink" href="#id804" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id805">
<h4>Parameters<a class="headerlink" href="#id805" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id806">
<h4>Returns<a class="headerlink" href="#id806" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.noise_variance_">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">noise_variance_</span></span><a class="headerlink" href="#pyod.models.pca.PCA.noise_variance_" title="Link to this definition">¶</a></dt>
<dd><p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See “Pattern Recognition and
Machine Learning” by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
computed the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
<p>Decorator for scikit-learn PCA attributes.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id807">
<h4>Parameters<a class="headerlink" href="#id807" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id808">
<h4>Returns<a class="headerlink" href="#id808" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id809">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id810">
<h4>Parameters<a class="headerlink" href="#id810" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id811">
<h4>Returns<a class="headerlink" href="#id811" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id812">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id813">
<h4>Parameters<a class="headerlink" href="#id813" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id814">
<h4>Returns<a class="headerlink" href="#id814" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id815">
<h4>Parameters<a class="headerlink" href="#id815" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id816">
<h4>Returns<a class="headerlink" href="#id816" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.pca.PCA.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.pca.PCA.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id817">
<h4>Returns<a class="headerlink" href="#id817" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.qmcd">
<span id="pyod-models-qmcd-module"></span><h2>pyod.models.qmcd module<a class="headerlink" href="#module-pyod.models.qmcd" title="Link to this heading">¶</a></h2>
<p>Quasi-Monte Carlo Discrepancy outlier detection (QMCD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.qmcd.</span></span><span class="sig-name descname"><span class="pre">QMCD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/qmcd.html#QMCD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.qmcd.QMCD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<dl class="simple">
<dt>The Wrap-around Quasi-Monte Carlo discrepancy is a uniformity criterion </dt><dd><p>which is used to assess the space filling of a number of samples in a hypercube. 
It quantifies the distance between the continuous uniform distribution on a 
hypercube and the discrete uniform distribution on distinct sample points. 
Therefore, lower discrepancy values for a sample point indicates that it provides 
better coverage of the parameter space with regard to the rest of the samples. This
method is kernel based and a higher discrepancy score is relative to the
rest of the samples, the higher the likelihood of it being an outlier. 
Read more in the <span id="id818">[<a class="reference internal" href="#id1165" title="Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. Journal of complexity, 17(4):608–624, 2001.">BFM01</a>]</span>.</p>
</dd>
</dl>
<section id="id819">
<h3>Parameters<a class="headerlink" href="#id819" title="Link to this heading">¶</a></h3>
</section>
<section id="id820">
<h3>Attributes<a class="headerlink" href="#id820" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1437"><span class="problematic" id="id1438">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1439"><span class="problematic" id="id1440">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The modified z-score to use as a threshold. Observations with
a modified z-score (based on the median absolute deviation) greater
than this value will be classified as outliers.</p>
</dd>
<dt><a href="#id1441"><span class="problematic" id="id1442">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id821">
<h4>Parameters<a class="headerlink" href="#id821" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id822">
<h4>Returns<a class="headerlink" href="#id822" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/qmcd.html#QMCD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.qmcd.QMCD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id823">
<h4>Parameters<a class="headerlink" href="#id823" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The independent and dependent/target samples with the target 
samples being the last column of the numpy array such that
eg: X = np.append(x, y.reshape(-1,1), axis=1). Sparse matrices are 
accepted only if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id824">
<h4>Returns<a class="headerlink" href="#id824" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/qmcd.html#QMCD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.qmcd.QMCD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector</p>
<section id="id825">
<h4>Parameters<a class="headerlink" href="#id825" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id826">
<h4>Parameters<a class="headerlink" href="#id826" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id827">
<h4>Returns<a class="headerlink" href="#id827" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id828">
<h4>Parameters<a class="headerlink" href="#id828" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id829">
<h4>Returns<a class="headerlink" href="#id829" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id830">
<h4>Parameters<a class="headerlink" href="#id830" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id831">
<h4>Returns<a class="headerlink" href="#id831" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id832">
<h4>Parameters<a class="headerlink" href="#id832" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id833">
<h4>Returns<a class="headerlink" href="#id833" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id834">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id835">
<h4>Parameters<a class="headerlink" href="#id835" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id836">
<h4>Returns<a class="headerlink" href="#id836" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id837">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id838">
<h4>Parameters<a class="headerlink" href="#id838" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id839">
<h4>Returns<a class="headerlink" href="#id839" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id840">
<h4>Parameters<a class="headerlink" href="#id840" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id841">
<h4>Returns<a class="headerlink" href="#id841" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.qmcd.QMCD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.qmcd.QMCD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id842">
<h4>Returns<a class="headerlink" href="#id842" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.rgraph">
<span id="pyod-models-rgraph-module"></span><h2>pyod.models.rgraph module<a class="headerlink" href="#module-pyod.models.rgraph" title="Link to this heading">¶</a></h2>
<p>R-graph</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.rgraph.</span></span><span class="sig-name descname"><span class="pre">RGraph</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transition_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_nz</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lasso_lars'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter_lasso</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">blocksize_test_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">support_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'L2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">support_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">active_support</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept_LR</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rgraph.html#RGraph"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rgraph.RGraph" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Outlier Detection via R-graph.
Paper: <a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/You_Provable_Self-Representation_Based_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/You_Provable_Self-Representation_Based_CVPR_2017_paper.pdf</a>
See <span id="id843">[<a class="reference internal" href="#id1162" title="Chong You, Daniel P Robinson, and René Vidal. Provable self-representation based outlier detection in a union of subspaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3395–3404. 2017.">BYRV17</a>]</span> for details.</p>
<section id="id844">
<h3>Parameters<a class="headerlink" href="#id844" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>transition_steps<span class="classifier">int, optional (default=20)</span></dt><dd><p>Number of transition steps that are taken in the graph, after which 
the outlier scores are determined.</p>
</dd>
</dl>
<p>gamma : float</p>
<dl>
<dt>gamma_nz<span class="classifier">boolean, default True</span></dt><dd><p>gamma and gamma_nz together determines the parameter alpha.
When <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">False</span></code>, alpha = gamma.
When <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">True</span></code>, then alpha = gamma * alpha0, where alpha0 is
the largest number such that the solution to the optimization problem
with alpha = alpha0 is the zero vector (see Proposition 1 in [1]).
Therefore, when <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">True</span></code>, gamma should be a value greater
than 1.0. A good choice is typically in the range [5, 500].</p>
</dd>
<dt>tau<span class="classifier">float, default 1.0</span></dt><dd><p>Parameter for elastic net penalty term. 
When tau = 1.0, the method reduces to sparse subspace clustering with
basis pursuit (SSC-BP) [2].
When tau = 0.0, the method reduces to least squares regression (LSR).</p>
</dd>
<dt>algorithm<span class="classifier">string, default <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code></span></dt><dd><p>Algorithm for computing the representation. Either lasso_lars or
lasso_cd.
Note: <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code> only support tau = 1.
For cases tau &lt;&lt; 1 linear regression is used.</p>
</dd>
<dt>fit_intercept_LR: bool, optional (default=False)</dt><dd><p>For  <code class="docutils literal notranslate"><span class="pre">gamma</span></code> &gt; 10000 linear regression is used instead of
<code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> or <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code>. This parameter determines whether the
intercept for the model is calculated.</p>
</dd>
<dt>maxiter_lasso<span class="classifier">int, default 1000</span></dt><dd><p>The maximum number of iterations for <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code>.</p>
</dd>
<dt>n_nonzero<span class="classifier">int, default 50</span></dt><dd><p>This is an upper bound on the number of nonzero entries of each
representation vector.
If there are more than n_nonzero nonzero entries,
only the top n_nonzero number of
entries with largest absolute value are kept.</p>
</dd>
<dt>active_support: boolean, default True</dt><dd><p>Set to True to use the active support algorithm in [1] for solving the
optimization problem. This should significantly reduce the running time
when n_samples is large.</p>
</dd>
<dt>active_support_params: dictionary of string to any, optional</dt><dd><p>Parameters (keyword arguments) and values for the active support
algorithm. It may be used to set the parameters <code class="docutils literal notranslate"><span class="pre">support_init</span></code>,
<code class="docutils literal notranslate"><span class="pre">support_size</span></code> and <code class="docutils literal notranslate"><span class="pre">maxiter</span></code>, see
<code class="docutils literal notranslate"><span class="pre">active_support_elastic_net</span></code> for details. 
Example: active_support_params={‘support_size’:50, ‘maxiter’:100}
Ignored when <code class="docutils literal notranslate"><span class="pre">active_support=False</span></code></p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply standardization on the data.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=1)</span></dt><dd><p>Verbosity mode.</p>
<ul class="simple">
<li><p>0 = silent</p></li>
<li><p>1 = progress bar</p></li>
<li><p>2 = one line per epoch.</p></li>
</ul>
<p>For verbose &gt;= 1, model summary may be printed.</p>
</dd>
<dt>random_state<span class="classifier">random_state: int, RandomState instance or None, optional</span></dt><dd><p>(default=None)
If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. When fitting this is used
to define the threshold on the decision function.</p>
</dd>
<dt>blocksize_test_data: int, optional (default=10)</dt><dd><p>Test set is splitted into blocks of the size <code class="docutils literal notranslate"><span class="pre">blocksize_test_data</span></code>
to at least partially separate test - and train set</p>
</dd>
</dl>
</section>
<section id="id845">
<h3>Attributes<a class="headerlink" href="#id845" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1443"><span class="problematic" id="id1444">transition_matrix_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>Transition matrix from the last fitted data, this might include 
training + test data</p>
</dd>
<dt><a href="#id1445"><span class="problematic" id="id1446">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1447"><span class="problematic" id="id1448">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1449"><span class="problematic" id="id1450">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.active_support_elastic_net">
<span class="sig-name descname"><span class="pre">active_support_elastic_net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lasso_lars'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">support_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'L2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">support_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter_lasso</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rgraph.html#RGraph.active_support_elastic_net"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rgraph.RGraph.active_support_elastic_net" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Source: <a class="reference external" href="https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py">https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py</a></dt><dd><p>An active support based algorithm for solving the elastic net optimization problem
min_{c} tau ||c||_1 + (1-tau)/2 ||c||_2^2 + alpha / 2 ||y - c X ||_2^2.</p>
</dd>
</dl>
<section id="id846">
<h4>Parameters<a class="headerlink" href="#id846" title="Link to this heading">¶</a></h4>
<p>X : array-like, shape (n_samples, n_features)</p>
<p>y : array-like, shape (1, n_features)</p>
<p>alpha : float</p>
<p>tau : float, default 1.0</p>
<dl>
<dt>algorithm<span class="classifier">string, default <code class="docutils literal notranslate"><span class="pre">spams</span></code></span></dt><dd><p>Algorithm for computing solving the subproblems. Either lasso_lars
or lasso_cd or spams
(installation of spams package is required).
Note: <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code> only support tau = 1.</p>
</dd>
<dt>support_init: string, default <code class="docutils literal notranslate"><span class="pre">knn</span></code></dt><dd><p>This determines how the active support is initialized.
It can be either <code class="docutils literal notranslate"><span class="pre">knn</span></code> or <code class="docutils literal notranslate"><span class="pre">L2</span></code>.</p>
</dd>
<dt>support_size: int, default 100</dt><dd><p>This determines the size of the working set.
A small support_size decreases the runtime per iteration while
increase the number of iterations.</p>
</dd>
<dt>maxiter: int default 40</dt><dd><p>Termination condition for active support update.</p>
</dd>
</dl>
</section>
<section id="id847">
<h4>Returns<a class="headerlink" href="#id847" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>c<span class="classifier">shape n_samples</span></dt><dd><p>The optimal solution to the optimization problem.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id848">
<h4>Parameters<a class="headerlink" href="#id848" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id849">
<h4>Returns<a class="headerlink" href="#id849" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rgraph.html#RGraph.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rgraph.RGraph.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id850">
<h4>Parameters<a class="headerlink" href="#id850" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id851">
<h4>Returns<a class="headerlink" href="#id851" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.elastic_net_subspace_clustering">
<span class="sig-name descname"><span class="pre">elastic_net_subspace_clustering</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_nz</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lasso_lars'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept_LR</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">active_support</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">active_support_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiter_lasso</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rgraph.html#RGraph.elastic_net_subspace_clustering"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering" title="Link to this definition">¶</a></dt>
<dd><p>Source: <a class="reference external" href="https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py">https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py</a></p>
<p>Elastic net subspace clustering (EnSC) [1]. 
Compute self-representation matrix C from solving the following optimization problem
min_{c_j} tau ||c_j||_1 + (1-tau)/2 ||c_j||_2^2 + alpha / 2 ||x_j - c_j X ||_2^2 s.t. c_jj = 0,
where c_j and x_j are the j-th rows of C and X, respectively.</p>
<p>Parameter <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> specifies the algorithm for solving the optimization problem.
<code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code> are algorithms implemented in sklearn, 
<code class="docutils literal notranslate"><span class="pre">spams</span></code> refers to the same algorithm as <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> but is implemented in 
spams package available at <a class="reference external" href="http://spams-devel.gforge.inria.fr/">http://spams-devel.gforge.inria.fr/</a> (installation required)
In principle, all three algorithms give the same result.    
For large scale data (e.g. with &gt; 5000 data points), use any of these algorithms in
conjunction with <code class="docutils literal notranslate"><span class="pre">active_support=True</span></code>. It adopts an efficient active support 
strategy that solves the optimization problem by breaking it into a sequence of 
small scale optimization problems as described in [1].
If tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2].
If tau = 0.0, the method reduces to least squares regression (LSR) [3].
Note: <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code> only support tau = 1.
Parameters
———–
X : array-like, shape (n_samples, n_features)</p>
<blockquote>
<div><p>Input data to be clustered</p>
</div></blockquote>
<p>gamma : float
gamma_nz : boolean, default True</p>
<blockquote>
<div><p>gamma and gamma_nz together determines the parameter alpha. When <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">False</span></code>, 
alpha = gamma. When <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">True</span></code>, then alpha = gamma * alpha0, where alpha0 is 
the largest number such that the solution to the optimization problem with alpha = alpha0
is the zero vector (see Proposition 1 in [1]). Therefore, when <code class="docutils literal notranslate"><span class="pre">gamma_nz</span> <span class="pre">=</span> <span class="pre">True</span></code>, gamma
should be a value greater than 1.0. A good choice is typically in the range [5, 500].</p>
</div></blockquote>
<dl>
<dt>tau<span class="classifier">float, default 1.0</span></dt><dd><p>Parameter for elastic net penalty term. 
When tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2].
When tau = 0.0, the method reduces to least squares regression (LSR) [3].</p>
</dd>
<dt>algorithm<span class="classifier">string, default <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code></span></dt><dd><p>Algorithm for computing the representation. Either lasso_lars or lasso_cd or spams 
(installation of spams package is required).
Note: <code class="docutils literal notranslate"><span class="pre">lasso_lars</span></code> and <code class="docutils literal notranslate"><span class="pre">lasso_cd</span></code> only support tau = 1.</p>
</dd>
<dt>n_nonzero<span class="classifier">int, default 50</span></dt><dd><p>This is an upper bound on the number of nonzero entries of each representation vector. 
If there are more than n_nonzero nonzero entries,  only the top n_nonzero number of
entries with largest absolute value are kept.</p>
</dd>
<dt>active_support: boolean, default True</dt><dd><p>Set to True to use the active support algorithm in [1] for solving the optimization problem.
This should significantly reduce the running time when n_samples is large.</p>
</dd>
<dt>active_support_params: dictionary of string to any, optional</dt><dd><p>Parameters (keyword arguments) and values for the active support algorithm. It may be
used to set the parameters <code class="docutils literal notranslate"><span class="pre">support_init</span></code>, <code class="docutils literal notranslate"><span class="pre">support_size</span></code> and <code class="docutils literal notranslate"><span class="pre">maxiter</span></code>, see
<code class="docutils literal notranslate"><span class="pre">active_support_elastic_net</span></code> for details. 
Example: active_support_params={‘support_size’:50, ‘maxiter’:100}
Ignored when <code class="docutils literal notranslate"><span class="pre">active_support=False</span></code></p>
</dd>
</dl>
<section id="id852">
<h4>Returns<a class="headerlink" href="#id852" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt><a href="#id1451"><span class="problematic" id="id1452">representation_matrix_</span></a><span class="classifier">csr matrix, shape: n_samples by n_samples</span></dt><dd><p>The self-representation matrix.</p>
</dd>
</dl>
</section>
<section id="references">
<h4>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h4>
<p>[1] C. You, C.-G. Li, D. Robinson, R. Vidal, Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering, CVPR 2016
[2] E. Elhaifar, R. Vidal, Sparse Subspace Clustering: Algorithm, Theory, and Applications, TPAMI 2013
[3] C. Lu, et al. Robust and efficient subspace segmentation via least squares regression, ECCV 2012</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rgraph.html#RGraph.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rgraph.RGraph.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The input samples.</p>
</div></blockquote>
<dl class="simple">
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
<section id="id853">
<h4>Returns<a class="headerlink" href="#id853" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id854">
<h4>Parameters<a class="headerlink" href="#id854" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id855">
<h4>Returns<a class="headerlink" href="#id855" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id856">
<h4>Parameters<a class="headerlink" href="#id856" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id857">
<h4>Returns<a class="headerlink" href="#id857" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id858">
<h4>Parameters<a class="headerlink" href="#id858" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id859">
<h4>Returns<a class="headerlink" href="#id859" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id860">
<h4>Parameters<a class="headerlink" href="#id860" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id861">
<h4>Returns<a class="headerlink" href="#id861" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id862">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id863">
<h4>Parameters<a class="headerlink" href="#id863" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id864">
<h4>Returns<a class="headerlink" href="#id864" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id865">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id866">
<h4>Parameters<a class="headerlink" href="#id866" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id867">
<h4>Returns<a class="headerlink" href="#id867" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id868">
<h4>Parameters<a class="headerlink" href="#id868" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id869">
<h4>Returns<a class="headerlink" href="#id869" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rgraph.RGraph.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rgraph.RGraph.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id870">
<h4>Returns<a class="headerlink" href="#id870" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.rod">
<span id="pyod-models-rod-module"></span><h2>pyod.models.rod module<a class="headerlink" href="#module-pyod.models.rod" title="Link to this heading">¶</a></h2>
<p>Rotation-based Outlier Detector (ROD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.rod.ROD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.rod.</span></span><span class="sig-name descname"><span class="pre">ROD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_execution</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rod.html#ROD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rod.ROD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Rotation-based Outlier Detection (ROD), is a robust and parameter-free
algorithm that requires no statistical distribution assumptions and
works intuitively in three-dimensional space, where the 3D-vectors,
representing the data points, are rotated about the geometric median
two times counterclockwise using Rodrigues rotation formula.
The results of the rotation are parallelepipeds where their volumes are
mathematically analyzed as cost functions and used to calculate the
Median Absolute Deviations to obtain the outlying score.
For high dimensions &gt; 3, the overall score is calculated by taking the
average of the overall 3D-subspaces scores, that were resulted from
decomposing the original data space.
See <span id="id871">[<a class="reference internal" href="#id1149" title="Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. IEEE Transactions on Knowledge and Data Engineering, 2020.">BABC20</a>]</span> for details.</p>
<section id="id872">
<h3>Parameters<a class="headerlink" href="#id872" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>parallel_execution: bool, optional (default=False).</dt><dd><p>If set to True, the algorithm will run in parallel,
for a better execution time. It is recommended to set
this parameter to True ONLY for high dimensional data &gt; 10,
and if a proper hardware is available.</p>
</dd>
</dl>
</section>
<section id="id873">
<h3>Attributes<a class="headerlink" href="#id873" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1453"><span class="problematic" id="id1454">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1455"><span class="problematic" id="id1456">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1457"><span class="problematic" id="id1458">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id874">
<h4>Parameters<a class="headerlink" href="#id874" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id875">
<h4>Returns<a class="headerlink" href="#id875" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rod.html#ROD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rod.ROD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id876">
<h4>Parameters<a class="headerlink" href="#id876" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id877">
<h4>Returns<a class="headerlink" href="#id877" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/rod.html#ROD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.rod.ROD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id878">
<h4>Parameters<a class="headerlink" href="#id878" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id879">
<h4>Returns<a class="headerlink" href="#id879" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id880">
<h4>Parameters<a class="headerlink" href="#id880" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id881">
<h4>Returns<a class="headerlink" href="#id881" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id882">
<h4>Parameters<a class="headerlink" href="#id882" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id883">
<h4>Returns<a class="headerlink" href="#id883" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id884">
<h4>Parameters<a class="headerlink" href="#id884" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id885">
<h4>Returns<a class="headerlink" href="#id885" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id886">
<h4>Parameters<a class="headerlink" href="#id886" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id887">
<h4>Returns<a class="headerlink" href="#id887" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id888">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id889">
<h4>Parameters<a class="headerlink" href="#id889" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id890">
<h4>Returns<a class="headerlink" href="#id890" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id891">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id892">
<h4>Parameters<a class="headerlink" href="#id892" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id893">
<h4>Returns<a class="headerlink" href="#id893" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id894">
<h4>Parameters<a class="headerlink" href="#id894" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id895">
<h4>Returns<a class="headerlink" href="#id895" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.rod.ROD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.rod.ROD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id896">
<h4>Returns<a class="headerlink" href="#id896" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.sampling">
<span id="pyod-models-sampling-module"></span><h2>pyod.models.sampling module<a class="headerlink" href="#module-pyod.models.sampling" title="Link to this heading">¶</a></h2>
<p>Outlier detection based on Sampling (SP)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.sampling.</span></span><span class="sig-name descname"><span class="pre">Sampling</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subset_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sampling.html#Sampling"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sampling.Sampling" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Sampling class for outlier detection.</p>
<p>Sugiyama, M., Borgwardt, K. M.: Rapid Distance-Based Outlier Detection via
Sampling, Advances in Neural Information Processing Systems (NIPS 2013),
467-475, 2013.</p>
<p>See <span id="id897">[<a class="reference internal" href="#id1157" title="Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. Advances in neural information processing systems, 2013.">BSB13</a>]</span> for details.</p>
<section id="id898">
<h3>Parameters<a class="headerlink" href="#id898" title="Link to this heading">¶</a></h3>
<dl>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>subset_size<span class="classifier">float in (0., 1.0) or int (0, n_samples), optional (default=20)</span></dt><dd><p>The size of subset of the data set.
Sampling subset from the data set is performed only once.</p>
</dd>
<dt>metric<span class="classifier">string or callable, default ‘minkowski’</span></dt><dd><p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’,
‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’,
‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</dd>
<dt>metric_params<span class="classifier">dict, optional (default = None)</span></dt><dd><p>Additional keyword arguments for the metric function.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional (default None)</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
</dl>
</section>
<section id="id899">
<h3>Attributes<a class="headerlink" href="#id899" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1459"><span class="problematic" id="id1460">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1461"><span class="problematic" id="id1462">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1463"><span class="problematic" id="id1464">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id900">
<h4>Parameters<a class="headerlink" href="#id900" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id901">
<h4>Returns<a class="headerlink" href="#id901" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sampling.html#Sampling.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sampling.Sampling.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id902">
<h4>Parameters<a class="headerlink" href="#id902" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The test input samples.</p>
</dd>
</dl>
</section>
<section id="id903">
<h4>Returns<a class="headerlink" href="#id903" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sampling.html#Sampling.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sampling.Sampling.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id904">
<h4>Parameters<a class="headerlink" href="#id904" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id905">
<h4>Returns<a class="headerlink" href="#id905" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id906">
<h4>Parameters<a class="headerlink" href="#id906" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id907">
<h4>Returns<a class="headerlink" href="#id907" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id908">
<h4>Parameters<a class="headerlink" href="#id908" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id909">
<h4>Returns<a class="headerlink" href="#id909" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id910">
<h4>Parameters<a class="headerlink" href="#id910" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id911">
<h4>Returns<a class="headerlink" href="#id911" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id912">
<h4>Parameters<a class="headerlink" href="#id912" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id913">
<h4>Returns<a class="headerlink" href="#id913" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id914">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id915">
<h4>Parameters<a class="headerlink" href="#id915" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id916">
<h4>Returns<a class="headerlink" href="#id916" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id917">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id918">
<h4>Parameters<a class="headerlink" href="#id918" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id919">
<h4>Returns<a class="headerlink" href="#id919" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id920">
<h4>Parameters<a class="headerlink" href="#id920" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id921">
<h4>Returns<a class="headerlink" href="#id921" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sampling.Sampling.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sampling.Sampling.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id922">
<h4>Returns<a class="headerlink" href="#id922" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.sod">
<span id="pyod-models-sod-module"></span><h2>pyod.models.sod module<a class="headerlink" href="#module-pyod.models.sod" title="Link to this heading">¶</a></h2>
<p>Subspace Outlier Detection (SOD)</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.sod.SOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.sod.</span></span><span class="sig-name descname"><span class="pre">SOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ref_set</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sod.html#SOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sod.SOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Subspace outlier detection (SOD) schema aims to detect outlier in
varying subspaces of a high dimensional feature space. For each data
object, SOD explores the axis-parallel subspace spanned by the data
object’s neighbors and determines how much the object deviates from the
neighbors in this subspace.</p>
<p>See <span id="id923">[<a class="reference internal" href="#id1139" title="Hans-Peter Kriegel, Peer Kröger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 831–838. Springer, 2009.">BKKrogerSZ09</a>]</span> for details.</p>
<section id="id924">
<h3>Parameters<a class="headerlink" href="#id924" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>n_neighbors<span class="classifier">int, optional (default=20)</span></dt><dd><p>Number of neighbors to use by default for k neighbors queries.</p>
</dd>
<dt>ref_set: int, optional (default=10)</dt><dd><p>specifies the number of shared nearest neighbors to create the
reference set. Note that ref_set must be smaller than n_neighbors.</p>
</dd>
<dt>alpha: float in (0., 1.), optional (default=0.8)</dt><dd><p>specifies the lower limit for selecting subspace.
0.8 is set as default as suggested in the original paper.</p>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
</dl>
</section>
<section id="id925">
<h3>Attributes<a class="headerlink" href="#id925" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1465"><span class="problematic" id="id1466">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1467"><span class="problematic" id="id1468">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1469"><span class="problematic" id="id1470">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id926">
<h4>Parameters<a class="headerlink" href="#id926" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id927">
<h4>Returns<a class="headerlink" href="#id927" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sod.html#SOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sod.SOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id928">
<h4>Parameters<a class="headerlink" href="#id928" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id929">
<h4>Returns<a class="headerlink" href="#id929" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sod.html#SOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sod.SOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id930">
<h4>Parameters<a class="headerlink" href="#id930" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id931">
<h4>Returns<a class="headerlink" href="#id931" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id932">
<h4>Parameters<a class="headerlink" href="#id932" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id933">
<h4>Returns<a class="headerlink" href="#id933" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id934">
<h4>Parameters<a class="headerlink" href="#id934" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id935">
<h4>Returns<a class="headerlink" href="#id935" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id936">
<h4>Parameters<a class="headerlink" href="#id936" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id937">
<h4>Returns<a class="headerlink" href="#id937" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id938">
<h4>Parameters<a class="headerlink" href="#id938" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id939">
<h4>Returns<a class="headerlink" href="#id939" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id940">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id941">
<h4>Parameters<a class="headerlink" href="#id941" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id942">
<h4>Returns<a class="headerlink" href="#id942" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id943">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id944">
<h4>Parameters<a class="headerlink" href="#id944" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id945">
<h4>Returns<a class="headerlink" href="#id945" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id946">
<h4>Parameters<a class="headerlink" href="#id946" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id947">
<h4>Returns<a class="headerlink" href="#id947" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sod.SOD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sod.SOD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id948">
<h4>Returns<a class="headerlink" href="#id948" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.so_gaal">
<span id="pyod-models-so-gaal-module"></span><h2>pyod.models.so_gaal module<a class="headerlink" href="#module-pyod.models.so_gaal" title="Link to this heading">¶</a></h2>
<p>Single-Objective Generative Adversarial Active Learning.
Part of the codes are adapted from
<a class="reference external" href="https://github.com/leibinghe/GAAL-based-outlier-detection">https://github.com/leibinghe/GAAL-based-outlier-detection</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.so_gaal.</span></span><span class="sig-name descname"><span class="pre">SO_GAAL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stop_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_g</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/so_gaal.html#SO_GAAL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Single-Objective Generative Adversarial Active Learning.</p>
<p>SO-GAAL directly generates informative potential outliers to assist the
classifier in describing a boundary that can separate outliers from normal
data effectively. Moreover, to prevent the generator from falling into the
mode collapsing problem, the network structure of SO-GAAL is expanded from
a single generator (SO-GAAL) to multiple generators with different
objectives (MO-GAAL) to generate a reasonable reference distribution for
the whole dataset.
Read more in the <span id="id949">[<a class="reference internal" href="#id1132" title="Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.">BLLZ+19</a>]</span>.</p>
<section id="id950">
<h3>Parameters<a class="headerlink" href="#id950" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>stop_epochs<span class="classifier">int, optional (default=20)</span></dt><dd><dl class="simple">
<dt>The number of epochs of training. The number of total epochs equals to</dt><dd><p>three times of stop_epochs.</p>
</dd>
</dl>
</dd>
<dt>lr_d<span class="classifier">float, optional (default=0.01)</span></dt><dd><p>The learn rate of the discriminator.</p>
</dd>
<dt>lr_g<span class="classifier">float, optional (default=0.0001)</span></dt><dd><p>The learn rate of the generator.</p>
</dd>
<dt>momentum<span class="classifier">float, optional (default=0.9)</span></dt><dd><p>The momentum parameter for SGD.</p>
</dd>
</dl>
</section>
<section id="id951">
<h3>Attributes<a class="headerlink" href="#id951" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1471"><span class="problematic" id="id1472">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1473"><span class="problematic" id="id1474">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1475"><span class="problematic" id="id1476">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id952">
<h4>Parameters<a class="headerlink" href="#id952" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id953">
<h4>Returns<a class="headerlink" href="#id953" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/so_gaal.html#SO_GAAL.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id954">
<h4>Parameters<a class="headerlink" href="#id954" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id955">
<h4>Returns<a class="headerlink" href="#id955" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/so_gaal.html#SO_GAAL.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id956">
<h4>Parameters<a class="headerlink" href="#id956" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id957">
<h4>Returns<a class="headerlink" href="#id957" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id958">
<h4>Parameters<a class="headerlink" href="#id958" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id959">
<h4>Returns<a class="headerlink" href="#id959" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id960">
<h4>Parameters<a class="headerlink" href="#id960" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id961">
<h4>Returns<a class="headerlink" href="#id961" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id962">
<h4>Parameters<a class="headerlink" href="#id962" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id963">
<h4>Returns<a class="headerlink" href="#id963" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id964">
<h4>Parameters<a class="headerlink" href="#id964" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id965">
<h4>Returns<a class="headerlink" href="#id965" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id966">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id967">
<h4>Parameters<a class="headerlink" href="#id967" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id968">
<h4>Returns<a class="headerlink" href="#id968" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id969">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id970">
<h4>Parameters<a class="headerlink" href="#id970" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id971">
<h4>Returns<a class="headerlink" href="#id971" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id972">
<h4>Parameters<a class="headerlink" href="#id972" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id973">
<h4>Returns<a class="headerlink" href="#id973" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.so_gaal.SO_GAAL.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.so_gaal.SO_GAAL.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id974">
<h4>Returns<a class="headerlink" href="#id974" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.sos">
<span id="pyod-models-sos-module"></span><h2>pyod.models.sos module<a class="headerlink" href="#module-pyod.models.sos" title="Link to this heading">¶</a></h2>
<p>Stochastic Outlier Selection (SOS).
Part of the codes are adapted from <a class="reference external" href="https://github.com/jeroenjanssens/scikit-sos">https://github.com/jeroenjanssens/scikit-sos</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.sos.SOS">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.sos.</span></span><span class="sig-name descname"><span class="pre">SOS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perplexity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sos.html#SOS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sos.SOS" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>Stochastic Outlier Selection.</p>
<p>SOS employs the concept of affinity to quantify
the relationship from one data point to another data point. Affinity is 
proportional to the similarity between two data points. So, a data point 
has little affinity with a dissimilar data point. A data point is 
selected as an outlier when all the other data points have insufficient
affinity with it.
Read more in the <span id="id975">[<a class="reference internal" href="#id1129" title="JHM Janssens, Ferenc Huszár, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.">BJHuszarPvdH12</a>]</span>.</p>
<section id="id976">
<h3>Parameters<a class="headerlink" href="#id976" title="Link to this heading">¶</a></h3>
<dl>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1) </span></dt><dd><p>The amount of contamination of the data set, i.e.
the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>perplexity<span class="classifier">float, optional (default=4.5)</span></dt><dd><p>A smooth measure of the effective number of neighbours. The perplexity
parameter is similar to the parameter <cite>k</cite> in kNN algorithm (the number
of nearest neighbors). The range of perplexity can be any real number
between 1 and n-1, where <cite>n</cite> is the number of samples.</p>
</dd>
<dt>metric: str, default ‘euclidean’</dt><dd><p>Metric used for the distance computation. Any metric from
scipy.spatial.distance can be used.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>‘euclidean’</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’,
‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’,
‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’,
‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’,
‘sokalsneath’, ‘sqeuclidean’, ‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics:
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/spatial.distance.html">http://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a></p>
</dd>
<dt>eps<span class="classifier">float, optional (default = 1e-5)</span></dt><dd><p>Tolerance threshold for floating point errors.</p>
</dd>
</dl>
</section>
<section id="id977">
<h3>Attributes<a class="headerlink" href="#id977" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1477"><span class="problematic" id="id1478">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1479"><span class="problematic" id="id1480">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1481"><span class="problematic" id="id1482">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
</section>
<section id="id978">
<h3>Examples<a class="headerlink" href="#id978" title="Link to this heading">¶</a></h3>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.models.sos</span><span class="w"> </span><span class="kn">import</span> <span class="n">SOS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pyod.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">50</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contamination</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_train</span><span class="o">=</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="n">n_test</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">contamination</span><span class="o">=</span><span class="n">contamination</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SOS</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="go">SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id979">
<h4>Parameters<a class="headerlink" href="#id979" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id980">
<h4>Returns<a class="headerlink" href="#id980" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sos.html#SOS.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sos.SOS.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id981">
<h4>Parameters<a class="headerlink" href="#id981" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id982">
<h4>Returns<a class="headerlink" href="#id982" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/sos.html#SOS.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.sos.SOS.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id983">
<h4>Parameters<a class="headerlink" href="#id983" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id984">
<h4>Returns<a class="headerlink" href="#id984" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id985">
<h4>Parameters<a class="headerlink" href="#id985" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id986">
<h4>Returns<a class="headerlink" href="#id986" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id987">
<h4>Parameters<a class="headerlink" href="#id987" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id988">
<h4>Returns<a class="headerlink" href="#id988" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id989">
<h4>Parameters<a class="headerlink" href="#id989" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id990">
<h4>Returns<a class="headerlink" href="#id990" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id991">
<h4>Parameters<a class="headerlink" href="#id991" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id992">
<h4>Returns<a class="headerlink" href="#id992" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id993">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id994">
<h4>Parameters<a class="headerlink" href="#id994" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id995">
<h4>Returns<a class="headerlink" href="#id995" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id996">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id997">
<h4>Parameters<a class="headerlink" href="#id997" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id998">
<h4>Returns<a class="headerlink" href="#id998" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id999">
<h4>Parameters<a class="headerlink" href="#id999" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id1000">
<h4>Returns<a class="headerlink" href="#id1000" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.sos.SOS.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.sos.SOS.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1001">
<h4>Returns<a class="headerlink" href="#id1001" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.suod">
<span id="pyod-models-suod-module"></span><h2>pyod.models.suod module<a class="headerlink" href="#module-pyod.models.suod" title="Link to this heading">¶</a></h2>
<p>SUOD</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.suod.</span></span><span class="sig-name descname"><span class="pre">SUOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'average'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rp_clf_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rp_ng_clf_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rp_flag_global</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_dim_frac</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jl_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bps_flag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approx_clf_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approx_ng_clf_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approx_flag_global</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approx_clf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/suod.html#SUOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.suod.SUOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>SUOD (Scalable Unsupervised Outlier Detection) is an acceleration
framework for large scale unsupervised outlier detector training and
prediction. See <span id="id1002">[<a class="reference internal" href="#id1150" title="Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.">BZHC+21</a>]</span> for details.</p>
<section id="id1003">
<h3>Parameters<a class="headerlink" href="#id1003" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>base_estimators<span class="classifier">list, length must be greater than 1</span></dt><dd><p>A list of base estimators. Certain methods must be present, e.g.,
<cite>fit</cite> and <cite>predict</cite>.</p>
</dd>
<dt>combination<span class="classifier">str, optional (default=’average’)</span></dt><dd><p>Decide how to aggregate the results from multiple models:</p>
<ul class="simple">
<li><p>“average” : average the results from all base detectors</p></li>
<li><p>“maximization” : output the max value across all base detectors</p></li>
</ul>
</dd>
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set,
i.e. the proportion of outliers in the data set. Used when fitting to
define the threshold on the decision function.</p>
</dd>
<dt>n_jobs<span class="classifier">optional (default=1)</span></dt><dd><p>The number of jobs to run in parallel for both <cite>fit</cite> and
<cite>predict</cite>. If -1, then the number of jobs is set to the
the number of jobs that can actually run in parallel.</p>
</dd>
<dt>rp_clf_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of outlier detection models to use random projection. The
detector name should be consistent with PyOD.</p>
</dd>
<dt>rp_ng_clf_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of outlier detection models NOT to use random projection. The
detector name should be consistent with PyOD.</p>
</dd>
<dt>rp_flag_global<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If set to False, random projection is turned off for all base models.</p>
</dd>
<dt>target_dim_frac<span class="classifier">float in (0., 1), optional (default=0.5)</span></dt><dd><p>The target compression ratio.</p>
</dd>
<dt>jl_method<span class="classifier">string, optional (default = ‘basic’)</span></dt><dd><p>The JL projection method:</p>
<ul class="simple">
<li><p>“basic”: each component of the transformation matrix is taken at
random in N(0,1).</p></li>
<li><p>“discrete”, each component of the transformation matrix is taken at
random in {-1,1}.</p></li>
<li><p>“circulant”: the first row of the transformation matrix is taken at
random in N(0,1), and each row is obtained from the previous one
by a one-left shift.</p></li>
<li><p>“toeplitz”: the first row and column of the transformation matrix
is taken at random in N(0,1), and each diagonal has a constant value
taken from these first vector.</p></li>
</ul>
</dd>
<dt>bps_flag<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If set to False, balanced parallel scheduling is turned off.</p>
</dd>
<dt>approx_clf_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of outlier detection models to use pseudo-supervised
approximation. The detector name should be consistent with PyOD.</p>
</dd>
<dt>approx_ng_clf_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of outlier detection models NOT to use pseudo-supervised
approximation. The detector name should be consistent with PyOD.</p>
</dd>
<dt>approx_flag_global<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If set to False, pseudo-supervised approximation is turned off.</p>
</dd>
<dt>approx_clf<span class="classifier">object, optional (default: sklearn RandomForestRegressor)</span></dt><dd><p>The supervised model used to approximate unsupervised models.</p>
</dd>
<dt>cost_forecast_loc_fit<span class="classifier">str, optional</span></dt><dd><p>The location of the pretrained cost prediction forecast for training.</p>
</dd>
<dt>cost_forecast_loc_pred<span class="classifier">str, optional</span></dt><dd><p>The location of the pretrained cost prediction forecast for prediction.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=0)</span></dt><dd><p>Controls the verbosity of the building process.</p>
</dd>
</dl>
</section>
<section id="id1004">
<h3>Attributes<a class="headerlink" href="#id1004" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1483"><span class="problematic" id="id1484">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is
fitted.</p>
</dd>
<dt><a href="#id1485"><span class="problematic" id="id1486">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1487"><span class="problematic" id="id1488">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id1005">
<h4>Parameters<a class="headerlink" href="#id1005" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id1006">
<h4>Returns<a class="headerlink" href="#id1006" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/suod.html#SUOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.suod.SUOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detectors.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.</p>
<section id="id1007">
<h4>Parameters<a class="headerlink" href="#id1007" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id1008">
<h4>Returns<a class="headerlink" href="#id1008" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/suod.html#SUOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.suod.SUOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id1009">
<h4>Parameters<a class="headerlink" href="#id1009" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id1010">
<h4>Returns<a class="headerlink" href="#id1010" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id1011">
<h4>Parameters<a class="headerlink" href="#id1011" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id1012">
<h4>Returns<a class="headerlink" href="#id1012" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id1013">
<h4>Parameters<a class="headerlink" href="#id1013" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id1014">
<h4>Returns<a class="headerlink" href="#id1014" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1015">
<h4>Parameters<a class="headerlink" href="#id1015" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id1016">
<h4>Returns<a class="headerlink" href="#id1016" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id1017">
<h4>Parameters<a class="headerlink" href="#id1017" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id1018">
<h4>Returns<a class="headerlink" href="#id1018" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id1019">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id1020">
<h4>Parameters<a class="headerlink" href="#id1020" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id1021">
<h4>Returns<a class="headerlink" href="#id1021" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id1022">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id1023">
<h4>Parameters<a class="headerlink" href="#id1023" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id1024">
<h4>Returns<a class="headerlink" href="#id1024" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id1025">
<h4>Parameters<a class="headerlink" href="#id1025" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id1026">
<h4>Returns<a class="headerlink" href="#id1026" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.suod.SUOD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.suod.SUOD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1027">
<h4>Returns<a class="headerlink" href="#id1027" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="pyod-models-thresholds-module">
<h2>pyod.models.thresholds module<a class="headerlink" href="#pyod-models-thresholds-module" title="Link to this heading">¶</a></h2>
<dl class="py function" id="module-pyod.models.thresholds">
<dt class="sig sig-object py" id="pyod.models.thresholds.AUCP">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">AUCP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#AUCP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.AUCP" title="Link to this definition">¶</a></dt>
<dd><p>AUCP class for Area Under Curve Precentage thresholder.</p>
<p>Use the area under the curve to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond where the auc of the kde is less
than the (mean + abs(mean-median)) percent of the total kde auc.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.BOOT">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">BOOT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#BOOT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.BOOT" title="Link to this definition">¶</a></dt>
<dd><p>BOOT class for Bootstrapping thresholder.</p>
<p>Use a boostrapping based method to find a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the mean of the confidence intervals.</p>
<section id="id1028">
<h3>Parameters<a class="headerlink" href="#id1028" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for bootstrapping a confidence interval. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.CHAU">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">CHAU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#CHAU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.CHAU" title="Link to this definition">¶</a></dt>
<dd><p>CHAU class for Chauvenet’s criterion thresholder.</p>
<p>Use the Chauvenet’s criterion to evaluate a non-parametric
means to threshold scores generated by the decision_scores
where outliers are set to any value below the Chauvenet’s
criterion.</p>
<section id="id1029">
<h3>Parameters<a class="headerlink" href="#id1029" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘mean’, ‘median’, ‘gmean’}, optional (default=’mean’)</span></dt><dd><p>Calculate the area normal to distance using a scaler</p>
<ul class="simple">
<li><p>‘mean’:  Construct a scaler with the mean of the scores</p></li>
<li><p>‘median: Construct a scaler with the median of the scores</p></li>
<li><p>‘gmean’: Construct a scaler with the geometric mean of the scores</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.CLF">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">CLF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#CLF"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.CLF" title="Link to this definition">¶</a></dt>
<dd><p>CLF class for Trained Classifier thresholder.</p>
<p>Use the trained linear classifier to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond 0.</p>
<section id="id1030">
<h3>Parameters<a class="headerlink" href="#id1030" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘simple’, ‘complex’}, optional (default=’complex’)</span></dt><dd><p>Type of linear model</p>
<ul class="simple">
<li><p>‘simple’:  Uses only the scores</p></li>
<li><p>‘complex’: Uses the scores, log of the scores, and the scores’ PDF</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.CLUST">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">CLUST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#CLUST"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.CLUST" title="Link to this definition">¶</a></dt>
<dd><p>CLUST class for clustering type thresholders.</p>
<p>Use the clustering methods to evaluate a non-parametric means to
threshold scores generated by the decision_scores where outliers
are set to any value not labelled as part of the main cluster.</p>
<section id="id1031">
<h3>Parameters<a class="headerlink" href="#id1031" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘agg’, ‘birch’, ‘bang’, ‘bgm’, ‘bsas’, ‘dbscan’, ‘ema’, ‘kmeans’, ‘mbsas’, ‘mshift’, ‘optics’, ‘somsc’, ‘spec’, ‘xmeans’}, optional (default=’spec’)</span></dt><dd><p>Clustering method</p>
<ul class="simple">
<li><p>‘agg’:    Agglomerative</p></li>
<li><p>‘birch’:  Balanced Iterative Reducing and Clustering using Hierarchies</p></li>
<li><p>‘bang’:   BANG</p></li>
<li><p>‘bgm’:    Bayesian Gaussian Mixture</p></li>
<li><p>‘bsas’:   Basic Sequential Algorithmic Scheme</p></li>
<li><p>‘dbscan’: Density-based spatial clustering of applications with noise</p></li>
<li><p>‘ema’:    Expectation-Maximization clustering algorithm for Gaussian Mixture Model</p></li>
<li><p>‘kmeans’: K-means</p></li>
<li><p>‘mbsas’:  Modified Basic Sequential Algorithmic Scheme</p></li>
<li><p>‘mshift’: Mean shift</p></li>
<li><p>‘optics’: Ordering Points To Identify Clustering Structure</p></li>
<li><p>‘somsc’:  Self-organized feature map</p></li>
<li><p>‘spec’:   Clustering to a projection of the normalized Laplacian</p></li>
<li><p>‘xmeans’: X-means</p></li>
</ul>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the BayesianGaussianMixture clustering (method=’bgm’). Can
also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.CPD">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">CPD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#CPD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.CPD" title="Link to this definition">¶</a></dt>
<dd><p>CPD class for Change Point Detection thresholder.</p>
<p>Use change point detection to find a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the detected change point.</p>
<section id="id1032">
<h3>Parameters<a class="headerlink" href="#id1032" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘Dynp’, ‘KernelCPD’, ‘Binseg’, ‘BottomUp’}, optional (default=’Dynp’)</span></dt><dd><p>Method for change point detection</p>
<ul class="simple">
<li><p>‘Dynp’:      Dynamic programming (optimal minimum sum of errors per partition)</p></li>
<li><p>‘KernelCPD’: RBF kernel function (optimal minimum sum of errors per partition)</p></li>
<li><p>‘Binseg’:    Binary segmentation</p></li>
<li><p>‘BottomUp’:  Bottom-up segmentation</p></li>
</ul>
</dd>
<dt>transform<span class="classifier">{‘cdf’, ‘kde’}, optional (default=’cdf’)</span></dt><dd><p>Data transformation method prior to fit</p>
<ul class="simple">
<li><p>‘cdf’: Use the cumulative distribution function</p></li>
<li><p>‘kde’: Use the kernel density estimation</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.DECOMP">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">DECOMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#DECOMP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.DECOMP" title="Link to this definition">¶</a></dt>
<dd><p>DECOMP class for Decomposition based thresholders.</p>
<p>Use decomposition to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the maximum of the decomposed
matrix that results from decomposing the cumulative distribution
function of the decision scores.</p>
<section id="id1033">
<h3>Parameters<a class="headerlink" href="#id1033" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘NMF’, ‘PCA’, ‘GRP’, ‘SRP’}, optional (default=’PCA’)</span></dt><dd><p>Method to use for decomposition</p>
<ul class="simple">
<li><p>‘NMF’:  Non-Negative Matrix Factorization</p></li>
<li><p>‘PCA’:  Principal Component Analysis</p></li>
<li><p>‘GRP’:  Gaussian Random Projection</p></li>
<li><p>‘SRP’:  Sparse Random Projection</p></li>
</ul>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the decomposition algorithm. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.DSN">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">DSN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#DSN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.DSN" title="Link to this definition">¶</a></dt>
<dd><p>DSN class for Distance Shift from Normal thresholder.</p>
<p>Use the distance shift from normal to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the distance calculated by the selected
metric.</p>
<section id="id1034">
<h3>Parameters<a class="headerlink" href="#id1034" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>metric<span class="classifier">{‘JS’, ‘WS’, ‘ENG’, ‘BHT’, ‘HLL’, ‘HI’, ‘LK’, ‘LP’, ‘MAH’, ‘TMT’, ‘RES’, ‘KS’, ‘INT’, ‘MMD’}, optional (default=’MAH’)</span></dt><dd><p>Metric to use for distance computation</p>
<ul class="simple">
<li><p>‘JS’:  Jensen-Shannon distance</p></li>
<li><p>‘WS’:  Wasserstein or Earth Movers distance</p></li>
<li><p>‘ENG’: Energy distance</p></li>
<li><p>‘BHT’: Bhattacharyya distance</p></li>
<li><p>‘HLL’: Hellinger distance</p></li>
<li><p>‘HI’:  Histogram intersection distance</p></li>
<li><p>‘LK’:  Lukaszyk-Karmowski metric for normal distributions</p></li>
<li><p>‘LP’:  Levy-Prokhorov metric</p></li>
<li><p>‘MAH’: Mahalanobis distance</p></li>
<li><p>‘TMT’: Tanimoto distance</p></li>
<li><p>‘RES’: Studentized residual distance</p></li>
<li><p>‘KS’:  Kolmogorov-Smirnov distance</p></li>
<li><p>‘INT’: Weighted spline interpolated distance</p></li>
<li><p>‘MMD’: Maximum Mean Discrepancy distance</p></li>
</ul>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the normal distribution. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.EB">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">EB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#EB"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.EB" title="Link to this definition">¶</a></dt>
<dd><p>EB class for Elliptical Boundary thresholder.</p>
<p>Use pseudo-random elliptical boundaries to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond a pseudo-random elliptical boundary set
between inliers and outliers.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.FGD">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">FGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#FGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.FGD" title="Link to this definition">¶</a></dt>
<dd><p>FGD class for Fixed Gradient Descent thresholder.</p>
<p>Use the fixed gradient descent to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond where the first derivative of the kde
with respect to the decision scores passes the mean of the first 
and second inflection points.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.FILTER">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">FILTER</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#FILTER"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.FILTER" title="Link to this definition">¶</a></dt>
<dd><p>FILTER class for Filtering based thresholders.</p>
<p>Use the filtering based methods to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the maximum filter value.
See <span id="id1035">[<a class="reference internal" href="#id1169" title="Navid Hashemi, Eduardo Verdugo German, Jonatan Pena Ramirez, and Justin Ruths. Filtering approaches for dealing with noise in anomaly detection. In 2019 IEEE 58th Conference on Decision and Control (CDC), 5356–5361. IEEE, December 2019. URL: http://dx.doi.org/10.1109/CDC40024.2019.9029258, doi:10.1109/cdc40024.2019.9029258.">BHGPRR19</a>]</span> for details.</p>
<section id="id1036">
<h3>Parameters<a class="headerlink" href="#id1036" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘gaussian’, ‘savgol’, ‘hilbert’, ‘wiener’, ‘medfilt’, ‘decimate’,’detrend’, ‘resample’}, optional (default=’savgol’)</span></dt><dd><p>Method to filter the scores</p>
<ul class="simple">
<li><p>‘gaussian’: use a gaussian based filter</p></li>
<li><p>‘savgol’:   use the savgol based filter</p></li>
<li><p>‘hilbert’:  use the hilbert based filter</p></li>
<li><p>‘wiener’:   use the wiener based filter</p></li>
<li><p>‘medfilt:   use a median based filter</p></li>
<li><p>‘decimate’: use a decimate based filter</p></li>
<li><p>‘detrend’:  use a detrend based filter</p></li>
<li><p>‘resample’: use a resampling based filter</p></li>
</ul>
</dd>
<dt>sigma<span class="classifier">int, optional (default=’auto’) </span></dt><dd><p>Variable specific to each filter type, default sets sigma to len(scores)*np.std(scores)</p>
<ul class="simple">
<li><p>‘gaussian’: standard deviation for Gaussian kernel</p></li>
<li><p>‘savgol’:   savgol filter window size</p></li>
<li><p>‘hilbert’:  number of Fourier components</p></li>
<li><p>‘medfilt:   kernel size</p></li>
<li><p>‘decimate’: downsampling factor</p></li>
<li><p>‘detrend’:  number of break points</p></li>
<li><p>‘resample’: resampling window size</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.FWFM">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">FWFM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#FWFM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.FWFM" title="Link to this definition">¶</a></dt>
<dd><p>FWFM class for Full Width at Full Minimum thresholder.</p>
<p>Use the full width at full minimum (aka base width) to evaluate
a non-parametric means to threshold scores generated by the
decision_scores where outliers are set to any value beyond the base
width.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.GESD">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">GESD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#GESD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.GESD" title="Link to this definition">¶</a></dt>
<dd><p>GESD class for Generalized Extreme Studentized Deviate thresholder.</p>
<p>Use the generalized extreme studentized deviate to evaluate a
non-parametric means to threshold scores generated by the decision_scores
where outliers are set to any less than the smallest detected outlier.</p>
<section id="id1037">
<h3>Parameters<a class="headerlink" href="#id1037" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>max_outliers<span class="classifier">int, optional (default=’auto’)</span></dt><dd><p>mamiximum number of outliers that the dataset may have. Default sets 
max_outliers to be half the size of the dataset</p>
</dd>
<dt>alpha<span class="classifier">float, optional (default=0.05)</span></dt><dd><p>significance level</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.HIST">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">HIST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#HIST"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.HIST" title="Link to this definition">¶</a></dt>
<dd><p>HIST class for Histogram based thresholders.</p>
<p>Use histograms methods as described in scikit-image.filters to
evaluate a non-parametric means to threshold scores generated by
the decision_scores where outliers are set by histogram generated
thresholds depending on the selected methods.</p>
<section id="id1038">
<h3>Parameters<a class="headerlink" href="#id1038" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>nbins<span class="classifier">int, optional (default=’auto’)</span></dt><dd><p>Number of bins to use in the hostogram, default set to int(len(scores)**0.7)</p>
</dd>
<dt>method<span class="classifier">{‘otsu’, ‘yen’, ‘isodata’, ‘li’, ‘minimum’, ‘triangle’}, optional (default=’triangle’)</span></dt><dd><p>Histogram filtering based method</p>
<ul class="simple">
<li><p>‘otsu’:     OTSU’s method for filtering</p></li>
<li><p>‘yen’:      Yen’s method for filtering</p></li>
<li><p>‘isodata’:  Ridler-Calvard or inter-means method for filtering</p></li>
<li><p>‘li’:       Li’s iterative Minimum Cross Entropy method for filtering</p></li>
<li><p>‘minimum’:  Minimum between two maxima via smoothing method for filtering</p></li>
<li><p>‘triangle’: Triangle algorithm method for filtering</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.IQR">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">IQR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#IQR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.IQR" title="Link to this definition">¶</a></dt>
<dd><p>IQR class for Inter-Qaurtile Region thresholder.</p>
<p>Use the inter-quartile region to evaluate a non-parametric
means to threshold scores generated by the decision_scores
where outliers are set to any value beyond the third quartile
plus 1.5 times the inter-quartile region.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.KARCH">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">KARCH</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#KARCH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.KARCH" title="Link to this definition">¶</a></dt>
<dd><p>KARCH class for Riemannian Center of Mass thresholder.</p>
<p>Use the Karcher mean (Riemannian Center of Mass) to evaluate a
non-parametric means to threshold scores generated by the
decision_scores where outliers are set to any value beyond the
Karcher mean + one standard deviation of the decision_scores.</p>
<section id="id1039">
<h3>Parameters<a class="headerlink" href="#id1039" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>ndim<span class="classifier">int, optional (default=2)</span></dt><dd><p>Number of dimensions to construct the Euclidean manifold</p>
</dd>
<dt>method<span class="classifier">{‘simple’, ‘complex’}, optional (default=’complex’)</span></dt><dd><p>Method for computing the Karcher mean</p>
<ul class="simple">
<li><p>‘simple’:  Compute the Karcher mean using the 1D array of scores</p></li>
<li><p>‘complex’: Compute the Karcher mean between a 2D array dot product of the scores and the sorted scores arrays</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.MAD">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">MAD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#MAD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.MAD" title="Link to this definition">¶</a></dt>
<dd><p>MAD class for Median Absolute Deviation thresholder.</p>
<p>Use the median absolute deviation to evaluate a non-parametric
means to threshold scores generated by the decision_scores
where outliers are set to any value beyond the mean plus the 
median absolute deviation over the standard deviation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.MCST">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">MCST</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#MCST"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.MCST" title="Link to this definition">¶</a></dt>
<dd><p>MCST class for Monte Carlo Shapiro Tests thresholder.</p>
<p>Use uniform random sampling and statstical testing to evaluate a
non-parametric means to threshold scores generated by the decision_scores
where outliers are set to any value beyond the minimum value left after
iterative Shapiro-Wilk tests have occured. Note** accuracy decreases with
array size. For good results the should be array&lt;1000. However still this
threshold method may fail at any array size.</p>
<section id="id1040">
<h3>Parameters<a class="headerlink" href="#id1040" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the uniform distribution. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.META">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">META</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#META"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.META" title="Link to this definition">¶</a></dt>
<dd><p>META class for Meta-modelling thresholder.</p>
<p>Use a trained meta-model to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set based on the trained meta-model classifier.</p>
<section id="id1041">
<h3>Parameters<a class="headerlink" href="#id1041" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘LIN’, ‘GNB’, ‘GNBC’, ‘GNBM’}, optional (default=’GNBM’)</span></dt><dd><p>select</p>
<ul class="simple">
<li><p>‘LIN’:  RidgeCV trained linear classifier meta-model on true labels</p></li>
<li><p>‘GNB’:  Gaussian Naive Bayes trained classifier meta-model on true labels</p></li>
<li><p>‘GNBC’: Gaussian Naive Bayes trained classifier meta-model on best contamination</p></li>
<li><p>‘GNBM’: Gaussian Naive Bayes multivariate trained classifier meta-model</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.MOLL">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">MOLL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#MOLL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.MOLL" title="Link to this definition">¶</a></dt>
<dd><p>MOLL class for Friedrichs’ mollifier thresholder.</p>
<p>Use the Friedrichs’ mollifier to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond one minus the  maximum of the smoothed
dataset via convolution.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.MTT">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">MTT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#MTT"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.MTT" title="Link to this definition">¶</a></dt>
<dd><p>MTT class for Modified Thompson Tau test thresholder.</p>
<p>Use the modified Thompson Tau test to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the smallest outlier detected by the test.</p>
<section id="id1042">
<h3>Parameters<a class="headerlink" href="#id1042" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>strictness<span class="classifier">[1,2,3,4,5], optional (default=4)</span></dt><dd><p>Level of strictness corresponding to the t-Student distribution map to sample</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.OCSVM">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">OCSVM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#OCSVM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.OCSVM" title="Link to this definition">¶</a></dt>
<dd><p>OCSVM class for One-Class Support Vector Machine thresholder.</p>
<p>Use a one-class svm to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are determined by the one-class svm using a polynomial kernel
with the polynomial degree either set or determined by regression
internally.</p>
<section id="id1043">
<h3>Parameters<a class="headerlink" href="#id1043" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">{‘poly’, ‘sgd’}, optional (default=’sgd’)</span></dt><dd><p>OCSVM model to apply</p>
<ul class="simple">
<li><p>‘poly’:  Use a polynomial kernel with a regular OCSVM</p></li>
<li><p>‘sgd’:   Used the Additive Chi2 kernel approximation with a SGDOneClassSVM</p></li>
</ul>
</dd>
<dt>degree<span class="classifier">int, optional (default=’auto’)</span></dt><dd><p>Polynomial degree to use for the one-class svm.
Default ‘auto’ finds the optimal degree with linear regression</p>
</dd>
<dt>gamma<span class="classifier">float, optional (default=’auto’)</span></dt><dd><p>Kernel coefficient for polynomial fit for the one-class svm.
Default ‘auto’ uses 1 / n_features</p>
</dd>
<dt>criterion<span class="classifier">{‘aic’, ‘bic’}, optional (default=’bic’)</span></dt><dd><p>regression performance metric. AIC is the Akaike Information Criterion,
and BIC is the Bayesian Information Criterion. This only applies
when degree is set to ‘auto’</p>
</dd>
<dt>nu<span class="classifier">float, optional (default=’auto’)</span></dt><dd><p>An upper bound on the fraction of training errors and a lower bound
of the fraction of support vectors. Default ‘auto’ sets nu as the ratio
between the any point that is less than or equal to the median plus
the absolute difference between the mean and geometric mean over the
the number of points in the entire dataset</p>
</dd>
<dt>tol<span class="classifier">float, optional (default=1e-3)</span></dt><dd><p>The stopping criterion for the one-class svm</p>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the SVM’s data sampling. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.QMCD">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">QMCD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#QMCD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.QMCD" title="Link to this definition">¶</a></dt>
<dd><p>QMCD class for Quasi-Monte Carlo Discreprancy thresholder.</p>
<p>Use the quasi-Monte Carlo discreprancy to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond and percentile or quantile of one minus the
descreperancy (Note** A discrepancy quantifies the distance between the
continuous uniform distribution on a hypercube and the discrete uniform
distribution on distinct sample points).</p>
<section id="id1044">
<h3>Parameters<a class="headerlink" href="#id1044" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘CD’, ‘WD’, ‘MD’, ‘L2-star’}, optional (default=’WD’)</span></dt><dd><p>Type of discrepancy</p>
<ul class="simple">
<li><p>‘CD’:      Centered Discrepancy</p></li>
<li><p>‘WD’:      Wrap-around Discrepancy</p></li>
<li><p>‘MD’:      Mix between CD/WD</p></li>
<li><p>‘L2-star’: L2-star discrepancy</p></li>
</ul>
</dd>
<dt>lim<span class="classifier">{‘Q’, ‘P’}, optional (default=’P’)</span></dt><dd><p>Filtering method to threshold scores using 1 - discrepancy</p>
<ul class="simple">
<li><p>‘Q’: Use quntile limiting</p></li>
<li><p>‘P’: Use percentile limiting</p></li>
</ul>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.REGR">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">REGR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#REGR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.REGR" title="Link to this definition">¶</a></dt>
<dd><p>REGR class for Regression based thresholder.</p>
<p>Use the regression to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the y-intercept value of the linear fit.</p>
<section id="id1045">
<h3>Parameters<a class="headerlink" href="#id1045" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>method<span class="classifier">{‘siegel’, ‘theil’}, optional (default=’siegel’)</span></dt><dd><p>Regression based method to calculate the y-intercept</p>
<ul class="simple">
<li><p>‘siegel’: implements a method for robust linear regression using repeated medians</p></li>
<li><p>‘theil’:  implements a method for robust linear regression using paired values</p></li>
</ul>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>random seed for the normal distribution. Can also be set to None</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.VAE">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">VAE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#VAE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.VAE" title="Link to this definition">¶</a></dt>
<dd><p>VAE class for Variational AutoEncoder thresholder.</p>
<p>Use a VAE to evaluate a non-parametric means
to threshold scores generated by the decision_scores where outliers
are set to any value beyond the maximum minus the minimum of the
reconstructed distribution probabilities after encoding.</p>
<section id="id1046">
<h3>Parameters<a class="headerlink" href="#id1046" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>verbose<span class="classifier">bool, optional (default=False)</span></dt><dd><p>display training progress</p>
</dd>
<dt>device<span class="classifier">str, optional (default=’cpu’)</span></dt><dd><p>device for pytorch</p>
</dd>
<dt>latent_dims<span class="classifier">int, optional (default=’auto’)</span></dt><dd><p>number of latent dimensions the encoder will map the scores to.
Default ‘auto’ applies automatic dimensionality selection using
a profile likelihood.</p>
</dd>
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>random seed for the normal distribution. Can also be set to None</p>
</dd>
<dt>epochs<span class="classifier">int, optional (default=100)</span></dt><dd><p>number of epochs to train the VAE</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=64)</span></dt><dd><p>batch size for the dataloader during training</p>
</dd>
<dt>loss<span class="classifier">str, optional (default=’kl’)</span></dt><dd><p>Loss function during training</p>
<ul class="simple">
<li><p>‘kl’ : use the combined negative log likelihood and Kullback-Leibler divergence</p></li>
<li><p>‘mmd’: use the combined negative log likelihood and maximum mean discrepancy</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id1047">
<h3>Attributes<a class="headerlink" href="#id1047" title="Link to this heading">¶</a></h3>
<p><a href="#id1489"><span class="problematic" id="id1490">thresh_</span></a> : threshold value that separates inliers from outliers</p>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.WIND">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">WIND</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#WIND"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.WIND" title="Link to this definition">¶</a></dt>
<dd><p>WIND class for topological Winding number thresholder.</p>
<p>Use the topological winding number (with respect to the origin) to
evaluate a non-parametric means to threshold scores generated by
the decision_scores where outliers are set to any value beyond the
mean intersection point calculated from the winding number.</p>
<section id="id1048">
<h3>Parameters<a class="headerlink" href="#id1048" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>random_state<span class="classifier">int, optional (default=1234)</span></dt><dd><p>Random seed for the normal distribution. Can also be set to None.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.YJ">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">YJ</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#YJ"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.YJ" title="Link to this definition">¶</a></dt>
<dd><p>YJ class for Yeo-Johnson transformation thresholder.</p>
<p>Use the Yeo-Johnson transformation to evaluate
a non-parametric means to threshold scores generated by the
decision_scores where outliers are set to any value beyond the
max value in the YJ transformed data.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pyod.models.thresholds.ZSCORE">
<span class="sig-prename descclassname"><span class="pre">pyod.models.thresholds.</span></span><span class="sig-name descname"><span class="pre">ZSCORE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/thresholds.html#ZSCORE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.thresholds.ZSCORE" title="Link to this definition">¶</a></dt>
<dd><p>ZSCORE class for ZSCORE thresholder.</p>
<p>Use the zscore to evaluate a non-parametric means to threshold
scores generated by the decision_scores where outliers are set
to any value beyond a zscore of one.</p>
</dd></dl>

</section>
<section id="module-pyod.models.vae">
<span id="pyod-models-vae-module"></span><h2>pyod.models.vae module<a class="headerlink" href="#module-pyod.models.vae" title="Link to this heading">¶</a></h2>
<p>Variational Auto Encoder (VAE)
and beta-VAE for Unsupervised Outlier Detection</p>
<dl>
<dt>Reference:</dt><dd><p><span id="id1049">[<a class="reference internal" href="#id1144" title="Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.">BKW13</a>]</span> Kingma, Diederik, Welling
‘Auto-Encodeing Variational Bayes’
<a class="reference external" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></p>
<p><span id="id1050">[<a class="reference internal" href="#id1146" title="Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.">BBHP+18</a>]</span> Burges et al
‘Understanding disentangling in beta-VAE’
<a class="reference external" href="https://arxiv.org/pdf/1804.03599.pdf">https://arxiv.org/pdf/1804.03599.pdf</a></p>
</dd>
</dl>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.vae.VAE">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.vae.</span></span><span class="sig-name descname"><span class="pre">VAE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocessing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_compile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">{'weight_decay':</span> <span class="pre">1e-05}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capacity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_neuron_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[128,</span> <span class="pre">64,</span> <span class="pre">32]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_neuron_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[32,</span> <span class="pre">64,</span> <span class="pre">128]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">latent_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_activation_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sigmoid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/vae.html#VAE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.vae.VAE" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDeepLearningDetector</span></code></p>
<p>Variational auto encoder
Encoder maps X onto a latent space Z
Decoder samples Z from N(0,1)
VAE_loss = Reconstruction_loss + KL_loss</p>
<p>Reference
See <span id="id1051">[<a class="reference internal" href="#id1144" title="Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.">BKW13</a>]</span> Kingma, Diederik, Welling
‘Auto-Encodeing Variational Bayes’
<a class="reference external" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a> for details.</p>
<p>beta VAE
In Loss, the emphasis is on KL_loss
and capacity of a bottleneck:
VAE_loss = Reconstruction_loss + beta * KL_loss</p>
<p>Reference
See <span id="id1052">[<a class="reference internal" href="#id1146" title="Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.">BBHP+18</a>]</span> Burges et al
‘Understanding disentangling in beta-VAE’
<a class="reference external" href="https://arxiv.org/pdf/1804.03599.pdf">https://arxiv.org/pdf/1804.03599.pdf</a> for details.</p>
<section id="id1053">
<h3>Parameters<a class="headerlink" href="#id1053" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>contamination<span class="classifier">float in (0., 0.5), optional (default=0.1)</span></dt><dd><p>The amount of contamination of the data set, 
i.e. the proportion of outliers in the data set. 
Used when fitting to define the threshold on the decision function.</p>
</dd>
<dt>preprocessing<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, apply the preprocessing procedure before training models.</p>
</dd>
<dt>lr<span class="classifier">float, optional (default=1e-3)</span></dt><dd><p>The initial learning rate for the optimizer.</p>
</dd>
<dt>epoch_num<span class="classifier">int, optional (default=30)</span></dt><dd><p>The number of epochs for training.</p>
</dd>
<dt>batch_size<span class="classifier">int, optional (default=32)</span></dt><dd><p>The batch size for training.</p>
</dd>
<dt>optimizer_name<span class="classifier">str, optional (default=’adam’)</span></dt><dd><p>The name of theoptimizer used to train the model.</p>
</dd>
<dt>device<span class="classifier">str, optional (default=None)</span></dt><dd><p>The device to use for the model. If None, it will be decided
automatically. If you want to use MPS, set it to ‘mps’.</p>
</dd>
<dt>random_state<span class="classifier">int, optional (default=42)</span></dt><dd><p>The random seed for reproducibility.</p>
</dd>
<dt>use_compile<span class="classifier">bool, optional (default=False)</span></dt><dd><p>Whether to compile the model.
If True, the model will be compiled before training.
This is only available for
PyTorch version &gt;= 2.0.0. and Python &lt; 3.12.</p>
</dd>
<dt>compile_mode<span class="classifier">str, optional (default=’default’)</span></dt><dd><p>The mode to compile the model.
Can be either “default”, “reduce-overhead”,
“max-autotune” or “max-autotune-no-cudagraphs”.
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile">https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile</a> for details.</p>
</dd>
<dt>verbose<span class="classifier">int, optional (default=1)</span></dt><dd><p>Verbosity mode.
- 0 = silent
- 1 = progress bar
- 2 = one line per epoch.</p>
</dd>
<dt>optimizer_params<span class="classifier">dict, optional (default={‘weight_decay’: 1e-5})</span></dt><dd><p>Additional parameters for the optimizer.
For example, <cite>optimizer_params={‘weight_decay’: 1e-5}</cite>.</p>
</dd>
<dt>beta<span class="classifier">float, optional (default=1.0)</span></dt><dd><p>Coefficient of beta VAE. The weight of KL divergence.
Default is regular VAE.</p>
</dd>
<dt>capacity<span class="classifier">float, optional (default=0.0)</span></dt><dd><p>The maximum capacity of a loss bottleneck.</p>
</dd>
<dt>encoder_neuron_list<span class="classifier">list, optional (default=[128, 64, 32])</span></dt><dd><p>The number of neurons per hidden layers in encoder.
So the encoder has the structure as [feature_size, 128, 64, 32, latent_dim].</p>
</dd>
<dt>decoder_neuron_list<span class="classifier">list, optional (default=[32, 64, 128])</span></dt><dd><p>The number of neurons per hidden layers in decoder.
So the decoder has the structure as [latent_dim, 32, 64, 128, feature_size].</p>
</dd>
<dt>latent_dim<span class="classifier">int, optional (default=2)</span></dt><dd><p>The dimension of latent space.</p>
</dd>
<dt>hidden_activation_name<span class="classifier">str, optional (default=’relu’)</span></dt><dd><p>The activation function used in hidden layers.</p>
</dd>
<dt>output_activation_name<span class="classifier">str, optional (default=’sigmoid’)</span></dt><dd><p>The activation function used in output layer.</p>
</dd>
<dt>batch_norm<span class="classifier">boolean, optional (default=False)</span></dt><dd><p>Whether to apply Batch Normalization,
See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html</a></p>
</dd>
<dt>dropout_rate<span class="classifier">float in (0., 1), optional (default=0.2)</span></dt><dd><p>The dropout to be used across all layers.</p>
</dd>
</dl>
</section>
<section id="id1054">
<h3>Attributes<a class="headerlink" href="#id1054" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>model<span class="classifier">torch.nn.Module</span></dt><dd><p>The underlying VAE model.</p>
</dd>
<dt>optimizer<span class="classifier">torch.optim</span></dt><dd><p>The optimizer used to train the model.</p>
</dd>
<dt>criterion<span class="classifier">python function</span></dt><dd><p>The loss function used to train the model.</p>
</dd>
<dt><a href="#id1491"><span class="problematic" id="id1492">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1493"><span class="problematic" id="id1494">threshold_</span></a><span class="classifier">float</span></dt><dd><p>The threshold is based on <code class="docutils literal notranslate"><span class="pre">contamination</span></code>. It is the
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">*</span> <span class="pre">contamination</span></code> most abnormal samples in
<code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>. The threshold is calculated for generating
binary outlier labels.</p>
</dd>
<dt><a href="#id1495"><span class="problematic" id="id1496">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.build_model">
<span class="sig-name descname"><span class="pre">build_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/vae.html#VAE.build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.vae.VAE.build_model" title="Link to this definition">¶</a></dt>
<dd><p>Need to define model in this method.
self.feature_size is the number of features in the input data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id1055">
<h4>Parameters<a class="headerlink" href="#id1055" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id1056">
<h4>Returns<a class="headerlink" href="#id1056" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly score of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on different
detector algorithms. For consistency, outliers are assigned with
larger anomaly scores.
Parameters
———-
X : numpy array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>The training input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</div></blockquote>
<dl class="simple">
<dt>batch_size<span class="classifier">int, optional (default=None)</span></dt><dd><p>The batch size for processing the input samples.
If not specified, the default batch size is used.</p>
</dd>
</dl>
<section id="id1057">
<h4>Returns<a class="headerlink" href="#id1057" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.decision_function_update">
<span class="sig-name descname"><span class="pre">decision_function_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">anomaly_scores</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.decision_function_update" title="Link to this definition">¶</a></dt>
<dd><p>For any additional operations after each decision function call.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.epoch_update">
<span class="sig-name descname"><span class="pre">epoch_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.epoch_update" title="Link to this definition">¶</a></dt>
<dd><p>For any additional operations after each epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_loader</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.evaluate" title="Link to this definition">¶</a></dt>
<dd><p>Evaluate the deep learning model.</p>
<section id="id1058">
<h4>Parameters<a class="headerlink" href="#id1058" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>data_loader<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>The data loader for evaluating the model.</p>
</dd>
</dl>
</section>
<section id="id1059">
<h4>Returns<a class="headerlink" href="#id1059" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.evaluating_forward">
<span class="sig-name descname"><span class="pre">evaluating_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/vae.html#VAE.evaluating_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.vae.VAE.evaluating_forward" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass for evaluating the model.
Abstract method to be implemented.</p>
<section id="id1060">
<h4>Parameters<a class="headerlink" href="#id1060" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>batch_data<span class="classifier">tuple</span></dt><dd><p>The batch data for evaluating the model.</p>
</dd>
</dl>
</section>
<section id="id1061">
<h4>Returns<a class="headerlink" href="#id1061" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>output<span class="classifier">numpy array</span></dt><dd><p>The output of the model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.evaluating_prepare">
<span class="sig-name descname"><span class="pre">evaluating_prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.evaluating_prepare" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector. y is ignored in unsupervised methods.</p>
<section id="id1062">
<h4>Parameters<a class="headerlink" href="#id1062" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">numpy array of shape (n_samples,), optional (default=None)</span></dt><dd><p>The ground truth of input samples. Not used in unsupervised methods.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id1063">
<h4>Parameters<a class="headerlink" href="#id1063" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id1064">
<h4>Returns<a class="headerlink" href="#id1064" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id1065">
<h4>Parameters<a class="headerlink" href="#id1065" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id1066">
<h4>Returns<a class="headerlink" href="#id1066" title="Link to this heading">¶</a></h4>
<p>score : float</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict_score</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency. Scoring could be done by
calling an evaluation method, e.g., AUC ROC.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1067">
<h4>Parameters<a class="headerlink" href="#id1067" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id1068">
<h4>Returns<a class="headerlink" href="#id1068" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.load">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.load" title="Link to this definition">¶</a></dt>
<dd><p>Load the model from the specified path.</p>
<section id="id1069">
<h4>Parameters<a class="headerlink" href="#id1069" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>path<span class="classifier">str</span></dt><dd><p>The path to load the model.</p>
</dd>
</dl>
</section>
<section id="id1070">
<h4>Returns<a class="headerlink" href="#id1070" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>model<span class="classifier">BaseDeepLearningDetector</span></dt><dd><p>The loaded model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<section id="id1071">
<h4>Parameters<a class="headerlink" href="#id1071" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id1072">
<h4>Returns<a class="headerlink" href="#id1072" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
<dt>confidence<span class="classifier">numpy array of shape (n_samples,).</span></dt><dd><p>Only if return_confidence is set to True.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id1073">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id1074">
<h4>Parameters<a class="headerlink" href="#id1074" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id1075">
<h4>Returns<a class="headerlink" href="#id1075" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_confidence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier. Two approaches
are possible:</p>
<ol class="arabic simple">
<li><p>simply use Min-max conversion to linearly transform the outlier
scores into the range of [0,1]. The model must be
fitted first.</p></li>
<li><p>use unifying scores, see <span id="id1076">[<a class="reference internal" href="#id1114" title="Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13–24. SIAM, 2011.">BKKSZ11</a>]</span>.</p></li>
</ol>
<section id="id1077">
<h4>Parameters<a class="headerlink" href="#id1077" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>method<span class="classifier">str, optional (default=’linear’)</span></dt><dd><p>probability conversion method. It must be one of
‘linear’ or ‘unify’.</p>
</dd>
<dt>return_confidence<span class="classifier">boolean, optional(default=False)</span></dt><dd><p>If True, also return the confidence of prediction.</p>
</dd>
</dl>
</section>
<section id="id1078">
<h4>Returns<a class="headerlink" href="#id1078" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_probability<span class="classifier">numpy array of shape (n_samples, n_classes)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1]. Note it depends on the number of classes, which is by
default 2 classes ([proba of normal, proba of outliers]).</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id1079">
<h4>Parameters<a class="headerlink" href="#id1079" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id1080">
<h4>Returns<a class="headerlink" href="#id1080" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.save" title="Link to this definition">¶</a></dt>
<dd><p>Save the model to the specified path.</p>
<section id="id1081">
<h4>Parameters<a class="headerlink" href="#id1081" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>path<span class="classifier">str</span></dt><dd><p>The path to save the model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1082">
<h4>Returns<a class="headerlink" href="#id1082" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_loader</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.train" title="Link to this definition">¶</a></dt>
<dd><p>Train the deep learning model.</p>
<section id="id1083">
<h4>Parameters<a class="headerlink" href="#id1083" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>train_loader<span class="classifier">torch.utils.data.DataLoader</span></dt><dd><p>The data loader for training the model.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.training_forward">
<span class="sig-name descname"><span class="pre">training_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/vae.html#VAE.training_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.vae.VAE.training_forward" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass for training the model.
Abstract method to be implemented.</p>
<section id="id1084">
<h4>Parameters<a class="headerlink" href="#id1084" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>batch_data<span class="classifier">tuple</span></dt><dd><p>The batch data for training the model.</p>
</dd>
</dl>
</section>
<section id="id1085">
<h4>Returns<a class="headerlink" href="#id1085" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>loss<span class="classifier">float or tuple of float</span></dt><dd><p>The loss.item of the model, or a tuple of loss.item 
if there are multiple losses.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.vae.VAE.training_prepare">
<span class="sig-name descname"><span class="pre">training_prepare</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.vae.VAE.training_prepare" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models.xgbod">
<span id="pyod-models-xgbod-module"></span><h2>pyod.models.xgbod module<a class="headerlink" href="#module-pyod.models.xgbod" title="Link to this heading">¶</a></h2>
<p>XGBOD: Improving Supervised Outlier Detection with Unsupervised
Representation Learning. A semi-supervised outlier detection framework.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyod.models.xgbod.</span></span><span class="sig-name descname"><span class="pre">XGBOD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">standardization_flag_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary:logistic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">booster</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gbtree'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nthread</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_child_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_delta_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colsample_bytree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colsample_bylevel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reg_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reg_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_pos_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api_cc.html#pyod.models.base.BaseDetector" title="pyod.models.base.BaseDetector"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseDetector</span></code></a></p>
<p>XGBOD class for outlier detection.
It first uses the passed in unsupervised outlier detectors to extract
richer representation of the data and then concatenates the newly
generated features to the original feature for constructing the augmented
feature space. An XGBoost classifier is then applied on this augmented
feature space. Read more in the <span id="id1086">[<a class="reference internal" href="#id1124" title="Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks (IJCNN). IEEE, 2018.">BZH18</a>]</span>.</p>
<section id="id1087">
<h3>Parameters<a class="headerlink" href="#id1087" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt>estimator_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of pyod detectors passed in for unsupervised learning</p>
</dd>
<dt>standardization_flag_list<span class="classifier">list, optional (default=None)</span></dt><dd><p>The list of boolean flags for indicating whether to perform
standardization for each detector.</p>
</dd>
<dt>max_depth<span class="classifier">int</span></dt><dd><p>Maximum tree depth for base learners.</p>
</dd>
<dt>learning_rate<span class="classifier">float</span></dt><dd><p>Boosting learning rate (xgb’s “eta”)</p>
</dd>
<dt>n_estimators<span class="classifier">int</span></dt><dd><p>Number of boosted trees to fit.</p>
</dd>
<dt>silent<span class="classifier">bool</span></dt><dd><p>Whether to print messages while running boosting.</p>
</dd>
<dt>objective<span class="classifier">string or callable</span></dt><dd><p>Specify the learning task and the corresponding learning objective or
a custom objective function to be used (see note below).</p>
</dd>
<dt>booster<span class="classifier">string</span></dt><dd><p>Specify which booster to use: gbtree, gblinear or dart.</p>
</dd>
<dt>n_jobs<span class="classifier">int</span></dt><dd><p>Number of parallel threads used to run xgboost.  (replaces <code class="docutils literal notranslate"><span class="pre">nthread</span></code>)</p>
</dd>
<dt>gamma<span class="classifier">float</span></dt><dd><p>Minimum loss reduction required to make a further partition on a leaf
node of the tree.</p>
</dd>
<dt>min_child_weight<span class="classifier">int</span></dt><dd><p>Minimum sum of instance weight(hessian) needed in a child.</p>
</dd>
<dt>max_delta_step<span class="classifier">int</span></dt><dd><p>Maximum delta step we allow each tree’s weight estimation to be.</p>
</dd>
<dt>subsample<span class="classifier">float</span></dt><dd><p>Subsample ratio of the training instance.</p>
</dd>
<dt>colsample_bytree<span class="classifier">float</span></dt><dd><p>Subsample ratio of columns when constructing each tree.</p>
</dd>
<dt>colsample_bylevel<span class="classifier">float</span></dt><dd><p>Subsample ratio of columns for each split, in each level.</p>
</dd>
<dt>reg_alpha<span class="classifier">float (xgb’s alpha)</span></dt><dd><p>L1 regularization term on weights.</p>
</dd>
<dt>reg_lambda<span class="classifier">float (xgb’s lambda)</span></dt><dd><p>L2 regularization term on weights.</p>
</dd>
<dt>scale_pos_weight<span class="classifier">float</span></dt><dd><p>Balancing of positive and negative weights.</p>
</dd>
<dt>base_score:</dt><dd><p>The initial prediction score of all instances, global bias.</p>
</dd>
<dt>random_state<span class="classifier">int</span></dt><dd><p>Random number seed.  (replaces seed)</p>
</dd>
</dl>
<p># missing : float, optional
#     Value in the data which needs to be present as a missing value. If
#     None, defaults to np.nan.</p>
<dl>
<dt>importance_type: string, default “gain”</dt><dd><p>The feature importance type for the <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code>
property: either “gain”,
“weight”, “cover”, “total_gain” or “total_cover”.</p>
</dd>
<dt>**kwargs<span class="classifier">dict, optional</span></dt><dd><p>Keyword arguments for XGBoost Booster object.  Full documentation of
parameters can be found here:
<a class="reference external" href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst">https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst</a>.
Attempting to set a parameter via the constructor args and **kwargs
dict simultaneously will result in a TypeError.</p>
<p>Note: **kwargs is unsupported by scikit-learn. We do not
guarantee that parameters passed via this argument will interact
properly with scikit-learn.</p>
</dd>
</dl>
</section>
<section id="id1088">
<h3>Attributes<a class="headerlink" href="#id1088" title="Link to this heading">¶</a></h3>
<dl class="simple">
<dt><a href="#id1497"><span class="problematic" id="id1498">n_detector_</span></a><span class="classifier">int</span></dt><dd><p>The number of unsupervised of detectors used.</p>
</dd>
<dt><a href="#id1499"><span class="problematic" id="id1500">clf_</span></a><span class="classifier">object</span></dt><dd><p>The XGBoost classifier.</p>
</dd>
<dt><a href="#id1501"><span class="problematic" id="id1502">decision_scores_</span></a><span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The outlier scores of the training data.
The higher, the more abnormal. Outliers tend to have higher
scores. This value is available once the detector is fitted.</p>
</dd>
<dt><a href="#id1503"><span class="problematic" id="id1504">labels_</span></a><span class="classifier">int, either 0 or 1</span></dt><dd><p>The binary labels of the training data. 0 stands for inliers
and 1 for outliers/anomalies. It is generated by applying
<code class="docutils literal notranslate"><span class="pre">threshold_</span></code> on <code class="docutils literal notranslate"><span class="pre">decision_scores_</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.compute_rejection_stats">
<span class="sig-name descname"><span class="pre">compute_rejection_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.xgbod.XGBOD.compute_rejection_stats" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Add reject option into the unsupervised detector. </dt><dd><p>This comes with guarantees: an estimate of the expected
rejection rate (return_rejectrate=True), an upper
bound of the rejection rate (return_ub_rejectrate= True),
and an upper bound on the cost (return_ub_cost=True).</p>
</dd>
</dl>
<section id="id1089">
<h4>Parameters<a class="headerlink" href="#id1089" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>T: int, optional(default=32)</dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive),</dt><dd><p>optional (default = [1,1, contamination])
costs for false positive predictions (c_fp),
false negative predictions (c_fn) and rejections (c_r).</p>
</dd>
<dt>verbose: bool, optional (default = False)</dt><dd><p>If true, it prints the expected rejection rate, the upper
bound rejection rate, and the upper bound of the cost.</p>
</dd>
</dl>
</section>
<section id="id1090">
<h4>Returns<a class="headerlink" href="#id1090" title="Link to this heading">¶</a></h4>
<p>expected_rejection_rate:   float, the expected rejection rate;
upperbound_rejection_rate: float, the upper bound for the rejection rate</p>
<blockquote>
<div><p>satisfied with probability 1-delta;</p>
</div></blockquote>
<p>upperbound_cost:           float, the upper bound for the cost;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.decision_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.decision_function" title="Link to this definition">¶</a></dt>
<dd><p>Predict raw anomaly scores of X using the fitted detector.</p>
<p>The anomaly score of an input sample is computed based on the fitted
detector. For consistency, outliers are assigned with
higher anomaly scores.</p>
<section id="id1091">
<h4>Parameters<a class="headerlink" href="#id1091" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Sparse matrices are accepted only
if they are supported by the base estimator.</p>
</dd>
</dl>
</section>
<section id="id1092">
<h4>Returns<a class="headerlink" href="#id1092" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>anomaly_scores<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The anomaly score of the input samples.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit the model using X and y as training data.</p>
<section id="id1093">
<h4>Parameters<a class="headerlink" href="#id1093" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>Training data.</p>
</dd>
<dt>y<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>The ground truth (binary label)</p>
<ul class="simple">
<li><p>0 : inliers</p></li>
<li><p>1 : outliers</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id1094">
<h4>Returns<a class="headerlink" href="#id1094" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.fit_predict">
<span class="sig-name descname"><span class="pre">fit_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.fit_predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.fit_predict" title="Link to this definition">¶</a></dt>
<dd><p>Fit detector first and then predict whether a particular sample
is an outlier or not. y is ignored in unsupervised models.</p>
<section id="id1095">
<h4>Parameters<a class="headerlink" href="#id1095" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
</dl>
</section>
<section id="id1096">
<h4>Returns<a class="headerlink" href="#id1096" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.6.9: </span><cite>fit_predict</cite> will be removed in pyod 0.8.0.; it will be
replaced by calling <cite>fit</cite> function first and then accessing
<cite>labels_</cite> attribute for consistency.</p>
</div>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.fit_predict_score">
<span class="sig-name descname"><span class="pre">fit_predict_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'roc_auc_score'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.fit_predict_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.fit_predict_score" title="Link to this definition">¶</a></dt>
<dd><p>Fit the detector, predict on samples, and evaluate the model by
predefined metrics, e.g., ROC.</p>
<section id="id1097">
<h4>Parameters<a class="headerlink" href="#id1097" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>y<span class="classifier">Ignored</span></dt><dd><p>Not used, present for API consistency by convention.</p>
</dd>
<dt>scoring<span class="classifier">str, optional (default=’roc_auc_score’)</span></dt><dd><p>Evaluation metric:</p>
<ul class="simple">
<li><p>‘roc_auc_score’: ROC score</p></li>
<li><p>‘prc_n_score’: Precision @ rank n score</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id1098">
<h4>Returns<a class="headerlink" href="#id1098" title="Link to this heading">¶</a></h4>
<p>score : float</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.xgbod.XGBOD.get_params" title="Link to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1099">
<h4>Parameters<a class="headerlink" href="#id1099" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>deep<span class="classifier">bool, optional (default=True)</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
</section>
<section id="id1100">
<h4>Returns<a class="headerlink" href="#id1100" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.
Calling xgboost <cite>predict</cite> function.</p>
<section id="id1101">
<h4>Parameters<a class="headerlink" href="#id1101" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id1102">
<h4>Returns<a class="headerlink" href="#id1102" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. 0 stands for inliers and 1 for outliers.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.predict_confidence">
<span class="sig-name descname"><span class="pre">predict_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.xgbod.XGBOD.predict_confidence" title="Link to this definition">¶</a></dt>
<dd><p>Predict the model’s confidence in making the same prediction
under slightly different training sets.
See <span id="id1103">[<a class="reference internal" href="#id1153" title="Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227–243. Springer, 2020.">BPVD20</a>]</span>.</p>
<section id="id1104">
<h4>Parameters<a class="headerlink" href="#id1104" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id1105">
<h4>Returns<a class="headerlink" href="#id1105" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>confidence<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells how consistently the model would
make the same prediction if the training set was perturbed.
Return a probability, ranging in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pyod/models/xgbod.html#XGBOD.predict_proba"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pyod.models.xgbod.XGBOD.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predict the probability of a sample being outlier.
Calling xgboost <cite>predict_proba</cite> function.</p>
<section id="id1106">
<h4>Parameters<a class="headerlink" href="#id1106" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
</dl>
</section>
<section id="id1107">
<h4>Returns<a class="headerlink" href="#id1107" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, tells whether or not
it should be considered as an outlier according to the
fitted model. Return the outlier probability, ranging
in [0,1].</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.predict_with_rejection">
<span class="sig-name descname"><span class="pre">predict_with_rejection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_stats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_r</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.xgbod.XGBOD.predict_with_rejection" title="Link to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Predict if a particular sample is an outlier or not, </dt><dd><p>allowing the detector to reject (i.e., output = -2) 
low confidence predictions.</p>
</dd>
</dl>
<section id="id1108">
<h4>Parameters<a class="headerlink" href="#id1108" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>X<span class="classifier">numpy array of shape (n_samples, n_features)</span></dt><dd><p>The input samples.</p>
</dd>
<dt>T<span class="classifier">int, optional(default=32)</span></dt><dd><p>It allows to set the rejection threshold to 1-2exp(-T).
The higher the value of T, the more rejections are made.</p>
</dd>
<dt>return_stats: bool, optional (default = False)</dt><dd><p>If true, it returns also three additional float values:
the estimated rejection rate, the upper bound rejection
rate, and the upper bound of the cost.</p>
</dd>
<dt>delta: float, optional (default = 0.1)</dt><dd><p>The upper bound rejection rate holds with probability 1-delta.</p>
</dd>
<dt>c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])</dt><dd><p>costs for false positive predictions (c_fp), false negative
predictions (c_fn) and rejections (c_r).</p>
</dd>
</dl>
</section>
<section id="id1109">
<h4>Returns<a class="headerlink" href="#id1109" title="Link to this heading">¶</a></h4>
<dl class="simple">
<dt>outlier_labels<span class="classifier">numpy array of shape (n_samples,)</span></dt><dd><p>For each observation, it tells whether it should be
considered as an outlier according to the fitted
model. 0 stands for inliers, 1 for outliers and
-2 for rejection.</p>
</dd>
</dl>
<p>expected_rejection_rate:   float, if return_stats is True;
upperbound_rejection_rate: float, if return_stats is True;
upperbound_cost:           float, if return_stats is True;</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyod.models.xgbod.XGBOD.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyod.models.xgbod.XGBOD.set_params" title="Link to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html">http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html</a>
and sklearn/base.py for more information.</p>
<section id="id1110">
<h4>Returns<a class="headerlink" href="#id1110" title="Link to this heading">¶</a></h4>
<p>self : object</p>
</section>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-pyod.models">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pyod.models" title="Link to this heading">¶</a></h2>
<p class="rubric">References</p>
<div class="docutils container" id="id1111">
<div role="list" class="citation-list">
<div class="citation" id="id1122" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BAgg15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id89">1</a>,<a role="doc-backlink" href="#id354">2</a>,<a role="doc-backlink" href="#id792">3</a>)</span>
<p>Charu&nbsp;C Aggarwal. Outlier analysis. In <em>Data mining</em>, 75–79. Springer, 2015.</p>
</div>
<div class="citation" id="id1115" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BAS15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id162">1</a>,<a role="doc-backlink" href="#id173">2</a>)</span>
<p>Charu&nbsp;C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. <em>ACM SIGKDD Explorations Newsletter</em>, 17(1):24–47, 2015.</p>
</div>
<div class="citation" id="id1149" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id871">BABC20</a><span class="fn-bracket">]</span></span>
<p>Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 2020.</p>
</div>
<div class="citation" id="id1117" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id480">BAP02</a><span class="fn-bracket">]</span></span>
<p>Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In <em>European Conference on Principles of Data Mining and Knowledge Discovery</em>, 15–27. Springer, 2002.</p>
</div>
<div class="citation" id="id1143" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id532">BAAR96</a><span class="fn-bracket">]</span></span>
<p>Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In <em>KDD</em>, volume 1141, 972–981. 1996.</p>
</div>
<div class="citation" id="id1158" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id428">BBTA+18</a><span class="fn-bracket">]</span></span>
<p>Tharindu&nbsp;R Bandaragoda, Kai&nbsp;Ming Ting, David Albrecht, Fei&nbsp;Tony Liu, Ye&nbsp;Zhu, and Jonathan&nbsp;R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. <em>Computational Intelligence</em>, 34(4):968–998, 2018.</p>
</div>
<div class="citation" id="id1152" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BBirgeR06<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id378">1</a>,<a role="doc-backlink" href="#id559">2</a>)</span>
<p>Lucien Birgé and Yves Rozenholc. How many bins should be put in a regular histogram. <em>ESAIM: Probability and Statistics</em>, 10:24–45, 2006.</p>
</div>
<div class="citation" id="id1123" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id585">BBKNS00</a><span class="fn-bracket">]</span></span>
<p>Markus&nbsp;M Breunig, Hans-Peter Kriegel, Raymond&nbsp;T Ng, and Jörg Sander. Lof: identifying density-based local outliers. In <em>ACM sigmod record</em>, volume&nbsp;29, 93–104. ACM, 2000.</p>
</div>
<div class="citation" id="id1146" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BBHP+18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1050">1</a>,<a role="doc-backlink" href="#id1052">2</a>)</span>
<p>Christopher&nbsp;P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. <em>arXiv preprint arXiv:1804.03599</em>, 2018.</p>
</div>
<div class="citation" id="id1155" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id176">BCoo77</a><span class="fn-bracket">]</span></span>
<p>R&nbsp;Dennis Cook. Detection of influential observation in linear regression. <em>Technometrics</em>, 19(1):15–18, 1977.</p>
</div>
<div class="citation" id="id1165" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id818">BFM01</a><span class="fn-bracket">]</span></span>
<p>Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. <em>Journal of complexity</em>, 17(4):608–624, 2001.</p>
</div>
<div class="citation" id="id1120" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id377">BGD12</a><span class="fn-bracket">]</span></span>
<p>Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. <em>KI-2012: Poster and Demo Track</em>, pages 59–63, 2012.</p>
</div>
<div class="citation" id="id1160" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id637">BGHNN22</a><span class="fn-bracket">]</span></span>
<p>Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee&nbsp;Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume&nbsp;36, 6737–6745. 2022.</p>
</div>
<div class="citation" id="id1126" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id714">BHR04</a><span class="fn-bracket">]</span></span>
<p>Johanna Hardin and David&nbsp;M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. <em>Computational Statistics &amp; Data Analysis</em>, 44(4):625–638, 2004.</p>
</div>
<div class="citation" id="id1169" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1035">BHGPRR19</a><span class="fn-bracket">]</span></span>
<p>Navid Hashemi, Eduardo&nbsp;Verdugo German, Jonatan Pena&nbsp;Ramirez, and Justin Ruths. Filtering approaches for dealing with noise in anomaly detection. In <em>2019 IEEE 58th Conference on Decision and Control (CDC)</em>, 5356–5361. IEEE, December 2019. URL: <a class="reference external" href="http://dx.doi.org/10.1109/CDC40024.2019.9029258">http://dx.doi.org/10.1109/CDC40024.2019.9029258</a>, <a class="reference external" href="https://doi.org/10.1109/cdc40024.2019.9029258">doi:10.1109/cdc40024.2019.9029258</a>.</p>
</div>
<div class="citation" id="id1127" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id116">BHXD03</a><span class="fn-bracket">]</span></span>
<p>Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. <em>Pattern Recognition Letters</em>, 24(9-10):1641–1650, 2003.</p>
</div>
<div class="citation" id="id1164" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id506">BHof07</a><span class="fn-bracket">]</span></span>
<p>Heiko Hoffmann. Kernel pca for novelty detection. <em>Pattern recognition</em>, 40(3):863–874, 2007.</p>
</div>
<div class="citation" id="id1147" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id688">BIH93</a><span class="fn-bracket">]</span></span>
<p>Boris Iglewicz and David&nbsp;Caster Hoaglin. <em>How to detect and handle outliers</em>. Volume&nbsp;16. Asq Press, 1993.</p>
</div>
<div class="citation" id="id1129" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id975">BJHuszarPvdH12</a><span class="fn-bracket">]</span></span>
<p>JHM Janssens, Ferenc Huszár, EO&nbsp;Postma, and HJ&nbsp;van&nbsp;den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.</p>
</div>
<div class="citation" id="id1144" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BKW13<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1049">1</a>,<a role="doc-backlink" href="#id1051">2</a>)</span>
<p>Diederik&nbsp;P Kingma and Max Welling. Auto-encoding variational bayes. <em>arXiv preprint arXiv:1312.6114</em>, 2013.</p>
</div>
<div class="citation" id="id1114" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BKKSZ11<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id16">1</a>,<a role="doc-backlink" href="#id37">2</a>,<a role="doc-backlink" href="#id58">3</a>,<a role="doc-backlink" href="#id83">4</a>,<a role="doc-backlink" href="#id107">5</a>,<a role="doc-backlink" href="#id134">6</a>,<a role="doc-backlink" href="#id157">7</a>,<a role="doc-backlink" href="#id194">8</a>,<a role="doc-backlink" href="#id220">9</a>,<a role="doc-backlink" href="#id246">10</a>,<a role="doc-backlink" href="#id269">11</a>,<a role="doc-backlink" href="#id295">12</a>,<a role="doc-backlink" href="#id322">13</a>,<a role="doc-backlink" href="#id348">14</a>,<a role="doc-backlink" href="#id372">15</a>,<a role="doc-backlink" href="#id398">16</a>,<a role="doc-backlink" href="#id423">17</a>,<a role="doc-backlink" href="#id448">18</a>,<a role="doc-backlink" href="#id474">19</a>,<a role="doc-backlink" href="#id500">20</a>,<a role="doc-backlink" href="#id526">21</a>,<a role="doc-backlink" href="#id552">22</a>,<a role="doc-backlink" href="#id579">23</a>,<a role="doc-backlink" href="#id605">24</a>,<a role="doc-backlink" href="#id631">25</a>,<a role="doc-backlink" href="#id655">26</a>,<a role="doc-backlink" href="#id682">27</a>,<a role="doc-backlink" href="#id708">28</a>,<a role="doc-backlink" href="#id734">29</a>,<a role="doc-backlink" href="#id760">30</a>,<a role="doc-backlink" href="#id786">31</a>,<a role="doc-backlink" href="#id812">32</a>,<a role="doc-backlink" href="#id837">33</a>,<a role="doc-backlink" href="#id865">34</a>,<a role="doc-backlink" href="#id891">35</a>,<a role="doc-backlink" href="#id917">36</a>,<a role="doc-backlink" href="#id943">37</a>,<a role="doc-backlink" href="#id969">38</a>,<a role="doc-backlink" href="#id996">39</a>,<a role="doc-backlink" href="#id1022">40</a>,<a role="doc-backlink" href="#id1076">41</a>)</span>
<p>Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In <em>Proceedings of the 2011 SIAM International Conference on Data Mining</em>, 13–24. SIAM, 2011.</p>
</div>
<div class="citation" id="id1139" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id923">BKKrogerSZ09</a><span class="fn-bracket">]</span></span>
<p>Hans-Peter Kriegel, Peer Kröger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 831–838. Springer, 2009.</p>
</div>
<div class="citation" id="id1118" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">BKZ+08</a><span class="fn-bracket">]</span></span>
<p>Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In <em>Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 444–452. ACM, 2008.</p>
</div>
<div class="citation" id="id1156" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id454">BLLP07</a><span class="fn-bracket">]</span></span>
<p>Longin&nbsp;Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In <em>International Workshop on Machine Learning and Data Mining in Pattern Recognition</em>, 61–75. Springer, 2007.</p>
</div>
<div class="citation" id="id1119" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id328">BLK05</a><span class="fn-bracket">]</span></span>
<p>Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In <em>Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</em>, 157–166. ACM, 2005.</p>
</div>
<div class="citation" id="id1148" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id199">BLZB+20</a><span class="fn-bracket">]</span></span>
<p>Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In <em>IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 2020.</p>
</div>
<div class="citation" id="id1154" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id301">BLZH+22</a><span class="fn-bracket">]</span></span>
<p>Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H.&nbsp;George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 2022.</p>
</div>
<div class="citation" id="id1112" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id404">BLTZ08</a><span class="fn-bracket">]</span></span>
<p>Fei&nbsp;Tony Liu, Kai&nbsp;Ming Ting, and Zhi-Hua Zhou. Isolation forest. In <em>Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on</em>, 413–422. IEEE, 2008.</p>
</div>
<div class="citation" id="id1113" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id404">BLTZ12</a><span class="fn-bracket">]</span></span>
<p>Fei&nbsp;Tony Liu, Kai&nbsp;Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>, 6(1):3, 2012.</p>
</div>
<div class="citation" id="id1132" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BLLZ+19<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id740">1</a>,<a role="doc-backlink" href="#id949">2</a>)</span>
<p>Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 2019.</p>
</div>
<div class="citation" id="id1130" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id611">BPKGF03</a><span class="fn-bracket">]</span></span>
<p>Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip&nbsp;B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In <em>Data Engineering, 2003. Proceedings. 19th International Conference on</em>, 315–326. IEEE, 2003.</p>
</div>
<div class="citation" id="id1153" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BPVD20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id34">2</a>,<a role="doc-backlink" href="#id55">3</a>,<a role="doc-backlink" href="#id80">4</a>,<a role="doc-backlink" href="#id104">5</a>,<a role="doc-backlink" href="#id131">6</a>,<a role="doc-backlink" href="#id154">7</a>,<a role="doc-backlink" href="#id191">8</a>,<a role="doc-backlink" href="#id217">9</a>,<a role="doc-backlink" href="#id243">10</a>,<a role="doc-backlink" href="#id266">11</a>,<a role="doc-backlink" href="#id292">12</a>,<a role="doc-backlink" href="#id319">13</a>,<a role="doc-backlink" href="#id345">14</a>,<a role="doc-backlink" href="#id369">15</a>,<a role="doc-backlink" href="#id395">16</a>,<a role="doc-backlink" href="#id420">17</a>,<a role="doc-backlink" href="#id445">18</a>,<a role="doc-backlink" href="#id471">19</a>,<a role="doc-backlink" href="#id497">20</a>,<a role="doc-backlink" href="#id523">21</a>,<a role="doc-backlink" href="#id549">22</a>,<a role="doc-backlink" href="#id576">23</a>,<a role="doc-backlink" href="#id602">24</a>,<a role="doc-backlink" href="#id628">25</a>,<a role="doc-backlink" href="#id652">26</a>,<a role="doc-backlink" href="#id679">27</a>,<a role="doc-backlink" href="#id705">28</a>,<a role="doc-backlink" href="#id731">29</a>,<a role="doc-backlink" href="#id757">30</a>,<a role="doc-backlink" href="#id783">31</a>,<a role="doc-backlink" href="#id809">32</a>,<a role="doc-backlink" href="#id834">33</a>,<a role="doc-backlink" href="#id862">34</a>,<a role="doc-backlink" href="#id888">35</a>,<a role="doc-backlink" href="#id914">36</a>,<a role="doc-backlink" href="#id940">37</a>,<a role="doc-backlink" href="#id966">38</a>,<a role="doc-backlink" href="#id993">39</a>,<a role="doc-backlink" href="#id1019">40</a>,<a role="doc-backlink" href="#id1073">41</a>,<a role="doc-backlink" href="#id1103">42</a>)</span>
<p>Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 227–243. Springer, 2020.</p>
</div>
<div class="citation" id="id1145" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id558">BPevny16</a><span class="fn-bracket">]</span></span>
<p>Tomáš Pevn\`y. Loda: lightweight on-line detector of anomalies. <em>Machine Learning</em>, 102(2):275–304, 2016.</p>
</div>
<div class="citation" id="id1116" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id480">BRRS00</a><span class="fn-bracket">]</span></span>
<p>Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In <em>ACM Sigmod Record</em>, volume&nbsp;29, 427–438. ACM, 2000.</p>
</div>
<div class="citation" id="id1125" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id714">BRD99</a><span class="fn-bracket">]</span></span>
<p>Peter&nbsp;J Rousseeuw and Katrien&nbsp;Van Driessen. A fast algorithm for the minimum covariance determinant estimator. <em>Technometrics</em>, 41(3):212–223, 1999.</p>
</div>
<div class="citation" id="id1151" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id226">BRVG+18</a><span class="fn-bracket">]</span></span>
<p>Lukas Ruff, Robert Vandermeulen, Nico Görnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. <em>International conference on machine learning</em>, 2018.</p>
</div>
<div class="citation" id="id1159" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">BSSeebockW+17</a><span class="fn-bracket">]</span></span>
<p>Thomas Schlegl, Philipp Seeböck, Sebastian&nbsp;M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In <em>International conference on information processing in medical imaging</em>, 146–157. Springer, 2017.</p>
</div>
<div class="citation" id="id1136" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id766">BScholkopfPST+01</a><span class="fn-bracket">]</span></span>
<p>Bernhard Schölkopf, John&nbsp;C Platt, John Shawe-Taylor, Alex&nbsp;J Smola, and Robert&nbsp;C Williamson. Estimating the support of a high-dimensional distribution. <em>Neural computation</em>, 13(7):1443–1471, 2001.</p>
</div>
<div class="citation" id="id1121" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id792">BSCSC03</a><span class="fn-bracket">]</span></span>
<p>Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.</p>
</div>
<div class="citation" id="id1157" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id897">BSB13</a><span class="fn-bracket">]</span></span>
<p>Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. <em>Advances in neural information processing systems</em>, 2013.</p>
</div>
<div class="citation" id="id1137" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id139">BTCFC02</a><span class="fn-bracket">]</span></span>
<p>Jian Tang, Zhixiang Chen, Ada&nbsp;Wai-Chee Fu, and David&nbsp;W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 535–548. Springer, 2002.</p>
</div>
<div class="citation" id="id1166" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id275">BXPWW23</a><span class="fn-bracket">]</span></span>
<p>Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. <em>IEEE Transactions on Knowledge and Data Engineering</em>, ():1–14, 2023. <a class="reference external" href="https://doi.org/10.1109/TKDE.2023.3270293">doi:10.1109/TKDE.2023.3270293</a>.</p>
</div>
<div class="citation" id="id1162" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id843">BYRV17</a><span class="fn-bracket">]</span></span>
<p>Chong You, Daniel&nbsp;P Robinson, and René Vidal. Provable self-representation based outlier detection in a union of subspaces. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 3395–3404. 2017.</p>
</div>
<div class="citation" id="id1163" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">BZRF+18</a><span class="fn-bracket">]</span></span>
<p>Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In <em>2018 IEEE International conference on data mining (ICDM)</em>, 727–736. IEEE, 2018.</p>
</div>
<div class="citation" id="id1124" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1086">BZH18</a><span class="fn-bracket">]</span></span>
<p>Yue Zhao and Maciej&nbsp;K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In <em>International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 2018.</p>
</div>
<div class="citation" id="id1150" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1002">BZHC+21</a><span class="fn-bracket">]</span></span>
<p>Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. <em>Proceedings of Machine Learning and Systems</em>, 2021.</p>
</div>
<div class="citation" id="id1131" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id661">BZNHL19</a><span class="fn-bracket">]</span></span>
<p>Yue Zhao, Zain Nasrullah, Maciej&nbsp;K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In <em>Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019</em>, 585–593. Calgary, Canada, May 2019. SIAM. URL: <a class="reference external" href="https://doi.org/10.1137/1.9781611975673.66">https://doi.org/10.1137/1.9781611975673.66</a>, <a class="reference external" href="https://doi.org/10.1137/1.9781611975673.66">doi:10.1137/1.9781611975673.66</a>.</p>
</div>
</div>
</div>
</section>
</section>

        <div class="raised" data-ea-type="text" data-ea-manual="true" data-ea-publisher="readthedocs" data-ea-keywords="anomaly-detection|outlier-detection|outlier-ensembles|python|readthedocs-project-219652|readthedocs-project-pyod" data-ea-campaign-types="community|house|paid" id="readthedocs-ea-text-nostyle-sphinx"></div></article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="pyod.utils.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Utility Functions</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="pyod.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">API Reference</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright © 2022, Yue Zhao
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">All Models</a><ul>
<li><a class="reference internal" href="#module-pyod.models.abod">pyod.models.abod module</a><ul>
<li><a class="reference internal" href="#pyod.models.abod.ABOD"><code class="docutils literal notranslate"><span class="pre">ABOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">ABOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.decision_function"><code class="docutils literal notranslate"><span class="pre">ABOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.fit"><code class="docutils literal notranslate"><span class="pre">ABOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">ABOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">ABOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.predict"><code class="docutils literal notranslate"><span class="pre">ABOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">ABOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">ABOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.abod.ABOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">ABOD.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.ae1svm">pyod.models.ae1svm module</a><ul>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM"><code class="docutils literal notranslate"><span class="pre">AE1SVM</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">AE1SVM.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.decision_function"><code class="docutils literal notranslate"><span class="pre">AE1SVM.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.fit"><code class="docutils literal notranslate"><span class="pre">AE1SVM.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.fit_predict"><code class="docutils literal notranslate"><span class="pre">AE1SVM.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">AE1SVM.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.predict"><code class="docutils literal notranslate"><span class="pre">AE1SVM.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.predict_confidence"><code class="docutils literal notranslate"><span class="pre">AE1SVM.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.predict_proba"><code class="docutils literal notranslate"><span class="pre">AE1SVM.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ae1svm.AE1SVM.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">AE1SVM.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.alad">pyod.models.alad module</a><ul>
<li><a class="reference internal" href="#pyod.models.alad.ALAD"><code class="docutils literal notranslate"><span class="pre">ALAD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">ALAD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.decision_function"><code class="docutils literal notranslate"><span class="pre">ALAD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.fit"><code class="docutils literal notranslate"><span class="pre">ALAD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.fit_predict"><code class="docutils literal notranslate"><span class="pre">ALAD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">ALAD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.plot_learning_curves"><code class="docutils literal notranslate"><span class="pre">ALAD.plot_learning_curves()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.predict"><code class="docutils literal notranslate"><span class="pre">ALAD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">ALAD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.predict_proba"><code class="docutils literal notranslate"><span class="pre">ALAD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.alad.ALAD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">ALAD.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.anogan">pyod.models.anogan module</a><ul>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN"><code class="docutils literal notranslate"><span class="pre">AnoGAN</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">AnoGAN.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.decision_function"><code class="docutils literal notranslate"><span class="pre">AnoGAN.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.fit"><code class="docutils literal notranslate"><span class="pre">AnoGAN.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.fit_predict"><code class="docutils literal notranslate"><span class="pre">AnoGAN.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">AnoGAN.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.get_params"><code class="docutils literal notranslate"><span class="pre">AnoGAN.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.plot_learning_curves"><code class="docutils literal notranslate"><span class="pre">AnoGAN.plot_learning_curves()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.predict"><code class="docutils literal notranslate"><span class="pre">AnoGAN.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.predict_confidence"><code class="docutils literal notranslate"><span class="pre">AnoGAN.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.predict_proba"><code class="docutils literal notranslate"><span class="pre">AnoGAN.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">AnoGAN.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.anogan.AnoGAN.set_params"><code class="docutils literal notranslate"><span class="pre">AnoGAN.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.auto_encoder">pyod.models.auto_encoder module</a><ul>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder"><code class="docutils literal notranslate"><span class="pre">AutoEncoder</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.build_model"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.build_model()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.decision_function"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.evaluate"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.evaluate()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.fit"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.fit_predict"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.predict"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.predict_confidence"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.predict_proba"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.save"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.save()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.train"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.train()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.training_forward"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.training_forward()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.auto_encoder.AutoEncoder.training_prepare"><code class="docutils literal notranslate"><span class="pre">AutoEncoder.training_prepare()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pyod-models-auto-encoder-torch-module">pyod.models.auto_encoder_torch module</a></li>
<li><a class="reference internal" href="#module-pyod.models.cblof">pyod.models.cblof module</a><ul>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF"><code class="docutils literal notranslate"><span class="pre">CBLOF</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">CBLOF.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.decision_function"><code class="docutils literal notranslate"><span class="pre">CBLOF.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.fit"><code class="docutils literal notranslate"><span class="pre">CBLOF.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.fit_predict"><code class="docutils literal notranslate"><span class="pre">CBLOF.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">CBLOF.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.predict"><code class="docutils literal notranslate"><span class="pre">CBLOF.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.predict_confidence"><code class="docutils literal notranslate"><span class="pre">CBLOF.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.predict_proba"><code class="docutils literal notranslate"><span class="pre">CBLOF.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cblof.CBLOF.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">CBLOF.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.cof">pyod.models.cof module</a><ul>
<li><a class="reference internal" href="#pyod.models.cof.COF"><code class="docutils literal notranslate"><span class="pre">COF</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.cof.COF.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">COF.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.decision_function"><code class="docutils literal notranslate"><span class="pre">COF.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.fit"><code class="docutils literal notranslate"><span class="pre">COF.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.fit_predict"><code class="docutils literal notranslate"><span class="pre">COF.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">COF.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.predict"><code class="docutils literal notranslate"><span class="pre">COF.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.predict_confidence"><code class="docutils literal notranslate"><span class="pre">COF.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.predict_proba"><code class="docutils literal notranslate"><span class="pre">COF.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cof.COF.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">COF.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.combination">pyod.models.combination module</a><ul>
<li><a class="reference internal" href="#pyod.models.combination.aom"><code class="docutils literal notranslate"><span class="pre">aom()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.combination.average"><code class="docutils literal notranslate"><span class="pre">average()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.combination.majority_vote"><code class="docutils literal notranslate"><span class="pre">majority_vote()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.combination.maximization"><code class="docutils literal notranslate"><span class="pre">maximization()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.combination.median"><code class="docutils literal notranslate"><span class="pre">median()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.combination.moa"><code class="docutils literal notranslate"><span class="pre">moa()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.cd">pyod.models.cd module</a><ul>
<li><a class="reference internal" href="#pyod.models.cd.CD"><code class="docutils literal notranslate"><span class="pre">CD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.cd.CD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">CD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.decision_function"><code class="docutils literal notranslate"><span class="pre">CD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.fit"><code class="docutils literal notranslate"><span class="pre">CD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.fit_predict"><code class="docutils literal notranslate"><span class="pre">CD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">CD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.predict"><code class="docutils literal notranslate"><span class="pre">CD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">CD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.predict_proba"><code class="docutils literal notranslate"><span class="pre">CD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.cd.CD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">CD.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.copod">pyod.models.copod module</a><ul>
<li><a class="reference internal" href="#pyod.models.copod.COPOD"><code class="docutils literal notranslate"><span class="pre">COPOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">COPOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.decision_function"><code class="docutils literal notranslate"><span class="pre">COPOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.explain_outlier"><code class="docutils literal notranslate"><span class="pre">COPOD.explain_outlier()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.fit"><code class="docutils literal notranslate"><span class="pre">COPOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">COPOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">COPOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.get_params"><code class="docutils literal notranslate"><span class="pre">COPOD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.predict"><code class="docutils literal notranslate"><span class="pre">COPOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">COPOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">COPOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">COPOD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.copod.COPOD.set_params"><code class="docutils literal notranslate"><span class="pre">COPOD.set_params()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pyod.models.copod.skew"><code class="docutils literal notranslate"><span class="pre">skew()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.deep_svdd">pyod.models.deep_svdd module</a><ul>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD"><code class="docutils literal notranslate"><span class="pre">DeepSVDD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.decision_function"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.fit"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.fit_predict"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.get_params"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.predict"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.predict_proba"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.deep_svdd.DeepSVDD.set_params"><code class="docutils literal notranslate"><span class="pre">DeepSVDD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.devnet">pyod.models.devnet module</a><ul>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet"><code class="docutils literal notranslate"><span class="pre">DevNet</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">DevNet.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.decision_function"><code class="docutils literal notranslate"><span class="pre">DevNet.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.fit"><code class="docutils literal notranslate"><span class="pre">DevNet.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.fit_predict"><code class="docutils literal notranslate"><span class="pre">DevNet.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">DevNet.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.get_params"><code class="docutils literal notranslate"><span class="pre">DevNet.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.predict"><code class="docutils literal notranslate"><span class="pre">DevNet.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.predict_confidence"><code class="docutils literal notranslate"><span class="pre">DevNet.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.predict_proba"><code class="docutils literal notranslate"><span class="pre">DevNet.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">DevNet.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.devnet.DevNet.set_params"><code class="docutils literal notranslate"><span class="pre">DevNet.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.dif">pyod.models.dif module</a><ul>
<li><a class="reference internal" href="#pyod.models.dif.DIF"><code class="docutils literal notranslate"><span class="pre">DIF</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.dif.DIF.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">DIF.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.decision_function"><code class="docutils literal notranslate"><span class="pre">DIF.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.fit"><code class="docutils literal notranslate"><span class="pre">DIF.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.fit_predict"><code class="docutils literal notranslate"><span class="pre">DIF.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">DIF.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.get_params"><code class="docutils literal notranslate"><span class="pre">DIF.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.predict"><code class="docutils literal notranslate"><span class="pre">DIF.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.predict_confidence"><code class="docutils literal notranslate"><span class="pre">DIF.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.predict_proba"><code class="docutils literal notranslate"><span class="pre">DIF.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">DIF.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.dif.DIF.set_params"><code class="docutils literal notranslate"><span class="pre">DIF.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.ecod">pyod.models.ecod module</a><ul>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD"><code class="docutils literal notranslate"><span class="pre">ECOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">ECOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.decision_function"><code class="docutils literal notranslate"><span class="pre">ECOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.explain_outlier"><code class="docutils literal notranslate"><span class="pre">ECOD.explain_outlier()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.fit"><code class="docutils literal notranslate"><span class="pre">ECOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">ECOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">ECOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.get_params"><code class="docutils literal notranslate"><span class="pre">ECOD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.predict"><code class="docutils literal notranslate"><span class="pre">ECOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">ECOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">ECOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">ECOD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ecod.ECOD.set_params"><code class="docutils literal notranslate"><span class="pre">ECOD.set_params()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pyod.models.ecod.skew"><code class="docutils literal notranslate"><span class="pre">skew()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.feature_bagging">pyod.models.feature_bagging module</a><ul>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging"><code class="docutils literal notranslate"><span class="pre">FeatureBagging</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.decision_function"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.fit"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.fit_predict"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.get_params"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.predict"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.predict_confidence"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.predict_proba"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.feature_bagging.FeatureBagging.set_params"><code class="docutils literal notranslate"><span class="pre">FeatureBagging.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.gmm">pyod.models.gmm module</a><ul>
<li><a class="reference internal" href="#pyod.models.gmm.GMM"><code class="docutils literal notranslate"><span class="pre">GMM</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">GMM.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.decision_function"><code class="docutils literal notranslate"><span class="pre">GMM.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.fit"><code class="docutils literal notranslate"><span class="pre">GMM.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.fit_predict"><code class="docutils literal notranslate"><span class="pre">GMM.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">GMM.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.precisions_"><code class="docutils literal notranslate"><span class="pre">GMM.precisions_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.precisions_cholesky_"><code class="docutils literal notranslate"><span class="pre">GMM.precisions_cholesky_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.predict"><code class="docutils literal notranslate"><span class="pre">GMM.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.predict_confidence"><code class="docutils literal notranslate"><span class="pre">GMM.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.predict_proba"><code class="docutils literal notranslate"><span class="pre">GMM.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.gmm.GMM.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">GMM.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.hbos">pyod.models.hbos module</a><ul>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS"><code class="docutils literal notranslate"><span class="pre">HBOS</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">HBOS.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.decision_function"><code class="docutils literal notranslate"><span class="pre">HBOS.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.fit"><code class="docutils literal notranslate"><span class="pre">HBOS.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.fit_predict"><code class="docutils literal notranslate"><span class="pre">HBOS.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">HBOS.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.get_params"><code class="docutils literal notranslate"><span class="pre">HBOS.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.predict"><code class="docutils literal notranslate"><span class="pre">HBOS.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.predict_confidence"><code class="docutils literal notranslate"><span class="pre">HBOS.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.predict_proba"><code class="docutils literal notranslate"><span class="pre">HBOS.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">HBOS.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.hbos.HBOS.set_params"><code class="docutils literal notranslate"><span class="pre">HBOS.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.iforest">pyod.models.iforest module</a><ul>
<li><a class="reference internal" href="#pyod.models.iforest.IForest"><code class="docutils literal notranslate"><span class="pre">IForest</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">IForest.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.decision_function"><code class="docutils literal notranslate"><span class="pre">IForest.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.feature_importances_"><code class="docutils literal notranslate"><span class="pre">IForest.feature_importances_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.fit"><code class="docutils literal notranslate"><span class="pre">IForest.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.fit_predict"><code class="docutils literal notranslate"><span class="pre">IForest.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">IForest.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.max_samples_"><code class="docutils literal notranslate"><span class="pre">IForest.max_samples_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.n_features_in_"><code class="docutils literal notranslate"><span class="pre">IForest.n_features_in_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.offset_"><code class="docutils literal notranslate"><span class="pre">IForest.offset_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.predict"><code class="docutils literal notranslate"><span class="pre">IForest.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.predict_confidence"><code class="docutils literal notranslate"><span class="pre">IForest.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.predict_proba"><code class="docutils literal notranslate"><span class="pre">IForest.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.iforest.IForest.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">IForest.predict_with_rejection()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.inne">pyod.models.inne module</a><ul>
<li><a class="reference internal" href="#pyod.models.inne.INNE"><code class="docutils literal notranslate"><span class="pre">INNE</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.inne.INNE.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">INNE.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.decision_function"><code class="docutils literal notranslate"><span class="pre">INNE.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.fit"><code class="docutils literal notranslate"><span class="pre">INNE.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.fit_predict"><code class="docutils literal notranslate"><span class="pre">INNE.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">INNE.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.get_params"><code class="docutils literal notranslate"><span class="pre">INNE.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.predict"><code class="docutils literal notranslate"><span class="pre">INNE.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.predict_confidence"><code class="docutils literal notranslate"><span class="pre">INNE.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.predict_proba"><code class="docutils literal notranslate"><span class="pre">INNE.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">INNE.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.inne.INNE.set_params"><code class="docutils literal notranslate"><span class="pre">INNE.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.kde">pyod.models.kde module</a><ul>
<li><a class="reference internal" href="#pyod.models.kde.KDE"><code class="docutils literal notranslate"><span class="pre">KDE</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.kde.KDE.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">KDE.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.decision_function"><code class="docutils literal notranslate"><span class="pre">KDE.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.fit"><code class="docutils literal notranslate"><span class="pre">KDE.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.fit_predict"><code class="docutils literal notranslate"><span class="pre">KDE.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">KDE.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.get_params"><code class="docutils literal notranslate"><span class="pre">KDE.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.predict"><code class="docutils literal notranslate"><span class="pre">KDE.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.predict_confidence"><code class="docutils literal notranslate"><span class="pre">KDE.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.predict_proba"><code class="docutils literal notranslate"><span class="pre">KDE.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">KDE.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kde.KDE.set_params"><code class="docutils literal notranslate"><span class="pre">KDE.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.knn">pyod.models.knn module</a><ul>
<li><a class="reference internal" href="#pyod.models.knn.KNN"><code class="docutils literal notranslate"><span class="pre">KNN</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.knn.KNN.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">KNN.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.decision_function"><code class="docutils literal notranslate"><span class="pre">KNN.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.fit"><code class="docutils literal notranslate"><span class="pre">KNN.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.fit_predict"><code class="docutils literal notranslate"><span class="pre">KNN.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">KNN.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.get_params"><code class="docutils literal notranslate"><span class="pre">KNN.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.predict"><code class="docutils literal notranslate"><span class="pre">KNN.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.predict_confidence"><code class="docutils literal notranslate"><span class="pre">KNN.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.predict_proba"><code class="docutils literal notranslate"><span class="pre">KNN.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">KNN.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.knn.KNN.set_params"><code class="docutils literal notranslate"><span class="pre">KNN.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.kpca">pyod.models.kpca module</a><ul>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA"><code class="docutils literal notranslate"><span class="pre">KPCA</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">KPCA.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.decision_function"><code class="docutils literal notranslate"><span class="pre">KPCA.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.fit"><code class="docutils literal notranslate"><span class="pre">KPCA.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.fit_predict"><code class="docutils literal notranslate"><span class="pre">KPCA.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">KPCA.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.get_params"><code class="docutils literal notranslate"><span class="pre">KPCA.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.predict"><code class="docutils literal notranslate"><span class="pre">KPCA.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.predict_confidence"><code class="docutils literal notranslate"><span class="pre">KPCA.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.predict_proba"><code class="docutils literal notranslate"><span class="pre">KPCA.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">KPCA.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.kpca.KPCA.set_params"><code class="docutils literal notranslate"><span class="pre">KPCA.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.lmdd">pyod.models.lmdd module</a><ul>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD"><code class="docutils literal notranslate"><span class="pre">LMDD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LMDD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.decision_function"><code class="docutils literal notranslate"><span class="pre">LMDD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.fit"><code class="docutils literal notranslate"><span class="pre">LMDD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.fit_predict"><code class="docutils literal notranslate"><span class="pre">LMDD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LMDD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.get_params"><code class="docutils literal notranslate"><span class="pre">LMDD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.predict"><code class="docutils literal notranslate"><span class="pre">LMDD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LMDD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.predict_proba"><code class="docutils literal notranslate"><span class="pre">LMDD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LMDD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lmdd.LMDD.set_params"><code class="docutils literal notranslate"><span class="pre">LMDD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.loda">pyod.models.loda module</a><ul>
<li><a class="reference internal" href="#pyod.models.loda.LODA"><code class="docutils literal notranslate"><span class="pre">LODA</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.loda.LODA.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LODA.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.decision_function"><code class="docutils literal notranslate"><span class="pre">LODA.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.fit"><code class="docutils literal notranslate"><span class="pre">LODA.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.fit_predict"><code class="docutils literal notranslate"><span class="pre">LODA.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LODA.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.get_params"><code class="docutils literal notranslate"><span class="pre">LODA.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.predict"><code class="docutils literal notranslate"><span class="pre">LODA.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LODA.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.predict_proba"><code class="docutils literal notranslate"><span class="pre">LODA.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LODA.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loda.LODA.set_params"><code class="docutils literal notranslate"><span class="pre">LODA.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.lof">pyod.models.lof module</a><ul>
<li><a class="reference internal" href="#pyod.models.lof.LOF"><code class="docutils literal notranslate"><span class="pre">LOF</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.lof.LOF.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LOF.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.decision_function"><code class="docutils literal notranslate"><span class="pre">LOF.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.fit"><code class="docutils literal notranslate"><span class="pre">LOF.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.fit_predict"><code class="docutils literal notranslate"><span class="pre">LOF.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LOF.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.get_params"><code class="docutils literal notranslate"><span class="pre">LOF.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.predict"><code class="docutils literal notranslate"><span class="pre">LOF.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LOF.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.predict_proba"><code class="docutils literal notranslate"><span class="pre">LOF.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LOF.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lof.LOF.set_params"><code class="docutils literal notranslate"><span class="pre">LOF.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.loci">pyod.models.loci module</a><ul>
<li><a class="reference internal" href="#pyod.models.loci.LOCI"><code class="docutils literal notranslate"><span class="pre">LOCI</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LOCI.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.decision_function"><code class="docutils literal notranslate"><span class="pre">LOCI.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.fit"><code class="docutils literal notranslate"><span class="pre">LOCI.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.fit_predict"><code class="docutils literal notranslate"><span class="pre">LOCI.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LOCI.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.get_params"><code class="docutils literal notranslate"><span class="pre">LOCI.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.predict"><code class="docutils literal notranslate"><span class="pre">LOCI.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LOCI.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.predict_proba"><code class="docutils literal notranslate"><span class="pre">LOCI.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LOCI.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.loci.LOCI.set_params"><code class="docutils literal notranslate"><span class="pre">LOCI.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.lunar">pyod.models.lunar module</a><ul>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR"><code class="docutils literal notranslate"><span class="pre">LUNAR</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LUNAR.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.decision_function"><code class="docutils literal notranslate"><span class="pre">LUNAR.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.fit"><code class="docutils literal notranslate"><span class="pre">LUNAR.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.fit_predict"><code class="docutils literal notranslate"><span class="pre">LUNAR.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LUNAR.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.get_params"><code class="docutils literal notranslate"><span class="pre">LUNAR.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.predict"><code class="docutils literal notranslate"><span class="pre">LUNAR.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LUNAR.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.predict_proba"><code class="docutils literal notranslate"><span class="pre">LUNAR.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LUNAR.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lunar.LUNAR.set_params"><code class="docutils literal notranslate"><span class="pre">LUNAR.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.lscp">pyod.models.lscp module</a><ul>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP"><code class="docutils literal notranslate"><span class="pre">LSCP</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">LSCP.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.decision_function"><code class="docutils literal notranslate"><span class="pre">LSCP.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.fit"><code class="docutils literal notranslate"><span class="pre">LSCP.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.fit_predict"><code class="docutils literal notranslate"><span class="pre">LSCP.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">LSCP.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.get_params"><code class="docutils literal notranslate"><span class="pre">LSCP.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.predict"><code class="docutils literal notranslate"><span class="pre">LSCP.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.predict_confidence"><code class="docutils literal notranslate"><span class="pre">LSCP.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.predict_proba"><code class="docutils literal notranslate"><span class="pre">LSCP.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">LSCP.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.lscp.LSCP.set_params"><code class="docutils literal notranslate"><span class="pre">LSCP.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.mad">pyod.models.mad module</a><ul>
<li><a class="reference internal" href="#pyod.models.mad.MAD"><code class="docutils literal notranslate"><span class="pre">MAD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.mad.MAD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">MAD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.decision_function"><code class="docutils literal notranslate"><span class="pre">MAD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.fit"><code class="docutils literal notranslate"><span class="pre">MAD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.fit_predict"><code class="docutils literal notranslate"><span class="pre">MAD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">MAD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.get_params"><code class="docutils literal notranslate"><span class="pre">MAD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.predict"><code class="docutils literal notranslate"><span class="pre">MAD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">MAD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.predict_proba"><code class="docutils literal notranslate"><span class="pre">MAD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">MAD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mad.MAD.set_params"><code class="docutils literal notranslate"><span class="pre">MAD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.mcd">pyod.models.mcd module</a><ul>
<li><a class="reference internal" href="#pyod.models.mcd.MCD"><code class="docutils literal notranslate"><span class="pre">MCD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">MCD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.decision_function"><code class="docutils literal notranslate"><span class="pre">MCD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.fit"><code class="docutils literal notranslate"><span class="pre">MCD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.fit_predict"><code class="docutils literal notranslate"><span class="pre">MCD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">MCD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.get_params"><code class="docutils literal notranslate"><span class="pre">MCD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.predict"><code class="docutils literal notranslate"><span class="pre">MCD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">MCD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.predict_proba"><code class="docutils literal notranslate"><span class="pre">MCD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">MCD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mcd.MCD.set_params"><code class="docutils literal notranslate"><span class="pre">MCD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.mo_gaal">pyod.models.mo_gaal module</a><ul>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL"><code class="docutils literal notranslate"><span class="pre">MO_GAAL</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.decision_function"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.fit"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.fit_predict"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.get_params"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.predict"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.predict_confidence"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.predict_proba"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.mo_gaal.MO_GAAL.set_params"><code class="docutils literal notranslate"><span class="pre">MO_GAAL.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.ocsvm">pyod.models.ocsvm module</a><ul>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM"><code class="docutils literal notranslate"><span class="pre">OCSVM</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">OCSVM.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.decision_function"><code class="docutils literal notranslate"><span class="pre">OCSVM.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.fit"><code class="docutils literal notranslate"><span class="pre">OCSVM.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.fit_predict"><code class="docutils literal notranslate"><span class="pre">OCSVM.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">OCSVM.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.get_params"><code class="docutils literal notranslate"><span class="pre">OCSVM.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.predict"><code class="docutils literal notranslate"><span class="pre">OCSVM.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.predict_confidence"><code class="docutils literal notranslate"><span class="pre">OCSVM.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.predict_proba"><code class="docutils literal notranslate"><span class="pre">OCSVM.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">OCSVM.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.ocsvm.OCSVM.set_params"><code class="docutils literal notranslate"><span class="pre">OCSVM.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.pca">pyod.models.pca module</a><ul>
<li><a class="reference internal" href="#pyod.models.pca.PCA"><code class="docutils literal notranslate"><span class="pre">PCA</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.pca.PCA.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">PCA.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.decision_function"><code class="docutils literal notranslate"><span class="pre">PCA.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.explained_variance_"><code class="docutils literal notranslate"><span class="pre">PCA.explained_variance_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.fit"><code class="docutils literal notranslate"><span class="pre">PCA.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.fit_predict"><code class="docutils literal notranslate"><span class="pre">PCA.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">PCA.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.get_params"><code class="docutils literal notranslate"><span class="pre">PCA.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.noise_variance_"><code class="docutils literal notranslate"><span class="pre">PCA.noise_variance_</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.predict"><code class="docutils literal notranslate"><span class="pre">PCA.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.predict_confidence"><code class="docutils literal notranslate"><span class="pre">PCA.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.predict_proba"><code class="docutils literal notranslate"><span class="pre">PCA.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">PCA.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.pca.PCA.set_params"><code class="docutils literal notranslate"><span class="pre">PCA.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.qmcd">pyod.models.qmcd module</a><ul>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD"><code class="docutils literal notranslate"><span class="pre">QMCD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">QMCD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.decision_function"><code class="docutils literal notranslate"><span class="pre">QMCD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.fit"><code class="docutils literal notranslate"><span class="pre">QMCD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.fit_predict"><code class="docutils literal notranslate"><span class="pre">QMCD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">QMCD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.get_params"><code class="docutils literal notranslate"><span class="pre">QMCD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.predict"><code class="docutils literal notranslate"><span class="pre">QMCD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">QMCD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.predict_proba"><code class="docutils literal notranslate"><span class="pre">QMCD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">QMCD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.qmcd.QMCD.set_params"><code class="docutils literal notranslate"><span class="pre">QMCD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.rgraph">pyod.models.rgraph module</a><ul>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph"><code class="docutils literal notranslate"><span class="pre">RGraph</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.active_support_elastic_net"><code class="docutils literal notranslate"><span class="pre">RGraph.active_support_elastic_net()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">RGraph.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.decision_function"><code class="docutils literal notranslate"><span class="pre">RGraph.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering"><code class="docutils literal notranslate"><span class="pre">RGraph.elastic_net_subspace_clustering()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.fit"><code class="docutils literal notranslate"><span class="pre">RGraph.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.fit_predict"><code class="docutils literal notranslate"><span class="pre">RGraph.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">RGraph.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.get_params"><code class="docutils literal notranslate"><span class="pre">RGraph.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.predict"><code class="docutils literal notranslate"><span class="pre">RGraph.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.predict_confidence"><code class="docutils literal notranslate"><span class="pre">RGraph.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.predict_proba"><code class="docutils literal notranslate"><span class="pre">RGraph.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">RGraph.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rgraph.RGraph.set_params"><code class="docutils literal notranslate"><span class="pre">RGraph.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.rod">pyod.models.rod module</a><ul>
<li><a class="reference internal" href="#pyod.models.rod.ROD"><code class="docutils literal notranslate"><span class="pre">ROD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.rod.ROD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">ROD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.decision_function"><code class="docutils literal notranslate"><span class="pre">ROD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.fit"><code class="docutils literal notranslate"><span class="pre">ROD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.fit_predict"><code class="docutils literal notranslate"><span class="pre">ROD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">ROD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.get_params"><code class="docutils literal notranslate"><span class="pre">ROD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.predict"><code class="docutils literal notranslate"><span class="pre">ROD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">ROD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.predict_proba"><code class="docutils literal notranslate"><span class="pre">ROD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">ROD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.rod.ROD.set_params"><code class="docutils literal notranslate"><span class="pre">ROD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.sampling">pyod.models.sampling module</a><ul>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling"><code class="docutils literal notranslate"><span class="pre">Sampling</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">Sampling.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.decision_function"><code class="docutils literal notranslate"><span class="pre">Sampling.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.fit"><code class="docutils literal notranslate"><span class="pre">Sampling.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.fit_predict"><code class="docutils literal notranslate"><span class="pre">Sampling.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">Sampling.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.get_params"><code class="docutils literal notranslate"><span class="pre">Sampling.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.predict"><code class="docutils literal notranslate"><span class="pre">Sampling.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.predict_confidence"><code class="docutils literal notranslate"><span class="pre">Sampling.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.predict_proba"><code class="docutils literal notranslate"><span class="pre">Sampling.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">Sampling.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sampling.Sampling.set_params"><code class="docutils literal notranslate"><span class="pre">Sampling.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.sod">pyod.models.sod module</a><ul>
<li><a class="reference internal" href="#pyod.models.sod.SOD"><code class="docutils literal notranslate"><span class="pre">SOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.sod.SOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">SOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.decision_function"><code class="docutils literal notranslate"><span class="pre">SOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.fit"><code class="docutils literal notranslate"><span class="pre">SOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">SOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">SOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.get_params"><code class="docutils literal notranslate"><span class="pre">SOD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.predict"><code class="docutils literal notranslate"><span class="pre">SOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">SOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">SOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">SOD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sod.SOD.set_params"><code class="docutils literal notranslate"><span class="pre">SOD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.so_gaal">pyod.models.so_gaal module</a><ul>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL"><code class="docutils literal notranslate"><span class="pre">SO_GAAL</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.decision_function"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.fit"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.fit_predict"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.get_params"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.predict"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.predict_confidence"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.predict_proba"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.so_gaal.SO_GAAL.set_params"><code class="docutils literal notranslate"><span class="pre">SO_GAAL.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.sos">pyod.models.sos module</a><ul>
<li><a class="reference internal" href="#pyod.models.sos.SOS"><code class="docutils literal notranslate"><span class="pre">SOS</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.sos.SOS.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">SOS.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.decision_function"><code class="docutils literal notranslate"><span class="pre">SOS.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.fit"><code class="docutils literal notranslate"><span class="pre">SOS.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.fit_predict"><code class="docutils literal notranslate"><span class="pre">SOS.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">SOS.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.get_params"><code class="docutils literal notranslate"><span class="pre">SOS.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.predict"><code class="docutils literal notranslate"><span class="pre">SOS.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.predict_confidence"><code class="docutils literal notranslate"><span class="pre">SOS.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.predict_proba"><code class="docutils literal notranslate"><span class="pre">SOS.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">SOS.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.sos.SOS.set_params"><code class="docutils literal notranslate"><span class="pre">SOS.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.suod">pyod.models.suod module</a><ul>
<li><a class="reference internal" href="#pyod.models.suod.SUOD"><code class="docutils literal notranslate"><span class="pre">SUOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">SUOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.decision_function"><code class="docutils literal notranslate"><span class="pre">SUOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.fit"><code class="docutils literal notranslate"><span class="pre">SUOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">SUOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">SUOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.get_params"><code class="docutils literal notranslate"><span class="pre">SUOD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.predict"><code class="docutils literal notranslate"><span class="pre">SUOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">SUOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">SUOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">SUOD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.suod.SUOD.set_params"><code class="docutils literal notranslate"><span class="pre">SUOD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pyod-models-thresholds-module">pyod.models.thresholds module</a><ul>
<li><a class="reference internal" href="#pyod.models.thresholds.AUCP"><code class="docutils literal notranslate"><span class="pre">AUCP()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.BOOT"><code class="docutils literal notranslate"><span class="pre">BOOT()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.CHAU"><code class="docutils literal notranslate"><span class="pre">CHAU()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.CLF"><code class="docutils literal notranslate"><span class="pre">CLF()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.CLUST"><code class="docutils literal notranslate"><span class="pre">CLUST()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.CPD"><code class="docutils literal notranslate"><span class="pre">CPD()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.DECOMP"><code class="docutils literal notranslate"><span class="pre">DECOMP()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.DSN"><code class="docutils literal notranslate"><span class="pre">DSN()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.EB"><code class="docutils literal notranslate"><span class="pre">EB()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.FGD"><code class="docutils literal notranslate"><span class="pre">FGD()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.FILTER"><code class="docutils literal notranslate"><span class="pre">FILTER()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.FWFM"><code class="docutils literal notranslate"><span class="pre">FWFM()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.GESD"><code class="docutils literal notranslate"><span class="pre">GESD()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.HIST"><code class="docutils literal notranslate"><span class="pre">HIST()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.IQR"><code class="docutils literal notranslate"><span class="pre">IQR()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.KARCH"><code class="docutils literal notranslate"><span class="pre">KARCH()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.MAD"><code class="docutils literal notranslate"><span class="pre">MAD()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.MCST"><code class="docutils literal notranslate"><span class="pre">MCST()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.META"><code class="docutils literal notranslate"><span class="pre">META()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.MOLL"><code class="docutils literal notranslate"><span class="pre">MOLL()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.MTT"><code class="docutils literal notranslate"><span class="pre">MTT()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.OCSVM"><code class="docutils literal notranslate"><span class="pre">OCSVM()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.QMCD"><code class="docutils literal notranslate"><span class="pre">QMCD()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.REGR"><code class="docutils literal notranslate"><span class="pre">REGR()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.VAE"><code class="docutils literal notranslate"><span class="pre">VAE()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.WIND"><code class="docutils literal notranslate"><span class="pre">WIND()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.YJ"><code class="docutils literal notranslate"><span class="pre">YJ()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.thresholds.ZSCORE"><code class="docutils literal notranslate"><span class="pre">ZSCORE()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.vae">pyod.models.vae module</a><ul>
<li><a class="reference internal" href="#pyod.models.vae.VAE"><code class="docutils literal notranslate"><span class="pre">VAE</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.vae.VAE.build_model"><code class="docutils literal notranslate"><span class="pre">VAE.build_model()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">VAE.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.decision_function"><code class="docutils literal notranslate"><span class="pre">VAE.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.decision_function_update"><code class="docutils literal notranslate"><span class="pre">VAE.decision_function_update()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.epoch_update"><code class="docutils literal notranslate"><span class="pre">VAE.epoch_update()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.evaluate"><code class="docutils literal notranslate"><span class="pre">VAE.evaluate()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.evaluating_forward"><code class="docutils literal notranslate"><span class="pre">VAE.evaluating_forward()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.evaluating_prepare"><code class="docutils literal notranslate"><span class="pre">VAE.evaluating_prepare()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.fit"><code class="docutils literal notranslate"><span class="pre">VAE.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.fit_predict"><code class="docutils literal notranslate"><span class="pre">VAE.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">VAE.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.get_params"><code class="docutils literal notranslate"><span class="pre">VAE.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.load"><code class="docutils literal notranslate"><span class="pre">VAE.load()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.predict"><code class="docutils literal notranslate"><span class="pre">VAE.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.predict_confidence"><code class="docutils literal notranslate"><span class="pre">VAE.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.predict_proba"><code class="docutils literal notranslate"><span class="pre">VAE.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">VAE.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.save"><code class="docutils literal notranslate"><span class="pre">VAE.save()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.set_params"><code class="docutils literal notranslate"><span class="pre">VAE.set_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.train"><code class="docutils literal notranslate"><span class="pre">VAE.train()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.training_forward"><code class="docutils literal notranslate"><span class="pre">VAE.training_forward()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.vae.VAE.training_prepare"><code class="docutils literal notranslate"><span class="pre">VAE.training_prepare()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models.xgbod">pyod.models.xgbod module</a><ul>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD"><code class="docutils literal notranslate"><span class="pre">XGBOD</span></code></a><ul>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.compute_rejection_stats"><code class="docutils literal notranslate"><span class="pre">XGBOD.compute_rejection_stats()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.decision_function"><code class="docutils literal notranslate"><span class="pre">XGBOD.decision_function()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.fit"><code class="docutils literal notranslate"><span class="pre">XGBOD.fit()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.fit_predict"><code class="docutils literal notranslate"><span class="pre">XGBOD.fit_predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.fit_predict_score"><code class="docutils literal notranslate"><span class="pre">XGBOD.fit_predict_score()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.get_params"><code class="docutils literal notranslate"><span class="pre">XGBOD.get_params()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.predict"><code class="docutils literal notranslate"><span class="pre">XGBOD.predict()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.predict_confidence"><code class="docutils literal notranslate"><span class="pre">XGBOD.predict_confidence()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.predict_proba"><code class="docutils literal notranslate"><span class="pre">XGBOD.predict_proba()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.predict_with_rejection"><code class="docutils literal notranslate"><span class="pre">XGBOD.predict_with_rejection()</span></code></a></li>
<li><a class="reference internal" href="#pyod.models.xgbod.XGBOD.set_params"><code class="docutils literal notranslate"><span class="pre">XGBOD.set_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pyod.models">Module contents</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="_static/documentation_options.js?v=09beee0d"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/furo.js?v=46bd48cc"></script>
    <readthedocs-flyout></readthedocs-flyout><readthedocs-notification class="raised toast"></readthedocs-notification><readthedocs-search class="raised floating"></readthedocs-search><readthedocs-hotkeys></readthedocs-hotkeys>
</body></html>