# Ê∑±Â∫¶Áà¨ÂèñÈ°πÁõÆ: pyod.readthedocs.io
**Ëµ∑ÂßãURL:** https://pyod.readthedocs.io/en/latest/index.html
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:11
**ÊÄªÈ°µÈù¢Êï∞:** 65
**ÊúÄÂ§ßÊ∑±Â∫¶:** 3

## üìã ÁõÆÂΩï
1. [pyod 2.0.5 documentation](#1-0)
  2. [pyod 2.0.5 documentation](#2-1)
  3. [About us - pyod 2.0.5 documentation](#3-1)
  4. [API CheatSheet - pyod 2.0.5 documentation](#4-1)
  5. [Benchmarks - pyod 2.0.5 documentation](#5-1)
  6. [Examples - pyod 2.0.5 documentation](#6-1)
  7. [Frequently Asked Questions - pyod 2.0.5 documentation](#7-1)
  8. [Fast Train with SUOD - pyod 2.0.5 documentation](#8-1)
  9. [Installation - pyod 2.0.5 documentation](#9-1)
  10. [Known Issues & Warnings - pyod 2.0.5 documentation](#10-1)
  11. [Model Save & Load - pyod 2.0.5 documentation](#11-1)
  12. [Citations & Achievements - pyod 2.0.5 documentation](#12-1)
  13. [API Reference - pyod 2.0.5 documentation](#13-1)
  14. [All Models - pyod 2.0.5 documentation](#14-1)
  15. [Utility Functions - pyod 2.0.5 documentation](#15-1)
  16. [Outlier Detection 101 - pyod 2.0.5 documentation](#16-1)
    17. [pyod.models.abod - pyod 2.0.5 documentation](#17-2)
    18. [pyod.models.ae1svm - pyod 2.0.5 documentation](#18-2)
    19. [pyod.models.alad - pyod 2.0.5 documentation](#19-2)
    20. [pyod.models.anogan - pyod 2.0.5 documentation](#20-2)
    21. [pyod.models.auto_encoder - pyod 2.0.5 documentation](#21-2)
    22. [pyod.models.base - pyod 2.0.5 documentation](#22-2)
    23. [pyod.models.cblof - pyod 2.0.5 documentation](#23-2)
    24. [pyod.models.cd - pyod 2.0.5 documentation](#24-2)
    25. [pyod.models.cof - pyod 2.0.5 documentation](#25-2)
    26. [pyod.models.combination - pyod 2.0.5 documentation](#26-2)
    27. [pyod.models.copod - pyod 2.0.5 documentation](#27-2)
    28. [pyod.models.deep_svdd - pyod 2.0.5 documentation](#28-2)
    29. [pyod.models.devnet - pyod 2.0.5 documentation](#29-2)
    30. [pyod.models.dif - pyod 2.0.5 documentation](#30-2)
    31. [pyod.models.ecod - pyod 2.0.5 documentation](#31-2)
    32. [pyod.models.feature_bagging - pyod 2.0.5 documentation](#32-2)
    33. [pyod.models.gmm - pyod 2.0.5 documentation](#33-2)
    34. [pyod.models.hbos - pyod 2.0.5 documentation](#34-2)
    35. [pyod.models.iforest - pyod 2.0.5 documentation](#35-2)
    36. [pyod.models.inne - pyod 2.0.5 documentation](#36-2)
    37. [pyod.models.kde - pyod 2.0.5 documentation](#37-2)
    38. [pyod.models.knn - pyod 2.0.5 documentation](#38-2)
    39. [pyod.models.kpca - pyod 2.0.5 documentation](#39-2)
    40. [pyod.models.lmdd - pyod 2.0.5 documentation](#40-2)
    41. [pyod.models.loci - pyod 2.0.5 documentation](#41-2)
    42. [pyod.models.loda - pyod 2.0.5 documentation](#42-2)
    43. [pyod.models.lof - pyod 2.0.5 documentation](#43-2)
    44. [pyod.models.lscp - pyod 2.0.5 documentation](#44-2)
    45. [pyod.models.lunar - pyod 2.0.5 documentation](#45-2)
    46. [pyod.models.mad - pyod 2.0.5 documentation](#46-2)
    47. [pyod.models.mcd - pyod 2.0.5 documentation](#47-2)
    48. [pyod.models.mo_gaal - pyod 2.0.5 documentation](#48-2)
    49. [pyod.models.ocsvm - pyod 2.0.5 documentation](#49-2)
    50. [pyod.models.pca - pyod 2.0.5 documentation](#50-2)
    51. [pyod.models.qmcd - pyod 2.0.5 documentation](#51-2)
    52. [pyod.models.rgraph - pyod 2.0.5 documentation](#52-2)
    53. [pyod.models.rod - pyod 2.0.5 documentation](#53-2)
    54. [pyod.models.sampling - pyod 2.0.5 documentation](#54-2)
    55. [pyod.models.so_gaal - pyod 2.0.5 documentation](#55-2)
    56. [pyod.models.sod - pyod 2.0.5 documentation](#56-2)
    57. [pyod.models.sos - pyod 2.0.5 documentation](#57-2)
    58. [pyod.models.suod - pyod 2.0.5 documentation](#58-2)
    59. [pyod.models.thresholds - pyod 2.0.5 documentation](#59-2)
    60. [pyod.models.vae - pyod 2.0.5 documentation](#60-2)
    61. [pyod.models.xgbod - pyod 2.0.5 documentation](#61-2)
    62. [pyod.utils.data - pyod 2.0.5 documentation](#62-2)
    63. [pyod.utils.example - pyod 2.0.5 documentation](#63-2)
    64. [pyod.utils.stat_models - pyod 2.0.5 documentation](#64-2)
    65. [pyod.utils.utility - pyod 2.0.5 documentation](#65-2)

---

## 1. pyod 2.0.5 documentation {#1-0}

**URL:** https://pyod.readthedocs.io/en/latest/index.html
**Ê∑±Â∫¶:** 0
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:52

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/index.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/index.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/index.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Welcome to PyOD V2 documentation![¬∂](https://pyod.readthedocs.io/en/latest/index.html#welcome-to-pyod-v2-documentation "Link to this heading")
**Deployment & Documentation & Stats & License**
[![PyPI version](https://img.shields.io/pypi/v/pyod.svg?color=brightgreen) ](https://pypi.org/project/pyod/) [![Anaconda version](https://anaconda.org/conda-forge/pyod/badges/version.svg) ](https://anaconda.org/conda-forge/pyod) [![Documentation status](https://readthedocs.org/projects/pyod/badge/?version=latest) ](https://pyod.readthedocs.io/en/latest/?badge=latest) [![GitHub stars](https://img.shields.io/github/stars/yzhao062/pyod.svg) ](https://github.com/yzhao062/pyod/stargazers) [![GitHub forks](https://img.shields.io/github/forks/yzhao062/pyod.svg?color=blue) ](https://github.com/yzhao062/pyod/network) [![Downloads](https://pepy.tech/badge/pyod) ](https://pepy.tech/project/pyod) [![Testing](https://github.com/yzhao062/pyod/actions/workflows/testing.yml/badge.svg) ](https://github.com/yzhao062/pyod/actions/workflows/testing.yml) [![Coverage Status](https://coveralls.io/repos/github/yzhao062/pyod/badge.svg) ](https://coveralls.io/github/yzhao062/pyod) [![Maintainability](https://api.codeclimate.com/v1/badges/bdc3d8d0454274c753c4/maintainability) ](https://codeclimate.com/github/yzhao062/Pyod/maintainability) [![License](https://img.shields.io/github/license/yzhao062/pyod.svg) ](https://github.com/yzhao062/pyod/blob/master/LICENSE) [![Benchmark](https://img.shields.io/badge/ADBench-benchmark_results-pink) ](https://github.com/Minqi824/ADBench)
* * *
###### Read Me First[¬∂](https://pyod.readthedocs.io/en/latest/index.html#read-me-first "Link to this heading")
Welcome to PyOD, a comprehensive but easy-to-use Python library for detecting anomalies in multivariate data. Whether you are working with a small-scale project or large datasets, PyOD provides a range of algorithms to suit your needs.
**PyOD Version 2 is now available** ([Paper](https://www.arxiv.org/abs/2412.12154)) [[ACQS+24](https://pyod.readthedocs.io/en/latest/index.html#id126 "Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, and others. Pyod 2: a python library for outlier detection with llm-powered model selection. arXiv preprint arXiv:2412.12154, 2024.")], featuring:
  * **Expanded Deep Learning Support** : Integrates 12 modern neural models into a single PyTorch-based framework, bringing the total number of outlier detection methods to 45.
  * **Enhanced Performance and Ease of Use** : Models are optimized for efficiency and consistent performance across different datasets.
  * **LLM-based Model Selection** : Automated model selection guided by a large language model reduces manual tuning and assists users who may have limited experience with outlier detection.


**Additional Resources** :
  * **NLP Anomaly Detection** : [NLP-ADBench](https://github.com/USC-FORTIS/NLP-ADBench) provides both NLP anomaly detection datasets and algorithms [[ALLX+24](https://pyod.readthedocs.io/en/latest/index.html#id127 "Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, and Yue Zhao. Nlp-adbench: nlp anomaly detection benchmark. arXiv preprint arXiv:2412.04784, 2024.")]
  * **Time-series Outlier Detection** : [TODS](https://github.com/datamllab/tods)
  * **Graph Outlier Detection** : [PyGOD](https://pygod.org/)
  * **Performance Comparison & Datasets**: We have a 45-page, comprehensive [anomaly detection benchmark paper](https://openreview.net/forum?id=foA_SFQ9zo0). The fully [open-sourced ADBench](https://github.com/Minqi824/ADBench) compares 30 anomaly detection algorithms on 57 benchmark datasets.
  * **PyOD on Distributed Systems** : You can also run [PyOD on Databricks](https://www.databricks.com/blog/2023/03/13/unsupervised-outlier-detection-databricks.html)
  * **Learn More** : [Anomaly Detection Resources](https://github.com/yzhao062/anomaly-detection-resources)


**Check out our latest research on LLM-based anomaly detection** [[AYNL+24](https://pyod.readthedocs.io/en/latest/index.html#id125 "Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, and others. Ad-llm: benchmarking large language models for anomaly detection. arXiv preprint arXiv:2412.11142, 2024.")]: [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/abs/2412.11142).
* * *
###### About PyOD[¬∂](https://pyod.readthedocs.io/en/latest/index.html#about-pyod "Link to this heading")
PyOD, established in 2017, has become a go-to **Python library** for **detecting anomalous/outlying objects** in multivariate data. This exciting yet challenging field is commonly referred to as [Outlier Detection](https://en.wikipedia.org/wiki/Anomaly_detection) or [Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection).
PyOD includes more than 50 detection algorithms, from classical LOF (SIGMOD 2000) to the cutting-edge ECOD and DIF (TKDE 2022 and 2023). Since 2017, PyOD has been successfully used in numerous academic research projects and commercial products with more than [26 million downloads](https://pepy.tech/project/pyod). It is also well acknowledged by the machine learning community with various dedicated posts/tutorials, including [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/), [KDnuggets](https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html), and [Towards Data Science](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1).
**PyOD is featured for** :
  * **Unified, User-Friendly Interface** across various algorithms.
  * **Wide Range of Models** , from classic techniques to the latest deep learning methods in **PyTorch**.
  * **High Performance & Efficiency**, leveraging [numba](https://github.com/numba/numba) and [joblib](https://github.com/joblib/joblib) for JIT compilation and parallel processing.
  * **Fast Training & Prediction**, achieved through the SUOD framework [[AZHC+21](https://pyod.readthedocs.io/en/latest/index.html#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")].


**Outlier Detection with 5 Lines of Code** :
```
### Example: Training an ECOD detector
frompyod.models.ecodimport ECOD
clf = ECOD()
clf.fit(X_train)
y_train_scores = clf.decision_scores_  ### Outlier scores for training data
y_test_scores = clf.decision_function(X_test)  ### Outlier scores for test data

```

**Selecting the Right Algorithm:** Unsure where to start? Consider these robust and interpretable options:
  * [ECOD](https://github.com/yzhao062/pyod/blob/master/examples/ecod_example.py): Example of using ECOD for outlier detection
  * [Isolation Forest](https://github.com/yzhao062/pyod/blob/master/examples/iforest_example.py): Example of using Isolation Forest for outlier detection


Alternatively, explore [MetaOD](https://github.com/yzhao062/MetaOD) for a data-driven approach.
**Citing PyOD** :
If you use PyOD in a scientific publication, we would appreciate citations to the following paper(s):
[PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection](https://arxiv.org/abs/2412.12154) is available as a preprint. If you use PyOD in a scientific publication, we would appreciate citations to the following paper:
```
@article{zhao2024pyod2,
    author  = {Chen, Sihan and Qian, Zhuangzhuang and Siu, Wingchun and Hu, Xingcan and Li, Jiaqi and Li, Shawn and Qin, Yuehan and Yang, Tiankai and Xiao, Zhuo and Ye, Wanghao and Zhang, Yichi and Dong, Yushun and Zhao, Yue},
    title   = {PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection},
    journal = {arXiv preprint arXiv:2412.12154},
    year    = {2024}
}

```

[PyOD paper](http://www.jmlr.org/papers/volume20/19-011/19-011.pdf) is published in [Journal of Machine Learning Research (JMLR)](http://www.jmlr.org/) (MLOSS track).:
```
@article{zhao2019pyod,
    author  = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
    title   = {PyOD: A Python Toolbox for Scalable Outlier Detection},
    journal = {Journal of Machine Learning Research},
    year    = {2019},
    volume  = {20},
    number  = {96},
    pages   = {1-7},
    url     = {http://jmlr.org/papers/v20/19-011.html}
}

```

or:
```
Zhao, Y., Nasrullah, Z. and Li, Z., 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of machine learning research (JMLR), 20(96), pp.1-7.

```

For a broader perspective on anomaly detection, see our NeurIPS papers [ADBench: Anomaly Detection Benchmark Paper](https://arxiv.org/abs/2206.09426) and [ADGym: Design Choices for Deep Anomaly Detection](https://arxiv.org/abs/2309.15376):
```
@article{han2022adbench,
    title={Adbench: Anomaly detection benchmark},
    author={Han, Songqiao and Hu, Xiyang and Huang, Hailiang and Jiang, Minqi and Zhao, Yue},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={32142--32159},
    year={2022}
}

@article{jiang2023adgym,
    title={ADGym: Design Choices for Deep Anomaly Detection},
    author={Jiang, Minqi and Hou, Chaochuan and Zheng, Ao and Han, Songqiao and Huang, Hailiang and Wen, Qingsong and Hu, Xiyang and Zhao, Yue},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2023}
}

```

* * *
###### ADBench Benchmark and Datasets[¬∂](https://pyod.readthedocs.io/en/latest/index.html#adbench-benchmark-and-datasets "Link to this heading")
We just released a 45-page, the most comprehensive [ADBench: Anomaly Detection Benchmark](https://arxiv.org/abs/2206.09426) [[AHHH+22](https://pyod.readthedocs.io/en/latest/index.html#id116 "Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: anomaly detection benchmark. arXiv preprint arXiv:2206.09426, 2022.")]. The fully [open-sourced ADBench](https://github.com/Minqi824/ADBench) compares 30 anomaly detection algorithms on 57 benchmark datasets.
The organization of **ADBench** is provided below:
[![benchmark-fig](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true) ](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true)
For a simpler visualization, we make **the comparison of selected models** via [compare_all_models.py](https://github.com/yzhao062/pyod/blob/master/examples/compare_all_models.py).
[![Comparison_of_All](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true) ](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true)
### Implemented Algorithms[¬∂](https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms "Link to this heading")
PyOD toolkit consists of three major functional groups:
**(i) Individual Detection Algorithms** :
Type | Abbr | Algorithm | Year | Class | Ref  
---|---|---|---|---|---  
Probabilistic | ECOD | Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions | 2022 | [`pyod.models.ecod.ECOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD "pyod.models.ecod.ECOD") | [[ALZH+22](https://pyod.readthedocs.io/en/latest/index.html#id109 "Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 2022.")]  
Probabilistic | COPOD | COPOD: Copula-Based Outlier Detection | 2020 | [`pyod.models.copod.COPOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD "pyod.models.copod.COPOD") | [[ALZB+20](https://pyod.readthedocs.io/en/latest/index.html#id103 "Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In IEEE International Conference on Data Mining \(ICDM\). IEEE, 2020.")]  
Probabilistic | ABOD | Angle-Based Outlier Detection | 2008 | [`pyod.models.abod.ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD "pyod.models.abod.ABOD") | [[AKZ+08](https://pyod.readthedocs.io/en/latest/index.html#id73 "Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444‚Äì452. ACM, 2008.")]  
Probabilistic | FastABOD | Fast Angle-Based Outlier Detection using approximation | 2008 | [`pyod.models.abod.ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD "pyod.models.abod.ABOD") | [[AKZ+08](https://pyod.readthedocs.io/en/latest/index.html#id73 "Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444‚Äì452. ACM, 2008.")]  
Probabilistic | MAD | Median Absolute Deviation (MAD) | 1993 | [`pyod.models.mad.MAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD "pyod.models.mad.MAD") | [[AIH93](https://pyod.readthedocs.io/en/latest/index.html#id102 "Boris Iglewicz and David Caster Hoaglin. How to detect and handle outliers. Volume 16. Asq Press, 1993.")]  
Probabilistic | SOS | Stochastic Outlier Selection | 2012 | [`pyod.models.sos.SOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS "pyod.models.sos.SOS") | [[AJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/index.html#id84 "JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.")]  
Probabilistic | QMCD | Quasi-Monte Carlo Discrepancy outlier detection | 2001 | [`pyod.models.qmcd.QMCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD "pyod.models.qmcd.QMCD") | [[AFM01](https://pyod.readthedocs.io/en/latest/index.html#id120 "Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. Journal of complexity, 17\(4\):608‚Äì624, 2001.")]  
Probabilistic | KDE | Outlier Detection with Kernel Density Functions | 2007 | [`pyod.models.kde.KDE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE "pyod.models.kde.KDE") | [[ALLP07](https://pyod.readthedocs.io/en/latest/index.html#id111 "Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition, 61‚Äì75. Springer, 2007.")]  
Probabilistic | Sampling | Rapid distance-based outlier detection via sampling | 2013 | [`pyod.models.sampling.Sampling`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling "pyod.models.sampling.Sampling") | [[ASB13](https://pyod.readthedocs.io/en/latest/index.html#id112 "Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. Advances in neural information processing systems, 2013.")]  
Probabilistic | GMM | Probabilistic Mixture Modeling for Outlier Analysis |  | [`pyod.models.gmm.GMM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM "pyod.models.gmm.GMM") | [[AAgg15](https://pyod.readthedocs.io/en/latest/index.html#id77 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] [Ch.2]  
Linear Model | PCA | Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes) | 2003 | [`pyod.models.pca.PCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA "pyod.models.pca.PCA") | [[ASCSC03](https://pyod.readthedocs.io/en/latest/index.html#id76 "Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.")]  
Linear Model | KPCA | Kernel Principal Component Analysis | 2007 | [`pyod.models.kpca.KPCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA "pyod.models.kpca.KPCA") | [[AHof07](https://pyod.readthedocs.io/en/latest/index.html#id119 "Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40\(3\):863‚Äì874, 2007.")]  
Linear Model | MCD | Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores) | 1999 | [`pyod.models.mcd.MCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD "pyod.models.mcd.MCD") | [[AHR04](https://pyod.readthedocs.io/en/latest/index.html#id81 "Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. Computational Statistics & Data Analysis, 44\(4\):625‚Äì638, 2004."), [ARD99](https://pyod.readthedocs.io/en/latest/index.html#id80 "Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. Technometrics, 41\(3\):212‚Äì223, 1999.")]  
Linear Model | CD | Use Cook‚Äôs distance for outlier detection | 1977 | [`pyod.models.cd.CD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD "pyod.models.cd.CD") | [[ACoo77](https://pyod.readthedocs.io/en/latest/index.html#id110 "R Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19\(1\):15‚Äì18, 1977.")]  
Linear Model | OCSVM | One-Class Support Vector Machines | 2001 | [`pyod.models.ocsvm.OCSVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM "pyod.models.ocsvm.OCSVM") | [[AScholkopfPST+01](https://pyod.readthedocs.io/en/latest/index.html#id91 "Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13\(7\):1443‚Äì1471, 2001.")]  
Linear Model | LMDD | Deviation-based Outlier Detection (LMDD) | 1996 | [`pyod.models.lmdd.LMDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD "pyod.models.lmdd.LMDD") | [[AAAR96](https://pyod.readthedocs.io/en/latest/index.html#id98 "Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In KDD, volume 1141, 972‚Äì981. 1996.")]  
Proximity-Based | LOF | Local Outlier Factor | 2000 | [`pyod.models.lof.LOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF "pyod.models.lof.LOF") | [[ABKNS00](https://pyod.readthedocs.io/en/latest/index.html#id78 "Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In ACM sigmod record, volume 29, 93‚Äì104. ACM, 2000.")]  
Proximity-Based | COF | Connectivity-Based Outlier Factor | 2002 | [`pyod.models.cof.COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF "pyod.models.cof.COF") | [[ATCFC02](https://pyod.readthedocs.io/en/latest/index.html#id92 "Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535‚Äì548. Springer, 2002.")]  
Proximity-Based | Incr. COF | Memory Efficient Connectivity-Based Outlier Factor (slower but reduce storage complexity) | 2002 | [`pyod.models.cof.COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF "pyod.models.cof.COF") | [[ATCFC02](https://pyod.readthedocs.io/en/latest/index.html#id92 "Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535‚Äì548. Springer, 2002.")]  
Proximity-Based | CBLOF | Clustering-Based Local Outlier Factor | 2003 | [`pyod.models.cblof.CBLOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF "pyod.models.cblof.CBLOF") | [[AHXD03](https://pyod.readthedocs.io/en/latest/index.html#id82 "Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. Pattern Recognition Letters, 24\(9-10\):1641‚Äì1650, 2003.")]  
Proximity-Based | LOCI | LOCI: Fast outlier detection using the local correlation integral | 2003 | [`pyod.models.loci.LOCI`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI "pyod.models.loci.LOCI") | [[APKGF03](https://pyod.readthedocs.io/en/latest/index.html#id85 "Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In Data Engineering, 2003. Proceedings. 19th International Conference on, 315‚Äì326. IEEE, 2003.")]  
Proximity-Based | HBOS | Histogram-based Outlier Score | 2012 | [`pyod.models.hbos.HBOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS "pyod.models.hbos.HBOS") | [[AGD12](https://pyod.readthedocs.io/en/latest/index.html#id75 "Markus Goldstein and Andreas Dengel. Histogram-based outlier score \(hbos\): a fast unsupervised anomaly detection algorithm. KI-2012: Poster and Demo Track, pages 59‚Äì63, 2012.")]  
Proximity-Based | kNN | k Nearest Neighbors (use the distance to the kth nearest neighbor as the outlier score | 2000 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/index.html#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/index.html#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | AvgKNN | Average kNN (use the average distance to k nearest neighbors as the outlier score) | 2002 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/index.html#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/index.html#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | MedKNN | Median kNN (use the median distance to k nearest neighbors as the outlier score) | 2002 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/index.html#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/index.html#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | SOD | Subspace Outlier Detection | 2009 | [`pyod.models.sod.SOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD "pyod.models.sod.SOD") | [[AKKrogerSZ09](https://pyod.readthedocs.io/en/latest/index.html#id94 "Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 831‚Äì838. Springer, 2009.")]  
Proximity-Based | ROD | Rotation-based Outlier Detection | 2020 | [`pyod.models.rod.ROD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD "pyod.models.rod.ROD") | [[AABC20](https://pyod.readthedocs.io/en/latest/index.html#id104 "Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. IEEE Transactions on Knowledge and Data Engineering, 2020.")]  
Outlier Ensembles | IForest | Isolation Forest | 2008 | [`pyod.models.iforest.IForest`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest "pyod.models.iforest.IForest") | [[ALTZ08](https://pyod.readthedocs.io/en/latest/index.html#id67 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, 413‚Äì422. IEEE, 2008."), [ALTZ12](https://pyod.readthedocs.io/en/latest/index.html#id68 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data \(TKDD\), 6\(1\):3, 2012.")]  
Outlier Ensembles | INNE | Isolation-based Anomaly Detection Using Nearest-Neighbor Ensembles | 2018 | [`pyod.models.inne.INNE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE "pyod.models.inne.INNE") | [[ABTA+18](https://pyod.readthedocs.io/en/latest/index.html#id113 "Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. Computational Intelligence, 34\(4\):968‚Äì998, 2018.")]  
Outlier Ensembles | DIF | Deep Isolation Forest for Anomaly Detection | 2023 | [`pyod.models.dif.DIF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF "pyod.models.dif.DIF") | [[AXPWW23](https://pyod.readthedocs.io/en/latest/index.html#id121 "Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, \(\):1-14, 2023. doi:10.1109/TKDE.2023.3270293.")]  
Outlier Ensembles | FB | Feature Bagging | 2005 | [`pyod.models.feature_bagging.FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging "pyod.models.feature_bagging.FeatureBagging") | [[ALK05](https://pyod.readthedocs.io/en/latest/index.html#id74 "Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157‚Äì166. ACM, 2005.")]  
Outlier Ensembles | LSCP | LSCP: Locally Selective Combination of Parallel Outlier Ensembles | 2019 | [`pyod.models.lscp.LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP "pyod.models.lscp.LSCP") | [[AZNHL19](https://pyod.readthedocs.io/en/latest/index.html#id86 "Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.")]  
Outlier Ensembles | XGBOD | Extreme Boosting Based Outlier Detection **(Supervised)** | 2018 | [`pyod.models.xgbod.XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD "pyod.models.xgbod.XGBOD") | [[AZH18](https://pyod.readthedocs.io/en/latest/index.html#id79 "Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks \(IJCNN\). IEEE, 2018.")]  
Outlier Ensembles | LODA | Lightweight On-line Detector of Anomalies | 2016 | [`pyod.models.loda.LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA "pyod.models.loda.LODA") | [[APevny16](https://pyod.readthedocs.io/en/latest/index.html#id100 "Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102\(2\):275‚Äì304, 2016.")]  
Outlier Ensembles | SUOD | SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection **(Acceleration)** | 2021 | [`pyod.models.suod.SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD "pyod.models.suod.SUOD") | [[AZHC+21](https://pyod.readthedocs.io/en/latest/index.html#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")]  
Neural Networks | AutoEncoder | Fully connected AutoEncoder (use reconstruction error as the outlier score) | 2015 | [`pyod.models.auto_encoder.AutoEncoder`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder "pyod.models.auto_encoder.AutoEncoder") | [[AAgg15](https://pyod.readthedocs.io/en/latest/index.html#id77 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] [Ch.3]  
Neural Networks | VAE | Variational AutoEncoder (use reconstruction error as the outlier score) | 2013 | [`pyod.models.vae.VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE "pyod.models.vae.VAE") | [[AKW13](https://pyod.readthedocs.io/en/latest/index.html#id99 "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.")]  
Neural Networks | Beta-VAE | Variational AutoEncoder (all customized loss term by varying gamma and capacity) | 2018 | [`pyod.models.vae.VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE "pyod.models.vae.VAE") | [[ABHP+18](https://pyod.readthedocs.io/en/latest/index.html#id101 "Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.")]  
Neural Networks | SO_GAAL | Single-Objective Generative Adversarial Active Learning | 2019 | [`pyod.models.so_gaal.SO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL "pyod.models.so_gaal.SO_GAAL") | [[ALLZ+19](https://pyod.readthedocs.io/en/latest/index.html#id87 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")]  
Neural Networks | MO_GAAL | Multiple-Objective Generative Adversarial Active Learning | 2019 | [`pyod.models.mo_gaal.MO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL "pyod.models.mo_gaal.MO_GAAL") | [[ALLZ+19](https://pyod.readthedocs.io/en/latest/index.html#id87 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")]  
Neural Networks | DeepSVDD | Deep One-Class Classification | 2018 | [`pyod.models.deep_svdd.DeepSVDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD "pyod.models.deep_svdd.DeepSVDD") | [[ARVG+18](https://pyod.readthedocs.io/en/latest/index.html#id106 "Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. International conference on machine learning, 2018.")]  
Neural Networks | AnoGAN | Anomaly Detection with Generative Adversarial Networks | 2017 | [`pyod.models.anogan.AnoGAN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN "pyod.models.anogan.AnoGAN") | [[ASSeebockW+17](https://pyod.readthedocs.io/en/latest/index.html#id114 "Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, 146‚Äì157. Springer, 2017.")]  
Neural Networks | ALAD | Adversarially learned anomaly detection | 2018 | [`pyod.models.alad.ALAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD "pyod.models.alad.ALAD") | [[AZRF+18](https://pyod.readthedocs.io/en/latest/index.html#id118 "Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining \(ICDM\), 727‚Äì736. IEEE, 2018.")]  
Neural Networks | DevNet | Deep Anomaly Detection with Deviation Networks | 2019 | [`pyod.models.devnet.DevNet`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet "pyod.models.devnet.DevNet") | [[APSVDH19](https://pyod.readthedocs.io/en/latest/index.html#id123 "Guansong Pang, Chunhua Shen, and Anton Van Den Hengel. Deep anomaly detection with deviation networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 353‚Äì362. 2019.")]  
Neural Networks | AE1SVM | Autoencoder-based One-class Support Vector Machine | 2019 | [`pyod.models.ae1svm.AE1SVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM "pyod.models.ae1svm.AE1SVM") | [[ANV19](https://pyod.readthedocs.io/en/latest/index.html#id122 "Minh-Nghia Nguyen and Ngo Anh Vien. Scalable and interpretable one-class svms with deep learning and random fourier features. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10‚Äì14, 2018, Proceedings, Part I 18, 157‚Äì172. Springer, 2019.")]  
Graph-based | R-Graph | Outlier detection by R-graph | 2017 | [`pyod.models.rgraph.RGraph`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph "pyod.models.rgraph.RGraph") | [[AYRV17](https://pyod.readthedocs.io/en/latest/index.html#id117 "Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3395‚Äì3404. 2017.")]  
Graph-based | LUNAR | LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks | 2022 | [`pyod.models.lunar.LUNAR`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR "pyod.models.lunar.LUNAR") | [[AGHNN22](https://pyod.readthedocs.io/en/latest/index.html#id115 "Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 6737‚Äì6745. 2022.")]  
**(ii) Outlier Ensembles & Outlier Detector Combination Frameworks**:
Type | Abbr | Algorithm | Year | Ref |   
---|---|---|---|---|---  
Outlier Ensembles |  | Feature Bagging | 2005 | [`pyod.models.feature_bagging.FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging "pyod.models.feature_bagging.FeatureBagging") | [[ALK05](https://pyod.readthedocs.io/en/latest/index.html#id74 "Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157‚Äì166. ACM, 2005.")]  
Outlier Ensembles | LSCP | LSCP: Locally Selective Combination of Parallel Outlier Ensembles | 2019 | [`pyod.models.lscp.LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP "pyod.models.lscp.LSCP") | [[AZNHL19](https://pyod.readthedocs.io/en/latest/index.html#id86 "Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.")]  
Outlier Ensembles | XGBOD | Extreme Boosting Based Outlier Detection **(Supervised)** | 2018 | [`pyod.models.xgbod.XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD "pyod.models.xgbod.XGBOD") | [[AZH18](https://pyod.readthedocs.io/en/latest/index.html#id79 "Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks \(IJCNN\). IEEE, 2018.")]  
Outlier Ensembles | LODA | Lightweight On-line Detector of Anomalies | 2016 | [`pyod.models.loda.LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA "pyod.models.loda.LODA") | [[APevny16](https://pyod.readthedocs.io/en/latest/index.html#id100 "Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102\(2\):275‚Äì304, 2016.")]  
Outlier Ensembles | SUOD | SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection **(Acceleration)** | 2021 | [`pyod.models.suod.SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD "pyod.models.suod.SUOD") | [[AZHC+21](https://pyod.readthedocs.io/en/latest/index.html#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")]  
Combination | Average | Simple combination by averaging the scores | 2015 | [`pyod.models.combination.average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average "pyod.models.combination.average") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Weighted Average | Simple combination by averaging the scores with detector weights | 2015 | [`pyod.models.combination.average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average "pyod.models.combination.average") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Maximization | Simple combination by taking the maximum scores | 2015 | [`pyod.models.combination.maximization()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization "pyod.models.combination.maximization") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | AOM | Average of Maximum | 2015 | [`pyod.models.combination.aom()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom "pyod.models.combination.aom") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | MOA | Maximum of Average | 2015 | [`pyod.models.combination.moa()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa "pyod.models.combination.moa") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Median | Simple combination by taking the median of the scores | 2015 | [`pyod.models.combination.median()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median "pyod.models.combination.median") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | majority Vote | Simple combination by taking the majority vote of the labels (weights can be used) | 2015 | [`pyod.models.combination.majority_vote()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote "pyod.models.combination.majority_vote") | [[AAS15](https://pyod.readthedocs.io/en/latest/index.html#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
**(iii) Utility Functions** :
Type | Name | Function  
---|---|---  
Data | [`pyod.utils.data.generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data "pyod.utils.data.generate_data") | Synthesized data generation; normal data is generated by a multivariate Gaussian and outliers are generated by a uniform distribution  
Data | [`pyod.utils.data.generate_data_clusters()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters "pyod.utils.data.generate_data_clusters") | Synthesized data generation in clusters; more complex data patterns can be created with multiple clusters  
Stat | [`pyod.utils.stat_models.wpearsonr()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr "pyod.utils.stat_models.wpearsonr") | Calculate the weighted Pearson correlation of two samples  
Utility | [`pyod.utils.utility.get_label_n()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n "pyod.utils.utility.get_label_n") | Turn raw outlier scores into binary labels by assign 1 to top n outlier scores  
Utility | [`pyod.utils.utility.precision_n_scores()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores "pyod.utils.utility.precision_n_scores") | calculate precision @ rank n  
### API Cheatsheet & Reference[¬∂](https://pyod.readthedocs.io/en/latest/index.html#api-cheatsheet-reference "Link to this heading")
The following APIs are applicable for all detector models for easy use.
  * [`pyod.models.base.BaseDetector.fit()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit "pyod.models.base.BaseDetector.fit"): Fit detector. y is ignored in unsupervised methods.
  * [`pyod.models.base.BaseDetector.decision_function()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function "pyod.models.base.BaseDetector.decision_function"): Predict raw anomaly score of X using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict "pyod.models.base.BaseDetector.predict"): Predict if a particular sample is an outlier or not using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_proba()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba "pyod.models.base.BaseDetector.predict_proba"): Predict the probability of a sample being outlier using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_confidence()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence "pyod.models.base.BaseDetector.predict_confidence"): Predict the model‚Äôs sample-wise confidence (available in predict and predict_proba).


Key Attributes of a fitted model:
  * `pyod.models.base.BaseDetector.decision_scores_`: The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores.
  * `pyod.models.base.BaseDetector.labels_`: The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies.


* * *
* * *
References
[AAgg15] ([1](https://pyod.readthedocs.io/en/latest/index.html#id16),[2](https://pyod.readthedocs.io/en/latest/index.html#id42))
Charu C Aggarwal. Outlier analysis. In _Data mining_ , 75‚Äì79. Springer, 2015.
[AAS15] ([1](https://pyod.readthedocs.io/en/latest/index.html#id59),[2](https://pyod.readthedocs.io/en/latest/index.html#id60),[3](https://pyod.readthedocs.io/en/latest/index.html#id61),[4](https://pyod.readthedocs.io/en/latest/index.html#id62),[5](https://pyod.readthedocs.io/en/latest/index.html#id63),[6](https://pyod.readthedocs.io/en/latest/index.html#id64),[7](https://pyod.readthedocs.io/en/latest/index.html#id65))
Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. _ACM SIGKDD Explorations Newsletter_ , 17(1):24‚Äì47, 2015.
[[AABC20](https://pyod.readthedocs.io/en/latest/index.html#id33)]
Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. _IEEE Transactions on Knowledge and Data Engineering_ , 2020.
[AAP02] ([1](https://pyod.readthedocs.io/en/latest/index.html#id29),[2](https://pyod.readthedocs.io/en/latest/index.html#id30),[3](https://pyod.readthedocs.io/en/latest/index.html#id31))
Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In _European Conference on Principles of Data Mining and Knowledge Discovery_ , 15‚Äì27. Springer, 2002.
[[AAAR96](https://pyod.readthedocs.io/en/latest/index.html#id22)]
Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In _KDD_ , volume 1141, 972‚Äì981. 1996.
[[ABTA+18](https://pyod.readthedocs.io/en/latest/index.html#id35)]
Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. _Computational Intelligence_ , 34(4):968‚Äì998, 2018.
[[ABKNS00](https://pyod.readthedocs.io/en/latest/index.html#id23)]
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In _ACM sigmod record_ , volume 29, 93‚Äì104. ACM, 2000.
[[ABHP+18](https://pyod.readthedocs.io/en/latest/index.html#id44)]
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. _arXiv preprint arXiv:1804.03599_ , 2018.
[[ACQS+24](https://pyod.readthedocs.io/en/latest/index.html#id1)]
Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, and others. Pyod 2: a python library for outlier detection with llm-powered model selection. _arXiv preprint arXiv:2412.12154_ , 2024.
[[ACoo77](https://pyod.readthedocs.io/en/latest/index.html#id20)]
R Dennis Cook. Detection of influential observation in linear regression. _Technometrics_ , 19(1):15‚Äì18, 1977.
[[AFM01](https://pyod.readthedocs.io/en/latest/index.html#id13)]
Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. _Journal of complexity_ , 17(4):608‚Äì624, 2001.
[[AGD12](https://pyod.readthedocs.io/en/latest/index.html#id28)]
Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. _KI-2012: Poster and Demo Track_ , pages 59‚Äì63, 2012.
[[AGHNN22](https://pyod.readthedocs.io/en/latest/index.html#id53)]
Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_ , volume 36, 6737‚Äì6745. 2022.
[[AHHH+22](https://pyod.readthedocs.io/en/latest/index.html#id5)]
Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: anomaly detection benchmark. _arXiv preprint arXiv:2206.09426_ , 2022.
[[AHR04](https://pyod.readthedocs.io/en/latest/index.html#id19)]
Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. _Computational Statistics & Data Analysis_, 44(4):625‚Äì638, 2004.
[[AHXD03](https://pyod.readthedocs.io/en/latest/index.html#id26)]
Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. _Pattern Recognition Letters_ , 24(9-10):1641‚Äì1650, 2003.
[[AHof07](https://pyod.readthedocs.io/en/latest/index.html#id18)]
Heiko Hoffmann. Kernel pca for novelty detection. _Pattern recognition_ , 40(3):863‚Äì874, 2007.
[[AIH93](https://pyod.readthedocs.io/en/latest/index.html#id11)]
Boris Iglewicz and David Caster Hoaglin. _How to detect and handle outliers_. Volume 16. Asq Press, 1993.
[[AJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/index.html#id12)]
JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.
[[AKW13](https://pyod.readthedocs.io/en/latest/index.html#id43)]
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ , 2013.
[[AKKrogerSZ09](https://pyod.readthedocs.io/en/latest/index.html#id32)]
Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 831‚Äì838. Springer, 2009.
[AKZ+08] ([1](https://pyod.readthedocs.io/en/latest/index.html#id9),[2](https://pyod.readthedocs.io/en/latest/index.html#id10))
Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_ , 444‚Äì452. ACM, 2008.
[[ALLP07](https://pyod.readthedocs.io/en/latest/index.html#id14)]
Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In _International Workshop on Machine Learning and Data Mining in Pattern Recognition_ , 61‚Äì75. Springer, 2007.
[ALK05] ([1](https://pyod.readthedocs.io/en/latest/index.html#id37),[2](https://pyod.readthedocs.io/en/latest/index.html#id54))
Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In _Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining_ , 157‚Äì166. ACM, 2005.
[[ALLX+24](https://pyod.readthedocs.io/en/latest/index.html#id2)]
Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, and Yue Zhao. Nlp-adbench: nlp anomaly detection benchmark. _arXiv preprint arXiv:2412.04784_ , 2024.
[[ALZB+20](https://pyod.readthedocs.io/en/latest/index.html#id8)]
Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In _IEEE International Conference on Data Mining (ICDM)_. IEEE, 2020.
[[ALZH+22](https://pyod.readthedocs.io/en/latest/index.html#id7)]
Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_ , 2022.
[[ALTZ08](https://pyod.readthedocs.io/en/latest/index.html#id34)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on_ , 413‚Äì422. IEEE, 2008.
[[ALTZ12](https://pyod.readthedocs.io/en/latest/index.html#id34)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_ , 6(1):3, 2012.
[ALLZ+19] ([1](https://pyod.readthedocs.io/en/latest/index.html#id45),[2](https://pyod.readthedocs.io/en/latest/index.html#id46))
Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. _IEEE Transactions on Knowledge and Data Engineering_ , 2019.
[[ANV19](https://pyod.readthedocs.io/en/latest/index.html#id51)]
Minh-Nghia Nguyen and Ngo Anh Vien. Scalable and interpretable one-class svms with deep learning and random fourier features. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10‚Äì14, 2018, Proceedings, Part I 18_ , 157‚Äì172. Springer, 2019.
[[APSVDH19](https://pyod.readthedocs.io/en/latest/index.html#id50)]
Guansong Pang, Chunhua Shen, and Anton Van Den Hengel. Deep anomaly detection with deviation networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, 353‚Äì362. 2019.
[[APKGF03](https://pyod.readthedocs.io/en/latest/index.html#id27)]
Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In _Data Engineering, 2003. Proceedings. 19th International Conference on_ , 315‚Äì326. IEEE, 2003.
[APVD20]
Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_ , 227‚Äì243. Springer, 2020.
[APevny16] ([1](https://pyod.readthedocs.io/en/latest/index.html#id40),[2](https://pyod.readthedocs.io/en/latest/index.html#id57))
Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. _Machine Learning_ , 102(2):275‚Äì304, 2016.
[ARRS00] ([1](https://pyod.readthedocs.io/en/latest/index.html#id29),[2](https://pyod.readthedocs.io/en/latest/index.html#id30),[3](https://pyod.readthedocs.io/en/latest/index.html#id31))
Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In _ACM Sigmod Record_ , volume 29, 427‚Äì438. ACM, 2000.
[[ARD99](https://pyod.readthedocs.io/en/latest/index.html#id19)]
Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. _Technometrics_ , 41(3):212‚Äì223, 1999.
[[ARVG+18](https://pyod.readthedocs.io/en/latest/index.html#id47)]
Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. _International conference on machine learning_ , 2018.
[[ASSeebockW+17](https://pyod.readthedocs.io/en/latest/index.html#id48)]
Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In _International conference on information processing in medical imaging_ , 146‚Äì157. Springer, 2017.
[[AScholkopfPST+01](https://pyod.readthedocs.io/en/latest/index.html#id21)]
Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_ , 13(7):1443‚Äì1471, 2001.
[[ASCSC03](https://pyod.readthedocs.io/en/latest/index.html#id17)]
Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.
[[ASB13](https://pyod.readthedocs.io/en/latest/index.html#id15)]
Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. _Advances in neural information processing systems_ , 2013.
[ATCFC02] ([1](https://pyod.readthedocs.io/en/latest/index.html#id24),[2](https://pyod.readthedocs.io/en/latest/index.html#id25))
Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 535‚Äì548. Springer, 2002.
[[AXPWW23](https://pyod.readthedocs.io/en/latest/index.html#id36)]
Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_ , ():1‚Äì14, 2023. [doi:10.1109/TKDE.2023.3270293](https://doi.org/10.1109/TKDE.2023.3270293).
[[AYNL+24](https://pyod.readthedocs.io/en/latest/index.html#id3)]
Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, and others. Ad-llm: benchmarking large language models for anomaly detection. _arXiv preprint arXiv:2412.11142_ , 2024.
[[AYRV17](https://pyod.readthedocs.io/en/latest/index.html#id52)]
Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ , 3395‚Äì3404. 2017.
[[AZRF+18](https://pyod.readthedocs.io/en/latest/index.html#id49)]
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In _2018 IEEE International conference on data mining (ICDM)_ , 727‚Äì736. IEEE, 2018.
[AZH18] ([1](https://pyod.readthedocs.io/en/latest/index.html#id39),[2](https://pyod.readthedocs.io/en/latest/index.html#id56))
Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In _International Joint Conference on Neural Networks (IJCNN)_. IEEE, 2018.
[AZHC+21] ([1](https://pyod.readthedocs.io/en/latest/index.html#id4),[2](https://pyod.readthedocs.io/en/latest/index.html#id41),[3](https://pyod.readthedocs.io/en/latest/index.html#id58))
Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. _Proceedings of Machine Learning and Systems_ , 2021.
[AZNHL19] ([1](https://pyod.readthedocs.io/en/latest/index.html#id38),[2](https://pyod.readthedocs.io/en/latest/index.html#id55))
Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In _Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019_ , 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: <https://doi.org/10.1137/1.9781611975673.66>, [doi:10.1137/1.9781611975673.66](https://doi.org/10.1137/1.9781611975673.66).
[ Next Installation ](https://pyod.readthedocs.io/en/latest/install.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Welcome to PyOD V2 documentation!](https://pyod.readthedocs.io/en/latest/index.html)
    * [Read Me First](https://pyod.readthedocs.io/en/latest/index.html#read-me-first)
    * [About PyOD](https://pyod.readthedocs.io/en/latest/index.html#about-pyod)
    * [ADBench Benchmark and Datasets](https://pyod.readthedocs.io/en/latest/index.html#adbench-benchmark-and-datasets)
  * [Implemented Algorithms](https://pyod.readthedocs.io/en/latest/index.html#implemented-algorithms)
  * [API Cheatsheet & Reference](https://pyod.readthedocs.io/en/latest/index.html#api-cheatsheet-reference)



---

## 2. pyod 2.0.5 documentation {#2-1}

**URL:** https://pyod.readthedocs.io/en/latest/?badge=latest
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:01

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/?badge=latest#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/?badge=latest)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/?badge=latest)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/?badge=latest)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/index.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Welcome to PyOD V2 documentation![¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#welcome-to-pyod-v2-documentation "Link to this heading")
**Deployment & Documentation & Stats & License**
[![PyPI version](https://img.shields.io/pypi/v/pyod.svg?color=brightgreen) ](https://pypi.org/project/pyod/) [![Anaconda version](https://anaconda.org/conda-forge/pyod/badges/version.svg) ](https://anaconda.org/conda-forge/pyod) [![Documentation status](https://readthedocs.org/projects/pyod/badge/?version=latest) ](https://pyod.readthedocs.io/en/latest/?badge=latest) [![GitHub stars](https://img.shields.io/github/stars/yzhao062/pyod.svg) ](https://github.com/yzhao062/pyod/stargazers) [![GitHub forks](https://img.shields.io/github/forks/yzhao062/pyod.svg?color=blue) ](https://github.com/yzhao062/pyod/network) [![Downloads](https://pepy.tech/badge/pyod) ](https://pepy.tech/project/pyod) [![Testing](https://github.com/yzhao062/pyod/actions/workflows/testing.yml/badge.svg) ](https://github.com/yzhao062/pyod/actions/workflows/testing.yml) [![Coverage Status](https://coveralls.io/repos/github/yzhao062/pyod/badge.svg) ](https://coveralls.io/github/yzhao062/pyod) [![Maintainability](https://api.codeclimate.com/v1/badges/bdc3d8d0454274c753c4/maintainability) ](https://codeclimate.com/github/yzhao062/Pyod/maintainability) [![License](https://img.shields.io/github/license/yzhao062/pyod.svg) ](https://github.com/yzhao062/pyod/blob/master/LICENSE) [![Benchmark](https://img.shields.io/badge/ADBench-benchmark_results-pink) ](https://github.com/Minqi824/ADBench)
* * *
###### Read Me First[¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#read-me-first "Link to this heading")
Welcome to PyOD, a comprehensive but easy-to-use Python library for detecting anomalies in multivariate data. Whether you are working with a small-scale project or large datasets, PyOD provides a range of algorithms to suit your needs.
**PyOD Version 2 is now available** ([Paper](https://www.arxiv.org/abs/2412.12154)) [[ACQS+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id126 "Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, and others. Pyod 2: a python library for outlier detection with llm-powered model selection. arXiv preprint arXiv:2412.12154, 2024.")], featuring:
  * **Expanded Deep Learning Support** : Integrates 12 modern neural models into a single PyTorch-based framework, bringing the total number of outlier detection methods to 45.
  * **Enhanced Performance and Ease of Use** : Models are optimized for efficiency and consistent performance across different datasets.
  * **LLM-based Model Selection** : Automated model selection guided by a large language model reduces manual tuning and assists users who may have limited experience with outlier detection.


**Additional Resources** :
  * **NLP Anomaly Detection** : [NLP-ADBench](https://github.com/USC-FORTIS/NLP-ADBench) provides both NLP anomaly detection datasets and algorithms [[ALLX+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id127 "Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, and Yue Zhao. Nlp-adbench: nlp anomaly detection benchmark. arXiv preprint arXiv:2412.04784, 2024.")]
  * **Time-series Outlier Detection** : [TODS](https://github.com/datamllab/tods)
  * **Graph Outlier Detection** : [PyGOD](https://pygod.org/)
  * **Performance Comparison & Datasets**: We have a 45-page, comprehensive [anomaly detection benchmark paper](https://openreview.net/forum?id=foA_SFQ9zo0). The fully [open-sourced ADBench](https://github.com/Minqi824/ADBench) compares 30 anomaly detection algorithms on 57 benchmark datasets.
  * **PyOD on Distributed Systems** : You can also run [PyOD on Databricks](https://www.databricks.com/blog/2023/03/13/unsupervised-outlier-detection-databricks.html)
  * **Learn More** : [Anomaly Detection Resources](https://github.com/yzhao062/anomaly-detection-resources)


**Check out our latest research on LLM-based anomaly detection** [[AYNL+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id125 "Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, and others. Ad-llm: benchmarking large language models for anomaly detection. arXiv preprint arXiv:2412.11142, 2024.")]: [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/abs/2412.11142).
* * *
###### About PyOD[¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#about-pyod "Link to this heading")
PyOD, established in 2017, has become a go-to **Python library** for **detecting anomalous/outlying objects** in multivariate data. This exciting yet challenging field is commonly referred to as [Outlier Detection](https://en.wikipedia.org/wiki/Anomaly_detection) or [Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection).
PyOD includes more than 50 detection algorithms, from classical LOF (SIGMOD 2000) to the cutting-edge ECOD and DIF (TKDE 2022 and 2023). Since 2017, PyOD has been successfully used in numerous academic research projects and commercial products with more than [26 million downloads](https://pepy.tech/project/pyod). It is also well acknowledged by the machine learning community with various dedicated posts/tutorials, including [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/), [KDnuggets](https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html), and [Towards Data Science](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1).
**PyOD is featured for** :
  * **Unified, User-Friendly Interface** across various algorithms.
  * **Wide Range of Models** , from classic techniques to the latest deep learning methods in **PyTorch**.
  * **High Performance & Efficiency**, leveraging [numba](https://github.com/numba/numba) and [joblib](https://github.com/joblib/joblib) for JIT compilation and parallel processing.
  * **Fast Training & Prediction**, achieved through the SUOD framework [[AZHC+21](https://pyod.readthedocs.io/en/latest/?badge=latest#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")].


**Outlier Detection with 5 Lines of Code** :
```
### Example: Training an ECOD detector
frompyod.models.ecodimport ECOD
clf = ECOD()
clf.fit(X_train)
y_train_scores = clf.decision_scores_  ### Outlier scores for training data
y_test_scores = clf.decision_function(X_test)  ### Outlier scores for test data

```

**Selecting the Right Algorithm:** Unsure where to start? Consider these robust and interpretable options:
  * [ECOD](https://github.com/yzhao062/pyod/blob/master/examples/ecod_example.py): Example of using ECOD for outlier detection
  * [Isolation Forest](https://github.com/yzhao062/pyod/blob/master/examples/iforest_example.py): Example of using Isolation Forest for outlier detection


Alternatively, explore [MetaOD](https://github.com/yzhao062/MetaOD) for a data-driven approach.
**Citing PyOD** :
If you use PyOD in a scientific publication, we would appreciate citations to the following paper(s):
[PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection](https://arxiv.org/abs/2412.12154) is available as a preprint. If you use PyOD in a scientific publication, we would appreciate citations to the following paper:
```
@article{zhao2024pyod2,
    author  = {Chen, Sihan and Qian, Zhuangzhuang and Siu, Wingchun and Hu, Xingcan and Li, Jiaqi and Li, Shawn and Qin, Yuehan and Yang, Tiankai and Xiao, Zhuo and Ye, Wanghao and Zhang, Yichi and Dong, Yushun and Zhao, Yue},
    title   = {PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection},
    journal = {arXiv preprint arXiv:2412.12154},
    year    = {2024}
}

```

[PyOD paper](http://www.jmlr.org/papers/volume20/19-011/19-011.pdf) is published in [Journal of Machine Learning Research (JMLR)](http://www.jmlr.org/) (MLOSS track).:
```
@article{zhao2019pyod,
    author  = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
    title   = {PyOD: A Python Toolbox for Scalable Outlier Detection},
    journal = {Journal of Machine Learning Research},
    year    = {2019},
    volume  = {20},
    number  = {96},
    pages   = {1-7},
    url     = {http://jmlr.org/papers/v20/19-011.html}
}

```

or:
```
Zhao, Y., Nasrullah, Z. and Li, Z., 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of machine learning research (JMLR), 20(96), pp.1-7.

```

For a broader perspective on anomaly detection, see our NeurIPS papers [ADBench: Anomaly Detection Benchmark Paper](https://arxiv.org/abs/2206.09426) and [ADGym: Design Choices for Deep Anomaly Detection](https://arxiv.org/abs/2309.15376):
```
@article{han2022adbench,
    title={Adbench: Anomaly detection benchmark},
    author={Han, Songqiao and Hu, Xiyang and Huang, Hailiang and Jiang, Minqi and Zhao, Yue},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={32142--32159},
    year={2022}
}

@article{jiang2023adgym,
    title={ADGym: Design Choices for Deep Anomaly Detection},
    author={Jiang, Minqi and Hou, Chaochuan and Zheng, Ao and Han, Songqiao and Huang, Hailiang and Wen, Qingsong and Hu, Xiyang and Zhao, Yue},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2023}
}

```

* * *
###### ADBench Benchmark and Datasets[¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#adbench-benchmark-and-datasets "Link to this heading")
We just released a 45-page, the most comprehensive [ADBench: Anomaly Detection Benchmark](https://arxiv.org/abs/2206.09426) [[AHHH+22](https://pyod.readthedocs.io/en/latest/?badge=latest#id116 "Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: anomaly detection benchmark. arXiv preprint arXiv:2206.09426, 2022.")]. The fully [open-sourced ADBench](https://github.com/Minqi824/ADBench) compares 30 anomaly detection algorithms on 57 benchmark datasets.
The organization of **ADBench** is provided below:
[![benchmark-fig](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true) ](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true)
For a simpler visualization, we make **the comparison of selected models** via [compare_all_models.py](https://github.com/yzhao062/pyod/blob/master/examples/compare_all_models.py).
[![Comparison_of_All](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true) ](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true)
### Implemented Algorithms[¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#implemented-algorithms "Link to this heading")
PyOD toolkit consists of three major functional groups:
**(i) Individual Detection Algorithms** :
Type | Abbr | Algorithm | Year | Class | Ref  
---|---|---|---|---|---  
Probabilistic | ECOD | Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions | 2022 | [`pyod.models.ecod.ECOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD "pyod.models.ecod.ECOD") | [[ALZH+22](https://pyod.readthedocs.io/en/latest/?badge=latest#id109 "Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 2022.")]  
Probabilistic | COPOD | COPOD: Copula-Based Outlier Detection | 2020 | [`pyod.models.copod.COPOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD "pyod.models.copod.COPOD") | [[ALZB+20](https://pyod.readthedocs.io/en/latest/?badge=latest#id103 "Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In IEEE International Conference on Data Mining \(ICDM\). IEEE, 2020.")]  
Probabilistic | ABOD | Angle-Based Outlier Detection | 2008 | [`pyod.models.abod.ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD "pyod.models.abod.ABOD") | [[AKZ+08](https://pyod.readthedocs.io/en/latest/?badge=latest#id73 "Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444‚Äì452. ACM, 2008.")]  
Probabilistic | FastABOD | Fast Angle-Based Outlier Detection using approximation | 2008 | [`pyod.models.abod.ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD "pyod.models.abod.ABOD") | [[AKZ+08](https://pyod.readthedocs.io/en/latest/?badge=latest#id73 "Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444‚Äì452. ACM, 2008.")]  
Probabilistic | MAD | Median Absolute Deviation (MAD) | 1993 | [`pyod.models.mad.MAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD "pyod.models.mad.MAD") | [[AIH93](https://pyod.readthedocs.io/en/latest/?badge=latest#id102 "Boris Iglewicz and David Caster Hoaglin. How to detect and handle outliers. Volume 16. Asq Press, 1993.")]  
Probabilistic | SOS | Stochastic Outlier Selection | 2012 | [`pyod.models.sos.SOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS "pyod.models.sos.SOS") | [[AJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/?badge=latest#id84 "JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.")]  
Probabilistic | QMCD | Quasi-Monte Carlo Discrepancy outlier detection | 2001 | [`pyod.models.qmcd.QMCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD "pyod.models.qmcd.QMCD") | [[AFM01](https://pyod.readthedocs.io/en/latest/?badge=latest#id120 "Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. Journal of complexity, 17\(4\):608‚Äì624, 2001.")]  
Probabilistic | KDE | Outlier Detection with Kernel Density Functions | 2007 | [`pyod.models.kde.KDE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE "pyod.models.kde.KDE") | [[ALLP07](https://pyod.readthedocs.io/en/latest/?badge=latest#id111 "Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition, 61‚Äì75. Springer, 2007.")]  
Probabilistic | Sampling | Rapid distance-based outlier detection via sampling | 2013 | [`pyod.models.sampling.Sampling`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling "pyod.models.sampling.Sampling") | [[ASB13](https://pyod.readthedocs.io/en/latest/?badge=latest#id112 "Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. Advances in neural information processing systems, 2013.")]  
Probabilistic | GMM | Probabilistic Mixture Modeling for Outlier Analysis |  | [`pyod.models.gmm.GMM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM "pyod.models.gmm.GMM") | [[AAgg15](https://pyod.readthedocs.io/en/latest/?badge=latest#id77 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] [Ch.2]  
Linear Model | PCA | Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes) | 2003 | [`pyod.models.pca.PCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA "pyod.models.pca.PCA") | [[ASCSC03](https://pyod.readthedocs.io/en/latest/?badge=latest#id76 "Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.")]  
Linear Model | KPCA | Kernel Principal Component Analysis | 2007 | [`pyod.models.kpca.KPCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA "pyod.models.kpca.KPCA") | [[AHof07](https://pyod.readthedocs.io/en/latest/?badge=latest#id119 "Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40\(3\):863‚Äì874, 2007.")]  
Linear Model | MCD | Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores) | 1999 | [`pyod.models.mcd.MCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD "pyod.models.mcd.MCD") | [[AHR04](https://pyod.readthedocs.io/en/latest/?badge=latest#id81 "Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. Computational Statistics & Data Analysis, 44\(4\):625‚Äì638, 2004."), [ARD99](https://pyod.readthedocs.io/en/latest/?badge=latest#id80 "Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. Technometrics, 41\(3\):212‚Äì223, 1999.")]  
Linear Model | CD | Use Cook‚Äôs distance for outlier detection | 1977 | [`pyod.models.cd.CD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD "pyod.models.cd.CD") | [[ACoo77](https://pyod.readthedocs.io/en/latest/?badge=latest#id110 "R Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19\(1\):15‚Äì18, 1977.")]  
Linear Model | OCSVM | One-Class Support Vector Machines | 2001 | [`pyod.models.ocsvm.OCSVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM "pyod.models.ocsvm.OCSVM") | [[AScholkopfPST+01](https://pyod.readthedocs.io/en/latest/?badge=latest#id91 "Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13\(7\):1443‚Äì1471, 2001.")]  
Linear Model | LMDD | Deviation-based Outlier Detection (LMDD) | 1996 | [`pyod.models.lmdd.LMDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD "pyod.models.lmdd.LMDD") | [[AAAR96](https://pyod.readthedocs.io/en/latest/?badge=latest#id98 "Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In KDD, volume 1141, 972‚Äì981. 1996.")]  
Proximity-Based | LOF | Local Outlier Factor | 2000 | [`pyod.models.lof.LOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF "pyod.models.lof.LOF") | [[ABKNS00](https://pyod.readthedocs.io/en/latest/?badge=latest#id78 "Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In ACM sigmod record, volume 29, 93‚Äì104. ACM, 2000.")]  
Proximity-Based | COF | Connectivity-Based Outlier Factor | 2002 | [`pyod.models.cof.COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF "pyod.models.cof.COF") | [[ATCFC02](https://pyod.readthedocs.io/en/latest/?badge=latest#id92 "Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535‚Äì548. Springer, 2002.")]  
Proximity-Based | Incr. COF | Memory Efficient Connectivity-Based Outlier Factor (slower but reduce storage complexity) | 2002 | [`pyod.models.cof.COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF "pyod.models.cof.COF") | [[ATCFC02](https://pyod.readthedocs.io/en/latest/?badge=latest#id92 "Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535‚Äì548. Springer, 2002.")]  
Proximity-Based | CBLOF | Clustering-Based Local Outlier Factor | 2003 | [`pyod.models.cblof.CBLOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF "pyod.models.cblof.CBLOF") | [[AHXD03](https://pyod.readthedocs.io/en/latest/?badge=latest#id82 "Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. Pattern Recognition Letters, 24\(9-10\):1641‚Äì1650, 2003.")]  
Proximity-Based | LOCI | LOCI: Fast outlier detection using the local correlation integral | 2003 | [`pyod.models.loci.LOCI`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI "pyod.models.loci.LOCI") | [[APKGF03](https://pyod.readthedocs.io/en/latest/?badge=latest#id85 "Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In Data Engineering, 2003. Proceedings. 19th International Conference on, 315‚Äì326. IEEE, 2003.")]  
Proximity-Based | HBOS | Histogram-based Outlier Score | 2012 | [`pyod.models.hbos.HBOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS "pyod.models.hbos.HBOS") | [[AGD12](https://pyod.readthedocs.io/en/latest/?badge=latest#id75 "Markus Goldstein and Andreas Dengel. Histogram-based outlier score \(hbos\): a fast unsupervised anomaly detection algorithm. KI-2012: Poster and Demo Track, pages 59‚Äì63, 2012.")]  
Proximity-Based | kNN | k Nearest Neighbors (use the distance to the kth nearest neighbor as the outlier score | 2000 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/?badge=latest#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/?badge=latest#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | AvgKNN | Average kNN (use the average distance to k nearest neighbors as the outlier score) | 2002 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/?badge=latest#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/?badge=latest#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | MedKNN | Median kNN (use the median distance to k nearest neighbors as the outlier score) | 2002 | [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") | [[AAP02](https://pyod.readthedocs.io/en/latest/?badge=latest#id72 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [ARRS00](https://pyod.readthedocs.io/en/latest/?badge=latest#id71 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")]  
Proximity-Based | SOD | Subspace Outlier Detection | 2009 | [`pyod.models.sod.SOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD "pyod.models.sod.SOD") | [[AKKrogerSZ09](https://pyod.readthedocs.io/en/latest/?badge=latest#id94 "Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 831‚Äì838. Springer, 2009.")]  
Proximity-Based | ROD | Rotation-based Outlier Detection | 2020 | [`pyod.models.rod.ROD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD "pyod.models.rod.ROD") | [[AABC20](https://pyod.readthedocs.io/en/latest/?badge=latest#id104 "Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. IEEE Transactions on Knowledge and Data Engineering, 2020.")]  
Outlier Ensembles | IForest | Isolation Forest | 2008 | [`pyod.models.iforest.IForest`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest "pyod.models.iforest.IForest") | [[ALTZ08](https://pyod.readthedocs.io/en/latest/?badge=latest#id67 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, 413‚Äì422. IEEE, 2008."), [ALTZ12](https://pyod.readthedocs.io/en/latest/?badge=latest#id68 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data \(TKDD\), 6\(1\):3, 2012.")]  
Outlier Ensembles | INNE | Isolation-based Anomaly Detection Using Nearest-Neighbor Ensembles | 2018 | [`pyod.models.inne.INNE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE "pyod.models.inne.INNE") | [[ABTA+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id113 "Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. Computational Intelligence, 34\(4\):968‚Äì998, 2018.")]  
Outlier Ensembles | DIF | Deep Isolation Forest for Anomaly Detection | 2023 | [`pyod.models.dif.DIF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF "pyod.models.dif.DIF") | [[AXPWW23](https://pyod.readthedocs.io/en/latest/?badge=latest#id121 "Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, \(\):1-14, 2023. doi:10.1109/TKDE.2023.3270293.")]  
Outlier Ensembles | FB | Feature Bagging | 2005 | [`pyod.models.feature_bagging.FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging "pyod.models.feature_bagging.FeatureBagging") | [[ALK05](https://pyod.readthedocs.io/en/latest/?badge=latest#id74 "Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157‚Äì166. ACM, 2005.")]  
Outlier Ensembles | LSCP | LSCP: Locally Selective Combination of Parallel Outlier Ensembles | 2019 | [`pyod.models.lscp.LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP "pyod.models.lscp.LSCP") | [[AZNHL19](https://pyod.readthedocs.io/en/latest/?badge=latest#id86 "Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.")]  
Outlier Ensembles | XGBOD | Extreme Boosting Based Outlier Detection **(Supervised)** | 2018 | [`pyod.models.xgbod.XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD "pyod.models.xgbod.XGBOD") | [[AZH18](https://pyod.readthedocs.io/en/latest/?badge=latest#id79 "Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks \(IJCNN\). IEEE, 2018.")]  
Outlier Ensembles | LODA | Lightweight On-line Detector of Anomalies | 2016 | [`pyod.models.loda.LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA "pyod.models.loda.LODA") | [[APevny16](https://pyod.readthedocs.io/en/latest/?badge=latest#id100 "Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102\(2\):275‚Äì304, 2016.")]  
Outlier Ensembles | SUOD | SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection **(Acceleration)** | 2021 | [`pyod.models.suod.SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD "pyod.models.suod.SUOD") | [[AZHC+21](https://pyod.readthedocs.io/en/latest/?badge=latest#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")]  
Neural Networks | AutoEncoder | Fully connected AutoEncoder (use reconstruction error as the outlier score) | 2015 | [`pyod.models.auto_encoder.AutoEncoder`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder "pyod.models.auto_encoder.AutoEncoder") | [[AAgg15](https://pyod.readthedocs.io/en/latest/?badge=latest#id77 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] [Ch.3]  
Neural Networks | VAE | Variational AutoEncoder (use reconstruction error as the outlier score) | 2013 | [`pyod.models.vae.VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE "pyod.models.vae.VAE") | [[AKW13](https://pyod.readthedocs.io/en/latest/?badge=latest#id99 "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.")]  
Neural Networks | Beta-VAE | Variational AutoEncoder (all customized loss term by varying gamma and capacity) | 2018 | [`pyod.models.vae.VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE "pyod.models.vae.VAE") | [[ABHP+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id101 "Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.")]  
Neural Networks | SO_GAAL | Single-Objective Generative Adversarial Active Learning | 2019 | [`pyod.models.so_gaal.SO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL "pyod.models.so_gaal.SO_GAAL") | [[ALLZ+19](https://pyod.readthedocs.io/en/latest/?badge=latest#id87 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")]  
Neural Networks | MO_GAAL | Multiple-Objective Generative Adversarial Active Learning | 2019 | [`pyod.models.mo_gaal.MO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL "pyod.models.mo_gaal.MO_GAAL") | [[ALLZ+19](https://pyod.readthedocs.io/en/latest/?badge=latest#id87 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")]  
Neural Networks | DeepSVDD | Deep One-Class Classification | 2018 | [`pyod.models.deep_svdd.DeepSVDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD "pyod.models.deep_svdd.DeepSVDD") | [[ARVG+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id106 "Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. International conference on machine learning, 2018.")]  
Neural Networks | AnoGAN | Anomaly Detection with Generative Adversarial Networks | 2017 | [`pyod.models.anogan.AnoGAN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN "pyod.models.anogan.AnoGAN") | [[ASSeebockW+17](https://pyod.readthedocs.io/en/latest/?badge=latest#id114 "Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, 146‚Äì157. Springer, 2017.")]  
Neural Networks | ALAD | Adversarially learned anomaly detection | 2018 | [`pyod.models.alad.ALAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD "pyod.models.alad.ALAD") | [[AZRF+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id118 "Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining \(ICDM\), 727‚Äì736. IEEE, 2018.")]  
Neural Networks | DevNet | Deep Anomaly Detection with Deviation Networks | 2019 | [`pyod.models.devnet.DevNet`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet "pyod.models.devnet.DevNet") | [[APSVDH19](https://pyod.readthedocs.io/en/latest/?badge=latest#id123 "Guansong Pang, Chunhua Shen, and Anton Van Den Hengel. Deep anomaly detection with deviation networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 353‚Äì362. 2019.")]  
Neural Networks | AE1SVM | Autoencoder-based One-class Support Vector Machine | 2019 | [`pyod.models.ae1svm.AE1SVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM "pyod.models.ae1svm.AE1SVM") | [[ANV19](https://pyod.readthedocs.io/en/latest/?badge=latest#id122 "Minh-Nghia Nguyen and Ngo Anh Vien. Scalable and interpretable one-class svms with deep learning and random fourier features. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10‚Äì14, 2018, Proceedings, Part I 18, 157‚Äì172. Springer, 2019.")]  
Graph-based | R-Graph | Outlier detection by R-graph | 2017 | [`pyod.models.rgraph.RGraph`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph "pyod.models.rgraph.RGraph") | [[AYRV17](https://pyod.readthedocs.io/en/latest/?badge=latest#id117 "Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3395‚Äì3404. 2017.")]  
Graph-based | LUNAR | LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks | 2022 | [`pyod.models.lunar.LUNAR`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR "pyod.models.lunar.LUNAR") | [[AGHNN22](https://pyod.readthedocs.io/en/latest/?badge=latest#id115 "Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 6737‚Äì6745. 2022.")]  
**(ii) Outlier Ensembles & Outlier Detector Combination Frameworks**:
Type | Abbr | Algorithm | Year | Ref |   
---|---|---|---|---|---  
Outlier Ensembles |  | Feature Bagging | 2005 | [`pyod.models.feature_bagging.FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging "pyod.models.feature_bagging.FeatureBagging") | [[ALK05](https://pyod.readthedocs.io/en/latest/?badge=latest#id74 "Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157‚Äì166. ACM, 2005.")]  
Outlier Ensembles | LSCP | LSCP: Locally Selective Combination of Parallel Outlier Ensembles | 2019 | [`pyod.models.lscp.LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP "pyod.models.lscp.LSCP") | [[AZNHL19](https://pyod.readthedocs.io/en/latest/?badge=latest#id86 "Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.")]  
Outlier Ensembles | XGBOD | Extreme Boosting Based Outlier Detection **(Supervised)** | 2018 | [`pyod.models.xgbod.XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD "pyod.models.xgbod.XGBOD") | [[AZH18](https://pyod.readthedocs.io/en/latest/?badge=latest#id79 "Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks \(IJCNN\). IEEE, 2018.")]  
Outlier Ensembles | LODA | Lightweight On-line Detector of Anomalies | 2016 | [`pyod.models.loda.LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA "pyod.models.loda.LODA") | [[APevny16](https://pyod.readthedocs.io/en/latest/?badge=latest#id100 "Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102\(2\):275‚Äì304, 2016.")]  
Outlier Ensembles | SUOD | SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection **(Acceleration)** | 2021 | [`pyod.models.suod.SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD "pyod.models.suod.SUOD") | [[AZHC+21](https://pyod.readthedocs.io/en/latest/?badge=latest#id105 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")]  
Combination | Average | Simple combination by averaging the scores | 2015 | [`pyod.models.combination.average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average "pyod.models.combination.average") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Weighted Average | Simple combination by averaging the scores with detector weights | 2015 | [`pyod.models.combination.average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average "pyod.models.combination.average") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Maximization | Simple combination by taking the maximum scores | 2015 | [`pyod.models.combination.maximization()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization "pyod.models.combination.maximization") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | AOM | Average of Maximum | 2015 | [`pyod.models.combination.aom()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom "pyod.models.combination.aom") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | MOA | Maximum of Average | 2015 | [`pyod.models.combination.moa()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa "pyod.models.combination.moa") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | Median | Simple combination by taking the median of the scores | 2015 | [`pyod.models.combination.median()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median "pyod.models.combination.median") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
Combination | majority Vote | Simple combination by taking the majority vote of the labels (weights can be used) | 2015 | [`pyod.models.combination.majority_vote()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote "pyod.models.combination.majority_vote") | [[AAS15](https://pyod.readthedocs.io/en/latest/?badge=latest#id70 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")]  
**(iii) Utility Functions** :
Type | Name | Function  
---|---|---  
Data | [`pyod.utils.data.generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data "pyod.utils.data.generate_data") | Synthesized data generation; normal data is generated by a multivariate Gaussian and outliers are generated by a uniform distribution  
Data | [`pyod.utils.data.generate_data_clusters()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters "pyod.utils.data.generate_data_clusters") | Synthesized data generation in clusters; more complex data patterns can be created with multiple clusters  
Stat | [`pyod.utils.stat_models.wpearsonr()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr "pyod.utils.stat_models.wpearsonr") | Calculate the weighted Pearson correlation of two samples  
Utility | [`pyod.utils.utility.get_label_n()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n "pyod.utils.utility.get_label_n") | Turn raw outlier scores into binary labels by assign 1 to top n outlier scores  
Utility | [`pyod.utils.utility.precision_n_scores()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores "pyod.utils.utility.precision_n_scores") | calculate precision @ rank n  
### API Cheatsheet & Reference[¬∂](https://pyod.readthedocs.io/en/latest/?badge=latest#api-cheatsheet-reference "Link to this heading")
The following APIs are applicable for all detector models for easy use.
  * [`pyod.models.base.BaseDetector.fit()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit "pyod.models.base.BaseDetector.fit"): Fit detector. y is ignored in unsupervised methods.
  * [`pyod.models.base.BaseDetector.decision_function()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function "pyod.models.base.BaseDetector.decision_function"): Predict raw anomaly score of X using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict "pyod.models.base.BaseDetector.predict"): Predict if a particular sample is an outlier or not using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_proba()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba "pyod.models.base.BaseDetector.predict_proba"): Predict the probability of a sample being outlier using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_confidence()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence "pyod.models.base.BaseDetector.predict_confidence"): Predict the model‚Äôs sample-wise confidence (available in predict and predict_proba).


Key Attributes of a fitted model:
  * `pyod.models.base.BaseDetector.decision_scores_`: The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores.
  * `pyod.models.base.BaseDetector.labels_`: The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies.


* * *
* * *
References
[AAgg15] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id16),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id42))
Charu C Aggarwal. Outlier analysis. In _Data mining_ , 75‚Äì79. Springer, 2015.
[AAS15] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id59),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id60),[3](https://pyod.readthedocs.io/en/latest/?badge=latest#id61),[4](https://pyod.readthedocs.io/en/latest/?badge=latest#id62),[5](https://pyod.readthedocs.io/en/latest/?badge=latest#id63),[6](https://pyod.readthedocs.io/en/latest/?badge=latest#id64),[7](https://pyod.readthedocs.io/en/latest/?badge=latest#id65))
Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. _ACM SIGKDD Explorations Newsletter_ , 17(1):24‚Äì47, 2015.
[[AABC20](https://pyod.readthedocs.io/en/latest/?badge=latest#id33)]
Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. _IEEE Transactions on Knowledge and Data Engineering_ , 2020.
[AAP02] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id29),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id30),[3](https://pyod.readthedocs.io/en/latest/?badge=latest#id31))
Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In _European Conference on Principles of Data Mining and Knowledge Discovery_ , 15‚Äì27. Springer, 2002.
[[AAAR96](https://pyod.readthedocs.io/en/latest/?badge=latest#id22)]
Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In _KDD_ , volume 1141, 972‚Äì981. 1996.
[[ABTA+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id35)]
Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. _Computational Intelligence_ , 34(4):968‚Äì998, 2018.
[[ABKNS00](https://pyod.readthedocs.io/en/latest/?badge=latest#id23)]
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In _ACM sigmod record_ , volume 29, 93‚Äì104. ACM, 2000.
[[ABHP+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id44)]
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. _arXiv preprint arXiv:1804.03599_ , 2018.
[[ACQS+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id1)]
Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, and others. Pyod 2: a python library for outlier detection with llm-powered model selection. _arXiv preprint arXiv:2412.12154_ , 2024.
[[ACoo77](https://pyod.readthedocs.io/en/latest/?badge=latest#id20)]
R Dennis Cook. Detection of influential observation in linear regression. _Technometrics_ , 19(1):15‚Äì18, 1977.
[[AFM01](https://pyod.readthedocs.io/en/latest/?badge=latest#id13)]
Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. _Journal of complexity_ , 17(4):608‚Äì624, 2001.
[[AGD12](https://pyod.readthedocs.io/en/latest/?badge=latest#id28)]
Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. _KI-2012: Poster and Demo Track_ , pages 59‚Äì63, 2012.
[[AGHNN22](https://pyod.readthedocs.io/en/latest/?badge=latest#id53)]
Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_ , volume 36, 6737‚Äì6745. 2022.
[[AHHH+22](https://pyod.readthedocs.io/en/latest/?badge=latest#id5)]
Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: anomaly detection benchmark. _arXiv preprint arXiv:2206.09426_ , 2022.
[[AHR04](https://pyod.readthedocs.io/en/latest/?badge=latest#id19)]
Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. _Computational Statistics & Data Analysis_, 44(4):625‚Äì638, 2004.
[[AHXD03](https://pyod.readthedocs.io/en/latest/?badge=latest#id26)]
Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. _Pattern Recognition Letters_ , 24(9-10):1641‚Äì1650, 2003.
[[AHof07](https://pyod.readthedocs.io/en/latest/?badge=latest#id18)]
Heiko Hoffmann. Kernel pca for novelty detection. _Pattern recognition_ , 40(3):863‚Äì874, 2007.
[[AIH93](https://pyod.readthedocs.io/en/latest/?badge=latest#id11)]
Boris Iglewicz and David Caster Hoaglin. _How to detect and handle outliers_. Volume 16. Asq Press, 1993.
[[AJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/?badge=latest#id12)]
JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.
[[AKW13](https://pyod.readthedocs.io/en/latest/?badge=latest#id43)]
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ , 2013.
[[AKKrogerSZ09](https://pyod.readthedocs.io/en/latest/?badge=latest#id32)]
Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 831‚Äì838. Springer, 2009.
[AKZ+08] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id9),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id10))
Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_ , 444‚Äì452. ACM, 2008.
[[ALLP07](https://pyod.readthedocs.io/en/latest/?badge=latest#id14)]
Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In _International Workshop on Machine Learning and Data Mining in Pattern Recognition_ , 61‚Äì75. Springer, 2007.
[ALK05] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id37),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id54))
Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In _Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining_ , 157‚Äì166. ACM, 2005.
[[ALLX+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id2)]
Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, and Yue Zhao. Nlp-adbench: nlp anomaly detection benchmark. _arXiv preprint arXiv:2412.04784_ , 2024.
[[ALZB+20](https://pyod.readthedocs.io/en/latest/?badge=latest#id8)]
Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In _IEEE International Conference on Data Mining (ICDM)_. IEEE, 2020.
[[ALZH+22](https://pyod.readthedocs.io/en/latest/?badge=latest#id7)]
Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_ , 2022.
[[ALTZ08](https://pyod.readthedocs.io/en/latest/?badge=latest#id34)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on_ , 413‚Äì422. IEEE, 2008.
[[ALTZ12](https://pyod.readthedocs.io/en/latest/?badge=latest#id34)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_ , 6(1):3, 2012.
[ALLZ+19] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id45),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id46))
Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. _IEEE Transactions on Knowledge and Data Engineering_ , 2019.
[[ANV19](https://pyod.readthedocs.io/en/latest/?badge=latest#id51)]
Minh-Nghia Nguyen and Ngo Anh Vien. Scalable and interpretable one-class svms with deep learning and random fourier features. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10‚Äì14, 2018, Proceedings, Part I 18_ , 157‚Äì172. Springer, 2019.
[[APSVDH19](https://pyod.readthedocs.io/en/latest/?badge=latest#id50)]
Guansong Pang, Chunhua Shen, and Anton Van Den Hengel. Deep anomaly detection with deviation networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, 353‚Äì362. 2019.
[[APKGF03](https://pyod.readthedocs.io/en/latest/?badge=latest#id27)]
Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In _Data Engineering, 2003. Proceedings. 19th International Conference on_ , 315‚Äì326. IEEE, 2003.
[APVD20]
Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_ , 227‚Äì243. Springer, 2020.
[APevny16] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id40),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id57))
Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. _Machine Learning_ , 102(2):275‚Äì304, 2016.
[ARRS00] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id29),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id30),[3](https://pyod.readthedocs.io/en/latest/?badge=latest#id31))
Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In _ACM Sigmod Record_ , volume 29, 427‚Äì438. ACM, 2000.
[[ARD99](https://pyod.readthedocs.io/en/latest/?badge=latest#id19)]
Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. _Technometrics_ , 41(3):212‚Äì223, 1999.
[[ARVG+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id47)]
Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. _International conference on machine learning_ , 2018.
[[ASSeebockW+17](https://pyod.readthedocs.io/en/latest/?badge=latest#id48)]
Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In _International conference on information processing in medical imaging_ , 146‚Äì157. Springer, 2017.
[[AScholkopfPST+01](https://pyod.readthedocs.io/en/latest/?badge=latest#id21)]
Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_ , 13(7):1443‚Äì1471, 2001.
[[ASCSC03](https://pyod.readthedocs.io/en/latest/?badge=latest#id17)]
Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.
[[ASB13](https://pyod.readthedocs.io/en/latest/?badge=latest#id15)]
Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. _Advances in neural information processing systems_ , 2013.
[ATCFC02] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id24),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id25))
Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 535‚Äì548. Springer, 2002.
[[AXPWW23](https://pyod.readthedocs.io/en/latest/?badge=latest#id36)]
Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_ , ():1‚Äì14, 2023. [doi:10.1109/TKDE.2023.3270293](https://doi.org/10.1109/TKDE.2023.3270293).
[[AYNL+24](https://pyod.readthedocs.io/en/latest/?badge=latest#id3)]
Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, and others. Ad-llm: benchmarking large language models for anomaly detection. _arXiv preprint arXiv:2412.11142_ , 2024.
[[AYRV17](https://pyod.readthedocs.io/en/latest/?badge=latest#id52)]
Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ , 3395‚Äì3404. 2017.
[[AZRF+18](https://pyod.readthedocs.io/en/latest/?badge=latest#id49)]
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In _2018 IEEE International conference on data mining (ICDM)_ , 727‚Äì736. IEEE, 2018.
[AZH18] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id39),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id56))
Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In _International Joint Conference on Neural Networks (IJCNN)_. IEEE, 2018.
[AZHC+21] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id4),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id41),[3](https://pyod.readthedocs.io/en/latest/?badge=latest#id58))
Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. _Proceedings of Machine Learning and Systems_ , 2021.
[AZNHL19] ([1](https://pyod.readthedocs.io/en/latest/?badge=latest#id38),[2](https://pyod.readthedocs.io/en/latest/?badge=latest#id55))
Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In _Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019_ , 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: <https://doi.org/10.1137/1.9781611975673.66>, [doi:10.1137/1.9781611975673.66](https://doi.org/10.1137/1.9781611975673.66).
[ Next Installation ](https://pyod.readthedocs.io/en/latest/install.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Welcome to PyOD V2 documentation!](https://pyod.readthedocs.io/en/latest/?badge=latest)
    * [Read Me First](https://pyod.readthedocs.io/en/latest/?badge=latest#read-me-first)
    * [About PyOD](https://pyod.readthedocs.io/en/latest/?badge=latest#about-pyod)
    * [ADBench Benchmark and Datasets](https://pyod.readthedocs.io/en/latest/?badge=latest#adbench-benchmark-and-datasets)
  * [Implemented Algorithms](https://pyod.readthedocs.io/en/latest/?badge=latest#implemented-algorithms)
  * [API Cheatsheet & Reference](https://pyod.readthedocs.io/en/latest/?badge=latest#api-cheatsheet-reference)



---

## 3. About us - pyod 2.0.5 documentation {#3-1}

**URL:** https://pyod.readthedocs.io/en/latest/about.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:09

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/about.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/about.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/about.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### About us[¬∂](https://pyod.readthedocs.io/en/latest/about.html#about-us "Link to this heading")
###### Core Development Team[¬∂](https://pyod.readthedocs.io/en/latest/about.html#core-development-team "Link to this heading")
Dr. Yue Zhao (Assistant Professor @ USC, Ph.D. @ CMU):
  * Initialized the project in 2017
  * [Homepage](https://viterbi-web.usc.edu/~yzhao010/)
  * [LinkedIn (Yue Zhao)](https://www.linkedin.com/in/yzhao062/)


Zain Nasrullah (Data Scientist at RBC; MSc in Computer Science from University of Toronto):
  * Joined in 2018
  * [LinkedIn (Zain Nasrullah)](https://www.linkedin.com/in/zain-nasrullah-097a2b85)


Winston (Zheng) Li (Founder of [arima](https://www.arimadata.com/), Part-time Instructor @ Northeastern University):
  * Joined in 2018
  * [LinkedIn (Winston Li)](https://www.linkedin.com/in/winstonl)


Yahya Almardeny (Senior AI/ML & Software Systems Engineer @ General Motors):
  * Joined in 2019
  * [LinkedIn (Yahya Almardeny)](https://www.linkedin.com/in/yahya-almardeny/)


Ant√¥nio Pedro Camargo (DOE Joint Genome Institute)
  * Joined in 2020 (our Conda maintainer)
  * [GitHub (Ant√¥nio Pedro Camargo)](https://github.com/apcamargo)


Dr. Andrij Vasylenko (Research Associate @ University of Liverpool)
  * Joined in 2020 (implemented the VAE and extend to Beta-VAE)
  * [Homepage (Dr Andrij Vasylenko)](https://www.liverpool.ac.uk/chemistry/staff/andrij-vasylenko/)


Roel Bouman (Ph.D. Student @ Radboud University):
  * Joined in 2021
  * [LinkedIn (Roel Bouman)](https://nl.linkedin.com/in/roel-bouman-18b5b9167)


Rafa≈Ç Bodziony (Data Scientist):
  * Joined in 2021 (implemented DeepSVDD)
  * [LinkedIn (Rafa≈Ç Bodziony)](https://pl.linkedin.com/in/rafalbodziony)


Dr. Akira Tamamori (Associate Professor @ Aichi Institute of Technology):
  * Joined in 2022 (implemented multiple OD algorithms such as KDE, sampling, and more)
  * [Homepage (Dr Akira Tamamori)](https://researchmap.jp/tamamori?lang=en)


Michiel Bongaerts (PhD student @ Erasmus Medical Centre Metabolomics & Genetics):
  * Joined in 2022 (implemented AnoGAN and more)
  * [GitHub (Michiel Bongaerts)](https://github.com/mbongaerts)


Adam Goodge (PhD Researcher @ National University of Singapore):
  * Joined in 2022 (implemented LUNAR)
  * [LinkedIn (Adam Goodge)](https://www.linkedin.com/in/adam-goodge-33908691/)


Daniel Kulik (Machine Learning Developer; MSc Student @ University of the Free State):
  * Joined 2022 (implemented integration with PyThresh and more)
  * [LinkedIn (Daniel Kulik)](https://www.linkedin.com/in/daniel-kulik-148256223)


Lorenzo Perini (Research Scientist, Meta):
  * Joined 2022 (implemented prediction confidence, rejection and more)
  * [Homepage (Lorenzo Perini)](https://lorenzo-perini.github.io/)


Tiankai Yang (PhD student @ USC):
  * Joined 2024 (implemented Pytorch version of algorithms)
  * [Homepage (Tiankai Yang)](https://www.linkedin.com/in/tiankai-yang/)


Yuehan Qin (PhD student @ USC):
  * Joined 2024 (implemented Pytorch version of algorithms)
  * [Homepage (Yuehan Qin)](https://github.com/yqin43)


Sihan Chen (MS student @ USC):
  * Joined 2024 (implemented Pytorch version of algorithms)
  * [Homepage (Sihan Chen)](https://www.linkedin.com/in/chensihanlaura/)


Zhuo Xiao (MS student @ USC):
  * Joined 2024 (implemented Pytorch version of algorithms)
  * [Homepage (Zhuo Xiao)](https://www.linkedin.com/in/zhuox5/)


Bourne Li (MS student @ USC):
  * Joined 2024 (implemented Pytorch version of algorithms)
  * [Homepage (Bourne Li)](https://www.linkedin.com/in/bourne-li-a9a899231/)


[ Previous Frequently Asked Questions ](https://pyod.readthedocs.io/en/latest/faq.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)
    * [Core Development Team](https://pyod.readthedocs.io/en/latest/about.html#core-development-team)



---

## 4. API CheatSheet - pyod 2.0.5 documentation {#4-1}

**URL:** https://pyod.readthedocs.io/en/latest/api_cc.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:08

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/api_cc.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/api_cc.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/api_cc.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### API CheatSheet[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#api-cheatsheet "Link to this heading")
The full API Reference is available at [PyOD Documentation](https://pyod.readthedocs.io/en/latest/pyod.html). Below is a quick cheatsheet for all detectors:
  * [`pyod.models.base.BaseDetector.fit()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit "pyod.models.base.BaseDetector.fit"): The parameter y is ignored in unsupervised methods.
  * [`pyod.models.base.BaseDetector.decision_function()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function "pyod.models.base.BaseDetector.decision_function"): Predict raw anomaly scores for X using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict "pyod.models.base.BaseDetector.predict"): Determine whether a sample is an outlier or not as binary labels using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_proba()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba "pyod.models.base.BaseDetector.predict_proba"): Estimate the probability of a sample being an outlier using the fitted detector.
  * [`pyod.models.base.BaseDetector.predict_confidence()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence "pyod.models.base.BaseDetector.predict_confidence"): Assess the model‚Äôs confidence on a per-sample basis (applicable in predict and predict_proba) [[APVD20](https://pyod.readthedocs.io/en/latest/index.html#id108 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].


**Key Attributes of a fitted model** :
  * `pyod.models.base.BaseDetector.decision_scores_`: Outlier scores of the training data. Higher scores typically indicate more abnormal behavior. Outliers usually have higher scores. Outliers tend to have higher scores.
  * `pyod.models.base.BaseDetector.labels_`: Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers/anomalies.


See base class definition below:
###### pyod.models.base module[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#module-pyod.models.base "Link to this heading")
Base class for all outlier detector models 

_class_ pyod.models.base.BaseDetector(_contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "Link to this definition") 
    
Bases: [`object`](https://docs.python.org/3/library/functions.html#object "\(in Python v3.13\)")
Abstract class for all outlier detection algorithms.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#parameters "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#attributes "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/api_cc.html#id24)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/api_cc.html#id26)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/api_cc.html#id28)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.compute_rejection_stats)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id2 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#returns "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

_abstractmethod_ decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function "Link to this definition") 
    
Predict raw anomaly scores of X using the fitted detector.
The anomaly score of an input sample is computed based on the fitted detector. For consistency, outliers are assigned with higher anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id3 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id4 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

_abstractmethod_ fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.fit)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id5 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id6 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.fit_predict)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id7 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id8 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.fit_predict_score)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id9 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id10 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.get_params)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id11 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id12 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id13 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id14 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict_confidence)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id16 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id17 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict_proba)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id19 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id20 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.predict_with_rejection)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id21 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id22 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#BaseDetector.set_params)[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/api_cc.html#id23 "Link to this heading")
self : object
[ Next API Reference ](https://pyod.readthedocs.io/en/latest/pyod.html) [ Previous Benchmarks ](https://pyod.readthedocs.io/en/latest/benchmark.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
    * [pyod.models.base module](https://pyod.readthedocs.io/en/latest/api_cc.html#module-pyod.models.base)
      * [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector)
        * [`BaseDetector.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.compute_rejection_stats)
        * [`BaseDetector.decision_function()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function)
        * [`BaseDetector.fit()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit)
        * [`BaseDetector.fit_predict()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict)
        * [`BaseDetector.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict_score)
        * [`BaseDetector.get_params()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.get_params)
        * [`BaseDetector.predict()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict)
        * [`BaseDetector.predict_confidence()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence)
        * [`BaseDetector.predict_proba()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba)
        * [`BaseDetector.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_with_rejection)
        * [`BaseDetector.set_params()`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.set_params)



---

## 5. Benchmarks - pyod 2.0.5 documentation {#5-1}

**URL:** https://pyod.readthedocs.io/en/latest/benchmark.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:07

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/benchmark.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/benchmark.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/benchmark.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Benchmarks[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#benchmarks "Link to this heading")
###### Latest ADBench (2022)[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#latest-adbench-2022 "Link to this heading")
We just released a 45-page, the most comprehensive [ADBench: Anomaly Detection Benchmark](https://arxiv.org/abs/2206.09426) [[#Han2022ADBench]_](https://pyod.readthedocs.io/en/latest/benchmark.html#id6). The fully [open-sourced ADBench](https://github.com/Minqi824/ADBench) compares 30 anomaly detection algorithms on 57 benchmark datasets.
The organization of **ADBench** is provided below:
[![benchmark-fig](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true) ](https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true)
For a simpler visualization, we make **the comparison of selected models** via [compare_all_models.py](https://github.com/yzhao062/pyod/blob/master/examples/compare_all_models.py).
[![Comparison_of_All](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true) ](https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true)
###### Old Results (2019)[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#old-results-2019 "Link to this heading")
A benchmark is supplied for select algorithms to provide an overview of the implemented models. In total, 17 benchmark datasets are used for comparison, which can be downloaded at [ODDS](http://odds.cs.stonybrook.edu/#table1).
For each dataset, it is first split into 60% for training and 40% for testing. All experiments are repeated 10 times independently with random splits. The mean of 10 trials is regarded as the final result. Three evaluation metrics are provided:
  * The area under receiver operating characteristic (ROC) curve
  * Precision @ rank n (P@N)
  * Execution time


You could replicate this process by running [benchmark.py](https://github.com/yzhao062/pyod/blob/master/notebooks/benchmark.py).
We also provide the hardware specification for reference.
Specification | Value  
---|---  
Platform | PC  
OS | Microsoft Windows 10 Enterprise  
CPU | Intel i7-6820HQ @ 2.70GHz  
RAM | 32GB  
Software | PyCharm 2018.02  
Python | Python 3.6.2  
Core | Single core (no parallelization)  
###### ROC Performance[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#roc-performance "Link to this heading")
ROC Performances (average of 10 independent trials)[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#id2 "Link to this table") Data | #Samples | ### Dimensions | Outlier Perc | ABOD | CBLOF | FB | HBOS | IForest | KNN | LOF | MCD | OCSVM | PCA  
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
arrhythmia | 452 | 274 | 14.6018 | 0.7688 | 0.7835 | 0.7781 | 0.8219 | 0.8005 | 0.7861 | 0.7787 | 0.7790 | 0.7812 | 0.7815  
cardio | 1831 | 21 | 9.6122 | 0.5692 | 0.9276 | 0.5867 | 0.8351 | 0.9213 | 0.7236 | 0.5736 | 0.8135 | 0.9348 | 0.9504  
glass | 214 | 9 | 4.2056 | 0.7951 | 0.8504 | 0.8726 | 0.7389 | 0.7569 | 0.8508 | 0.8644 | 0.7901 | 0.6324 | 0.6747  
ionosphere | 351 | 33 | 35.8974 | 0.9248 | 0.8134 | 0.8730 | 0.5614 | 0.8499 | 0.9267 | 0.8753 | 0.9557 | 0.8419 | 0.7962  
letter | 1600 | 32 | 6.2500 | 0.8783 | 0.5070 | 0.8660 | 0.5927 | 0.6420 | 0.8766 | 0.8594 | 0.8074 | 0.6118 | 0.5283  
lympho | 148 | 18 | 4.0541 | 0.9110 | 0.9728 | 0.9753 | 0.9957 | 0.9941 | 0.9745 | 0.9771 | 0.9000 | 0.9759 | 0.9847  
mnist | 7603 | 100 | 9.2069 | 0.7815 | 0.8009 | 0.7205 | 0.5742 | 0.8159 | 0.8481 | 0.7161 | 0.8666 | 0.8529 | 0.8527  
musk | 3062 | 166 | 3.1679 | 0.1844 | 0.9879 | 0.5263 | 1.0000 | 0.9999 | 0.7986 | 0.5287 | 0.9998 | 1.0000 | 1.0000  
optdigits | 5216 | 64 | 2.8758 | 0.4667 | 0.5089 | 0.4434 | 0.8732 | 0.7253 | 0.3708 | 0.4500 | 0.3979 | 0.4997 | 0.5086  
pendigits | 6870 | 16 | 2.2707 | 0.6878 | 0.9486 | 0.4595 | 0.9238 | 0.9435 | 0.7486 | 0.4698 | 0.8344 | 0.9303 | 0.9352  
pima | 768 | 8 | 34.8958 | 0.6794 | 0.7348 | 0.6235 | 0.7000 | 0.6806 | 0.7078 | 0.6271 | 0.6753 | 0.6215 | 0.6481  
satellite | 6435 | 36 | 31.6395 | 0.5714 | 0.6693 | 0.5572 | 0.7581 | 0.7022 | 0.6836 | 0.5573 | 0.8030 | 0.6622 | 0.5988  
satimage-2 | 5803 | 36 | 1.2235 | 0.8190 | 0.9917 | 0.4570 | 0.9804 | 0.9947 | 0.9536 | 0.4577 | 0.9959 | 0.9978 | 0.9822  
shuttle | 49097 | 9 | 7.1511 | 0.6234 | 0.6272 | 0.4724 | 0.9855 | 0.9971 | 0.6537 | 0.5264 | 0.9903 | 0.9917 | 0.9898  
vertebral | 240 | 6 | 12.5000 | 0.4262 | 0.3486 | 0.4166 | 0.3263 | 0.3905 | 0.3817 | 0.4081 | 0.3906 | 0.4431 | 0.4027  
vowels | 1456 | 12 | 3.4341 | 0.9606 | 0.5856 | 0.9425 | 0.6727 | 0.7585 | 0.9680 | 0.9410 | 0.8076 | 0.7802 | 0.6027  
wbc | 378 | 30 | 5.5556 | 0.9047 | 0.9227 | 0.9325 | 0.9516 | 0.9310 | 0.9366 | 0.9349 | 0.9210 | 0.9319 | 0.9159  
###### P@N Performance[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#p-n-performance "Link to this heading")
Precision @ N Performances (average of 10 independent trials)[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#id3 "Link to this table") Data | #Samples | ### Dimensions | Outlier Perc | ABOD | CBLOF | FB | HBOS | IForest | KNN | LOF | MCD | OCSVM | PCA  
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
arrhythmia | 452 | 274 | 14.6018 | 0.3808 | 0.4539 | 0.4230 | 0.5111 | 0.4961 | 0.4464 | 0.4334 | 0.3995 | 0.4614 | 0.4613  
cardio | 1831 | 21 | 9.6122 | 0.2374 | 0.5876 | 0.1690 | 0.4476 | 0.5041 | 0.3323 | 0.1541 | 0.4317 | 0.5011 | 0.6090  
glass | 214 | 9 | 4.2056 | 0.1702 | 0.0726 | 0.1476 | 0.0000 | 0.0726 | 0.0726 | 0.1476 | 0.0000 | 0.1726 | 0.0726  
ionosphere | 351 | 33 | 35.8974 | 0.8442 | 0.6088 | 0.7056 | 0.3295 | 0.6369 | 0.8602 | 0.7063 | 0.8806 | 0.7000 | 0.5729  
letter | 1600 | 32 | 6.2500 | 0.3801 | 0.0749 | 0.3642 | 0.0715 | 0.1003 | 0.3312 | 0.3641 | 0.1933 | 0.1510 | 0.0875  
lympho | 148 | 18 | 4.0541 | 0.4483 | 0.7517 | 0.7517 | 0.8467 | 0.9267 | 0.7517 | 0.7517 | 0.5183 | 0.7517 | 0.7517  
mnist | 7603 | 100 | 9.2069 | 0.3555 | 0.3348 | 0.3299 | 0.1188 | 0.3135 | 0.4204 | 0.3343 | 0.3462 | 0.3962 | 0.3846  
musk | 3062 | 166 | 3.1679 | 0.0507 | 0.7766 | 0.2230 | 0.9783 | 0.9680 | 0.2733 | 0.1695 | 0.9742 | 1.0000 | 0.9799  
optdigits | 5216 | 64 | 2.8758 | 0.0060 | 0.0000 | 0.0244 | 0.2194 | 0.0301 | 0.0000 | 0.0234 | 0.0000 | 0.0000 | 0.0000  
pendigits | 6870 | 16 | 2.2707 | 0.0812 | 0.2768 | 0.0658 | 0.2979 | 0.3422 | 0.0984 | 0.0653 | 0.0893 | 0.3287 | 0.3187  
pima | 768 | 8 | 34.8958 | 0.5193 | 0.5413 | 0.4480 | 0.5424 | 0.5111 | 0.5413 | 0.4555 | 0.4962 | 0.4704 | 0.4943  
satellite | 6435 | 36 | 31.6395 | 0.3902 | 0.4152 | 0.3902 | 0.5690 | 0.5676 | 0.4994 | 0.3893 | 0.6845 | 0.5346 | 0.4784  
satimage-2 | 5803 | 36 | 1.2235 | 0.2130 | 0.8846 | 0.0555 | 0.6939 | 0.8754 | 0.3809 | 0.0555 | 0.6481 | 0.9356 | 0.8041  
shuttle | 49097 | 9 | 7.1511 | 0.1977 | 0.2943 | 0.0695 | 0.9551 | 0.9546 | 0.2184 | 0.1424 | 0.7506 | 0.9542 | 0.9501  
vertebral | 240 | 6 | 12.5000 | 0.0601 | 0.0000 | 0.0644 | 0.0071 | 0.0343 | 0.0238 | 0.0506 | 0.0071 | 0.0238 | 0.0226  
vowels | 1456 | 12 | 3.4341 | 0.5710 | 0.0831 | 0.3224 | 0.1297 | 0.1875 | 0.5093 | 0.3551 | 0.2186 | 0.2791 | 0.1364  
wbc | 378 | 30 | 5.5556 | 0.3060 | 0.5055 | 0.5188 | 0.5817 | 0.5088 | 0.4952 | 0.5188 | 0.4577 | 0.5125 | 0.4767  
###### Execution Time[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#execution-time "Link to this heading")
Time Elapsed in Seconds (average of 10 independent trials)[¬∂](https://pyod.readthedocs.io/en/latest/benchmark.html#id4 "Link to this table") Data | #Samples | ### Dimensions | Outlier Perc | ABOD | CBLOF | FB | HBOS | IForest | KNN | LOF | MCD | OCSVM | PCA  
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
arrhythmia | 452 | 274 | 14.6018 | 0.3667 | 0.2123 | 0.5651 | 0.1383 | 0.2669 | 0.1075 | 0.0743 | 1.4165 | 0.0473 | 0.0596  
cardio | 1831 | 21 | 9.6122 | 0.3824 | 0.1255 | 0.7741 | 0.0053 | 0.2672 | 0.2249 | 0.0993 | 0.5418 | 0.0883 | 0.0035  
glass | 214 | 9 | 4.2056 | 0.0352 | 0.0359 | 0.0317 | 0.0022 | 0.1724 | 0.0173 | 0.0025 | 0.0325 | 0.0010 | 0.0011  
ionosphere | 351 | 33 | 35.8974 | 0.0645 | 0.0459 | 0.0728 | 0.0082 | 0.1864 | 0.0302 | 0.0070 | 0.0718 | 0.0048 | 0.0018  
letter | 1600 | 32 | 6.2500 | 0.3435 | 0.1014 | 0.7361 | 0.0080 | 0.2617 | 0.1882 | 0.0935 | 1.1942 | 0.0888 | 0.0041  
lympho | 148 | 18 | 4.0541 | 0.0277 | 0.0353 | 0.0266 | 0.0037 | 0.1712 | 0.0111 | 0.0021 | 0.0327 | 0.0014 | 0.0012  
mnist | 7603 | 100 | 9.2069 | 7.4192 | 1.1339 | 48.2750 | 0.0480 | 1.9314 | 7.3431 | 6.7901 | 4.7448 | 5.0203 | 0.1569  
musk | 3062 | 166 | 3.1679 | 2.3860 | 0.4134 | 13.8610 | 0.0587 | 1.2736 | 2.2057 | 1.9835 | 25.5501 | 1.3774 | 0.1637  
optdigits | 5216 | 64 | 2.8758 | 2.7279 | 0.4977 | 14.2399 | 0.0303 | 0.7783 | 2.1205 | 1.7799 | 1.8599 | 1.5618 | 0.0519  
pendigits | 6870 | 16 | 2.2707 | 1.4339 | 0.2847 | 3.8185 | 0.0090 | 0.5879 | 0.8659 | 0.5936 | 2.2209 | 0.9666 | 0.0062  
pima | 768 | 8 | 34.8958 | 0.1357 | 0.0698 | 0.0908 | 0.0019 | 0.1923 | 0.0590 | 0.0102 | 0.0474 | 0.0087 | 0.0013  
satellite | 6435 | 36 | 31.6395 | 1.7970 | 0.4269 | 7.5566 | 0.0161 | 0.6449 | 1.2578 | 0.9868 | 2.6916 | 1.3697 | 0.0245  
satimage-2 | 5803 | 36 | 1.2235 | 1.5209 | 0.3705 | 5.6561 | 0.0148 | 0.5529 | 1.0587 | 0.7525 | 2.3935 | 1.1114 | 0.0151  
shuttle | 49097 | 9 | 7.1511 | 14.3611 | 1.2524 | 59.2131 | 0.0953 | 3.3906 | 9.4958 | 11.1500 | 12.1449 | 44.6830 | 0.0378  
vertebral | 240 | 6 | 12.5000 | 0.0529 | 0.0444 | 0.0339 | 0.0014 | 0.1786 | 0.0161 | 0.0025 | 0.0446 | 0.0015 | 0.0010  
vowels | 1456 | 12 | 3.4341 | 0.3380 | 0.0889 | 0.3125 | 0.0044 | 0.2751 | 0.1125 | 0.0367 | 0.9745 | 0.0469 | 0.0023  
wbc | 378 | 30 | 5.5556 | 0.1014 | 0.0691 | 0.0771 | 0.0063 | 0.2030 | 0.0287 | 0.0078 | 0.0864 | 0.0062 | 0.0035  
[ Next API CheatSheet ](https://pyod.readthedocs.io/en/latest/api_cc.html) [ Previous Examples ](https://pyod.readthedocs.io/en/latest/example.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)
    * [Latest ADBench (2022)](https://pyod.readthedocs.io/en/latest/benchmark.html#latest-adbench-2022)
    * [Old Results (2019)](https://pyod.readthedocs.io/en/latest/benchmark.html#old-results-2019)
    * [ROC Performance](https://pyod.readthedocs.io/en/latest/benchmark.html#roc-performance)
    * [P@N Performance](https://pyod.readthedocs.io/en/latest/benchmark.html#p-n-performance)
    * [Execution Time](https://pyod.readthedocs.io/en/latest/benchmark.html#execution-time)



---

## 6. Examples - pyod 2.0.5 documentation {#6-1}

**URL:** https://pyod.readthedocs.io/en/latest/example.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:59

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/example.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/example.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/example.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Examples[¬∂](https://pyod.readthedocs.io/en/latest/example.html#examples "Link to this heading")
* * *
###### Featured Tutorials[¬∂](https://pyod.readthedocs.io/en/latest/example.html#featured-tutorials "Link to this heading")
PyOD has been well acknowledged by the machine learning community with a few featured posts and tutorials.
**Analytics Vidhya** : [An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/)
**KDnuggets** : [Intuitive Visualization of Outlier Detection Methods](https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html)
**Towards Data Science** : [Anomaly Detection for Dummies](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)
**awesome-machine-learning** : [General-Purpose Machine Learning](https://github.com/josephmisiti/awesome-machine-learning#python-general-purpose)
* * *
###### kNN Example[¬∂](https://pyod.readthedocs.io/en/latest/example.html#knn-example "Link to this heading")
Full example: [knn_example.py](https://github.com/yzhao062/Pyod/blob/master/examples/knn_example.py)
  1. Import models
> ```
frompyod.models.knnimport KNN   ### kNN detector

```

  2. Generate sample data with [`pyod.utils.data.generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data "pyod.utils.data.generate_data"):
> ```
contamination = 0.1  ### percentage of outliers
n_train = 200  ### number of training points
n_test = 100  ### number of testing points

X_train, X_test, y_train, y_test = generate_data(
    n_train=n_train, n_test=n_test, contamination=contamination)

```

  3. Initialize a [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") detector, fit the model, and make the prediction.
> ```
### train kNN detector
clf_name = 'KNN'
clf = KNN()
clf.fit(X_train)

### get the prediction labels and outlier scores of the training data
y_train_pred = clf.labels_  ### binary labels (0: inliers, 1: outliers)
y_train_scores = clf.decision_scores_  ### raw outlier scores

### get the prediction on the test data
y_test_pred = clf.predict(X_test)  ### outlier labels (0 or 1)
y_test_scores = clf.decision_function(X_test)  ### outlier scores

### it is possible to get the prediction confidence as well
y_test_pred, y_test_pred_confidence = clf.predict(X_test, return_confidence=True)  ### outlier labels (0 or 1) and confidence in the range of [0,1]

```

  4. Evaluate the prediction using ROC and Precision @ Rank n [`pyod.utils.data.evaluate_print()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.evaluate_print "pyod.utils.data.evaluate_print").
> ```
frompyod.utils.dataimport evaluate_print
### evaluate and print the results
print("\nOn Training Data:")
evaluate_print(clf_name, y_train, y_train_scores)
print("\nOn Test Data:")
evaluate_print(clf_name, y_test, y_test_scores)

```

  5. See sample outputs on both training and test data.
  6. Generate the visualizations by visualize function included in all examples.
> ```
visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,
          y_test_pred, show_figure=True, save_figure=False)

```


![kNN demo](https://pyod.readthedocs.io/en/latest/_images/KNN.png)
* * *
###### Model Combination Example[¬∂](https://pyod.readthedocs.io/en/latest/example.html#model-combination-example "Link to this heading")
Outlier detection often suffers from model instability due to its unsupervised nature. Thus, it is recommended to combine various detector outputs, e.g., by averaging, to improve its robustness. Detector combination is a subfield of outlier ensembles; refer [[BKalayciE18](https://pyod.readthedocs.io/en/latest/example.html#id26 "ƒ∞lker Kalaycƒ± and Tuncay Ercan. Anomaly detection in wireless sensor networks data by using histogram based outlier score method. In 2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies \(ISMSIT\), 1‚Äì6. IEEE, 2018.")] for more information.
Four score combination mechanisms are shown in this demo:
  1. **Average** : average scores of all detectors.
  2. **maximization** : maximum score across all detectors.
  3. **Average of Maximum (AOM)** : divide base detectors into subgroups and take the maximum score for each subgroup. The final score is the average of all subgroup scores.
  4. **Maximum of Average (MOA)** : divide base detectors into subgroups and take the average score for each subgroup. The final score is the maximum of all subgroup scores.


‚Äúexamples/comb_example.py‚Äù illustrates the API for combining the output of multiple base detectors ([comb_example.py](https://github.com/yzhao062/pyod/blob/master/examples/comb_example.py), [Jupyter Notebooks](https://mybinder.org/v2/gh/yzhao062/pyod/master)). For Jupyter Notebooks, please navigate to **‚Äú/notebooks/Model Combination.ipynb‚Äù**
  1. Import models and generate sample data.
> ```
frompyod.models.knnimport KNN  ### kNN detector
frompyod.models.combinationimport aom, moa, average, maximization
frompyod.utils.dataimport generate_data

X, y= generate_data(train_only=True)  ### load data

```

  2. Initialize 20 kNN outlier detectors with different k (10 to 200), and get the outlier scores.
> ```
### initialize 20 base detectors for combination
k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
            150, 160, 170, 180, 190, 200]
n_clf = len(k_list) ### Number of classifiers being trained

train_scores = np.zeros([X_train.shape[0], n_clf])
test_scores = np.zeros([X_test.shape[0], n_clf])

for i in range(n_clf):
    k = k_list[i]

    clf = KNN(n_neighbors=k, method='largest')
    clf.fit(X_train_norm)

    train_scores[:, i] = clf.decision_scores_
    test_scores[:, i] = clf.decision_function(X_test_norm)

```

  3. Then the output scores are standardized into zero average and unit std before combination. This step is crucial to adjust the detector outputs to the same scale.
> ```
frompyod.utils.utilityimport standardizer

### scores have to be normalized before combination
train_scores_norm, test_scores_norm = standardizer(train_scores, test_scores)

```

  4. Four different combination algorithms are applied as described above:
> ```
comb_by_average = average(test_scores_norm)
comb_by_maximization = maximization(test_scores_norm)
comb_by_aom = aom(test_scores_norm, 5) ### 5 groups
comb_by_moa = moa(test_scores_norm, 5) ### 5 groups

```

  5. Finally, all four combination methods are evaluated by ROC and Precision @ Rank n:
> ```
20
```



###### Thresholding Example[¬∂](https://pyod.readthedocs.io/en/latest/example.html#thresholding-example "Link to this heading")
Full example: [threshold_example.py](https://github.com/yzhao062/Pyod/blob/master/examples/threshold_example.py)
  1. Import models
> ```
frompyod.models.knnimport KNN   ### kNN detector
frompyod.models.thresholdsimport FILTER  ### Filter thresholder

```

  2. Generate sample data with [`pyod.utils.data.generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data "pyod.utils.data.generate_data"):
> ```
contamination = 0.1  ### percentage of outliers
n_train = 200  ### number of training points
n_test = 100  ### number of testing points

X_train, X_test, y_train, y_test = generate_data(
    n_train=n_train, n_test=n_test, contamination=contamination)

```

  3. Initialize a [`pyod.models.knn.KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "pyod.models.knn.KNN") detector, fit the model, and make the prediction.
> ```
### train kNN detector and apply FILTER thresholding
clf_name = 'KNN'
clf = KNN(contamination=FILTER())
clf.fit(X_train)

### get the prediction labels and outlier scores of the training data
y_train_pred = clf.labels_  ### binary labels (0: inliers, 1: outliers)
y_train_scores = clf.decision_scores_  ### raw outlier scores

```



References
[[BKalayciE18](https://pyod.readthedocs.io/en/latest/example.html#id1)]
ƒ∞lker Kalaycƒ± and Tuncay Ercan. Anomaly detection in wireless sensor networks data by using histogram based outlier score method. In _2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)_ , 1‚Äì6. IEEE, 2018.
[ Next Benchmarks ](https://pyod.readthedocs.io/en/latest/benchmark.html) [ Previous Fast Train with SUOD ](https://pyod.readthedocs.io/en/latest/fast_train.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
    * [Featured Tutorials](https://pyod.readthedocs.io/en/latest/example.html#featured-tutorials)
    * [kNN Example](https://pyod.readthedocs.io/en/latest/example.html#knn-example)
    * [Model Combination Example](https://pyod.readthedocs.io/en/latest/example.html#model-combination-example)
    * [Thresholding Example](https://pyod.readthedocs.io/en/latest/example.html#thresholding-example)



---

## 7. Frequently Asked Questions - pyod 2.0.5 documentation {#7-1}

**URL:** https://pyod.readthedocs.io/en/latest/faq.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:02

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/faq.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/faq.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/faq.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Frequently Asked Questions[¬∂](https://pyod.readthedocs.io/en/latest/faq.html#frequently-asked-questions "Link to this heading")
* * *
###### What is the Next?[¬∂](https://pyod.readthedocs.io/en/latest/faq.html#what-is-the-next "Link to this heading")
This is the central place to track important things to be fixed/added:
  * GPU support (it is noted that keras with TensorFlow backend will automatically run on GPU; auto_encoder_example.py takes around 96.95 seconds on a RTX 2060 GPU).
  * Installation efficiency improvement, such as using docker
  * Add contact channel with [Gitter](https://gitter.im)
  * Support additional languages, see [Manage Translations](https://docs.readthedocs.io/en/latest/guides/manage-translations.html)
  * Fix the bug that numba enabled function may be excluded from code coverage
  * Decide which Python interpreter should readthedocs use. 3.X invokes Python 3.7 which has no TF supported for now.


Feel free to open on issue report if needed. See [Issues](https://github.com/yzhao062/pyod/issues).
* * *
###### How to Contribute[¬∂](https://pyod.readthedocs.io/en/latest/faq.html#how-to-contribute "Link to this heading")
You are welcome to contribute to this exciting project:
  * Please first check Issue lists for ‚Äúhelp wanted‚Äù tag and comment the one you are interested. We will assign the issue to you.
  * Fork the master branch and add your improvement/modification/fix.
  * Create a pull request to **development branch** and follow the pull request template [PR template](https://github.com/yzhao062/pyod/blob/master/PULL_REQUEST_TEMPLATE.md)
  * Automatic tests will be triggered. Make sure all tests are passed. Please make sure all added modules are accompanied with proper test functions.


To make sure the code has the same style and standard, please refer to abod.py, hbos.py, or feature_bagging.py for example.
You are also welcome to share your ideas by opening an issue or dropping me an email at zhaoy@cmu.edu :)
###### Inclusion Criteria[¬∂](https://pyod.readthedocs.io/en/latest/faq.html#inclusion-criteria "Link to this heading")
Similarly to [scikit-learn](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms), We mainly consider well-established algorithms for inclusion. A rule of thumb is at least two years since publication, 50+ citations, and usefulness.
However, we encourage the author(s) of newly proposed models to share and add your implementation into PyOD for boosting ML accessibility and reproducibility. This exception only applies if you could commit to the maintenance of your model for at least two year period.
[ Next About us ](https://pyod.readthedocs.io/en/latest/about.html) [ Previous Citations & Achievements ](https://pyod.readthedocs.io/en/latest/pubs.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
    * [What is the Next?](https://pyod.readthedocs.io/en/latest/faq.html#what-is-the-next)
    * [How to Contribute](https://pyod.readthedocs.io/en/latest/faq.html#how-to-contribute)
    * [Inclusion Criteria](https://pyod.readthedocs.io/en/latest/faq.html#inclusion-criteria)



---

## 8. Fast Train with SUOD - pyod 2.0.5 documentation {#8-1}

**URL:** https://pyod.readthedocs.io/en/latest/fast_train.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:01

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/fast_train.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/fast_train.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/fast_train.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Fast Train with SUOD[¬∂](https://pyod.readthedocs.io/en/latest/fast_train.html#fast-train-with-suod "Link to this heading")
**Fast training and prediction** : it is possible to train and predict with a large number of detection models in PyOD by leveraging SUOD framework. See [SUOD Paper](https://proceedings.mlsys.org/paper_files/paper/2021/file/37385144cac01dff38247ab11c119e3c-Paper.pdf) and [SUOD example](https://github.com/yzhao062/pyod/blob/master/examples/suod_example.py).
```
frompyod.models.suodimport SUOD

### initialized a group of outlier detectors for acceleration
detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20),
                 LOF(n_neighbors=25), LOF(n_neighbors=35),
                 COPOD(), IForest(n_estimators=100),
                 IForest(n_estimators=200)]

### decide the number of parallel process, and the combination method
### then clf can be used as any outlier detection model
clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average',
           verbose=False)

```

[ Next Examples ](https://pyod.readthedocs.io/en/latest/example.html) [ Previous Model Save & Load ](https://pyod.readthedocs.io/en/latest/model_persistence.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 9. Installation - pyod 2.0.5 documentation {#9-1}

**URL:** https://pyod.readthedocs.io/en/latest/install.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:03

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/install.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/install.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/install.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Installation[¬∂](https://pyod.readthedocs.io/en/latest/install.html#installation "Link to this heading")
PyOD is designed for easy installation using either **pip** or **conda**. We recommend using the latest version of PyOD due to frequent updates and enhancements:
```
### normal install
pip### or update if needed

```

Alternatively, you can clone and run the setup.py file:
```
cd
```

**Required Dependencies** :
  * Python 3.8 or higher
  * joblib
  * matplotlib
  * numpy>=1.19
  * numba>=0.51
  * scipy>=1.5.1
  * scikit_learn>=0.22.0


**Optional Dependencies (see details below)** :
  * combo (optional, required for models/combination.py and FeatureBagging)
  * pytorch (optional, required for deep learning models)
  * suod (optional, required for running SUOD model)
  * xgboost (optional, required for XGBOD)
  * pythresh (optional, required for thresholding)


Warning
PyOD includes several neural network-based models, such as AutoEncoders, implemented in PyTorch. These deep learning libraries are not automatically installed by PyOD to avoid conflicts with existing installations. If you plan to use neural-net based models, please ensure these libraries are installed. See the [neural-net FAQ](https://github.com/yzhao062/pyod/wiki/Setting-up-Keras-and-Tensorflow-for-Neural-net-Based-models) for guidance. Additionally, xgboost is not installed by default but is required for models like XGBOD.
[ Next Model Save & Load ](https://pyod.readthedocs.io/en/latest/model_persistence.html) [ Previous Home ](https://pyod.readthedocs.io/en/latest/index.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 10. Known Issues & Warnings - pyod 2.0.5 documentation {#10-1}

**URL:** https://pyod.readthedocs.io/en/latest/issues.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:55

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/issues.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/issues.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/issues.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Known Issues & Warnings[¬∂](https://pyod.readthedocs.io/en/latest/issues.html#known-issues-warnings "Link to this heading")
This is the central place to track known issues.
###### Installation[¬∂](https://pyod.readthedocs.io/en/latest/issues.html#installation "Link to this heading")
There are some known dependency issues/notes. Refer [installation](https://pyod.readthedocs.io/en/latest/install.html) for more information.
###### Neural Networks[¬∂](https://pyod.readthedocs.io/en/latest/issues.html#neural-networks "Link to this heading")
SO_GAAL and MO_GAAL may only work under Python 3.5+.
###### Differences between PyOD and scikit-learn[¬∂](https://pyod.readthedocs.io/en/latest/issues.html#differences-between-pyod-and-scikit-learn "Link to this heading")
Although PyOD is built on top of scikit-learn and inspired by its API design, some differences should be noted:
  * All models in PyOD follow the tradition that the outlying objects come with higher scores while the normal objects have lower scores. scikit-learn has an inverted design‚Äìlower scores stand for outlying objects.
  * PyOD uses ‚Äú0‚Äù to represent inliers and ‚Äú1‚Äù to represent outliers. Differently, scikit-learn returns ‚Äú-1‚Äù for anomalies/outliers and ‚Äú1‚Äù for inliers.
  * Although Isolation Forests, One-class SVM, and Local Outlier Factor are implemented in both PyOD and scikit-learn, users are not advised to mix the use of them, e.g., calling one model from PyOD and another model from scikit-learn. It is recommended to only use one library for consistency (for three models, the PyOD implementation is indeed a set of wrapper functions of scikit-learn).
  * PyOD models may not work with scikit-learn‚Äôs check_estimator function. Similarly, scikit-learn models would not work with PyOD‚Äôs check_estimator function.


[ Next Outlier Detection 101 ](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html) [ Previous Utility Functions ](https://pyod.readthedocs.io/en/latest/pyod.utils.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
    * [Installation](https://pyod.readthedocs.io/en/latest/issues.html#installation)
    * [Neural Networks](https://pyod.readthedocs.io/en/latest/issues.html#neural-networks)
    * [Differences between PyOD and scikit-learn](https://pyod.readthedocs.io/en/latest/issues.html#differences-between-pyod-and-scikit-learn)



---

## 11. Model Save & Load - pyod 2.0.5 documentation {#11-1}

**URL:** https://pyod.readthedocs.io/en/latest/model_persistence.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:04

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/model_persistence.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/model_persistence.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/model_persistence.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Model Save & Load[¬∂](https://pyod.readthedocs.io/en/latest/model_persistence.html#model-save-load "Link to this heading")
PyOD takes a similar approach of sklearn regarding model persistence. See [model persistence](https://scikit-learn.org/stable/modules/model_persistence.html) for clarification.
In short, we recommend to use joblib or pickle for saving and loading PyOD models. See [‚Äúexamples/save_load_model_example.py‚Äù](https://github.com/yzhao062/pyod/blob/master/examples/save_load_model_example.py) for an example. In short, it is simple as below:
```
fromjoblibimport dump, load

### save the model
dump(clf, 'clf.joblib')
### load the model
clf = load('clf.joblib')

```

It is known that there are challenges in saving neural network models. Check [#328](https://github.com/yzhao062/pyod/issues/328#issuecomment-917192704) and [#88](https://github.com/yzhao062/pyod/issues/88#issuecomment-615343139) for temporary workaround.
[ Next Fast Train with SUOD ](https://pyod.readthedocs.io/en/latest/fast_train.html) [ Previous Installation ](https://pyod.readthedocs.io/en/latest/install.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 12. Citations & Achievements - pyod 2.0.5 documentation {#12-1}

**URL:** https://pyod.readthedocs.io/en/latest/pubs.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:06

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/pubs.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/pubs.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/pubs.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Citations & Achievements[¬∂](https://pyod.readthedocs.io/en/latest/pubs.html#citations-achievements "Link to this heading")
* * *
###### Citing PyOD[¬∂](https://pyod.readthedocs.io/en/latest/pubs.html#citing-pyod "Link to this heading")
[PyOD paper](http://www.jmlr.org/papers/volume20/19-011/19-011.pdf) is published in [JMLR](http://www.jmlr.org/) (machine learning open-source software track). If you use PyOD in a scientific publication, we would appreciate citations to the following paper:
```
@article{zhao2019pyod,
  author  = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
  title   = {PyOD: A Python Toolbox for Scalable Outlier Detection},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {96},
  pages   = {1-7},
  url     = {http://jmlr.org/papers/v20/19-011.html}
}

```

or:
```
Zhao, Y., Nasrullah, Z. and Li, Z., 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of machine learning research (JMLR), 20(96), pp.1-7.

```

* * *
###### Scientific Work Using or Referencing PyOD[¬∂](https://pyod.readthedocs.io/en/latest/pubs.html#scientific-work-using-or-referencing-pyod "Link to this heading")
We are appreciated that PyOD has been increasingly referred and cited in scientific works. Since its release, PyOD has been used in hundred of academic projects. See [an incomplete list here](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3726241381117726876).
* * *
###### Featured Posts & Achievements[¬∂](https://pyod.readthedocs.io/en/latest/pubs.html#featured-posts-achievements "Link to this heading")
PyOD has been well acknowledged by the machine learning community with a few featured posts and tutorials.
**Analytics Vidhya** : [An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/)
**KDnuggets** : [Intuitive Visualization of Outlier Detection Methods](https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html)
**KDnuggets** : [An Overview of Outlier Detection Methods from PyOD](https://www.kdnuggets.com/2019/06/overview-outlier-detection-methods-pyod.html)
**Towards Data Science** : [Anomaly Detection for Dummies](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)
**Computer Vision News (March 2019)** : [Python Open Source Toolbox for Outlier Detection](https://rsipvision.com/ComputerVisionNews-2019March/18/)
**FLOYDHUB** : [Introduction to Anomaly Detection in Python](https://blog.floydhub.com/introduction-to-anomaly-detection-in-python/)
**awesome-machine-learning** : [General-Purpose Machine Learning](https://github.com/josephmisiti/awesome-machine-learning#python-general-purpose)
**Lecture on anomaly detection with PyOD by Dr.Hadi Fanaee** : [Anomaly Detection Lecture](https://www.youtube.com/watch?v=sF2DeSPrGfc)
**Workshop/Showcase using PyOD** :
  * [Detecting the Unexpected: An Introduction to Anomaly Detection Methods](http://www.kiss.caltech.edu/workshops/technosignatures/presentations/Wagstaff.pdf), _KISS Technosignatures Workshop_ by Dr. Kiri Wagstaff @ Jet Propulsion Laboratory, California Institute of Technology. [[Workshop Video](https://www.youtube.com/watch?v=brWqY4Wads4)] [[PDF](http://www.kiss.caltech.edu/workshops/technosignatures/presentations/Wagstaff.pdf)]


**GitHub Python Trending** :
  * 2019: Jul 8th-9th, Apr 5th-6th, Feb 10th-11th, Jan 23th-24th, Jan 10th-14th
  * 2018: Jun 15, Dec 8th-9th


**Miscellaneous** :
  * [PythonAwesome](https://pythonawesome.com/a-python-toolkit-for-scalable-outlier-detection/)
  * [awesome-python](https://github.com/uhub/awesome-python)
  * [PapersWithCode](https://paperswithcode.com/task/anomaly-detection)


[ Next Frequently Asked Questions ](https://pyod.readthedocs.io/en/latest/faq.html) [ Previous Outlier Detection 101 ](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
    * [Citing PyOD](https://pyod.readthedocs.io/en/latest/pubs.html#citing-pyod)
    * [Scientific Work Using or Referencing PyOD](https://pyod.readthedocs.io/en/latest/pubs.html#scientific-work-using-or-referencing-pyod)
    * [Featured Posts & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html#featured-posts-achievements)



---

## 13. API Reference - pyod 2.0.5 documentation {#13-1}

**URL:** https://pyod.readthedocs.io/en/latest/pyod.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:55

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/pyod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/pyod.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/pyod.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### API Reference[¬∂](https://pyod.readthedocs.io/en/latest/pyod.html#api-reference "Link to this heading")
  * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [pyod.models.abod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod)
      * [`ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD)
        * [`ABOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.compute_rejection_stats)
        * [`ABOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.decision_function)
        * [`ABOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit)
        * [`ABOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict)
        * [`ABOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict_score)
        * [`ABOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict)
        * [`ABOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_confidence)
        * [`ABOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_proba)
        * [`ABOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_with_rejection)
    * [pyod.models.ae1svm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ae1svm)
      * [`AE1SVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM)
        * [`AE1SVM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.compute_rejection_stats)
        * [`AE1SVM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.decision_function)
        * [`AE1SVM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit)
        * [`AE1SVM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict)
        * [`AE1SVM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict_score)
        * [`AE1SVM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict)
        * [`AE1SVM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_confidence)
        * [`AE1SVM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_proba)
        * [`AE1SVM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_with_rejection)
    * [pyod.models.alad module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.alad)
      * [`ALAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD)
        * [`ALAD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.compute_rejection_stats)
        * [`ALAD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.decision_function)
        * [`ALAD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit)
        * [`ALAD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict)
        * [`ALAD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict_score)
        * [`ALAD.plot_learning_curves()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.plot_learning_curves)
        * [`ALAD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict)
        * [`ALAD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_confidence)
        * [`ALAD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_proba)
        * [`ALAD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_with_rejection)
    * [pyod.models.anogan module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.anogan)
      * [`AnoGAN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN)
        * [`AnoGAN.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.compute_rejection_stats)
        * [`AnoGAN.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.decision_function)
        * [`AnoGAN.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit)
        * [`AnoGAN.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict)
        * [`AnoGAN.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict_score)
        * [`AnoGAN.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.get_params)
        * [`AnoGAN.plot_learning_curves()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.plot_learning_curves)
        * [`AnoGAN.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict)
        * [`AnoGAN.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_confidence)
        * [`AnoGAN.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_proba)
        * [`AnoGAN.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_with_rejection)
        * [`AnoGAN.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.set_params)
    * [pyod.models.auto_encoder module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.auto_encoder)
      * [`AutoEncoder`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder)
        * [`AutoEncoder.build_model()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.build_model)
        * [`AutoEncoder.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats)
        * [`AutoEncoder.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.decision_function)
        * [`AutoEncoder.evaluate()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.evaluate)
        * [`AutoEncoder.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit)
        * [`AutoEncoder.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict)
        * [`AutoEncoder.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict_score)
        * [`AutoEncoder.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict)
        * [`AutoEncoder.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_confidence)
        * [`AutoEncoder.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_proba)
        * [`AutoEncoder.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_with_rejection)
        * [`AutoEncoder.save()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.save)
        * [`AutoEncoder.train()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.train)
        * [`AutoEncoder.training_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_forward)
        * [`AutoEncoder.training_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_prepare)
    * [pyod.models.auto_encoder_torch module](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-auto-encoder-torch-module)
    * [pyod.models.cblof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)
      * [`CBLOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF)
        * [`CBLOF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.compute_rejection_stats)
        * [`CBLOF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.decision_function)
        * [`CBLOF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit)
        * [`CBLOF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict)
        * [`CBLOF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict_score)
        * [`CBLOF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict)
        * [`CBLOF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_confidence)
        * [`CBLOF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_proba)
        * [`CBLOF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_with_rejection)
    * [pyod.models.cof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof)
      * [`COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF)
        * [`COF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.compute_rejection_stats)
        * [`COF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.decision_function)
        * [`COF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit)
        * [`COF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict)
        * [`COF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict_score)
        * [`COF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict)
        * [`COF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_confidence)
        * [`COF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_proba)
        * [`COF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_with_rejection)
    * [pyod.models.combination module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.combination)
      * [`aom()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom)
      * [`average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average)
      * [`majority_vote()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote)
      * [`maximization()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization)
      * [`median()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median)
      * [`moa()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa)
    * [pyod.models.cd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd)
      * [`CD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD)
        * [`CD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.compute_rejection_stats)
        * [`CD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.decision_function)
        * [`CD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit)
        * [`CD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict)
        * [`CD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict_score)
        * [`CD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict)
        * [`CD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_confidence)
        * [`CD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_proba)
        * [`CD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_with_rejection)
    * [pyod.models.copod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod)
      * [`COPOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD)
        * [`COPOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.compute_rejection_stats)
        * [`COPOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.decision_function)
        * [`COPOD.explain_outlier()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.explain_outlier)
        * [`COPOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit)
        * [`COPOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict)
        * [`COPOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict_score)
        * [`COPOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.get_params)
        * [`COPOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict)
        * [`COPOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_confidence)
        * [`COPOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_proba)
        * [`COPOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_with_rejection)
        * [`COPOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.set_params)
      * [`skew()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.skew)
    * [pyod.models.deep_svdd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.deep_svdd)
      * [`DeepSVDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD)
        * [`DeepSVDD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats)
        * [`DeepSVDD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.decision_function)
        * [`DeepSVDD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit)
        * [`DeepSVDD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict)
        * [`DeepSVDD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict_score)
        * [`DeepSVDD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.get_params)
        * [`DeepSVDD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict)
        * [`DeepSVDD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_confidence)
        * [`DeepSVDD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_proba)
        * [`DeepSVDD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_with_rejection)
        * [`DeepSVDD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.set_params)
    * [pyod.models.devnet module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.devnet)
      * [`DevNet`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet)
        * [`DevNet.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.compute_rejection_stats)
        * [`DevNet.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.decision_function)
        * [`DevNet.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit)
        * [`DevNet.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict)
        * [`DevNet.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict_score)
        * [`DevNet.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.get_params)
        * [`DevNet.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict)
        * [`DevNet.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_confidence)
        * [`DevNet.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_proba)
        * [`DevNet.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_with_rejection)
        * [`DevNet.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.set_params)
    * [pyod.models.dif module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.dif)
      * [`DIF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF)
        * [`DIF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.compute_rejection_stats)
        * [`DIF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.decision_function)
        * [`DIF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit)
        * [`DIF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict)
        * [`DIF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict_score)
        * [`DIF.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.get_params)
        * [`DIF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict)
        * [`DIF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_confidence)
        * [`DIF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_proba)
        * [`DIF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_with_rejection)
        * [`DIF.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.set_params)
    * [pyod.models.ecod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod)
      * [`ECOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD)
        * [`ECOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.compute_rejection_stats)
        * [`ECOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.decision_function)
        * [`ECOD.explain_outlier()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.explain_outlier)
        * [`ECOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit)
        * [`ECOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict)
        * [`ECOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict_score)
        * [`ECOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.get_params)
        * [`ECOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict)
        * [`ECOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_confidence)
        * [`ECOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_proba)
        * [`ECOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_with_rejection)
        * [`ECOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.set_params)
      * [`skew()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.skew)
    * [pyod.models.feature_bagging module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.feature_bagging)
      * [`FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging)
        * [`FeatureBagging.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats)
        * [`FeatureBagging.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.decision_function)
        * [`FeatureBagging.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit)
        * [`FeatureBagging.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict)
        * [`FeatureBagging.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict_score)
        * [`FeatureBagging.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.get_params)
        * [`FeatureBagging.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict)
        * [`FeatureBagging.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_confidence)
        * [`FeatureBagging.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_proba)
        * [`FeatureBagging.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_with_rejection)
        * [`FeatureBagging.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.set_params)
    * [pyod.models.gmm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm)
      * [`GMM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM)
        * [`GMM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.compute_rejection_stats)
        * [`GMM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.decision_function)
        * [`GMM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit)
        * [`GMM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict)
        * [`GMM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict_score)
        * [`GMM.precisions_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_)
        * [`GMM.precisions_cholesky_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_cholesky_)
        * [`GMM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict)
        * [`GMM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_confidence)
        * [`GMM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_proba)
        * [`GMM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_with_rejection)
    * [pyod.models.hbos module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos)
      * [`HBOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS)
        * [`HBOS.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.compute_rejection_stats)
        * [`HBOS.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.decision_function)
        * [`HBOS.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit)
        * [`HBOS.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict)
        * [`HBOS.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict_score)
        * [`HBOS.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.get_params)
        * [`HBOS.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict)
        * [`HBOS.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_confidence)
        * [`HBOS.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_proba)
        * [`HBOS.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_with_rejection)
        * [`HBOS.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.set_params)
    * [pyod.models.iforest module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest)
      * [`IForest`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest)
        * [`IForest.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.compute_rejection_stats)
        * [`IForest.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.decision_function)
        * [`IForest.feature_importances_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.feature_importances_)
        * [`IForest.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit)
        * [`IForest.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict)
        * [`IForest.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict_score)
        * [`IForest.max_samples_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.max_samples_)
        * [`IForest.n_features_in_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.n_features_in_)
        * [`IForest.offset_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.offset_)
        * [`IForest.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict)
        * [`IForest.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_confidence)
        * [`IForest.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_proba)
        * [`IForest.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_with_rejection)
    * [pyod.models.inne module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne)
      * [`INNE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE)
        * [`INNE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.compute_rejection_stats)
        * [`INNE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.decision_function)
        * [`INNE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit)
        * [`INNE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict)
        * [`INNE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict_score)
        * [`INNE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.get_params)
        * [`INNE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict)
        * [`INNE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_confidence)
        * [`INNE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_proba)
        * [`INNE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_with_rejection)
        * [`INNE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.set_params)
    * [pyod.models.kde module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde)
      * [`KDE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE)
        * [`KDE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.compute_rejection_stats)
        * [`KDE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.decision_function)
        * [`KDE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit)
        * [`KDE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict)
        * [`KDE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict_score)
        * [`KDE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.get_params)
        * [`KDE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict)
        * [`KDE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_confidence)
        * [`KDE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_proba)
        * [`KDE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_with_rejection)
        * [`KDE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.set_params)
    * [pyod.models.knn module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn)
      * [`KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN)
        * [`KNN.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.compute_rejection_stats)
        * [`KNN.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.decision_function)
        * [`KNN.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit)
        * [`KNN.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict)
        * [`KNN.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict_score)
        * [`KNN.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.get_params)
        * [`KNN.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict)
        * [`KNN.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_confidence)
        * [`KNN.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_proba)
        * [`KNN.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_with_rejection)
        * [`KNN.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.set_params)
    * [pyod.models.kpca module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kpca)
      * [`KPCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA)
        * [`KPCA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.compute_rejection_stats)
        * [`KPCA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.decision_function)
        * [`KPCA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit)
        * [`KPCA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict)
        * [`KPCA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict_score)
        * [`KPCA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.get_params)
        * [`KPCA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict)
        * [`KPCA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_confidence)
        * [`KPCA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_proba)
        * [`KPCA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_with_rejection)
        * [`KPCA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.set_params)
    * [pyod.models.lmdd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd)
      * [`LMDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD)
        * [`LMDD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.compute_rejection_stats)
        * [`LMDD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.decision_function)
        * [`LMDD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit)
        * [`LMDD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict)
        * [`LMDD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict_score)
        * [`LMDD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.get_params)
        * [`LMDD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict)
        * [`LMDD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_confidence)
        * [`LMDD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_proba)
        * [`LMDD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_with_rejection)
        * [`LMDD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.set_params)
    * [pyod.models.loda module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda)
      * [`LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA)
        * [`LODA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.compute_rejection_stats)
        * [`LODA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.decision_function)
        * [`LODA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit)
        * [`LODA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict)
        * [`LODA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict_score)
        * [`LODA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.get_params)
        * [`LODA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict)
        * [`LODA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_confidence)
        * [`LODA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_proba)
        * [`LODA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_with_rejection)
        * [`LODA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.set_params)
    * [pyod.models.lof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)
      * [`LOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF)
        * [`LOF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.compute_rejection_stats)
        * [`LOF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.decision_function)
        * [`LOF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit)
        * [`LOF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict)
        * [`LOF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict_score)
        * [`LOF.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.get_params)
        * [`LOF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict)
        * [`LOF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_confidence)
        * [`LOF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_proba)
        * [`LOF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_with_rejection)
        * [`LOF.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.set_params)
    * [pyod.models.loci module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci)
      * [`LOCI`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI)
        * [`LOCI.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.compute_rejection_stats)
        * [`LOCI.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.decision_function)
        * [`LOCI.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit)
        * [`LOCI.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict)
        * [`LOCI.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict_score)
        * [`LOCI.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.get_params)
        * [`LOCI.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict)
        * [`LOCI.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_confidence)
        * [`LOCI.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_proba)
        * [`LOCI.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_with_rejection)
        * [`LOCI.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.set_params)
    * [pyod.models.lunar module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lunar)
      * [`LUNAR`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR)
        * [`LUNAR.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.compute_rejection_stats)
        * [`LUNAR.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.decision_function)
        * [`LUNAR.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit)
        * [`LUNAR.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict)
        * [`LUNAR.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict_score)
        * [`LUNAR.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.get_params)
        * [`LUNAR.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict)
        * [`LUNAR.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_confidence)
        * [`LUNAR.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_proba)
        * [`LUNAR.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_with_rejection)
        * [`LUNAR.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.set_params)
    * [pyod.models.lscp module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lscp)
      * [`LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP)
        * [`LSCP.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.compute_rejection_stats)
        * [`LSCP.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.decision_function)
        * [`LSCP.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit)
        * [`LSCP.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict)
        * [`LSCP.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict_score)
        * [`LSCP.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.get_params)
        * [`LSCP.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict)
        * [`LSCP.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_confidence)
        * [`LSCP.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_proba)
        * [`LSCP.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_with_rejection)
        * [`LSCP.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.set_params)
    * [pyod.models.mad module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mad)
      * [`MAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD)
        * [`MAD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.compute_rejection_stats)
        * [`MAD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.decision_function)
        * [`MAD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit)
        * [`MAD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict)
        * [`MAD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict_score)
        * [`MAD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.get_params)
        * [`MAD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict)
        * [`MAD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_confidence)
        * [`MAD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_proba)
        * [`MAD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_with_rejection)
        * [`MAD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.set_params)
    * [pyod.models.mcd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd)
      * [`MCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD)
        * [`MCD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.compute_rejection_stats)
        * [`MCD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.decision_function)
        * [`MCD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit)
        * [`MCD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict)
        * [`MCD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict_score)
        * [`MCD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.get_params)
        * [`MCD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict)
        * [`MCD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_confidence)
        * [`MCD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_proba)
        * [`MCD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_with_rejection)
        * [`MCD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.set_params)
    * [pyod.models.mo_gaal module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mo_gaal)
      * [`MO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL)
        * [`MO_GAAL.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats)
        * [`MO_GAAL.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.decision_function)
        * [`MO_GAAL.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit)
        * [`MO_GAAL.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict)
        * [`MO_GAAL.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict_score)
        * [`MO_GAAL.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.get_params)
        * [`MO_GAAL.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict)
        * [`MO_GAAL.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_confidence)
        * [`MO_GAAL.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_proba)
        * [`MO_GAAL.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_with_rejection)
        * [`MO_GAAL.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.set_params)
    * [pyod.models.ocsvm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm)
      * [`OCSVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM)
        * [`OCSVM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.compute_rejection_stats)
        * [`OCSVM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.decision_function)
        * [`OCSVM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit)
        * [`OCSVM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict)
        * [`OCSVM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict_score)
        * [`OCSVM.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.get_params)
        * [`OCSVM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict)
        * [`OCSVM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_confidence)
        * [`OCSVM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_proba)
        * [`OCSVM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_with_rejection)
        * [`OCSVM.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.set_params)
    * [pyod.models.pca module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca)
      * [`PCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA)
        * [`PCA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.compute_rejection_stats)
        * [`PCA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.decision_function)
        * [`PCA.explained_variance_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.explained_variance_)
        * [`PCA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit)
        * [`PCA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict)
        * [`PCA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict_score)
        * [`PCA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.get_params)
        * [`PCA.noise_variance_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.noise_variance_)
        * [`PCA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict)
        * [`PCA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_confidence)
        * [`PCA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_proba)
        * [`PCA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_with_rejection)
        * [`PCA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.set_params)
    * [pyod.models.qmcd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.qmcd)
      * [`QMCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD)
        * [`QMCD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.compute_rejection_stats)
        * [`QMCD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.decision_function)
        * [`QMCD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit)
        * [`QMCD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict)
        * [`QMCD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict_score)
        * [`QMCD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.get_params)
        * [`QMCD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict)
        * [`QMCD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_confidence)
        * [`QMCD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_proba)
        * [`QMCD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_with_rejection)
        * [`QMCD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.set_params)
    * [pyod.models.rgraph module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rgraph)
      * [`RGraph`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph)
        * [`RGraph.active_support_elastic_net()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.active_support_elastic_net)
        * [`RGraph.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.compute_rejection_stats)
        * [`RGraph.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.decision_function)
        * [`RGraph.elastic_net_subspace_clustering()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering)
        * [`RGraph.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit)
        * [`RGraph.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict)
        * [`RGraph.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict_score)
        * [`RGraph.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.get_params)
        * [`RGraph.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict)
        * [`RGraph.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_confidence)
        * [`RGraph.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_proba)
        * [`RGraph.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_with_rejection)
        * [`RGraph.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.set_params)
    * [pyod.models.rod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod)
      * [`ROD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD)
        * [`ROD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.compute_rejection_stats)
        * [`ROD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.decision_function)
        * [`ROD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit)
        * [`ROD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict)
        * [`ROD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict_score)
        * [`ROD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.get_params)
        * [`ROD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict)
        * [`ROD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_confidence)
        * [`ROD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_proba)
        * [`ROD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_with_rejection)
        * [`ROD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.set_params)
    * [pyod.models.sampling module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sampling)
      * [`Sampling`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling)
        * [`Sampling.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.compute_rejection_stats)
        * [`Sampling.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.decision_function)
        * [`Sampling.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit)
        * [`Sampling.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict)
        * [`Sampling.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict_score)
        * [`Sampling.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.get_params)
        * [`Sampling.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict)
        * [`Sampling.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_confidence)
        * [`Sampling.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_proba)
        * [`Sampling.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_with_rejection)
        * [`Sampling.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.set_params)
    * [pyod.models.sod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)
      * [`SOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD)
        * [`SOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.compute_rejection_stats)
        * [`SOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.decision_function)
        * [`SOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit)
        * [`SOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict)
        * [`SOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict_score)
        * [`SOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.get_params)
        * [`SOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict)
        * [`SOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_confidence)
        * [`SOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_proba)
        * [`SOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_with_rejection)
        * [`SOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.set_params)
    * [pyod.models.so_gaal module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.so_gaal)
      * [`SO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL)
        * [`SO_GAAL.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.compute_rejection_stats)
        * [`SO_GAAL.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.decision_function)
        * [`SO_GAAL.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit)
        * [`SO_GAAL.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict)
        * [`SO_GAAL.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict_score)
        * [`SO_GAAL.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.get_params)
        * [`SO_GAAL.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict)
        * [`SO_GAAL.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_confidence)
        * [`SO_GAAL.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_proba)
        * [`SO_GAAL.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_with_rejection)
        * [`SO_GAAL.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.set_params)
    * [pyod.models.sos module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos)
      * [`SOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS)
        * [`SOS.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.compute_rejection_stats)
        * [`SOS.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.decision_function)
        * [`SOS.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit)
        * [`SOS.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict)
        * [`SOS.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict_score)
        * [`SOS.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.get_params)
        * [`SOS.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict)
        * [`SOS.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_confidence)
        * [`SOS.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_proba)
        * [`SOS.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_with_rejection)
        * [`SOS.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.set_params)
    * [pyod.models.suod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.suod)
      * [`SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD)
        * [`SUOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.compute_rejection_stats)
        * [`SUOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.decision_function)
        * [`SUOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit)
        * [`SUOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict)
        * [`SUOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict_score)
        * [`SUOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.get_params)
        * [`SUOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict)
        * [`SUOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_confidence)
        * [`SUOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_proba)
        * [`SUOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_with_rejection)
        * [`SUOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.set_params)
    * [pyod.models.thresholds module](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-thresholds-module)
      * [`AUCP()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.AUCP)
      * [`BOOT()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.BOOT)
      * [`CHAU()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CHAU)
      * [`CLF()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLF)
      * [`CLUST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLUST)
      * [`CPD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CPD)
      * [`DECOMP()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DECOMP)
      * [`DSN()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DSN)
      * [`EB()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.EB)
      * [`FGD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FGD)
      * [`FILTER()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FILTER)
      * [`FWFM()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FWFM)
      * [`GESD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.GESD)
      * [`HIST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.HIST)
      * [`IQR()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.IQR)
      * [`KARCH()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.KARCH)
      * [`MAD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MAD)
      * [`MCST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MCST)
      * [`META()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.META)
      * [`MOLL()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MOLL)
      * [`MTT()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MTT)
      * [`OCSVM()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.OCSVM)
      * [`QMCD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.QMCD)
      * [`REGR()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.REGR)
      * [`VAE()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.VAE)
      * [`WIND()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.WIND)
      * [`YJ()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.YJ)
      * [`ZSCORE()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.ZSCORE)
    * [pyod.models.vae module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.vae)
      * [`VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE)
        * [`VAE.build_model()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.build_model)
        * [`VAE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.compute_rejection_stats)
        * [`VAE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function)
        * [`VAE.decision_function_update()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function_update)
        * [`VAE.epoch_update()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.epoch_update)
        * [`VAE.evaluate()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluate)
        * [`VAE.evaluating_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_forward)
        * [`VAE.evaluating_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_prepare)
        * [`VAE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit)
        * [`VAE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict)
        * [`VAE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict_score)
        * [`VAE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.get_params)
        * [`VAE.load()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.load)
        * [`VAE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict)
        * [`VAE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_confidence)
        * [`VAE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_proba)
        * [`VAE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_with_rejection)
        * [`VAE.save()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.save)
        * [`VAE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.set_params)
        * [`VAE.train()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.train)
        * [`VAE.training_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_forward)
        * [`VAE.training_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_prepare)
    * [pyod.models.xgbod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.xgbod)
      * [`XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD)
        * [`XGBOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.compute_rejection_stats)
        * [`XGBOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.decision_function)
        * [`XGBOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit)
        * [`XGBOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict)
        * [`XGBOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict_score)
        * [`XGBOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.get_params)
        * [`XGBOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict)
        * [`XGBOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_confidence)
        * [`XGBOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_proba)
        * [`XGBOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_with_rejection)
        * [`XGBOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.set_params)
    * [Module contents](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models)
  * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)
    * [pyod.utils.data module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.data)
      * [`check_consistent_shape()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.check_consistent_shape)
      * [`evaluate_print()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.evaluate_print)
      * [`generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data)
      * [`generate_data_categorical()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_categorical)
      * [`generate_data_clusters()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters)
      * [`get_outliers_inliers()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.get_outliers_inliers)
    * [pyod.utils.example module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.example)
      * [`data_visualize()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.data_visualize)
      * [`visualize()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.visualize)
    * [pyod.utils.stat_models module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.stat_models)
      * [`column_ecdf()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.column_ecdf)
      * [`ecdf_terminate_equals_inplace()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.ecdf_terminate_equals_inplace)
      * [`pairwise_distances_no_broadcast()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pairwise_distances_no_broadcast)
      * [`pearsonr_mat()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pearsonr_mat)
      * [`wpearsonr()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr)
    * [pyod.utils.utility module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility)
      * [`argmaxn()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.argmaxn)
      * [`check_detector()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_detector)
      * [`check_parameter()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_parameter)
      * [`generate_bagging_indices()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_bagging_indices)
      * [`generate_indices()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_indices)
      * [`get_diff_elements()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_diff_elements)
      * [`get_intersection()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_intersection)
      * [`get_label_n()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n)
      * [`get_list_diff()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_list_diff)
      * [`get_optimal_n_bins()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_optimal_n_bins)
      * [`invert_order()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.invert_order)
      * [`precision_n_scores()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores)
      * [`score_to_label()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.score_to_label)
      * [`standardizer()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.standardizer)


###### Module contents[¬∂](https://pyod.readthedocs.io/en/latest/pyod.html#module-pyod "Link to this heading")
[ Next All Models ](https://pyod.readthedocs.io/en/latest/pyod.models.html) [ Previous API CheatSheet ](https://pyod.readthedocs.io/en/latest/api_cc.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
    * [Module contents](https://pyod.readthedocs.io/en/latest/pyod.html#module-pyod)



---

## 14. All Models - pyod 2.0.5 documentation {#14-1}

**URL:** https://pyod.readthedocs.io/en/latest/pyod.models.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:57

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/pyod.models.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/pyod.models.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/pyod.models.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### All Models[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#all-models "Link to this heading")
###### pyod.models.abod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod "Link to this heading")
Angle-based Outlier Detector (ABOD) 

_class_ pyod.models.abod.ABOD(_contamination =0.1_, _n_neighbors =5_, _method ='fast'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html#ABOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
ABOD class for Angle-base Outlier Detection. For an observation, the variance of its weighted cosine scores to all neighbors could be viewed as the outlying score. See [[BKZ+08](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1118 "Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 444‚Äì452. ACM, 2008.")] for details.
Two version of ABOD are supported:
  * Fast ABOD: use k nearest neighbors to approximate.
  * Original ABOD: consider all training points with high time complexity at O(n^3).


######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#parameters "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_neighborsint, optional (default=10) 
    
Number of neighbors to use by default for k neighbors queries. 

method: str, optional (default=‚Äôfast‚Äô)
    
Valid values for metric are:
  * ‚Äòfast‚Äô: fast ABOD. Only consider n_neighbors of training points
  * ‚Äòdefault‚Äô: original ABOD with all training points, which could be slow


######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#attributes "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1173)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1175)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1177)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id2 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#returns "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html#ABOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id3 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id4 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html#ABOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id5 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id6 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id7 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id8 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id9 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id10 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id11 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id12 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id14 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id15 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id17 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id18 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id19 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id20 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.ae1svm module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ae1svm "Link to this heading")
Using AE-1SVM with Outlier Detection (PyTorch) Source: <https://arxiv.org/pdf/1804.04888> There is another implementation of this model by Minh Nghia: <https://github.com/minh-nghia/AE-1SVM> (Tensorflow) 

_class_ pyod.models.ae1svm.AE1SVM(_hidden_neurons =None_, _hidden_activation ='relu'_, _batch_norm =True_, _learning_rate =0.001_, _epochs =50_, _batch_size =32_, _dropout_rate =0.2_, _weight_decay =1e-05_, _preprocessing =True_, _loss_fn =None_, _contamination =0.1_, _alpha =1.0_, _sigma =1.0_, _nu =0.1_, _kernel_approx_features =1000_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html#AE1SVM)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Auto Encoder with One-class SVM for anomaly detection.
Note: self.device is needed or all tensors may not be on the same device (if device w/ GPU running)
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id21 "Link to this heading") 

hidden_neuronslist, optional (default=[64, 32]) 
    
Number of neurons in each hidden layer. 

hidden_activationstr, optional (default=‚Äôrelu‚Äô) 
    
Activation function for the hidden layers. 

batch_normbool, optional (default=True) 
    
Whether to use batch normalization. 

learning_ratefloat, optional (default=1e-3) 
    
Learning rate for training the model. 

epochsint, optional (default=50) 
    
Number of training epochs. 

batch_sizeint, optional (default=32) 
    
Size of each training batch. 

dropout_ratefloat, optional (default=0.2) 
    
Dropout rate for regularization. 

weight_decayfloat, optional (default=1e-5) 
    
Weight decay (L2 penalty) for the optimizer. 

preprocessingbool, optional (default=True) 
    
Whether to apply standard scaling to the input data. 

loss_fncallable, optional (default=torch.nn.MSELoss) 
    
Loss function to use for reconstruction loss. 

contaminationfloat, optional (default=0.1) 
    
Proportion of outliers in the data. 

alphafloat, optional (default=1.0) 
    
Weight for the reconstruction loss in the final loss computation. 

sigmafloat, optional (default=1.0) 
    
Scaling factor for the random Fourier features. 

nufloat, optional (default=0.1) 
    
Parameter for the SVM loss. 

kernel_approx_featuresint, optional (default=1000) 
    
Number of random Fourier features to approximate the kernel. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id22 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id23 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html#AE1SVM.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id24 "Link to this heading") 

Xnumpy.ndarray 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id25 "Link to this heading") 

numpy.ndarray
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html#AE1SVM.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit "Link to this definition") 
    
Fit the model to the data.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id26 "Link to this heading") 

Xnumpy.ndarray 
    
Input data. 

yNone 
    
Ignored, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id27 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id28 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id29 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id30 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id31 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id32 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id33 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id35 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id36 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id38 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id39 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id40 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id41 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.alad module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.alad "Link to this heading")
Using Adversarially Learned Anomaly Detection 

_class_ pyod.models.alad.ALAD(_activation_hidden_gen ='tanh'_, _activation_hidden_disc ='tanh'_, _output_activation =None_, _dropout_rate =0.2_, _latent_dim =2_, _dec_layers =[5, 10, 25]_, _enc_layers =[25, 10, 5]_, _disc_xx_layers =[25, 10, 5]_, _disc_zz_layers =[25, 10, 5]_, _disc_xz_layers =[25, 10, 5]_, _learning_rate_gen =0.0001_, _learning_rate_disc =0.0001_, _add_recon_loss =False_, _lambda_recon_loss =0.1_, _epochs =200_, _verbose =0_, _preprocessing =False_, _add_disc_zz_loss =True_, _spectral_normalization =False_, _batch_size =32_, _contamination =0.1_, _device =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html#ALAD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Adversarially Learned Anomaly Detection (ALAD). Paper: <https://arxiv.org/pdf/1812.02288.pdf>
See [[BZRF+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1163 "Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining \(ICDM\), 727‚Äì736. IEEE, 2018.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id43 "Link to this heading") 

output_activationstr, optional (default=None) 
    
Activation function to use for output layers for encoder and dector. 

activation_hidden_discstr, optional (default=‚Äôtanh‚Äô) 
    
Activation function to use for hidden layers in discrimators. 

activation_hidden_genstr, optional (default=‚Äôtanh‚Äô) 
    
Activation function to use for hidden layers in encoder and decoder (i.e. generator). 

epochsint, optional (default=500) 
    
Number of epochs to train the model. 

batch_sizeint, optional (default=32) 
    
Number of samples per gradient update. 

dropout_ratefloat in (0., 1), optional (default=0.2) 
    
The dropout to be used across all layers. 

dec_layerslist, optional (default=[5,10,25]) 
    
List that indicates the number of nodes per hidden layer for the d ecoder network. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

enc_layerslist, optional (default=[25,10,5]) 
    
List that indicates the number of nodes per hidden layer for the encoder network. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

disc_xx_layerslist, optional (default=[25,10,5]) 
    
List that indicates the number of nodes per hidden layer for discriminator_xx. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

disc_zz_layerslist, optional (default=[25,10,5]) 
    
List that indicates the number of nodes per hidden layer for discriminator_zz. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

disc_xz_layerslist, optional (default=[25,10,5]) 
    
List that indicates the number of nodes per hidden layer for discriminator_xz. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

learning_rate_gen: float in (0., 1), optional (default=0.001)
    
learning rate of training the encoder and decoder 

learning_rate_disc: float in (0., 1), optional (default=0.001)
    
learning rate of training the discriminators 

add_recon_loss: bool optional (default=False)
    
add an extra loss for encoder and decoder based on the reconstruction error 

lambda_recon_loss: float in (0., 1), optional (default=0.1)
    
if `add_recon_loss= True`, the reconstruction loss gets multiplied by `lambda_recon_loss` and added to the total loss for the generator
> (i.e. encoder and decoder). 

preprocessingbool, optional (default=True) 
    
If True, apply standardization on the data. 

verboseint, optional (default=1) 
    
Verbosity mode. - 0 = silent - 1 = progress bar 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. 

devicestr or None, optional (default=None) 
    
The device to use for computation. If None, the default device will be used. Possible values include ‚Äòcpu‚Äô or ‚Äògpu‚Äô. This parameter allows the user to specify the preferred device for running the model.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id44 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1179)numpy array of shape (n_samples,) 
    
The outlier scores of the training data [0,1]. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1181)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1183)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id45 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id46 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html#ALAD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id47 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_, _noise_std =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html#ALAD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id48 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id49 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id50 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id51 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id52 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

plot_learning_curves(_start_ind =0_, _window_smoothening =10_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html#ALAD.plot_learning_curves)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.plot_learning_curves "Link to this definition") 


predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id53 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id54 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id56 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id57 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id59 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id60 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id61 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id62 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.anogan module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.anogan "Link to this heading")
Anomaly Detection with Generative Adversarial Networks (AnoGAN) Paper: <https://arxiv.org/pdf/1703.05921.pdf> Note, that this is another implementation of AnoGAN as the one from <https://github.com/fuchami/ANOGAN> 

_class_ pyod.models.anogan.AnoGAN(_activation_hidden ='tanh'_, _dropout_rate =0.2_, _latent_dim_G =2_, _G_layers =[20, 10, 3, 10, 20]_, _verbose =0_, _D_layers =[20, 10, 5]_, _index_D_layer_for_recon_error =1_, _epochs =500_, _preprocessing =False_, _learning_rate =0.001_, _learning_rate_query =0.01_, _epochs_query =20_, _batch_size =32_, _output_activation =None_, _contamination =0.1_, _device =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html#AnoGAN)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Anomaly Detection with Generative Adversarial Networks (AnoGAN). See the original paper ‚ÄúUnsupervised anomaly detection with generative adversarial networks to guide marker discovery‚Äù.
See [[BSSeebockW+17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1159 "Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, 146‚Äì157. Springer, 2017.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id64 "Link to this heading") 

output_activationstr, optional (default=None) 
    
Activation function to use for output layer. 

activation_hiddenstr, optional (default=‚Äôtanh‚Äô) 
    
Activation function to use for output layer. 

epochsint, optional (default=500) 
    
Number of epochs to train the model. 

batch_sizeint, optional (default=32) 
    
Number of samples per gradient update. 

dropout_ratefloat in (0., 1), optional (default=0.2) 
    
The dropout to be used across all layers. 

G_layerslist, optional (default=[20,10,3,10,20]) 
    
List that indicates the number of nodes per hidden layer for the generator. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

D_layerslist, optional (default=[20,10,5]) 
    
List that indicates the number of nodes per hidden layer for the discriminator. Thus, [10,10] indicates 2 hidden layers having each 10 nodes. 

learning_rate: float in (0., 1), optional (default=0.001)
    
learning rate of training the network 

index_D_layer_for_recon_error: int, optional (default = 1)
    
This is the index of the hidden layer in the discriminator for which the reconstruction error will be determined between query sample and the sample created from the latent space. 

learning_rate_query: float in (0., 1), optional (default=0.001)
    
learning rate for the backpropagation steps needed to find a point in the latent space of the generator that approximate the query sample 

epochs_query: int, optional (default=20) 
    
Number of epochs to approximate the query sample in the latent space of the generator 

preprocessingbool, optional (default=True) 
    
If True, apply standardization on the data. 

verboseint, optional (default=1) 
    
Verbosity mode. - 0 = silent - 1 = progress bar 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id65 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1185)numpy array of shape (n_samples,) 
    
The outlier scores of the training data [0,1]. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1187)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1189)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id66 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id67 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html#AnoGAN.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id68 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id69 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html#AnoGAN.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id70 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id71 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id72 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id73 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id74 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id75 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id76 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id77 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

plot_learning_curves(_start_ind =0_, _window_smoothening =10_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html#AnoGAN.plot_learning_curves)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.plot_learning_curves "Link to this definition") 


predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id78 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id79 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id81 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id82 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id84 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id85 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id86 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id87 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id88 "Link to this heading")
self : object
###### pyod.models.auto_encoder module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.auto_encoder "Link to this heading")
Using AutoEncoder with Outlier Detection 

_class_ pyod.models.auto_encoder.AutoEncoder(_contamination =0.1_, _preprocessing =True_, _lr =0.001_, _epoch_num =10_, _batch_size =32_, _optimizer_name ='adam'_, _device =None_, _random_state =42_, _use_compile =False_, _compile_mode ='default'_, _verbose =1_, _optimizer_params :[dict](https://docs.python.org/3/library/stdtypes.html#dict "\(in Python v3.13\)")={'weight_decay': 1e-05}_, _hidden_neuron_list =[64, 32]_, _hidden_activation_name ='relu'_, _batch_norm =True_, _dropout_rate =0.2_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html#AutoEncoder)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder "Link to this definition") 
    
Bases: `BaseDeepLearningDetector`
Auto Encoder (AE) is a type of neural networks for learning useful data representations in an unsupervised manner. Similar to PCA, AE could be used to detect outlying objects in the data by calculating the reconstruction errors. See [[BAgg15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1122 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] Chapter 3 for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id90 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

preprocessingbool, optional (default=True) 
    
If True, apply the preprocessing procedure before training models. 

lrfloat, optional (default=1e-3) 
    
The initial learning rate for the optimizer. 

epoch_numint, optional (default=10) 
    
The number of epochs for training. 

batch_sizeint, optional (default=32) 
    
The batch size for training. 

optimizer_namestr, optional (default=‚Äôadam‚Äô) 
    
The name of theoptimizer used to train the model. 

devicestr, optional (default=None) 
    
The device to use for the model. If None, it will be decided automatically. If you want to use MPS, set it to ‚Äòmps‚Äô. 

random_stateint, optional (default=42) 
    
The random seed for reproducibility. 

use_compilebool, optional (default=False) 
    
Whether to compile the model. If True, the model will be compiled before training. This is only available for PyTorch version >= 2.0.0. and Python < 3.12. 

compile_modestr, optional (default=‚Äôdefault‚Äô) 
    
The mode to compile the model. Can be either ‚Äúdefault‚Äù, ‚Äúreduce-overhead‚Äù, ‚Äúmax-autotune‚Äù or ‚Äúmax-autotune-no-cudagraphs‚Äù. See <https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile> for details. 

verboseint, optional (default=1) 
    
Verbosity mode. - 0 = silent - 1 = progress bar - 2 = one line per epoch. 

optimizer_paramsdict, optional (default={‚Äòweight_decay‚Äô: 1e-5}) 
    
Additional parameters for the optimizer. For example, optimizer_params={‚Äòweight_decay‚Äô: 1e-5}. 

hidden_neuron_listlist, optional (default=[64, 32]) 
    
The number of neurons per hidden layers. So the network has the structure as [feature_size, 64, 32, 32, 64, feature_size]. 

hidden_activation_namestr, optional (default=‚Äôrelu‚Äô) 
    
The activation function used in hidden layers. 

batch_normboolean, optional (default=True) 
    
Whether to apply Batch Normalization, See <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> 

dropout_ratefloat in (0., 1), optional (default=0.2) 
    
The dropout to be used across all layers.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id91 "Link to this heading") 

modeltorch.nn.Module 
    
The underlying AutoEncoder model. 

optimizertorch.optim 
    
The optimizer used to train the model. 

criteriontorch.nn.modules 
    
The loss function used to train the model. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1191)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1193)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1195)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

build_model()[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html#AutoEncoder.build_model)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.build_model "Link to this definition") 
    
Need to define model in this method. self.feature_size is the number of features in the input data. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id92 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id93 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_ , _batch_size =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. 

batch_sizeint, optional (default=None) 
    
The batch size for processing the input samples. If not specified, the default batch size is used.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id94 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

evaluate(_data_loader_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.evaluate "Link to this definition") 
    
Evaluate the deep learning model.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id95 "Link to this heading") 

data_loadertorch.utils.data.DataLoader 
    
The data loader for evaluating the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id96 "Link to this heading") 

outlier_scoresnumpy array of shape (n_samples,) 
    
The outlier scores of the input samples. 

fit(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id97 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

ynumpy array of shape (n_samples,), optional (default=None) 
    
The ground truth of input samples. Not used in unsupervised methods. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id98 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id99 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id100 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id101 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id102 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id103 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id105 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id106 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id108 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id109 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id110 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id111 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

save(_path_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.save "Link to this definition") 
    
Save the model to the specified path.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id112 "Link to this heading") 

pathstr 
    
The path to save the model. 

train(_train_loader_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.train "Link to this definition") 
    
Train the deep learning model.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id113 "Link to this heading") 

train_loadertorch.utils.data.DataLoader 
    
The data loader for training the model. 

training_forward(_batch_data_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html#AutoEncoder.training_forward)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_forward "Link to this definition") 
    
Forward pass for training the model. Abstract method to be implemented.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id114 "Link to this heading") 

batch_datatuple 
    
The batch data for training the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id115 "Link to this heading") 

lossfloat or tuple of float 
    
The loss.item of the model, or a tuple of loss.item if there are multiple losses. 

training_prepare()[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_prepare "Link to this definition") 

###### pyod.models.auto_encoder_torch module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-auto-encoder-torch-module "Link to this heading")
###### pyod.models.cblof module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof "Link to this heading")
Clustering Based Local Outlier Factor (CBLOF) 

_class_ pyod.models.cblof.CBLOF(_n_clusters =8_, _contamination =0.1_, _clustering_estimator =None_, _alpha =0.9_, _beta =5_, _use_weights =False_, _check_estimator =False_, _random_state =None_, _n_jobs =1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html#CBLOF)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
The CBLOF operator calculates the outlier score based on cluster-based local outlier factor.
CBLOF takes as an input the data set and the cluster model that was generated by a clustering algorithm. It classifies the clusters into small clusters and large clusters using the parameters alpha and beta. The anomaly score is then calculated based on the size of the cluster the point belongs to as well as the distance to the nearest large cluster.
Use weighting for outlier factor based on the sizes of the clusters as proposed in the original publication. Since this might lead to unexpected behavior (outliers close to small clusters are not found), it is disabled by default.Outliers scores are solely computed based on their distance to the closest large cluster center.
By default, kMeans is used for clustering algorithm instead of Squeezer algorithm mentioned in the original paper for multiple reasons.
See [[BHXD03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1127 "Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. Pattern Recognition Letters, 24\(9-10\):1641‚Äì1650, 2003.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id117 "Link to this heading") 

n_clustersint, optional (default=8) 
    
The number of clusters to form as well as the number of centroids to generate. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

clustering_estimatorEstimator, optional (default=None) 
    
The base clustering algorithm for performing data clustering. A valid clustering algorithm should be passed in. The estimator should have standard sklearn APIs, fit() and predict(). The estimator should have attributes `labels_` and `cluster_centers_`. If `cluster_centers_` is not in the attributes once the model is fit, it is calculated as the mean of the samples in a cluster.
If not set, CBLOF uses KMeans for scalability. See <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html> 

alphafloat in (0.5, 1), optional (default=0.9) 
    
Coefficient for deciding small and large clusters. The ratio of the number of samples in large clusters to the number of samples in small clusters. 

betaint or float in (1,), optional (default=5). 
    
Coefficient for deciding small and large clusters. For a list sorted clusters by size |C1|, |C2|, ‚Ä¶, |Cn|, beta = |Ck|/|Ck-1| 

use_weightsbool, optional (default=False) 
    
If set to True, the size of clusters are used as weights in outlier score calculation. 

check_estimatorbool, optional (default=False) 
    
If set to True, check whether the base estimator is consistent with sklearn standard.
Warning
check_estimator may throw errors with scikit-learn 0.20 above. 

random_stateint, RandomState or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id118 "Link to this heading") 

[clustering_estimator_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1197)Estimator, sklearn instance 
    
Base estimator for clustering. 

[cluster_labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1199)list of shape (n_samples,) 
    
Cluster assignment for the training samples. 

[n_clusters_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1201)int 
    
Actual number of clusters (possibly different from n_clusters). 

[cluster_sizes_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1203)list of shape ([n_clusters_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1205),) 
    
The size of each cluster once fitted with the training data. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1207)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[cluster_centers_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1209)numpy array of shape ([n_clusters_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1211), n_features) 
    
The center of each cluster. 

[small_cluster_labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1213)list of clusters numbers 
    
The cluster assignments belonging to small clusters. 

[large_cluster_labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1215)list of clusters numbers 
    
The cluster assignments belonging to large clusters. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1217)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1219)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id119 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id120 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html#CBLOF.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id121 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id122 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html#CBLOF.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id123 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id124 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id125 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id126 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id127 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id128 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id129 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id130 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id132 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id133 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id135 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id136 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id137 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id138 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.cof module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof "Link to this heading")
Connectivity-Based Outlier Factor (COF) Algorithm 

_class_ pyod.models.cof.COF(_contamination =0.1_, _n_neighbors =20_, _method ='fast'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html#COF)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Connectivity-Based Outlier Factor (COF) COF uses the ratio of average chaining distance of data point and the average of average chaining distance of k nearest neighbor of the data point, as the outlier score for observations.
See [[BTCFC02](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1137 "Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 535‚Äì548. Springer, 2002.")] for details.
Two version of COF are supported:
  * Fast COF: computes the entire pairwise distance matrix at the cost of a O(n^2) memory requirement.
  * Memory efficient COF: calculates pairwise distances incrementally. Use this implementation when it is not feasible to fit the n-by-n distance in memory. This leads to a linear overhead because many distances will have to be recalculated.


######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id140 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_neighborsint, optional (default=20) 
    
Number of neighbors to use by default for k neighbors queries. Note that n_neighbors should be less than the number of samples. If n_neighbors is larger than the number of samples provided, all samples will be used. 

methodstring, optional (default=‚Äôfast‚Äô) 
    
Valid values for method are:
  * ‚Äòfast‚Äô Fast COF, computes the full pairwise distance matrix up front.
  * ‚Äòmemory‚Äô Memory-efficient COF, computes pairwise distances only when needed at the cost of computational speed.


######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id141 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1221)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1223)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1225)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

[n_neighbors_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1227): int
    
Number of neighbors to use by default for k neighbors queries. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id142 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id143 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html#COF.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id144 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id145 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html#COF.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id146 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id147 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id148 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id149 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id150 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id151 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id152 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id153 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id155 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id156 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id158 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id159 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id160 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id161 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.combination module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.combination "Link to this heading")
A collection of model combination functionalities. 

pyod.models.combination.aom(_scores_ , _n_buckets =5_, _method ='static'_, _bootstrap_estimators =False_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#aom)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom "Link to this definition") 
    
Average of Maximum - An ensemble method for combining multiple estimators. See [[BAS15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1115 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")] for details.
First dividing estimators into subgroups, take the maximum score as the subgroup score. Finally, take the average of all subgroup outlier scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id163 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
The score matrix outputted from various estimators 

n_bucketsint, optional (default=5) 
    
The number of subgroups to build 

methodstr, optional (default=‚Äôstatic‚Äô) 
    
{‚Äòstatic‚Äô, ‚Äòdynamic‚Äô}, if ‚Äòdynamic‚Äô, build subgroups randomly with dynamic bucket size. 

bootstrap_estimatorsbool, optional (default=False) 
    
Whether estimators are drawn with replacement. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id164 "Link to this heading") 

combined_scoresNumpy array of shape (n_samples,) 
    
The combined outlier scores. 

pyod.models.combination.average(_scores_ , _estimator_weights =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#average)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average "Link to this definition") 
    
Combination method to merge the outlier scores from multiple estimators by taking the average.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id165 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
Score matrix from multiple estimators on the same samples. 

estimator_weightslist of shape (1, n_estimators) 
    
If specified, using weighted average
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id166 "Link to this heading") 

combined_scoresnumpy array of shape (n_samples, ) 
    
The combined outlier scores. 

pyod.models.combination.majority_vote(_scores_ , _weights =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#majority_vote)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote "Link to this definition") 
    
Combination method to merge the scores from multiple estimators by majority vote.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id167 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
Score matrix from multiple estimators on the same samples. 

weightsnumpy array of shape (1, n_estimators) 
    
If specified, using weighted majority weight.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id168 "Link to this heading") 

combined_scoresnumpy array of shape (n_samples, ) 
    
The combined scores. 

pyod.models.combination.maximization(_scores_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#maximization)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization "Link to this definition") 
    
Combination method to merge the outlier scores from multiple estimators by taking the maximum.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id169 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
Score matrix from multiple estimators on the same samples.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id170 "Link to this heading") 

combined_scoresnumpy array of shape (n_samples, ) 
    
The combined outlier scores. 

pyod.models.combination.median(_scores_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#median)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median "Link to this definition") 
    
Combination method to merge the scores from multiple estimators by taking the median.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id171 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
Score matrix from multiple estimators on the same samples.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id172 "Link to this heading") 

combined_scoresnumpy array of shape (n_samples, ) 
    
The combined scores. 

pyod.models.combination.moa(_scores_ , _n_buckets =5_, _method ='static'_, _bootstrap_estimators =False_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#moa)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa "Link to this definition") 
    
Maximization of Average - An ensemble method for combining multiple estimators. See [[BAS15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1115 "Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. ACM SIGKDD Explorations Newsletter, 17\(1\):24‚Äì47, 2015.")] for details.
First dividing estimators into subgroups, take the average score as the subgroup score. Finally, take the maximization of all subgroup outlier scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id174 "Link to this heading") 

scoresnumpy array of shape (n_samples, n_estimators) 
    
The score matrix outputted from various estimators 

n_bucketsint, optional (default=5) 
    
The number of subgroups to build 

methodstr, optional (default=‚Äôstatic‚Äô) 
    
{‚Äòstatic‚Äô, ‚Äòdynamic‚Äô}, if ‚Äòdynamic‚Äô, build subgroups randomly with dynamic bucket size. 

bootstrap_estimatorsbool, optional (default=False) 
    
Whether estimators are drawn with replacement. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id175 "Link to this heading") 

combined_scoresNumpy array of shape (n_samples,) 
    
The combined outlier scores.
###### pyod.models.cd module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd "Link to this heading")
Cook‚Äôs distance outlier detection (CD) 

_class_ pyod.models.cd.CD(_contamination =0.1_, _model =LinearRegression()_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html#CD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector") 

Cook‚Äôs distance can be used to identify points that negatively
    
affect a regression model. A combination of each observation‚Äôs leverage and residual values are used in the measurement. Higher leverage and residuals relate to higher Cook‚Äôs distances. Note that this method is unsupervised and requires at least two features for X with which to calculate the mean Cook‚Äôs distance for each datapoint. Read more in the [[BCoo77](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1155 "R Dennis Cook. Detection of influential observation in linear regression. Technometrics, 19\(1\):15‚Äì18, 1977.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id177 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

modelobject, optional (default=LinearRegression()) 
    
Regression model used to calculate the Cook‚Äôs distance
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id178 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1229)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1231)float 
    
The modified z-score to use as a threshold. Observations with a modified z-score (based on the median absolute deviation) greater than this value will be classified as outliers. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1233)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id179 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id180 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html#CD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id181 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id182 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html#CD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit "Link to this definition") 
    
‚ÄúFit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id183 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id184 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id185 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id186 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id187 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id188 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id189 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id190 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id192 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id193 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id195 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id196 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id197 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id198 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.copod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod "Link to this heading")
Copula Based Outlier Detector (COPOD) 

_class_ pyod.models.copod.COPOD(_contamination =0.1_, _n_jobs =1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#COPOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
COPOD class for Copula Based Outlier Detector. COPOD is a parameter-free, highly interpretable outlier detection algorithm based on empirical copula models. See [[BLZB+20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1148 "Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In IEEE International Conference on Data Mining \(ICDM\). IEEE, 2020.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id200 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_jobsoptional (default=1) 
    
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id201 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1235)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1237)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1239)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id202 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id203 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#COPOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.decision_function "Link to this definition") 
     

Predict raw anomaly score of X using the fitted detector.
    
For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id204 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id205 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

explain_outlier(_ind_ , _columns =None_, _cutoffs =None_, _feature_names =None_, _file_name =None_, _file_type =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#COPOD.explain_outlier)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.explain_outlier "Link to this definition") 
    
Plot dimensional outlier graph for a given data point within the dataset.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id206 "Link to this heading") 

indint 
    
The index of the data point one wishes to obtain a dimensional outlier graph for. 

columnslist 
    
Specify a list of features/dimensions for plotting. If not specified, use all features. 

cutoffslist of floats in (0., 1), optional (default=[0.95, 0.99]) 
    
The significance cutoff bands of the dimensional outlier graph. 

feature_nameslist of strings 
    
The display names of all columns of the dataset, to show on the x-axis of the plot. 

file_namestring 
    
The name to save the figure 

file_typestring 
    
The file type to save the figure
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id207 "Link to this heading") 

Plotmatplotlib plot 
    
The dimensional outlier graph for data point with index ind. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#COPOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id208 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id209 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id210 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id211 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id212 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id213 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id214 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id215 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id216 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id218 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id219 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id221 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id222 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id223 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id224 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id225 "Link to this heading")
self : object 

pyod.models.copod.skew(_X_ , _axis =0_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#skew)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.skew "Link to this definition") 

###### pyod.models.deep_svdd module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.deep_svdd "Link to this heading")
Deep One-Class Classification for outlier detection 

_class_ pyod.models.deep_svdd.DeepSVDD(_n_features_ , _c =None_, _use_ae =False_, _hidden_neurons =None_, _hidden_activation ='relu'_, _output_activation ='sigmoid'_, _optimizer ='adam'_, _epochs =100_, _batch_size =32_, _dropout_rate =0.2_, _l2_regularizer =0.1_, _validation_size =0.1_, _preprocessing =True_, _verbose =1_, _random_state =None_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html#DeepSVDD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Deep One-Class Classifier with AutoEncoder (AE) is a type of neural networks for learning useful data representations in an unsupervised way. DeepSVDD trains a neural network while minimizing the volume of a hypersphere that encloses the network representations of the data, forcing the network to extract the common factors of variation. Similar to PCA, DeepSVDD could be used to detect outlying objects in the data by calculating the distance from center See [[BRVG+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1151 "Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. International conference on machine learning, 2018.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id227 "Link to this heading") 

n_features: int, 
    
Number of features in the input data. 

c: float, optional (default=‚Äôforwad_nn_pass‚Äô)
    
Deep SVDD center, the default will be calculated based on network initialization first forward pass. To get repeated results set random_state if c is set to None. 

use_ae: bool, optional (default=False)
    
The AutoEncoder type of DeepSVDD it reverse neurons from hidden_neurons if set to True. 

hidden_neuronslist, optional (default=[64, 32]) 
    
The number of neurons per hidden layers. if use_ae is True, neurons will be reversed eg. [64, 32] -> [64, 32, 32, 64, n_features] 

hidden_activationstr, optional (default=‚Äôrelu‚Äô) 
    
Activation function to use for hidden layers. All hidden layers are forced to use the same type of activation. See <https://keras.io/activations/> 

output_activationstr, optional (default=‚Äôsigmoid‚Äô) 
    
Activation function to use for output layer. See <https://keras.io/activations/> 

optimizerstr, optional (default=‚Äôadam‚Äô) 
    
String (name of optimizer) or optimizer instance. See <https://keras.io/optimizers/> 

epochsint, optional (default=100) 
    
Number of epochs to train the model. 

batch_sizeint, optional (default=32) 
    
Number of samples per gradient update. 

dropout_ratefloat in (0., 1), optional (default=0.2) 
    
The dropout to be used across all layers. 

l2_regularizerfloat in (0., 1), optional (default=0.1) 
    
The regularization strength of activity_regularizer applied on each layer. By default, l2 regularizer is used. See <https://keras.io/regularizers/> 

validation_sizefloat in (0., 1), optional (default=0.1) 
    
The percentage of data to be used for validation. 

preprocessingbool, optional (default=True) 
    
If True, apply standardization on the data. 

random_staterandom_state: int, RandomState instance or None, optional 
    
(default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id228 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1241)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1243)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1245)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id229 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id230 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html#DeepSVDD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id231 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id232 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html#DeepSVDD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id233 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id234 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id235 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id236 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id237 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id238 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id239 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id240 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id241 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id242 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id244 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id245 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id247 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id248 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id249 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id250 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id251 "Link to this heading")
self : object
###### pyod.models.devnet module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.devnet "Link to this heading")
Deep anomaly detection with deviation networks Part of the codes are adapted from <https://github.com/GuansongPang/deviation-network> 

_class_ pyod.models.devnet.DevNet(_network_depth =2_, _batch_size =512_, _epochs =50_, _nb_batch =20_, _known_outliers =30_, _cont_rate =0.02_, _data_format =0_, _random_seed =42_, _device =None_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html#DevNet)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector") 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id252 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id253 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html#DevNet.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.decision_function "Link to this definition") 
    
Predict raw anomaly scores of X using the fitted detector.
The anomaly score of an input sample is computed based on the fitted detector. For consistency, outliers are assigned with higher anomaly scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id254 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. Sparse matrices are accepted only if they are supported by the base estimator.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id255 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html#DevNet.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id256 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id257 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id258 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id259 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html#DevNet.fit_predict_score)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict_score "Link to this definition") 
    
Fit the detector with labels, predict on samples, and evaluate the model by predefined metrics.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id260 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

ynumpy array of shape (n_samples,) 
    
The labels or target values corresponding to X. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric: - ‚Äòroc_auc_score‚Äô: ROC score - ‚Äòprc_n_score‚Äô: Precision @ rank n score
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id261 "Link to this heading")
score : float 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id262 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id263 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id264 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id265 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id267 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id268 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id270 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id271 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id272 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id273 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id274 "Link to this heading")
self : object
###### pyod.models.dif module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.dif "Link to this heading")
Deep Isolation Forest for Anomaly Detection (DIF) 

_class_ pyod.models.dif.DIF(_batch_size =1000_, _representation_dim =20_, _hidden_neurons =None_, _hidden_activation ='tanh'_, _skip_connection =False_, _n_ensemble =50_, _n_estimators =6_, _max_samples =256_, _contamination =0.1_, _random_state =None_, _device =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html#DIF)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Deep Isolation Forest (DIF) is an extension of iForest. It uses deep representation ensemble to achieve non-linear isolation on original data space. See [[BXPWW23](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1166 "Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, \(\):1-14, 2023. doi:10.1109/TKDE.2023.3270293.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id276 "Link to this heading") 

batch_sizeint, optional (default=1000) 
    
Number of samples per gradient update. 

representation_dim, int, optional (default=20)
    
Dimensionality of the representation space. 

hidden_neurons, list, optional (default=[64, 32])
    
The number of neurons per hidden layers. So the network has the structure as [n_features, hidden_neurons[0], hidden_neurons[1], ‚Ä¶, representation_dim] 

hidden_activation, str, optional (default=‚Äôtanh‚Äô)
    
Activation function to use for hidden layers. All hidden layers are forced to use the same type of activation. See <https://pytorch.org/docs/stable/nn.html> for details. Currently only ‚Äòrelu‚Äô: nn.ReLU() ‚Äòsigmoid‚Äô: nn.Sigmoid() ‚Äòtanh‚Äô: nn.Tanh() are supported. See pyod/utils/torch_utility.py for details. 

skip_connection, boolean, optional (default=False)
    
If True, apply skip-connection in the neural network structure. 

n_ensemble, int, optional (default=50)
    
The number of deep representation ensemble members. 

n_estimators, int, optional (default=6)
    
The number of isolation forest of each representation. 

max_samples, int, optional (default=256)
    
The number of samples to draw from X to train each base isolation tree. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

random_stateint or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

device, ‚Äòcuda‚Äô, ‚Äòcpu‚Äô, or None, optional (default=None)
    
if ‚Äòcuda‚Äô, use GPU acceleration in torch if ‚Äòcpu‚Äô, use cpu in torch if None, automatically determine whether GPU is available
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id277 "Link to this heading") 

net_lstlist of torch.Module 
    
The list of representation neural networks. 

iForest_lstlist of iForest 
    
The list of instantiated iForest model. 

x_reduced_lst: list of numpy array
    
The list of training data representations 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1247)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1249)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1251)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id278 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id279 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html#DIF.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id280 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id281 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html#DIF.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id282 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id283 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id284 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id285 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id286 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id287 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id288 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id289 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id290 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id291 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id293 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id294 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id296 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id297 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id298 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id299 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id300 "Link to this heading")
self : object
###### pyod.models.ecod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod "Link to this heading")
Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions (ECOD) 

_class_ pyod.models.ecod.ECOD(_contamination =0.1_, _n_jobs =1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#ECOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
ECOD class for Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions (ECOD) ECOD is a parameter-free, highly interpretable outlier detection algorithm based on empirical CDF functions. See [[BLZH+22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1154 "Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 2022.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id302 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_jobsoptional (default=1) 
    
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id303 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1253)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1255)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1257)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id304 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id305 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#ECOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.decision_function "Link to this definition") 
     

Predict raw anomaly score of X using the fitted detector.
    
For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id306 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id307 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

explain_outlier(_ind_ , _columns =None_, _cutoffs =None_, _feature_names =None_, _file_name =None_, _file_type =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#ECOD.explain_outlier)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.explain_outlier "Link to this definition") 
    
Plot dimensional outlier graph for a given data point within the dataset.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id308 "Link to this heading") 

indint 
    
The index of the data point one wishes to obtain a dimensional outlier graph for. 

columnslist 
    
Specify a list of features/dimensions for plotting. If not specified, use all features. 

cutoffslist of floats in (0., 1), optional (default=[0.95, 0.99]) 
    
The significance cutoff bands of the dimensional outlier graph. 

feature_nameslist of strings 
    
The display names of all columns of the dataset, to show on the x-axis of the plot. 

file_namestring 
    
The name to save the figure 

file_typestring 
    
The file type to save the figure
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id309 "Link to this heading") 

Plotmatplotlib plot 
    
The dimensional outlier graph for data point with index ind. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#ECOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id310 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id311 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id312 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id313 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id314 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id315 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id316 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id317 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id318 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id320 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id321 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id323 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id324 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id325 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id326 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id327 "Link to this heading")
self : object 

pyod.models.ecod.skew(_X_ , _axis =0_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#skew)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.skew "Link to this definition") 

###### pyod.models.feature_bagging module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.feature_bagging "Link to this heading")
Feature bagging detector 

_class_ pyod.models.feature_bagging.FeatureBagging(_base_estimator =None_, _n_estimators =10_, _contamination =0.1_, _max_features =1.0_, _bootstrap_features =False_, _check_detector =True_, _check_estimator =False_, _n_jobs =1_, _random_state =None_, _combination ='average'_, _verbose =0_, _estimator_params =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html#FeatureBagging)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
A feature bagging detector is a meta estimator that fits a number of base detectors on various sub-samples of the dataset and use averaging or other combination methods to improve the predictive accuracy and control over-fitting.
The sub-sample size is always the same as the original input sample size but the features are randomly sampled from half of the features to all features.
By default, LOF is used as the base estimator. However, any estimator could be used as the base estimator, such as kNN and ABOD.
Feature bagging first construct n subsamples by random selecting a subset of features, which induces the diversity of base estimators.
Finally, the prediction score is generated by averaging/taking the maximum of all base detectors. See [[BLK05](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1119 "Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 157‚Äì166. ACM, 2005.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id329 "Link to this heading") 

base_estimatorobject or None, optional (default=None) 
    
The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a LOF detector. 

n_estimatorsint, optional (default=10) 
    
The number of base estimators in the ensemble. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

max_featuresint or float, optional (default=1.0) 
    
The number of features to draw from X to train each base estimator.
  * If int, then draw max_features features.
  * If float, then draw max_features * X.shape[1] features.



bootstrap_featuresbool, optional (default=False) 
    
Whether features are drawn with replacement. 

check_detectorbool, optional (default=True) 
    
If set to True, check whether the base estimator is consistent with pyod standard. 

check_estimatorbool, optional (default=False) 
    
If set to True, check whether the base estimator is consistent with sklearn standard.
Deprecated since version 0.6.9: check_estimator will be removed in pyod 0.8.0.; it will be replaced by check_detector. 

n_jobsoptional (default=1) 
    
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores. 

random_stateint, RandomState or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

combinationstr, optional (default=‚Äôaverage‚Äô) 
    
The method of combination:
  * if ‚Äòaverage‚Äô: take the average of all detectors
  * if ‚Äòmax‚Äô: take the maximum scores of all detectors



verboseint, optional (default=0) 
    
Controls the verbosity of the building process. 

estimator_paramsdict, optional (default=None) 
    
The list of attributes to use as parameters when instantiating a new base estimator. If none are given, default parameters are used.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id330 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1259)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1261)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1263)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id331 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id332 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html#FeatureBagging.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id333 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id334 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html#FeatureBagging.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id335 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id336 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id337 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id338 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id339 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id340 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id341 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id342 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id343 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id344 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id346 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id347 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id349 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id350 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id351 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id352 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id353 "Link to this heading")
self : object
###### pyod.models.gmm module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm "Link to this heading")
Outlier detection based on Gaussian Mixture Model (GMM). 

_class_ pyod.models.gmm.GMM(_n_components =1_, _covariance_type ='full'_, _tol =0.001_, _reg_covar =1e-06_, _max_iter =100_, _n_init =1_, _init_params ='kmeans'_, _weights_init =None_, _means_init =None_, _precisions_init =None_, _random_state =None_, _warm_start =False_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html#GMM)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Wrapper of scikit-learn Gaussian Mixture Model with more functionalities. Unsupervised Outlier Detection.
See [[BAgg15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1122 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015.")] Chapter 2 for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id355 "Link to this heading") 

n_componentsint, default=1 
    
The number of mixture components. 

covariance_type{‚Äòfull‚Äô, ‚Äòtied‚Äô, ‚Äòdiag‚Äô, ‚Äòspherical‚Äô}, default=‚Äôfull‚Äô 
    
String describing the type of covariance parameters to use. 

tolfloat, default=1e-3 
    
The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold. 

reg_covarfloat, default=1e-6 
    
Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive. 

max_iterint, default=100 
    
The number of EM iterations to perform. 

n_initint, default=1 
    
The number of initializations to perform. The best results are kept. 

init_params{‚Äòkmeans‚Äô, ‚Äòrandom‚Äô}, default=‚Äôkmeans‚Äô 
    
The method used to initialize the weights, the means and the precisions. 

weights_initarray-like of shape (n_components, ), default=None 
    
The user-provided initial weights. If it is None, weights are initialized using the init_params method. 

means_initarray-like of shape (n_components, n_features), default=None 
    
The user-provided initial means, If it is None, means are initialized using the init_params method. 

precisions_initarray-like, default=None 
    
The user-provided initial precisions (inverse of the covariance matrices). If it is None, precisions are initialized using the ‚Äòinit_params‚Äô method. 

random_stateint, RandomState instance or None, default=None 
    
Controls the random seed given to the method chosen to initialize the parameters. 

warm_startbool, default=False 
    
If ‚Äòwarm_start‚Äô is True, the solution of the last fitting is used as initialization for the next call of fit(). 

verboseint, default=0 
    
Enable verbose output. 

verbose_intervalint, default=10 
    
Number of iteration done before the next print. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id356 "Link to this heading") 

[weights_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1265)array-like of shape (n_components,) 
    
The weights of each mixture components. 

[means_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1267)array-like of shape (n_components, n_features) 
    
The mean of each mixture component. 

[covariances_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1269)array-like 
    
The covariance of each mixture component. 

[precisions_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1271)array-like 
    
The precision matrices for each component in the mixture. 

[precisions_cholesky_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1273)array-like 
    
The cholesky decomposition of the precision matrices of each mixture component. 

[converged_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1275)bool 
    
True when convergence was reached in fit(), False otherwise. 

[n_iter_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1277)int 
    
Number of step used by the best fit of EM to reach the convergence. 

[lower_bound_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1279)float 
    
Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1281)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1283)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1285)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id357 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id358 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html#GMM.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id359 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id360 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html#GMM.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id361 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

sample_weightarray-like, shape (n_samples,) 
    
Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id362 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id363 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id364 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id365 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id366 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

_property_ precisions_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_ "Link to this definition") 
    
The precision matrices for each component in the mixture. Decorator for scikit-learn Gaussian Mixture Model attributes. 

_property_ precisions_cholesky_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_cholesky_ "Link to this definition") 
     

The cholesky decomposition of the precision matrices
    
of each mixture component.
Decorator for scikit-learn Gaussian Mixture Model attributes. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id367 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id368 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id370 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id371 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id373 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id374 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id375 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id376 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.hbos module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos "Link to this heading")
Histogram-based Outlier Detection (HBOS) 

_class_ pyod.models.hbos.HBOS(_n_bins =10_, _alpha =0.1_, _tol =0.5_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html#HBOS)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Histogram- based outlier detection (HBOS) is an efficient unsupervised method. It assumes the feature independence and calculates the degree of outlyingness by building histograms. See [[BGD12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1120 "Markus Goldstein and Andreas Dengel. Histogram-based outlier score \(hbos\): a fast unsupervised anomaly detection algorithm. KI-2012: Poster and Demo Track, pages 59‚Äì63, 2012.")] for details.
Two versions of HBOS are supported: - Static number of bins: uses a static number of bins for all features. - Automatic number of bins: every feature uses a number of bins deemed to
> be optimal according to the Birge-Rozenblac method ([[BBirgeR06](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1152 "Lucien Birg√© and Yves Rozenholc. How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10:24‚Äì45, 2006.")]).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id379 "Link to this heading") 

n_binsint or string, optional (default=10) 
    
The number of bins. ‚Äúauto‚Äù uses the birge-rozenblac method for automatic selection of the optimal number of bins for each feature. 

alphafloat in (0, 1), optional (default=0.1) 
    
The regularizer for preventing overflow. 

tolfloat in (0, 1), optional (default=0.5) 
    
The parameter to decide the flexibility while dealing the samples falling outside the bins. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id380 "Link to this heading") 

[bin_edges_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1287)numpy array of shape (n_bins + 1, n_features ) 
    
The edges of the bins. 

[hist_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1289)numpy array of shape (n_bins, n_features) 
    
The density of each histogram. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1291)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1293)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1295)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id381 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id382 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html#HBOS.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id383 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id384 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html#HBOS.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id385 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id386 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id387 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id388 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id389 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id390 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id391 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id392 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id393 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id394 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id396 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id397 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id399 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id400 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id401 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id402 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id403 "Link to this heading")
self : object
###### pyod.models.iforest module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest "Link to this heading")
IsolationForest Outlier Detector. Implemented on scikit-learn library. 

_class_ pyod.models.iforest.IForest(_n_estimators =100_, _max_samples ='auto'_, _contamination =0.1_, _max_features =1.0_, _bootstrap =False_, _n_jobs =1_, _behaviour ='old'_, _random_state =None_, _verbose =0_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html#IForest)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Wrapper of scikit-learn Isolation Forest with more functionalities.
The IsolationForest ‚Äòisolates‚Äô observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. See [[BLTZ08](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1112 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, 413‚Äì422. IEEE, 2008."), [BLTZ12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1113 "Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data \(TKDD\), 6\(1\):3, 2012.")] for details.
Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id405 "Link to this heading") 

n_estimatorsint, optional (default=100) 
    
The number of base estimators in the ensemble. 

max_samplesint or float, optional (default=‚Äùauto‚Äù) 
    
The number of samples to draw from X to train each base estimator.
>   * If int, then draw max_samples samples.
>   * If float, then draw max_samples * X.shape[0] samples.
>   * If ‚Äúauto‚Äù, then max_samples=min(256, n_samples).
> 

If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling). 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

max_featuresint or float, optional (default=1.0) 
    
The number of features to draw from X to train each base estimator.
>   * If int, then draw max_features features.
>   * If float, then draw max_features * X.shape[1] features.
> 


bootstrapbool, optional (default=False) 
    
If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed. 

n_jobsinteger, optional (default=1) 
    
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores. 

behaviourstr, default=‚Äôold‚Äô 
    
Behaviour of the `decision_function` which can be either ‚Äòold‚Äô or ‚Äònew‚Äô. Passing `behaviour='new'` makes the `decision_function` change to match other anomaly detection algorithm API which will be the default behaviour in the future. As explained in details in the `offset_` attribute documentation, the `decision_function` becomes dependent on the contamination parameter, in such a way that 0 becomes its natural threshold to detect outliers.
Added in version 0.7.0: `behaviour` is added in 0.7.0 for back-compatibility purpose.
Deprecated since version 0.20: `behaviour='old'` is deprecated in sklearn 0.20 and will not be possible in 0.22.
Deprecated since version 0.22: `behaviour` parameter will be deprecated in sklearn 0.22 and removed in 0.24.
Warning
Only applicable for sklearn 0.20 above. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

verboseint, optional (default=0) 
    
Controls the verbosity of the tree building process.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id406 "Link to this heading") 

[estimators_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1297)list of DecisionTreeClassifier 
    
The collection of fitted sub-estimators. 

[estimators_samples_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1299)list of arrays 
    
The subset of drawn samples (i.e., the in-bag samples) for each base estimator. 

[max_samples_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1301)integer 
    
The actual number of samples 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1303)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1305)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1307)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id407 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id408 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html#IForest.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id409 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id410 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

_property_ feature_importances_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.feature_importances_ "Link to this definition") 
    
The impurity-based feature importance. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.
impurity-based feature importance can be misleading for high cardinality features (many unique values). See <https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html> as an alternative.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id411 "Link to this heading") 

[feature_importances_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1309)ndarray of shape (n_features,) 
    
The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, in which case it will be an array of zeros. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html#IForest.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id412 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id413 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id414 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id415 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id416 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id417 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

_property_ max_samples_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.max_samples_ "Link to this definition") 
    
The actual number of samples. Decorator for scikit-learn Isolation Forest attributes. 

_property_ n_features_in_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.n_features_in_ "Link to this definition") 
    
The number of features seen during the fit. Decorator for scikit-learn Isolation Forest attributes. 

_property_ offset_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.offset_ "Link to this definition") 
    
Offset used to define the decision function from the raw scores. Decorator for scikit-learn Isolation Forest attributes. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id418 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id419 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id421 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id422 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id424 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id425 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id426 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id427 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True;
###### pyod.models.inne module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne "Link to this heading")
Isolation-based anomaly detection using nearest-neighbor ensembles. Part of the codes are adapted from <https://github.com/xhan97/inne> 

_class_ pyod.models.inne.INNE(_n_estimators =200_, _max_samples ='auto'_, _contamination =0.1_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html#INNE)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Isolation-based anomaly detection using nearest-neighbor ensembles.
The INNE algorithm uses the nearest neighbour ensemble to isolate anomalies. It partitions the data space into regions using a subsample and determines an isolation score for each region. As each region adapts to local distribution, the calculated isolation score is a local measure that is relative to the local neighbourhood, enabling it to detect both global and local anomalies. INNE has linear time complexity to efficiently handle large and high-dimensional datasets with complex distributions.
See [[BBTA+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1158 "Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. Computational Intelligence, 34\(4\):968‚Äì998, 2018.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id429 "Link to this heading") 

n_estimatorsint, default=200 
    
The number of base estimators in the ensemble. 

max_samplesint or float, optional (default=‚Äùauto‚Äù) 
    
The number of samples to draw from X to train each base estimator.
>   * If int, then draw max_samples samples.
>   * If float, then draw max_samples * X.shape[0]` samples.
>   * If ‚Äúauto‚Äù, then max_samples=min(8, n_samples).
> 


contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id430 "Link to this heading") 

[max_samples_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1311)integer 
    
The actual number of samples 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1313)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1315)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1317)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id431 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id432 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html#INNE.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id433 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id434 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html#INNE.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id435 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id436 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id437 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id438 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id439 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id440 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id441 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id442 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id443 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id444 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id446 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id447 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id449 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id450 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id451 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id452 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id453 "Link to this heading")
self : object
###### pyod.models.kde module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde "Link to this heading")
Kernel Density Estimation (KDE) for Unsupervised Outlier Detection. 

_class_ pyod.models.kde.KDE(_contamination =0.1_, _bandwidth =1.0_, _algorithm ='auto'_, _leaf_size =30_, _metric ='minkowski'_, _metric_params =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html#KDE)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
KDE class for outlier detection.
For an observation, its negative log probability density could be viewed as the outlying score.
See [[BLLP07](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1156 "Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition, 61‚Äì75. Springer, 2007.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id455 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

bandwidthfloat, optional (default=1.0) 
    
The bandwidth of the kernel. 

algorithm{‚Äòauto‚Äô, ‚Äòball_tree‚Äô, ‚Äòkd_tree‚Äô}, optional 
    
Algorithm used to compute the kernel density estimator:
  * ‚Äòball_tree‚Äô will use BallTree
  * ‚Äòkd_tree‚Äô will use KDTree
  * ‚Äòauto‚Äô will attempt to decide the most appropriate algorithm based on the values passed to [`fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit "pyod.models.kde.KDE.fit") method.



leaf_sizeint, optional (default = 30) 
    
Leaf size passed to BallTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. 

metricstring or callable, default ‚Äòminkowski‚Äô 
    
metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy‚Äôs metrics, but is less efficient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
  * from scikit-learn: [‚Äòcityblock‚Äô, ‚Äòcosine‚Äô, ‚Äòeuclidean‚Äô, ‚Äòl1‚Äô, ‚Äòl2‚Äô, ‚Äòmanhattan‚Äô]
  * from scipy.spatial.distance: [‚Äòbraycurtis‚Äô, ‚Äòcanberra‚Äô, ‚Äòchebyshev‚Äô, ‚Äòcorrelation‚Äô, ‚Äòdice‚Äô, ‚Äòhamming‚Äô, ‚Äòjaccard‚Äô, ‚Äòkulsinski‚Äô, ‚Äòmahalanobis‚Äô, ‚Äòmatching‚Äô, ‚Äòminkowski‚Äô, ‚Äòrogerstanimoto‚Äô, ‚Äòrussellrao‚Äô, ‚Äòseuclidean‚Äô, ‚Äòsokalmichener‚Äô, ‚Äòsokalsneath‚Äô, ‚Äòsqeuclidean‚Äô, ‚Äòyule‚Äô]


See the documentation for scipy.spatial.distance for details on these metrics. 

metric_paramsdict, optional (default = None) 
    
Additional keyword arguments for the metric function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id456 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1319)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1321)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1323)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id457 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id458 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html#KDE.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id459 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id460 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html#KDE.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id461 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id462 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id463 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id464 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id465 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id466 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id467 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id468 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id469 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id470 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id472 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id473 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id475 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id476 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id477 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id478 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id479 "Link to this heading")
self : object
###### pyod.models.knn module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn "Link to this heading")
k-Nearest Neighbors Detector (kNN) 

_class_ pyod.models.knn.KNN(_contamination =0.1_, _n_neighbors =5_, _method ='largest'_, _radius =1.0_, _algorithm ='auto'_, _leaf_size =30_, _metric ='minkowski'_, _p =2_, _metric_params =None_, _n_jobs =1_, _** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html#KNN)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
kNN class for outlier detection. For an observation, its distance to its kth nearest neighbor could be viewed as the outlying score. It could be viewed as a way to measure the density. See [[BAP02](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1117 "Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, 15‚Äì27. Springer, 2002."), [BRRS00](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1116 "Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In ACM Sigmod Record, volume 29, 427‚Äì438. ACM, 2000.")] for details.
Three kNN detectors are supported: largest: use the distance to the kth neighbor as the outlier score mean: use the average of all k neighbors as the outlier score median: use the median of the distance to k neighbors as the outlier score
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id481 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_neighborsint, optional (default = 5) 
    
Number of neighbors to use by default for k neighbors queries. 

methodstr, optional (default=‚Äôlargest‚Äô) 
    
{‚Äòlargest‚Äô, ‚Äòmean‚Äô, ‚Äòmedian‚Äô}
  * ‚Äòlargest‚Äô: use the distance to the kth neighbor as the outlier score
  * ‚Äòmean‚Äô: use the average of all k neighbors as the outlier score
  * ‚Äòmedian‚Äô: use the median of the distance to k neighbors as the outlier score



radiusfloat, optional (default = 1.0) 
    
Range of parameter space to use by default for radius_neighbors queries. 

algorithm{‚Äòauto‚Äô, ‚Äòball_tree‚Äô, ‚Äòkd_tree‚Äô, ‚Äòbrute‚Äô}, optional 
    
Algorithm used to compute the nearest neighbors:
  * ‚Äòball_tree‚Äô will use BallTree
  * ‚Äòkd_tree‚Äô will use KDTree
  * ‚Äòbrute‚Äô will use a brute-force search.
  * ‚Äòauto‚Äô will attempt to decide the most appropriate algorithm based on the values passed to [`fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit "pyod.models.knn.KNN.fit") method.


Note: fitting on sparse input will override the setting of this parameter, using brute force.
Deprecated since version 0.74: `algorithm` is deprecated in PyOD 0.7.4 and will not be possible in 0.7.6. It has to use BallTree for consistency. 

leaf_sizeint, optional (default = 30) 
    
Leaf size passed to BallTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. 

metricstring or callable, default ‚Äòminkowski‚Äô 
    
metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy‚Äôs metrics, but is less efficient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
  * from scikit-learn: [‚Äòcityblock‚Äô, ‚Äòeuclidean‚Äô, ‚Äòl1‚Äô, ‚Äòl2‚Äô, ‚Äòmanhattan‚Äô]
  * from scipy.spatial.distance: [‚Äòbraycurtis‚Äô, ‚Äòcanberra‚Äô, ‚Äòchebyshev‚Äô, ‚Äòcorrelation‚Äô, ‚Äòdice‚Äô, ‚Äòhamming‚Äô, ‚Äòjaccard‚Äô, ‚Äòkulsinski‚Äô, ‚Äòmahalanobis‚Äô, ‚Äòmatching‚Äô, ‚Äòminkowski‚Äô, ‚Äòrogerstanimoto‚Äô, ‚Äòrussellrao‚Äô, ‚Äòseuclidean‚Äô, ‚Äòsokalmichener‚Äô, ‚Äòsokalsneath‚Äô, ‚Äòsqeuclidean‚Äô, ‚Äòyule‚Äô]


See the documentation for scipy.spatial.distance for details on these metrics. 

pinteger, optional (default = 2) 
    
Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. See <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances> 

metric_paramsdict, optional (default = None) 
    
Additional keyword arguments for the metric function. 

n_jobsint, optional (default = 1) 
    
The number of parallel jobs to run for neighbors search. If `-1`, then the number of jobs is set to the number of CPU cores. Affects only kneighbors and kneighbors_graph methods.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id482 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1325)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1327)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1329)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id483 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id484 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html#KNN.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id485 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id486 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html#KNN.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id487 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id488 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id489 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id490 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id491 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id492 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id493 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id494 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id495 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id496 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id498 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id499 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id501 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id502 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id503 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id504 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id505 "Link to this heading")
self : object
###### pyod.models.kpca module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kpca "Link to this heading")
Kernel Principal Component Analysis (KPCA) Outlier Detector 

_class_ pyod.models.kpca.KPCA(_contamination =0.1_, _n_components =None_, _n_selected_components =None_, _kernel ='rbf'_, _gamma =None_, _degree =3_, _coef0 =1_, _kernel_params =None_, _alpha =1.0_, _eigen_solver ='auto'_, _tol =0_, _max_iter =None_, _remove_zero_eig =False_, _copy_X =True_, _n_jobs =None_, _sampling =False_, _subset_size =20_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html#KPCA)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
KPCA class for outlier detection.
PCA is performed on the feature space uniquely determined by the kernel, and the reconstruction error on the feature space is used as the anomaly score.
See [[BHof07](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1164 "Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40\(3\):863‚Äì874, 2007.")] Heiko Hoffmann, ‚ÄúKernel PCA for novelty detection,‚Äù Pattern Recognition, vol.40, no.3, pp. 863-874, 2007. <https://www.sciencedirect.com/science/article/pii/S0031320306003414> for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id507 "Link to this heading") 

n_componentsint, optional (default=None) 
    
Number of components. If None, all non-zero components are kept. 

n_selected_componentsint, optional (default=None) 
    
Number of selected principal components for calculating the outlier scores. It is not necessarily equal to the total number of the principal components. If not set, use all principal components. 

kernelstring {‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô, 
    
> ‚Äòcosine‚Äô, ‚Äòprecomputed‚Äô}, optional (default=‚Äôrbf‚Äô)
Kernel used for PCA. 

gammafloat, optional (default=None) 
    
Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels. If `gamma` is `None`, then it is set to `1/n_features`. 

degreeint, optional (default=3) 
    
Degree for poly kernels. Ignored by other kernels. 

coef0float, optional (default=1) 
    
Independent term in poly and sigmoid kernels. Ignored by other kernels. 

kernel_paramsdict, optional (default=None) 
    
Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. 

alphafloat, optional (default=1.0) 
    
Hyperparameter of the ridge regression that learns the inverse transform (when inverse_transform=True). 

eigen_solverstring, {‚Äòauto‚Äô, ‚Äòdense‚Äô, ‚Äòarpack‚Äô, ‚Äòrandomized‚Äô}, default=‚Äôauto‚Äô 
    
Select eigensolver to use. If n_components is much less than the number of training samples, randomized (or arpack to a smaller extend) may be more efficient than the dense eigensolver. Randomized SVD is performed according to the method of Halko et al. 

auto :
    
the solver is selected by a default policy based on n_samples (the number of training samples) and n_components: if the number of components to extract is less than 10 (strict) and the number of samples is more than 200 (strict), the ‚Äòarpack‚Äô method is enabled. Otherwise the exact full eigenvalue decomposition is computed and optionally truncated afterwards (‚Äòdense‚Äô method). 

dense :
    
run exact full eigenvalue decomposition calling the standard LAPACK solver via scipy.linalg.eigh, and select the components by postprocessing. 

arpack :
    
run SVD truncated to n_components calling ARPACK solver using scipy.sparse.linalg.eigsh. It requires strictly 0 < n_components < n_samples 

randomized :
    
run randomized SVD. implementation selects eigenvalues based on their module; therefore using this method can lead to unexpected results if the kernel is not positive semi-definite. 

tolfloat, optional (default=0) 
    
Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack. 

max_iterint, optional (default=None) 
    
Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack. 

remove_zero_eigbool, optional (default=False) 
    
If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless. 

copy_Xbool, optional (default=True) 
    
If True, input X is copied and stored by the model in the X_fit_ attribute. If no further changes will be done to X, setting copy_X=False saves memory by storing a reference. 

n_jobsint, optional (default=None) 
    
The number of parallel jobs to run. `None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using all processors. 

samplingbool, optional (default=False) 
    
If True, sampling subset from the dataset is performed only once, in order to reduce time complexity while keeping detection performance. 

subset_sizefloat in (0., 1.0) or int (0, n_samples), optional (default=20) 
    
If sampling is True, the size of subset is specified. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id508 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1331)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1333)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1335)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id509 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id510 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html#KPCA.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id511 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id512 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html#KPCA.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id513 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id514 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id515 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id516 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id517 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id518 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id519 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id520 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id521 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id522 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id524 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id525 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id527 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id528 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id529 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id530 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id531 "Link to this heading")
self : object
###### pyod.models.lmdd module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd "Link to this heading")
Linear Model Deviation-base outlier detection (LMDD). 

_class_ pyod.models.lmdd.LMDD(_contamination =0.1_, _n_iter =50_, _dis_measure ='aad'_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html#LMDD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Linear Method for Deviation-based Outlier Detection.
LMDD employs the concept of the smoothing factor which indicates how much the dissimilarity can be reduced by removing a subset of elements from the data-set. Read more in the [[BAAR96](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1143 "Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In KDD, volume 1141, 972‚Äì981. 1996.")].
Note: this implementation has minor modification to make it output scores instead of labels.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id533 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_iterint, optional (default=50) 
    
Number of iterations where in each iteration, the process is repeated after randomizing the order of the input. Note that n_iter is a very important factor that affects the accuracy. The higher the better the accuracy and the longer the execution. 

dis_measure: str, optional (default=‚Äôaad‚Äô)
    
Dissimilarity measure to be used in calculating the smoothing factor for points, options available:
  * ‚Äòaad‚Äô: Average Absolute Deviation
  * ‚Äòvar‚Äô: Variance
  * ‚Äòiqr‚Äô: Interquartile Range



random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id534 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1337)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1339)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1341)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id535 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id536 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html#LMDD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id537 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id538 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html#LMDD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id539 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id540 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id541 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id542 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id543 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id544 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id545 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id546 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id547 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id548 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id550 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id551 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id553 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id554 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id555 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id556 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id557 "Link to this heading")
self : object
###### pyod.models.loda module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda "Link to this heading")
Loda: Lightweight on-line detector of anomalies Adapted from tilitools (<https://github.com/nicococo/tilitools>) by 

_class_ pyod.models.loda.LODA(_contamination =0.1_, _n_bins =10_, _n_random_cuts =100_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html#LODA)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Loda: Lightweight on-line detector of anomalies. See [[BPevny16](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1145 "Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. Machine Learning, 102\(2\):275‚Äì304, 2016.")] for more information.
Two versions of LODA are supported: - Static number of bins: uses a static number of bins for all random cuts. - Automatic number of bins: every random cut uses a number of bins deemed
> to be optimal according to the Birge-Rozenblac method ([[BBirgeR06](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1152 "Lucien Birg√© and Yves Rozenholc. How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10:24‚Äì45, 2006.")]).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id560 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_binsint or string, optional (default = 10) 
    
The number of bins for the histogram. If set to ‚Äúauto‚Äù, the Birge-Rozenblac method will be used to automatically determine the optimal number of bins. 

n_random_cutsint, optional (default = 100) 
    
The number of random cuts.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id561 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1343)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1345)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1347)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id562 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id563 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html#LODA.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id564 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id565 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html#LODA.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id566 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id567 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id568 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id569 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id570 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id571 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id572 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id573 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id574 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id575 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id577 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id578 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id580 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id581 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id582 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id583 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id584 "Link to this heading")
self : object
###### pyod.models.lof module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof "Link to this heading")
Local Outlier Factor (LOF). Implemented on scikit-learn library. 

_class_ pyod.models.lof.LOF(_n_neighbors =20_, _algorithm ='auto'_, _leaf_size =30_, _metric ='minkowski'_, _p =2_, _metric_params =None_, _contamination =0.1_, _n_jobs =1_, _novelty =True_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html#LOF)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Wrapper of scikit-learn LOF Class with more functionalities. Unsupervised Outlier Detection using Local Outlier Factor (LOF).
The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers. See [[BBKNS00](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1123 "Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In ACM sigmod record, volume 29, 93‚Äì104. ACM, 2000.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id586 "Link to this heading") 

n_neighborsint, optional (default=20) 
    
Number of neighbors to use by default for kneighbors queries. If n_neighbors is larger than the number of samples provided, all samples will be used. 

algorithm{‚Äòauto‚Äô, ‚Äòball_tree‚Äô, ‚Äòkd_tree‚Äô, ‚Äòbrute‚Äô}, optional 
    
Algorithm used to compute the nearest neighbors:
  * ‚Äòball_tree‚Äô will use BallTree
  * ‚Äòkd_tree‚Äô will use KDTree
  * ‚Äòbrute‚Äô will use a brute-force search.
  * ‚Äòauto‚Äô will attempt to decide the most appropriate algorithm based on the values passed to [`fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit "pyod.models.lof.LOF.fit") method.


Note: fitting on sparse input will override the setting of this parameter, using brute force. 

leaf_sizeint, optional (default=30) 
    
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. 

metricstring or callable, default ‚Äòminkowski‚Äô 
    
metric used for the distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.
If ‚Äòprecomputed‚Äô, the training input X is expected to be a distance matrix.
If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy‚Äôs metrics, but is less efficient than passing the metric name as a string.
Valid values for metric are:
  * from scikit-learn: [‚Äòcityblock‚Äô, ‚Äòcosine‚Äô, ‚Äòeuclidean‚Äô, ‚Äòl1‚Äô, ‚Äòl2‚Äô, ‚Äòmanhattan‚Äô]
  * from scipy.spatial.distance: [‚Äòbraycurtis‚Äô, ‚Äòcanberra‚Äô, ‚Äòchebyshev‚Äô, ‚Äòcorrelation‚Äô, ‚Äòdice‚Äô, ‚Äòhamming‚Äô, ‚Äòjaccard‚Äô, ‚Äòkulsinski‚Äô, ‚Äòmahalanobis‚Äô, ‚Äòmatching‚Äô, ‚Äòminkowski‚Äô, ‚Äòrogerstanimoto‚Äô, ‚Äòrussellrao‚Äô, ‚Äòseuclidean‚Äô, ‚Äòsokalmichener‚Äô, ‚Äòsokalsneath‚Äô, ‚Äòsqeuclidean‚Äô, ‚Äòyule‚Äô]


See the documentation for scipy.spatial.distance for details on these metrics: <http://docs.scipy.org/doc/scipy/reference/spatial.distance.html> 

pinteger, optional (default = 2) 
    
Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. See <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances> 

metric_paramsdict, optional (default = None) 
    
Additional keyword arguments for the metric function. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. 

n_jobsint, optional (default = 1) 
    
The number of parallel jobs to run for neighbors search. If `-1`, then the number of jobs is set to the number of CPU cores. Affects only kneighbors and kneighbors_graph methods. 

noveltybool (default=False) 
    
By default, LocalOutlierFactor is only meant to be used for outlier detection (novelty=False). Set novelty to True if you want to use LocalOutlierFactor for novelty detection. In this case be aware that that you should only use predict, decision_function and score_samples on new unseen data and not on the training set.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id587 "Link to this heading") 

[n_neighbors_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1349)int 
    
The actual number of neighbors used for kneighbors queries. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1351)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1353)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1355)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id588 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id589 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html#LOF.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id590 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id591 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html#LOF.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id592 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id593 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id594 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id595 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id596 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id597 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id598 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id599 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id600 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id601 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id603 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id604 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id606 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id607 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id608 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id609 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id610 "Link to this heading")
self : object
###### pyod.models.loci module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci "Link to this heading")
Local Correlation Integral (LOCI). Part of the codes are adapted from <https://github.com/Cloudy10/loci> 

_class_ pyod.models.loci.LOCI(_contamination =0.1_, _alpha =0.5_, _k =3_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html#LOCI)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Local Correlation Integral.
LOCI is highly effective for detecting outliers and groups of outliers ( a.k.a.micro-clusters), which offers the following advantages and novelties: (a) It provides an automatic, data-dictated cut-off to determine whether a point is an outlier‚Äîin contrast, previous methods force users to pick cut-offs, without any hints as to what cut-off value is best for a given dataset. (b) It can provide a LOCI plot for each point; this plot summarizes a wealth of information about the data in the vicinity of the point, determining clusters, micro-clusters, their diameters and their inter-cluster distances. None of the existing outlier-detection methods can match this feature, because they output only a single number for each point: its outlierness score.(c) It can be computed as quickly as the best previous methods Read more in the [[BPKGF03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1130 "Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In Data Engineering, 2003. Proceedings. 19th International Conference on, 315‚Äì326. IEEE, 2003.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id612 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1)  
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

alphaint, default = 0.5 
    
The neighbourhood parameter measures how large of a neighbourhood should be considered ‚Äúlocal‚Äù. 

k: int, default = 3
    
An outlier cutoff threshold for determine whether or not a point should be considered an outlier.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id613 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1357)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1359)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1361)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`.
######### Examples[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#examples "Link to this heading")
```
>>> frompyod.models.lociimport LOCI
>>> frompyod.utils.dataimport generate_data
>>> n_train = 50
>>> n_test = 50
>>> contamination = 0.1
>>> X_train, y_train, X_test, y_test = generate_data(
...     n_train=n_train, n_test=n_test,
...     contamination=contamination, random_state=42)
>>> clf = LOCI()
>>> clf.fit(X_train)
LOCI(alpha=0.5, contamination=0.1, k=None)

```


compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id614 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id615 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html#LOCI.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.decision_function "Link to this definition") 
    
Predict raw anomaly scores of X using the fitted detector.
The anomaly score of an input sample is computed based on the fitted detector. For consistency, outliers are assigned with higher anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id616 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id617 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html#LOCI.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit "Link to this definition") 
    
Fit the model using X as training data.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id618 "Link to this heading") 

Xarray, shape (n_samples, n_features) 
    
Training data. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id619 "Link to this heading")
self : object 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id620 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id621 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id622 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id623 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id624 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id625 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id626 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id627 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id629 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id630 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id632 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id633 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id634 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id635 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id636 "Link to this heading")
self : object
###### pyod.models.lunar module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lunar "Link to this heading")
LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks 

_class_ pyod.models.lunar.LUNAR(_model_type ='WEIGHT'_, _n_neighbours =5_, _negative_sampling ='MIXED'_, _val_size =0.1_, _scaler =MinMaxScaler()_, _epsilon =0.1_, _proportion =1.0_, _n_epochs =200_, _lr =0.001_, _wd =0.1_, _verbose =0_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html#LUNAR)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
LUNAR class for outlier detection. See <https://www.aaai.org/AAAI22Papers/AAAI-51.GoodgeA.pdf> for details. For an observation, its ordered list of distances to its k nearest neighbours is input to a neural network, with one of the following outputs:
>   1. SCORE_MODEL: network directly outputs the anomaly score.
>   2. 

WEIGHT_MODEL: network outputs a set of weights for the k distances, the anomaly score is then the
    
> sum of weighted distances.
> 

See [[BGHNN22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1160 "Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 6737‚Äì6745. 2022.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id638 "Link to this heading") 

model_type: str in [‚ÄòWEIGHT‚Äô, ‚ÄòSCORE‚Äô], optional (default = ‚ÄòWEIGHT‚Äô)
    
Whether to use WEIGHT_MODEL or SCORE_MODEL for anomaly scoring. 

n_neighbors: int, optional (default = 5)
    
Number of neighbors to use by default for k neighbors queries. 

negative_sampling: str in [‚ÄòUNIFORM‚Äô, ‚ÄòSUBSPACE‚Äô, MIXED‚Äô], optional (default = ‚ÄòMIXED)
    
Type of negative samples to use between:
  * ‚ÄòUNIFORM‚Äô: uniformly distributed samples
  * ‚ÄòSUBSPACE‚Äô: subspace perturbation (additive random noise in a subset of features)
  * ‚ÄòMIXED‚Äô: a combination of both types of samples



val_size: float in [0,1], optional (default = 0.1)
    
Proportion of samples to be used for model validation 

scaler: object in {StandardScaler(), MinMaxScaler(), optional (default = MinMaxScaler())
    
Method of data normalization 

epsilon: float, optional (default = 0.1)
    
Hyper-parameter for the generation of negative samples. A smaller epsilon results in negative samples more similar to normal samples. 

proportion: float, optional (default = 1.0)
    
Hyper-parameter for the proprotion of negative samples to use relative to the number of normal training samples. 

n_epochs: int, optional (default = 200)
    
Number of epochs to train neural network. 

lr: float, optional (default = 0.001)
    
Learning rate. 

wd: float, optional (default = 0.1)
    
Weight decay. 

verbose: int in {0,1}, optional (default = 0):
    
To view or hide training progress
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id639 "Link to this heading") 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id640 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id641 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html#LUNAR.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. For consistency, outliers are assigned with larger anomaly scores. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The training input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id642 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html#LUNAR.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit "Link to this definition") 
    
Fit detector. y is assumed to be 0 for all training samples. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The input samples. 

yIgnored 
    
Overwritten with 0 for all training samples (assumed to be normal).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id643 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id644 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id645 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id646 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id647 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id648 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id649 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id650 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id651 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id653 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id654 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id656 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id657 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id658 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id659 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id660 "Link to this heading")
self : object
###### pyod.models.lscp module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lscp "Link to this heading")
Locally Selective Combination of Parallel Outlier Ensembles (LSCP). Adapted from the original implementation. 

_class_ pyod.models.lscp.LSCP(_detector_list_ , _local_region_size =30_, _local_max_features =1.0_, _n_bins =10_, _random_state =None_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html#LSCP)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Locally Selection Combination in Parallel Outlier Ensembles
LSCP is an unsupervised parallel outlier detection ensemble which selects competent detectors in the local region of a test instance. This implementation uses an Average of Maximum strategy. First, a heterogeneous list of base detectors is fit to the training data and then generates a pseudo ground truth for each train instance is generated by taking the maximum outlier score.
For each test instance: 1) The local region is defined to be the set of nearest training points in randomly sampled feature subspaces which occur more frequently than a defined threshold over multiple iterations.
2) Using the local region, a local pseudo ground truth is defined and the pearson correlation is calculated between each base detector‚Äôs training outlier scores and the pseudo ground truth.
3) A histogram is built out of pearson correlation scores; detectors in the largest bin are selected as competent base detectors for the given test instance.
4) The average outlier score of the selected competent detectors is taken to be the final score.
See [[BZNHL19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1131 "Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: https://doi.org/10.1137/1.9781611975673.66, doi:10.1137/1.9781611975673.66.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id662 "Link to this heading") 

detector_listList, length must be greater than 1 
    
Base unsupervised outlier detectors from PyOD. (Note: requires fit and decision_function methods) 

local_region_sizeint, optional (default=30) 
    
Number of training points to consider in each iteration of the local region generation process (30 by default). 

local_max_featuresfloat in (0.5, 1.), optional (default=1.0) 
    
Maximum proportion of number of features to consider when defining the local region (1.0 by default). 

n_binsint, optional (default=10) 
    
Number of bins to use when selecting the local region 

random_stateRandomState, optional (default=None) 
    
A random number generator instance to define the state of the random permutations generator. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function (0.1 by default).
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id663 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1363)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1365)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1367)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`.
######### Examples[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id664 "Link to this heading")
```
>>> frompyod.utils.dataimport generate_data
>>> frompyod.utils.utilityimport standardizer
>>> frompyod.models.lscpimport LSCP
>>> frompyod.models.lofimport LOF
>>> X_train, y_train, X_test, y_test = generate_data(
...     n_train=50, n_test=50,
...     contamination=0.1, random_state=42)
>>> X_train, X_test = standardizer(X_train, X_test)
>>> detector_list = [LOF(), LOF()]
>>> clf = LSCP(detector_list)
>>> clf.fit(X_train)
LSCP(...)

```


compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id665 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id666 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html#LSCP.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id667 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id668 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html#LSCP.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id669 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id670 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id671 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id672 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id673 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id674 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id675 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id676 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id677 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id678 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id680 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id681 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id683 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id684 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id685 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id686 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id687 "Link to this heading")
self : object
###### pyod.models.mad module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mad "Link to this heading")
Median Absolute deviation (MAD) Algorithm. Strictly for Univariate Data. 

_class_ pyod.models.mad.MAD(_threshold =3.5_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html#MAD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Median Absolute Deviation: for measuring the distances between data points and the median in terms of median distance. See [[BIH93](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1147 "Boris Iglewicz and David Caster Hoaglin. How to detect and handle outliers. Volume 16. Asq Press, 1993.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id689 "Link to this heading") 

thresholdfloat, optional (default=3.5) 
    
The modified z-score to use as a threshold. Observations with a modified z-score (based on the median absolute deviation) greater than this value will be classified as outliers.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id690 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1369)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1371)float 
    
The modified z-score to use as a threshold. Observations with a modified z-score (based on the median absolute deviation) greater than this value will be classified as outliers. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1373)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id691 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id692 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html#MAD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id693 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. Note that n_features must equal 1.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id694 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html#MAD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id695 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. Note that n_features must equal 1. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id696 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id697 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id698 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id699 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id700 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id701 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id702 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id703 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id704 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id706 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id707 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id709 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id710 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id711 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id712 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id713 "Link to this heading")
self : object
###### pyod.models.mcd module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd "Link to this heading")
Outlier Detection with Minimum Covariance Determinant (MCD) 

_class_ pyod.models.mcd.MCD(_contamination =0.1_, _store_precision =True_, _assume_centered =False_, _support_fraction =None_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html#MCD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Detecting outliers in a Gaussian distributed dataset using Minimum Covariance Determinant (MCD): robust estimator of covariance.
The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.
First fit a minimum covariance determinant model and then compute the Mahalanobis distance as the outlier degree of the data
See [[BHR04](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1126 "Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. Computational Statistics & Data Analysis, 44\(4\):625‚Äì638, 2004."), [BRD99](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1125 "Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. Technometrics, 41\(3\):212‚Äì223, 1999.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id715 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

store_precisionbool 
    
Specify if the estimated precision is stored. 

assume_centeredbool 
    
If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment. 

support_fractionfloat, 0 < support_fraction < 1 
    
The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id716 "Link to this heading") 

[raw_location_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1375)array-like, shape (n_features,) 
    
The raw robust estimated location before correction and re-weighting. 

[raw_covariance_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1377)array-like, shape (n_features, n_features) 
    
The raw robust estimated covariance before correction and re-weighting. 

[raw_support_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1379)array-like, shape (n_samples,) 
    
A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting. 

[location_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1381)array-like, shape (n_features,) 
    
Estimated robust location 

[covariance_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1383)array-like, shape (n_features, n_features) 
    
Estimated robust covariance matrix 

[precision_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1385)array-like, shape (n_features, n_features) 
    
Estimated pseudo inverse matrix. (stored only if store_precision is True) 

[support_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1387)array-like, shape (n_samples,) 
    
A mask of the observations that have been used to compute the robust estimates of location and shape. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1389)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. Mahalanobis distances of the training set (on which :meth:`fit is called) observations. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1391)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1393)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id717 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id718 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html#MCD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id719 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id720 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html#MCD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id721 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id722 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id723 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id724 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id725 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id726 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id727 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id728 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id729 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id730 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id732 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id733 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id735 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id736 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id737 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id738 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id739 "Link to this heading")
self : object
###### pyod.models.mo_gaal module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mo_gaal "Link to this heading")
Multiple-Objective Generative Adversarial Active Learning. Part of the codes are adapted from <https://github.com/leibinghe/GAAL-based-outlier-detection> 

_class_ pyod.models.mo_gaal.MO_GAAL(_k =10_, _stop_epochs =20_, _lr_d =0.01_, _lr_g =0.0001_, _momentum =0.9_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html#MO_GAAL)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Multi-Objective Generative Adversarial Active Learning.
MO_GAAL directly generates informative potential outliers to assist the classifier in describing a boundary that can separate outliers from normal data effectively. Moreover, to prevent the generator from falling into the mode collapsing problem, the network structure of SO-GAAL is expanded from a single generator (SO-GAAL) to multiple generators with different objectives (MO-GAAL) to generate a reasonable reference distribution for the whole dataset. Read more in the [[BLLZ+19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1132 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id741 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

kint, optional (default=10) 
    
The number of sub generators. 

stop_epochsint, optional (default=20) 
    
The number of epochs of training. The number of total epochs equals to three times of stop_epochs. 

lr_dfloat, optional (default=0.01) 
    
The learn rate of the discriminator. 

lr_gfloat, optional (default=0.0001) 
    
The learn rate of the generator. 

momentumfloat, optional (default=0.9) 
    
The momentum parameter for SGD.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id742 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1395)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1397)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1399)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id743 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id744 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html#MO_GAAL.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id745 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id746 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html#MO_GAAL.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id747 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id748 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id749 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id750 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id751 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id752 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id753 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id754 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id755 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id756 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id758 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id759 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id761 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id762 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id763 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id764 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id765 "Link to this heading")
self : object
###### pyod.models.ocsvm module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm "Link to this heading")
One-class SVM detector. Implemented on scikit-learn library. 

_class_ pyod.models.ocsvm.OCSVM(_kernel ='rbf'_, _degree =3_, _gamma ='auto'_, _coef0 =0.0_, _tol =0.001_, _nu =0.5_, _shrinking =True_, _cache_size =200_, _verbose =False_, _max_iter =-1_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html#OCSVM)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Wrapper of scikit-learn one-class SVM Class with more functionalities. Unsupervised Outlier Detection.
Estimate the support of a high-dimensional distribution.
The implementation is based on libsvm. See <http://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection> and [[BScholkopfPST+01](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1136 "Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13\(7\):1443‚Äì1471, 2001.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id767 "Link to this heading") 

kernelstring, optional (default=‚Äôrbf‚Äô) 
    
Specifies the kernel type to be used in the algorithm. It must be one of ‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô, ‚Äòprecomputed‚Äô or a callable. If none is given, ‚Äòrbf‚Äô will be used. If a callable is given it is used to precompute the kernel matrix. 

nufloat, optional 
    
An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken. 

degreeint, optional (default=3) 
    
Degree of the polynomial kernel function (‚Äòpoly‚Äô). Ignored by all other kernels. 

gammafloat, optional (default=‚Äôauto‚Äô) 
    
Kernel coefficient for ‚Äòrbf‚Äô, ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô. If gamma is ‚Äòauto‚Äô then 1/n_features will be used instead. 

coef0float, optional (default=0.0) 
    
Independent term in kernel function. It is only significant in ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô. 

tolfloat, optional 
    
Tolerance for stopping criterion. 

shrinkingbool, optional 
    
Whether to use the shrinking heuristic. 

cache_sizefloat, optional 
    
Specify the size of the kernel cache (in MB). 

verbosebool, default: False 
    
Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. 

max_iterint, optional (default=-1) 
    
Hard limit on iterations within solver, or -1 for no limit. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id768 "Link to this heading") 

[support_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1401)array-like, shape = [n_SV] 
    
Indices of support vectors. 

[support_vectors_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1403)array-like, shape = [nSV, n_features] 
    
Support vectors. 

[dual_coef_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1405)array, shape = [1, n_SV] 
    
Coefficients of the support vectors in the decision function. 

[coef_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1407)array, shape = [1, n_features] 
    
Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.
coef_ is readonly property derived from dual_coef_ and support_vectors_ 

[intercept_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1409)array, shape = [1,] 
    
Constant in the decision function. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1411)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1413)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1415)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id769 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id770 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html#OCSVM.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id771 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id772 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_, _sample_weight =None_, _** params_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html#OCSVM.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id773 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

sample_weightarray-like, shape (n_samples,) 
    
Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id774 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id775 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id776 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id777 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id778 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id779 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id780 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id781 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id782 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id784 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id785 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id787 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id788 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id789 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id790 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id791 "Link to this heading")
self : object
###### pyod.models.pca module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca "Link to this heading")
Principal Component Analysis (PCA) Outlier Detector 

_class_ pyod.models.pca.PCA(_n_components =None_, _n_selected_components =None_, _contamination =0.1_, _copy =True_, _whiten =False_, _svd_solver ='auto'_, _tol =0.0_, _iterated_power ='auto'_, _random_state =None_, _weighted =True_, _standardization =True_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html#PCA)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Principal component analysis (PCA) can be used in detecting outliers. PCA is a linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.
In this procedure, covariance matrix of the data can be decomposed to orthogonal vectors, called eigenvectors, associated with eigenvalues. The eigenvectors with high eigenvalues capture most of the variance in the data.
Therefore, a low dimensional hyperplane constructed by k eigenvectors can capture most of the variance in the data. However, outliers are different from normal data points, which is more obvious on the hyperplane constructed by the eigenvectors with small eigenvalues.
Therefore, outlier scores can be obtained as the sum of the projected distance of a sample on all eigenvectors. See [[BAgg15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1122 "Charu C Aggarwal. Outlier analysis. In Data mining, 75‚Äì79. Springer, 2015."), [BSCSC03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1121 "Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.")] for details.
Score(X) = Sum of weighted euclidean distance between each sample to the hyperplane constructed by the selected eigenvectors
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id793 "Link to this heading") 

n_componentsint, float, None or string 
    
Number of components to keep. if n_components is not set all components are kept:
```
n_components == min(n_samples, n_features)

```

if n_components == ‚Äòmle‚Äô and svd_solver == ‚Äòfull‚Äô, Minka‚Äôs MLE is used to guess the dimension if `0 < n_components < 1` and svd_solver == ‚Äòfull‚Äô, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components n_components cannot be equal to n_features for svd_solver == ‚Äòarpack‚Äô. 

n_selected_componentsint, optional (default=None) 
    
Number of selected principal components for calculating the outlier scores. It is not necessarily equal to the total number of the principal components. If not set, use all principal components. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

copybool (default True) 
    
If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. 

whitenbool, optional (default False) 
    
When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.
Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. 

svd_solverstring {‚Äòauto‚Äô, ‚Äòfull‚Äô, ‚Äòarpack‚Äô, ‚Äòrandomized‚Äô} 
     

auto :
    
the solver is selected by a default policy based on X.shape and n_components: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ‚Äòrandomized‚Äô method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards. 

full :
    
run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing 

arpack :
    
run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < X.shape[1] 

randomized :
    
run randomized SVD by the method of Halko et al. 

tolfloat >= 0, optional (default .0) 
    
Tolerance for singular values computed by svd_solver == ‚Äòarpack‚Äô. 

iterated_powerint >= 0, or ‚Äòauto‚Äô, (default ‚Äòauto‚Äô) 
    
Number of iterations for the power method computed by svd_solver == ‚Äòrandomized‚Äô. 

random_stateint, RandomState instance or None, optional (default None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when `svd_solver` == ‚Äòarpack‚Äô or ‚Äòrandomized‚Äô. 

weightedbool, optional (default=True) 
    
If True, the eigenvalues are used in score computation. The eigenvectors with small eigenvalues comes with more importance in outlier score calculation. 

standardizationbool, optional (default=True) 
    
If True, perform standardization first to convert data to zero mean and unit variance. See <http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html>
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id794 "Link to this heading") 

[components_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1417)array, shape (n_components, n_features) 
    
Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by `explained_variance_`. 

[explained_variance_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1419)array, shape (n_components,) 
    
The amount of variance explained by each of the selected components.
Equal to n_components largest eigenvalues of the covariance matrix of X. 

[explained_variance_ratio_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1421)array, shape (n_components,) 
    
Percentage of variance explained by each of the selected components.
If `n_components` is not set then all components are stored and the sum of explained variances is equal to 1.0. 

[singular_values_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1423)array, shape (n_components,) 
    
The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the `n_components` variables in the lower-dimensional space. 

[mean_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1425)array, shape (n_features,) 
    
Per-feature empirical mean, estimated from the training set.
Equal to X.mean(axis=0). 

[n_components_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1427)int 
    
The estimated number of components. When n_components is set to ‚Äòmle‚Äô or a number between 0 and 1 (with svd_solver == ‚Äòfull‚Äô) this number is estimated from input data. Otherwise it equals the parameter n_components, or n_features if n_components is None. 

[noise_variance_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1429)float 
    
The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See ‚ÄúPattern Recognition and Machine Learning‚Äù by C. Bishop, 12.2.1 p. 574 or <http://www.miketipping.com/papers/met-mppca.pdf>. It is required to computed the estimated data covariance and score samples.
Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1431)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1433)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1435)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id795 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id796 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html#PCA.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id797 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id798 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

_property_ explained_variance_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.explained_variance_ "Link to this definition") 
    
The amount of variance explained by each of the selected components.
Equal to n_components largest eigenvalues of the covariance matrix of X.
Decorator for scikit-learn PCA attributes. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html#PCA.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id799 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id800 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id801 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id802 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id803 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id804 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id805 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id806 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

_property_ noise_variance_[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.noise_variance_ "Link to this definition") 
    
The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See ‚ÄúPattern Recognition and Machine Learning‚Äù by C. Bishop, 12.2.1 p. 574 or <http://www.miketipping.com/papers/met-mppca.pdf>. It is required to computed the estimated data covariance and score samples.
Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.
Decorator for scikit-learn PCA attributes. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id807 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id808 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id810 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id811 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id813 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id814 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id815 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id816 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id817 "Link to this heading")
self : object
###### pyod.models.qmcd module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.qmcd "Link to this heading")
Quasi-Monte Carlo Discrepancy outlier detection (QMCD) 

_class_ pyod.models.qmcd.QMCD(_contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html#QMCD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector") 

The Wrap-around Quasi-Monte Carlo discrepancy is a uniformity criterion 
    
which is used to assess the space filling of a number of samples in a hypercube. It quantifies the distance between the continuous uniform distribution on a hypercube and the discrete uniform distribution on distinct sample points. Therefore, lower discrepancy values for a sample point indicates that it provides better coverage of the parameter space with regard to the rest of the samples. This method is kernel based and a higher discrepancy score is relative to the rest of the samples, the higher the likelihood of it being an outlier. Read more in the [[BFM01](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1165 "Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. Journal of complexity, 17\(4\):608‚Äì624, 2001.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id819 "Link to this heading")
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id820 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1437)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1439)float 
    
The modified z-score to use as a threshold. Observations with a modified z-score (based on the median absolute deviation) greater than this value will be classified as outliers. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1441)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id821 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id822 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html#QMCD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id823 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The independent and dependent/target samples with the target samples being the last column of the numpy array such that eg: X = np.append(x, y.reshape(-1,1), axis=1). Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id824 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html#QMCD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit "Link to this definition") 
    
Fit detector
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id825 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id826 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id827 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id828 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id829 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id830 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id831 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id832 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id833 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id835 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id836 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id838 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id839 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id840 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id841 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id842 "Link to this heading")
self : object
###### pyod.models.rgraph module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rgraph "Link to this heading")
R-graph 

_class_ pyod.models.rgraph.RGraph(_transition_steps =10_, _n_nonzero =10_, _gamma =50.0_, _gamma_nz =True_, _algorithm ='lasso_lars'_, _tau =1.0_, _maxiter_lasso =1000_, _preprocessing =True_, _contamination =0.1_, _blocksize_test_data =10_, _support_init ='L2'_, _maxiter =40_, _support_size =100_, _active_support =True_, _fit_intercept_LR =False_, _verbose =True_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#RGraph)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Outlier Detection via R-graph. Paper: <https://openaccess.thecvf.com/content_cvpr_2017/papers/You_Provable_Self-Representation_Based_CVPR_2017_paper.pdf> See [[BYRV17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1162 "Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3395‚Äì3404. 2017.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id844 "Link to this heading") 

transition_stepsint, optional (default=20) 
    
Number of transition steps that are taken in the graph, after which the outlier scores are determined.
gamma : float 

gamma_nzboolean, default True 
    
gamma and gamma_nz together determines the parameter alpha. When `gamma_nz = False`, alpha = gamma. When `gamma_nz = True`, then alpha = gamma * alpha0, where alpha0 is the largest number such that the solution to the optimization problem with alpha = alpha0 is the zero vector (see Proposition 1 in [1]). Therefore, when `gamma_nz = True`, gamma should be a value greater than 1.0. A good choice is typically in the range [5, 500]. 

taufloat, default 1.0 
    
Parameter for elastic net penalty term. When tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2]. When tau = 0.0, the method reduces to least squares regression (LSR). 

algorithmstring, default `lasso_lars` 
    
Algorithm for computing the representation. Either lasso_lars or lasso_cd. Note: `lasso_lars` and `lasso_cd` only support tau = 1. For cases tau << 1 linear regression is used. 

fit_intercept_LR: bool, optional (default=False)
    
For `gamma` > 10000 linear regression is used instead of `lasso_lars` or `lasso_cd`. This parameter determines whether the intercept for the model is calculated. 

maxiter_lassoint, default 1000 
    
The maximum number of iterations for `lasso_lars` and `lasso_cd`. 

n_nonzeroint, default 50 
    
This is an upper bound on the number of nonzero entries of each representation vector. If there are more than n_nonzero nonzero entries, only the top n_nonzero number of entries with largest absolute value are kept. 

active_support: boolean, default True
    
Set to True to use the active support algorithm in [1] for solving the optimization problem. This should significantly reduce the running time when n_samples is large. 

active_support_params: dictionary of string to any, optional
    
Parameters (keyword arguments) and values for the active support algorithm. It may be used to set the parameters `support_init`, `support_size` and `maxiter`, see `active_support_elastic_net` for details. Example: active_support_params={‚Äòsupport_size‚Äô:50, ‚Äòmaxiter‚Äô:100} Ignored when `active_support=False` 

preprocessingbool, optional (default=True) 
    
If True, apply standardization on the data. 

verboseint, optional (default=1) 
    
Verbosity mode.
  * 0 = silent
  * 1 = progress bar
  * 2 = one line per epoch.


For verbose >= 1, model summary may be printed. 

random_staterandom_state: int, RandomState instance or None, optional 
    
(default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. 

blocksize_test_data: int, optional (default=10)
    
Test set is splitted into blocks of the size `blocksize_test_data` to at least partially separate test - and train set
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id845 "Link to this heading") 

[transition_matrix_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1443)numpy array of shape (n_samples,) 
    
Transition matrix from the last fitted data, this might include training + test data 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1445)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1447)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1449)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

active_support_elastic_net(_X_ , _y_ , _alpha_ , _tau =1.0_, _algorithm ='lasso_lars'_, _support_init ='L2'_, _support_size =100_, _maxiter =40_, _maxiter_lasso =1000_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#RGraph.active_support_elastic_net)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.active_support_elastic_net "Link to this definition") 
     

Source: <https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py> 
    
An active support based algorithm for solving the elastic net optimization problem min_{c} tau ||c||_1 + (1-tau)/2 ||c||_2^2 + alpha / 2 ||y - c X ||_2^2.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id846 "Link to this heading")
X : array-like, shape (n_samples, n_features)
y : array-like, shape (1, n_features)
alpha : float
tau : float, default 1.0 

algorithmstring, default `spams` 
    
Algorithm for computing solving the subproblems. Either lasso_lars or lasso_cd or spams (installation of spams package is required). Note: `lasso_lars` and `lasso_cd` only support tau = 1. 

support_init: string, default `knn` 
    
This determines how the active support is initialized. It can be either `knn` or `L2`. 

support_size: int, default 100
    
This determines the size of the working set. A small support_size decreases the runtime per iteration while increase the number of iterations. 

maxiter: int default 40
    
Termination condition for active support update.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id847 "Link to this heading") 

cshape n_samples 
    
The optimal solution to the optimization problem. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id848 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id849 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#RGraph.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id850 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id851 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

elastic_net_subspace_clustering(_X_ , _gamma =50.0_, _gamma_nz =True_, _tau =1.0_, _algorithm ='lasso_lars'_, _fit_intercept_LR =False_, _active_support =True_, _active_support_params =None_, _n_nonzero =50_, _maxiter_lasso =1000_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#RGraph.elastic_net_subspace_clustering)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering "Link to this definition") 
    
Source: <https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py>
Elastic net subspace clustering (EnSC) [1]. Compute self-representation matrix C from solving the following optimization problem min_{c_j} tau ||c_j||_1 + (1-tau)/2 ||c_j||_2^2 + alpha / 2 ||x_j - c_j X ||_2^2 s.t. c_jj = 0, where c_j and x_j are the j-th rows of C and X, respectively.
Parameter `algorithm` specifies the algorithm for solving the optimization problem. `lasso_lars` and `lasso_cd` are algorithms implemented in sklearn, `spams` refers to the same algorithm as `lasso_lars` but is implemented in spams package available at <http://spams-devel.gforge.inria.fr/> (installation required) In principle, all three algorithms give the same result. For large scale data (e.g. with > 5000 data points), use any of these algorithms in conjunction with `active_support=True`. It adopts an efficient active support strategy that solves the optimization problem by breaking it into a sequence of small scale optimization problems as described in [1]. If tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2]. If tau = 0.0, the method reduces to least squares regression (LSR) [3]. Note: `lasso_lars` and `lasso_cd` only support tau = 1. Parameters ‚Äî‚Äî‚Äî‚Äì X : array-like, shape (n_samples, n_features)
> Input data to be clustered
gamma : float gamma_nz : boolean, default True
> gamma and gamma_nz together determines the parameter alpha. When `gamma_nz = False`, alpha = gamma. When `gamma_nz = True`, then alpha = gamma * alpha0, where alpha0 is the largest number such that the solution to the optimization problem with alpha = alpha0 is the zero vector (see Proposition 1 in [1]). Therefore, when `gamma_nz = True`, gamma should be a value greater than 1.0. A good choice is typically in the range [5, 500]. 

taufloat, default 1.0 
    
Parameter for elastic net penalty term. When tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2]. When tau = 0.0, the method reduces to least squares regression (LSR) [3]. 

algorithmstring, default `lasso_lars` 
    
Algorithm for computing the representation. Either lasso_lars or lasso_cd or spams (installation of spams package is required). Note: `lasso_lars` and `lasso_cd` only support tau = 1. 

n_nonzeroint, default 50 
    
This is an upper bound on the number of nonzero entries of each representation vector. If there are more than n_nonzero nonzero entries, only the top n_nonzero number of entries with largest absolute value are kept. 

active_support: boolean, default True
    
Set to True to use the active support algorithm in [1] for solving the optimization problem. This should significantly reduce the running time when n_samples is large. 

active_support_params: dictionary of string to any, optional
    
Parameters (keyword arguments) and values for the active support algorithm. It may be used to set the parameters `support_init`, `support_size` and `maxiter`, see `active_support_elastic_net` for details. Example: active_support_params={‚Äòsupport_size‚Äô:50, ‚Äòmaxiter‚Äô:100} Ignored when `active_support=False`
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id852 "Link to this heading") 

[representation_matrix_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1451)csr matrix, shape: n_samples by n_samples 
    
The self-representation matrix.
############ References[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#references "Link to this heading")
[1] C. You, C.-G. Li, D. Robinson, R. Vidal, Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering, CVPR 2016 [2] E. Elhaifar, R. Vidal, Sparse Subspace Clustering: Algorithm, Theory, and Applications, TPAMI 2013 [3] C. Lu, et al. Robust and efficient subspace segmentation via least squares regression, ECCV 2012 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#RGraph.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id853 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id854 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id855 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id856 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id857 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id858 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id859 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id860 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id861 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id863 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id864 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id866 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id867 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id868 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id869 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id870 "Link to this heading")
self : object
###### pyod.models.rod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod "Link to this heading")
Rotation-based Outlier Detector (ROD) 

_class_ pyod.models.rod.ROD(_contamination =0.1_, _parallel_execution =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html#ROD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Rotation-based Outlier Detection (ROD), is a robust and parameter-free algorithm that requires no statistical distribution assumptions and works intuitively in three-dimensional space, where the 3D-vectors, representing the data points, are rotated about the geometric median two times counterclockwise using Rodrigues rotation formula. The results of the rotation are parallelepipeds where their volumes are mathematically analyzed as cost functions and used to calculate the Median Absolute Deviations to obtain the outlying score. For high dimensions > 3, the overall score is calculated by taking the average of the overall 3D-subspaces scores, that were resulted from decomposing the original data space. See [[BABC20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1149 "Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. IEEE Transactions on Knowledge and Data Engineering, 2020.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id872 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

parallel_execution: bool, optional (default=False).
    
If set to True, the algorithm will run in parallel, for a better execution time. It is recommended to set this parameter to True ONLY for high dimensional data > 10, and if a proper hardware is available.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id873 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1453)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1455)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1457)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id874 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id875 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html#ROD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id876 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id877 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html#ROD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id878 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id879 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id880 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id881 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id882 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id883 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id884 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id885 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id886 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id887 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id889 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id890 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id892 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id893 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id894 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id895 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id896 "Link to this heading")
self : object
###### pyod.models.sampling module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sampling "Link to this heading")
Outlier detection based on Sampling (SP) 

_class_ pyod.models.sampling.Sampling(_contamination =0.1_, _subset_size =20_, _metric ='minkowski'_, _metric_params =None_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html#Sampling)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Sampling class for outlier detection.
Sugiyama, M., Borgwardt, K. M.: Rapid Distance-Based Outlier Detection via Sampling, Advances in Neural Information Processing Systems (NIPS 2013), 467-475, 2013.
See [[BSB13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1157 "Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. Advances in neural information processing systems, 2013.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id898 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

subset_sizefloat in (0., 1.0) or int (0, n_samples), optional (default=20) 
    
The size of subset of the data set. Sampling subset from the data set is performed only once. 

metricstring or callable, default ‚Äòminkowski‚Äô 
    
metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy‚Äôs metrics, but is less efficient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
  * from scikit-learn: [‚Äòcityblock‚Äô, ‚Äòcosine‚Äô, ‚Äòeuclidean‚Äô, ‚Äòl1‚Äô, ‚Äòl2‚Äô, ‚Äòmanhattan‚Äô]
  * from scipy.spatial.distance: [‚Äòbraycurtis‚Äô, ‚Äòcanberra‚Äô, ‚Äòchebyshev‚Äô, ‚Äòcorrelation‚Äô, ‚Äòdice‚Äô, ‚Äòhamming‚Äô, ‚Äòjaccard‚Äô, ‚Äòkulsinski‚Äô, ‚Äòmahalanobis‚Äô, ‚Äòmatching‚Äô, ‚Äòminkowski‚Äô, ‚Äòrogerstanimoto‚Äô, ‚Äòrussellrao‚Äô, ‚Äòseuclidean‚Äô, ‚Äòsokalmichener‚Äô, ‚Äòsokalsneath‚Äô, ‚Äòsqeuclidean‚Äô, ‚Äòyule‚Äô]


See the documentation for scipy.spatial.distance for details on these metrics. 

metric_paramsdict, optional (default = None) 
    
Additional keyword arguments for the metric function. 

random_stateint, RandomState instance or None, optional (default None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id899 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1459)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1461)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1463)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id900 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id901 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html#Sampling.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id902 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The test input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id903 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html#Sampling.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id904 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id905 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id906 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id907 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id908 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id909 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id910 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id911 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id912 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id913 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id915 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id916 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id918 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id919 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id920 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id921 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id922 "Link to this heading")
self : object
###### pyod.models.sod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod "Link to this heading")
Subspace Outlier Detection (SOD) 

_class_ pyod.models.sod.SOD(_contamination =0.1_, _n_neighbors =20_, _ref_set =10_, _alpha =0.8_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html#SOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Subspace outlier detection (SOD) schema aims to detect outlier in varying subspaces of a high dimensional feature space. For each data object, SOD explores the axis-parallel subspace spanned by the data object‚Äôs neighbors and determines how much the object deviates from the neighbors in this subspace.
See [[BKKrogerSZ09](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1139 "Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, 831‚Äì838. Springer, 2009.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id924 "Link to this heading") 

n_neighborsint, optional (default=20) 
    
Number of neighbors to use by default for k neighbors queries. 

ref_set: int, optional (default=10)
    
specifies the number of shared nearest neighbors to create the reference set. Note that ref_set must be smaller than n_neighbors. 

alpha: float in (0., 1.), optional (default=0.8)
    
specifies the lower limit for selecting subspace. 0.8 is set as default as suggested in the original paper. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id925 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1465)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1467)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1469)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id926 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id927 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html#SOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector. The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id928 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id929 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html#SOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id930 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id931 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id932 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id933 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id934 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id935 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id936 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id937 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id938 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id939 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id941 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id942 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id944 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id945 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id946 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id947 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id948 "Link to this heading")
self : object
###### pyod.models.so_gaal module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.so_gaal "Link to this heading")
Single-Objective Generative Adversarial Active Learning. Part of the codes are adapted from <https://github.com/leibinghe/GAAL-based-outlier-detection> 

_class_ pyod.models.so_gaal.SO_GAAL(_stop_epochs =20_, _lr_d =0.01_, _lr_g =0.0001_, _momentum =0.9_, _contamination =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html#SO_GAAL)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Single-Objective Generative Adversarial Active Learning.
SO-GAAL directly generates informative potential outliers to assist the classifier in describing a boundary that can separate outliers from normal data effectively. Moreover, to prevent the generator from falling into the mode collapsing problem, the network structure of SO-GAAL is expanded from a single generator (SO-GAAL) to multiple generators with different objectives (MO-GAAL) to generate a reasonable reference distribution for the whole dataset. Read more in the [[BLLZ+19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1132 "Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 2019.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id950 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

stop_epochsint, optional (default=20) 
     

The number of epochs of training. The number of total epochs equals to
    
three times of stop_epochs. 

lr_dfloat, optional (default=0.01) 
    
The learn rate of the discriminator. 

lr_gfloat, optional (default=0.0001) 
    
The learn rate of the generator. 

momentumfloat, optional (default=0.9) 
    
The momentum parameter for SGD.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id951 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1471)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1473)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1475)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id952 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id953 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html#SO_GAAL.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id954 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id955 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html#SO_GAAL.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id956 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id957 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id958 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id959 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id960 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id961 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id962 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id963 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id964 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id965 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id967 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id968 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id970 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id971 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id972 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id973 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id974 "Link to this heading")
self : object
###### pyod.models.sos module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos "Link to this heading")
Stochastic Outlier Selection (SOS). Part of the codes are adapted from <https://github.com/jeroenjanssens/scikit-sos> 

_class_ pyod.models.sos.SOS(_contamination =0.1_, _perplexity =4.5_, _metric ='euclidean'_, _eps =1e-05_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html#SOS)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
Stochastic Outlier Selection.
SOS employs the concept of affinity to quantify the relationship from one data point to another data point. Affinity is proportional to the similarity between two data points. So, a data point has little affinity with a dissimilar data point. A data point is selected as an outlier when all the other data points have insufficient affinity with it. Read more in the [[BJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1129 "JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id976 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1)  
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

perplexityfloat, optional (default=4.5) 
    
A smooth measure of the effective number of neighbours. The perplexity parameter is similar to the parameter k in kNN algorithm (the number of nearest neighbors). The range of perplexity can be any real number between 1 and n-1, where n is the number of samples. 

metric: str, default ‚Äòeuclidean‚Äô
    
Metric used for the distance computation. Any metric from scipy.spatial.distance can be used.
Valid values for metric are:
  * ‚Äòeuclidean‚Äô
  * from scipy.spatial.distance: [‚Äòbraycurtis‚Äô, ‚Äòcanberra‚Äô, ‚Äòchebyshev‚Äô, ‚Äòcorrelation‚Äô, ‚Äòdice‚Äô, ‚Äòhamming‚Äô, ‚Äòjaccard‚Äô, ‚Äòkulsinski‚Äô, ‚Äòmahalanobis‚Äô, ‚Äòmatching‚Äô, ‚Äòminkowski‚Äô, ‚Äòrogerstanimoto‚Äô, ‚Äòrussellrao‚Äô, ‚Äòseuclidean‚Äô, ‚Äòsokalmichener‚Äô, ‚Äòsokalsneath‚Äô, ‚Äòsqeuclidean‚Äô, ‚Äòyule‚Äô]


See the documentation for scipy.spatial.distance for details on these metrics: <http://docs.scipy.org/doc/scipy/reference/spatial.distance.html> 

epsfloat, optional (default = 1e-5) 
    
Tolerance threshold for floating point errors.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id977 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1477)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1479)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1481)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`.
######### Examples[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id978 "Link to this heading")
```
>>> frompyod.models.sosimport SOS
>>> frompyod.utils.dataimport generate_data
>>> n_train = 50
>>> n_test = 50
>>> contamination = 0.1
>>> X_train, y_train, X_test, y_test = generate_data(
...     n_train=n_train, n_test=n_test,
...     contamination=contamination, random_state=42)
>>>
>>> clf = SOS()
>>> clf.fit(X_train)
SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5)

```


compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id979 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id980 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html#SOS.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id981 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id982 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html#SOS.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id983 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id984 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id985 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id986 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id987 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id988 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id989 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id990 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id991 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id992 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id994 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id995 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id997 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id998 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id999 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1000 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1001 "Link to this heading")
self : object
###### pyod.models.suod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.suod "Link to this heading")
SUOD 

_class_ pyod.models.suod.SUOD(_base_estimators =None_, _contamination =0.1_, _combination ='average'_, _n_jobs =None_, _rp_clf_list =None_, _rp_ng_clf_list =None_, _rp_flag_global =True_, _target_dim_frac =0.5_, _jl_method ='basic'_, _bps_flag =True_, _approx_clf_list =None_, _approx_ng_clf_list =None_, _approx_flag_global =True_, _approx_clf =None_, _verbose =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html#SUOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
SUOD (Scalable Unsupervised Outlier Detection) is an acceleration framework for large scale unsupervised outlier detector training and prediction. See [[BZHC+21](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1150 "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. Proceedings of Machine Learning and Systems, 2021.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1003 "Link to this heading") 

base_estimatorslist, length must be greater than 1 
    
A list of base estimators. Certain methods must be present, e.g., fit and predict. 

combinationstr, optional (default=‚Äôaverage‚Äô) 
    
Decide how to aggregate the results from multiple models:
  * ‚Äúaverage‚Äù : average the results from all base detectors
  * ‚Äúmaximization‚Äù : output the max value across all base detectors



contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

n_jobsoptional (default=1) 
    
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the the number of jobs that can actually run in parallel. 

rp_clf_listlist, optional (default=None) 
    
The list of outlier detection models to use random projection. The detector name should be consistent with PyOD. 

rp_ng_clf_listlist, optional (default=None) 
    
The list of outlier detection models NOT to use random projection. The detector name should be consistent with PyOD. 

rp_flag_globalbool, optional (default=True) 
    
If set to False, random projection is turned off for all base models. 

target_dim_fracfloat in (0., 1), optional (default=0.5) 
    
The target compression ratio. 

jl_methodstring, optional (default = ‚Äòbasic‚Äô) 
    
The JL projection method:
  * ‚Äúbasic‚Äù: each component of the transformation matrix is taken at random in N(0,1).
  * ‚Äúdiscrete‚Äù, each component of the transformation matrix is taken at random in {-1,1}.
  * ‚Äúcirculant‚Äù: the first row of the transformation matrix is taken at random in N(0,1), and each row is obtained from the previous one by a one-left shift.
  * ‚Äútoeplitz‚Äù: the first row and column of the transformation matrix is taken at random in N(0,1), and each diagonal has a constant value taken from these first vector.



bps_flagbool, optional (default=True) 
    
If set to False, balanced parallel scheduling is turned off. 

approx_clf_listlist, optional (default=None) 
    
The list of outlier detection models to use pseudo-supervised approximation. The detector name should be consistent with PyOD. 

approx_ng_clf_listlist, optional (default=None) 
    
The list of outlier detection models NOT to use pseudo-supervised approximation. The detector name should be consistent with PyOD. 

approx_flag_globalbool, optional (default=True) 
    
If set to False, pseudo-supervised approximation is turned off. 

approx_clfobject, optional (default: sklearn RandomForestRegressor) 
    
The supervised model used to approximate unsupervised models. 

cost_forecast_loc_fitstr, optional 
    
The location of the pretrained cost prediction forecast for training. 

cost_forecast_loc_predstr, optional 
    
The location of the pretrained cost prediction forecast for prediction. 

verboseint, optional (default=0) 
    
Controls the verbosity of the building process.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1004 "Link to this heading") 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1483)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1485)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1487)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1005 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1006 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html#SUOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detectors.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1007 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1008 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html#SUOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1009 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1010 "Link to this heading") 

selfobject 
    
Fitted estimator. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1011 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1012 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1013 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1014 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1015 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1016 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1017 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1018 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1020 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1021 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1023 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1024 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1025 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1026 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1027 "Link to this heading")
self : object
###### pyod.models.thresholds module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-thresholds-module "Link to this heading") 

pyod.models.thresholds.AUCP(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#AUCP)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.AUCP "Link to this definition") 
    
AUCP class for Area Under Curve Precentage thresholder.
Use the area under the curve to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond where the auc of the kde is less than the (mean + abs(mean-median)) percent of the total kde auc. 

pyod.models.thresholds.BOOT(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#BOOT)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.BOOT "Link to this definition") 
    
BOOT class for Bootstrapping thresholder.
Use a boostrapping based method to find a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the mean of the confidence intervals.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1028 "Link to this heading") 

random_stateint, optional (default=1234) 
    
Random seed for bootstrapping a confidence interval. Can also be set to None. 

pyod.models.thresholds.CHAU(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#CHAU)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CHAU "Link to this definition") 
    
CHAU class for Chauvenet‚Äôs criterion thresholder.
Use the Chauvenet‚Äôs criterion to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value below the Chauvenet‚Äôs criterion.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1029 "Link to this heading") 

method{‚Äòmean‚Äô, ‚Äòmedian‚Äô, ‚Äògmean‚Äô}, optional (default=‚Äômean‚Äô) 
    
Calculate the area normal to distance using a scaler
  * ‚Äòmean‚Äô: Construct a scaler with the mean of the scores
  * ‚Äòmedian: Construct a scaler with the median of the scores
  * ‚Äògmean‚Äô: Construct a scaler with the geometric mean of the scores



pyod.models.thresholds.CLF(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#CLF)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLF "Link to this definition") 
    
CLF class for Trained Classifier thresholder.
Use the trained linear classifier to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond 0.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1030 "Link to this heading") 

method{‚Äòsimple‚Äô, ‚Äòcomplex‚Äô}, optional (default=‚Äôcomplex‚Äô) 
    
Type of linear model
  * ‚Äòsimple‚Äô: Uses only the scores
  * ‚Äòcomplex‚Äô: Uses the scores, log of the scores, and the scores‚Äô PDF



pyod.models.thresholds.CLUST(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#CLUST)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLUST "Link to this definition") 
    
CLUST class for clustering type thresholders.
Use the clustering methods to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value not labelled as part of the main cluster.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1031 "Link to this heading") 

method{‚Äòagg‚Äô, ‚Äòbirch‚Äô, ‚Äòbang‚Äô, ‚Äòbgm‚Äô, ‚Äòbsas‚Äô, ‚Äòdbscan‚Äô, ‚Äòema‚Äô, ‚Äòkmeans‚Äô, ‚Äòmbsas‚Äô, ‚Äòmshift‚Äô, ‚Äòoptics‚Äô, ‚Äòsomsc‚Äô, ‚Äòspec‚Äô, ‚Äòxmeans‚Äô}, optional (default=‚Äôspec‚Äô) 
    
Clustering method
  * ‚Äòagg‚Äô: Agglomerative
  * ‚Äòbirch‚Äô: Balanced Iterative Reducing and Clustering using Hierarchies
  * ‚Äòbang‚Äô: BANG
  * ‚Äòbgm‚Äô: Bayesian Gaussian Mixture
  * ‚Äòbsas‚Äô: Basic Sequential Algorithmic Scheme
  * ‚Äòdbscan‚Äô: Density-based spatial clustering of applications with noise
  * ‚Äòema‚Äô: Expectation-Maximization clustering algorithm for Gaussian Mixture Model
  * ‚Äòkmeans‚Äô: K-means
  * ‚Äòmbsas‚Äô: Modified Basic Sequential Algorithmic Scheme
  * ‚Äòmshift‚Äô: Mean shift
  * ‚Äòoptics‚Äô: Ordering Points To Identify Clustering Structure
  * ‚Äòsomsc‚Äô: Self-organized feature map
  * ‚Äòspec‚Äô: Clustering to a projection of the normalized Laplacian
  * ‚Äòxmeans‚Äô: X-means



random_stateint, optional (default=1234) 
    
Random seed for the BayesianGaussianMixture clustering (method=‚Äôbgm‚Äô). Can also be set to None. 

pyod.models.thresholds.CPD(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#CPD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CPD "Link to this definition") 
    
CPD class for Change Point Detection thresholder.
Use change point detection to find a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the detected change point.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1032 "Link to this heading") 

method{‚ÄòDynp‚Äô, ‚ÄòKernelCPD‚Äô, ‚ÄòBinseg‚Äô, ‚ÄòBottomUp‚Äô}, optional (default=‚ÄôDynp‚Äô) 
    
Method for change point detection
  * ‚ÄòDynp‚Äô: Dynamic programming (optimal minimum sum of errors per partition)
  * ‚ÄòKernelCPD‚Äô: RBF kernel function (optimal minimum sum of errors per partition)
  * ‚ÄòBinseg‚Äô: Binary segmentation
  * ‚ÄòBottomUp‚Äô: Bottom-up segmentation



transform{‚Äòcdf‚Äô, ‚Äòkde‚Äô}, optional (default=‚Äôcdf‚Äô) 
    
Data transformation method prior to fit
  * ‚Äòcdf‚Äô: Use the cumulative distribution function
  * ‚Äòkde‚Äô: Use the kernel density estimation



pyod.models.thresholds.DECOMP(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#DECOMP)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DECOMP "Link to this definition") 
    
DECOMP class for Decomposition based thresholders.
Use decomposition to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the maximum of the decomposed matrix that results from decomposing the cumulative distribution function of the decision scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1033 "Link to this heading") 

method{‚ÄòNMF‚Äô, ‚ÄòPCA‚Äô, ‚ÄòGRP‚Äô, ‚ÄòSRP‚Äô}, optional (default=‚ÄôPCA‚Äô) 
    
Method to use for decomposition
  * ‚ÄòNMF‚Äô: Non-Negative Matrix Factorization
  * ‚ÄòPCA‚Äô: Principal Component Analysis
  * ‚ÄòGRP‚Äô: Gaussian Random Projection
  * ‚ÄòSRP‚Äô: Sparse Random Projection



random_stateint, optional (default=1234) 
    
Random seed for the decomposition algorithm. Can also be set to None. 

pyod.models.thresholds.DSN(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#DSN)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DSN "Link to this definition") 
    
DSN class for Distance Shift from Normal thresholder.
Use the distance shift from normal to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the distance calculated by the selected metric.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1034 "Link to this heading") 

metric{‚ÄòJS‚Äô, ‚ÄòWS‚Äô, ‚ÄòENG‚Äô, ‚ÄòBHT‚Äô, ‚ÄòHLL‚Äô, ‚ÄòHI‚Äô, ‚ÄòLK‚Äô, ‚ÄòLP‚Äô, ‚ÄòMAH‚Äô, ‚ÄòTMT‚Äô, ‚ÄòRES‚Äô, ‚ÄòKS‚Äô, ‚ÄòINT‚Äô, ‚ÄòMMD‚Äô}, optional (default=‚ÄôMAH‚Äô) 
    
Metric to use for distance computation
  * ‚ÄòJS‚Äô: Jensen-Shannon distance
  * ‚ÄòWS‚Äô: Wasserstein or Earth Movers distance
  * ‚ÄòENG‚Äô: Energy distance
  * ‚ÄòBHT‚Äô: Bhattacharyya distance
  * ‚ÄòHLL‚Äô: Hellinger distance
  * ‚ÄòHI‚Äô: Histogram intersection distance
  * ‚ÄòLK‚Äô: Lukaszyk-Karmowski metric for normal distributions
  * ‚ÄòLP‚Äô: Levy-Prokhorov metric
  * ‚ÄòMAH‚Äô: Mahalanobis distance
  * ‚ÄòTMT‚Äô: Tanimoto distance
  * ‚ÄòRES‚Äô: Studentized residual distance
  * ‚ÄòKS‚Äô: Kolmogorov-Smirnov distance
  * ‚ÄòINT‚Äô: Weighted spline interpolated distance
  * ‚ÄòMMD‚Äô: Maximum Mean Discrepancy distance



random_stateint, optional (default=1234) 
    
Random seed for the normal distribution. Can also be set to None. 

pyod.models.thresholds.EB(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#EB)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.EB "Link to this definition") 
    
EB class for Elliptical Boundary thresholder.
Use pseudo-random elliptical boundaries to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond a pseudo-random elliptical boundary set between inliers and outliers. 

pyod.models.thresholds.FGD(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#FGD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FGD "Link to this definition") 
    
FGD class for Fixed Gradient Descent thresholder.
Use the fixed gradient descent to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond where the first derivative of the kde with respect to the decision scores passes the mean of the first and second inflection points. 

pyod.models.thresholds.FILTER(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#FILTER)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FILTER "Link to this definition") 
    
FILTER class for Filtering based thresholders.
Use the filtering based methods to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the maximum filter value. See [[BHGPRR19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1169 "Navid Hashemi, Eduardo Verdugo German, Jonatan Pena Ramirez, and Justin Ruths. Filtering approaches for dealing with noise in anomaly detection. In 2019 IEEE 58th Conference on Decision and Control \(CDC\), 5356‚Äì5361. IEEE, December 2019. URL: http://dx.doi.org/10.1109/CDC40024.2019.9029258, doi:10.1109/cdc40024.2019.9029258.")] for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1036 "Link to this heading") 

method{‚Äògaussian‚Äô, ‚Äòsavgol‚Äô, ‚Äòhilbert‚Äô, ‚Äòwiener‚Äô, ‚Äòmedfilt‚Äô, ‚Äòdecimate‚Äô,‚Äôdetrend‚Äô, ‚Äòresample‚Äô}, optional (default=‚Äôsavgol‚Äô) 
    
Method to filter the scores
  * ‚Äògaussian‚Äô: use a gaussian based filter
  * ‚Äòsavgol‚Äô: use the savgol based filter
  * ‚Äòhilbert‚Äô: use the hilbert based filter
  * ‚Äòwiener‚Äô: use the wiener based filter
  * ‚Äòmedfilt: use a median based filter
  * ‚Äòdecimate‚Äô: use a decimate based filter
  * ‚Äòdetrend‚Äô: use a detrend based filter
  * ‚Äòresample‚Äô: use a resampling based filter



sigmaint, optional (default=‚Äôauto‚Äô)  
    
Variable specific to each filter type, default sets sigma to len(scores)*np.std(scores)
  * ‚Äògaussian‚Äô: standard deviation for Gaussian kernel
  * ‚Äòsavgol‚Äô: savgol filter window size
  * ‚Äòhilbert‚Äô: number of Fourier components
  * ‚Äòmedfilt: kernel size
  * ‚Äòdecimate‚Äô: downsampling factor
  * ‚Äòdetrend‚Äô: number of break points
  * ‚Äòresample‚Äô: resampling window size



pyod.models.thresholds.FWFM(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#FWFM)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FWFM "Link to this definition") 
    
FWFM class for Full Width at Full Minimum thresholder.
Use the full width at full minimum (aka base width) to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the base width. 

pyod.models.thresholds.GESD(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#GESD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.GESD "Link to this definition") 
    
GESD class for Generalized Extreme Studentized Deviate thresholder.
Use the generalized extreme studentized deviate to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any less than the smallest detected outlier.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1037 "Link to this heading") 

max_outliersint, optional (default=‚Äôauto‚Äô) 
    
mamiximum number of outliers that the dataset may have. Default sets max_outliers to be half the size of the dataset 

alphafloat, optional (default=0.05) 
    
significance level 

pyod.models.thresholds.HIST(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#HIST)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.HIST "Link to this definition") 
    
HIST class for Histogram based thresholders.
Use histograms methods as described in scikit-image.filters to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set by histogram generated thresholds depending on the selected methods.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1038 "Link to this heading") 

nbinsint, optional (default=‚Äôauto‚Äô) 
    
Number of bins to use in the hostogram, default set to int(len(scores)**0.7) 

method{‚Äòotsu‚Äô, ‚Äòyen‚Äô, ‚Äòisodata‚Äô, ‚Äòli‚Äô, ‚Äòminimum‚Äô, ‚Äòtriangle‚Äô}, optional (default=‚Äôtriangle‚Äô) 
    
Histogram filtering based method
  * ‚Äòotsu‚Äô: OTSU‚Äôs method for filtering
  * ‚Äòyen‚Äô: Yen‚Äôs method for filtering
  * ‚Äòisodata‚Äô: Ridler-Calvard or inter-means method for filtering
  * ‚Äòli‚Äô: Li‚Äôs iterative Minimum Cross Entropy method for filtering
  * ‚Äòminimum‚Äô: Minimum between two maxima via smoothing method for filtering
  * ‚Äòtriangle‚Äô: Triangle algorithm method for filtering



pyod.models.thresholds.IQR(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#IQR)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.IQR "Link to this definition") 
    
IQR class for Inter-Qaurtile Region thresholder.
Use the inter-quartile region to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the third quartile plus 1.5 times the inter-quartile region. 

pyod.models.thresholds.KARCH(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#KARCH)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.KARCH "Link to this definition") 
    
KARCH class for Riemannian Center of Mass thresholder.
Use the Karcher mean (Riemannian Center of Mass) to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the Karcher mean + one standard deviation of the decision_scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1039 "Link to this heading") 

ndimint, optional (default=2) 
    
Number of dimensions to construct the Euclidean manifold 

method{‚Äòsimple‚Äô, ‚Äòcomplex‚Äô}, optional (default=‚Äôcomplex‚Äô) 
    
Method for computing the Karcher mean
  * ‚Äòsimple‚Äô: Compute the Karcher mean using the 1D array of scores
  * ‚Äòcomplex‚Äô: Compute the Karcher mean between a 2D array dot product of the scores and the sorted scores arrays



pyod.models.thresholds.MAD(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#MAD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MAD "Link to this definition") 
    
MAD class for Median Absolute Deviation thresholder.
Use the median absolute deviation to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the mean plus the median absolute deviation over the standard deviation. 

pyod.models.thresholds.MCST(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#MCST)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MCST "Link to this definition") 
    
MCST class for Monte Carlo Shapiro Tests thresholder.
Use uniform random sampling and statstical testing to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the minimum value left after iterative Shapiro-Wilk tests have occured. Note** accuracy decreases with array size. For good results the should be array<1000. However still this threshold method may fail at any array size.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1040 "Link to this heading") 

random_stateint, optional (default=1234) 
    
Random seed for the uniform distribution. Can also be set to None. 

pyod.models.thresholds.META(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#META)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.META "Link to this definition") 
    
META class for Meta-modelling thresholder.
Use a trained meta-model to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set based on the trained meta-model classifier.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1041 "Link to this heading") 

method{‚ÄòLIN‚Äô, ‚ÄòGNB‚Äô, ‚ÄòGNBC‚Äô, ‚ÄòGNBM‚Äô}, optional (default=‚ÄôGNBM‚Äô) 
    
select
  * ‚ÄòLIN‚Äô: RidgeCV trained linear classifier meta-model on true labels
  * ‚ÄòGNB‚Äô: Gaussian Naive Bayes trained classifier meta-model on true labels
  * ‚ÄòGNBC‚Äô: Gaussian Naive Bayes trained classifier meta-model on best contamination
  * ‚ÄòGNBM‚Äô: Gaussian Naive Bayes multivariate trained classifier meta-model



pyod.models.thresholds.MOLL(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#MOLL)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MOLL "Link to this definition") 
    
MOLL class for Friedrichs‚Äô mollifier thresholder.
Use the Friedrichs‚Äô mollifier to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond one minus the maximum of the smoothed dataset via convolution. 

pyod.models.thresholds.MTT(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#MTT)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MTT "Link to this definition") 
    
MTT class for Modified Thompson Tau test thresholder.
Use the modified Thompson Tau test to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the smallest outlier detected by the test.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1042 "Link to this heading") 

strictness[1,2,3,4,5], optional (default=4) 
    
Level of strictness corresponding to the t-Student distribution map to sample 

pyod.models.thresholds.OCSVM(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#OCSVM)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.OCSVM "Link to this definition") 
    
OCSVM class for One-Class Support Vector Machine thresholder.
Use a one-class svm to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are determined by the one-class svm using a polynomial kernel with the polynomial degree either set or determined by regression internally.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1043 "Link to this heading") 

model{‚Äòpoly‚Äô, ‚Äòsgd‚Äô}, optional (default=‚Äôsgd‚Äô) 
    
OCSVM model to apply
  * ‚Äòpoly‚Äô: Use a polynomial kernel with a regular OCSVM
  * ‚Äòsgd‚Äô: Used the Additive Chi2 kernel approximation with a SGDOneClassSVM



degreeint, optional (default=‚Äôauto‚Äô) 
    
Polynomial degree to use for the one-class svm. Default ‚Äòauto‚Äô finds the optimal degree with linear regression 

gammafloat, optional (default=‚Äôauto‚Äô) 
    
Kernel coefficient for polynomial fit for the one-class svm. Default ‚Äòauto‚Äô uses 1 / n_features 

criterion{‚Äòaic‚Äô, ‚Äòbic‚Äô}, optional (default=‚Äôbic‚Äô) 
    
regression performance metric. AIC is the Akaike Information Criterion, and BIC is the Bayesian Information Criterion. This only applies when degree is set to ‚Äòauto‚Äô 

nufloat, optional (default=‚Äôauto‚Äô) 
    
An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Default ‚Äòauto‚Äô sets nu as the ratio between the any point that is less than or equal to the median plus the absolute difference between the mean and geometric mean over the the number of points in the entire dataset 

tolfloat, optional (default=1e-3) 
    
The stopping criterion for the one-class svm 

random_stateint, optional (default=1234) 
    
Random seed for the SVM‚Äôs data sampling. Can also be set to None. 

pyod.models.thresholds.QMCD(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#QMCD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.QMCD "Link to this definition") 
    
QMCD class for Quasi-Monte Carlo Discreprancy thresholder.
Use the quasi-Monte Carlo discreprancy to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond and percentile or quantile of one minus the descreperancy (Note** A discrepancy quantifies the distance between the continuous uniform distribution on a hypercube and the discrete uniform distribution on distinct sample points).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1044 "Link to this heading") 

method{‚ÄòCD‚Äô, ‚ÄòWD‚Äô, ‚ÄòMD‚Äô, ‚ÄòL2-star‚Äô}, optional (default=‚ÄôWD‚Äô) 
    
Type of discrepancy
  * ‚ÄòCD‚Äô: Centered Discrepancy
  * ‚ÄòWD‚Äô: Wrap-around Discrepancy
  * ‚ÄòMD‚Äô: Mix between CD/WD
  * ‚ÄòL2-star‚Äô: L2-star discrepancy



lim{‚ÄòQ‚Äô, ‚ÄòP‚Äô}, optional (default=‚ÄôP‚Äô) 
    
Filtering method to threshold scores using 1 - discrepancy
  * ‚ÄòQ‚Äô: Use quntile limiting
  * ‚ÄòP‚Äô: Use percentile limiting



pyod.models.thresholds.REGR(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#REGR)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.REGR "Link to this definition") 
    
REGR class for Regression based thresholder.
Use the regression to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the y-intercept value of the linear fit.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1045 "Link to this heading") 

method{‚Äòsiegel‚Äô, ‚Äòtheil‚Äô}, optional (default=‚Äôsiegel‚Äô) 
    
Regression based method to calculate the y-intercept
  * ‚Äòsiegel‚Äô: implements a method for robust linear regression using repeated medians
  * ‚Äòtheil‚Äô: implements a method for robust linear regression using paired values



random_stateint, optional (default=1234) 
    
random seed for the normal distribution. Can also be set to None 

pyod.models.thresholds.VAE(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#VAE)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.VAE "Link to this definition") 
    
VAE class for Variational AutoEncoder thresholder.
Use a VAE to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the maximum minus the minimum of the reconstructed distribution probabilities after encoding.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1046 "Link to this heading") 

verbosebool, optional (default=False) 
    
display training progress 

devicestr, optional (default=‚Äôcpu‚Äô) 
    
device for pytorch 

latent_dimsint, optional (default=‚Äôauto‚Äô) 
    
number of latent dimensions the encoder will map the scores to. Default ‚Äòauto‚Äô applies automatic dimensionality selection using a profile likelihood. 

random_stateint, optional (default=1234) 
    
random seed for the normal distribution. Can also be set to None 

epochsint, optional (default=100) 
    
number of epochs to train the VAE 

batch_sizeint, optional (default=64) 
    
batch size for the dataloader during training 

lossstr, optional (default=‚Äôkl‚Äô) 
    
Loss function during training
  * ‚Äòkl‚Äô : use the combined negative log likelihood and Kullback-Leibler divergence
  * ‚Äòmmd‚Äô: use the combined negative log likelihood and maximum mean discrepancy


######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1047 "Link to this heading")
[thresh_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1489) : threshold value that separates inliers from outliers 

pyod.models.thresholds.WIND(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#WIND)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.WIND "Link to this definition") 
    
WIND class for topological Winding number thresholder.
Use the topological winding number (with respect to the origin) to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the mean intersection point calculated from the winding number.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1048 "Link to this heading") 

random_stateint, optional (default=1234) 
    
Random seed for the normal distribution. Can also be set to None. 

pyod.models.thresholds.YJ(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#YJ)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.YJ "Link to this definition") 
    
YJ class for Yeo-Johnson transformation thresholder.
Use the Yeo-Johnson transformation to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond the max value in the YJ transformed data. 

pyod.models.thresholds.ZSCORE(_** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#ZSCORE)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.ZSCORE "Link to this definition") 
    
ZSCORE class for ZSCORE thresholder.
Use the zscore to evaluate a non-parametric means to threshold scores generated by the decision_scores where outliers are set to any value beyond a zscore of one.
###### pyod.models.vae module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.vae "Link to this heading")
Variational Auto Encoder (VAE) and beta-VAE for Unsupervised Outlier Detection 

Reference:
    
[[BKW13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1144 "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.")] Kingma, Diederik, Welling ‚ÄòAuto-Encodeing Variational Bayes‚Äô <https://arxiv.org/abs/1312.6114>
[[BBHP+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1146 "Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.")] Burges et al ‚ÄòUnderstanding disentangling in beta-VAE‚Äô <https://arxiv.org/pdf/1804.03599.pdf> 

_class_ pyod.models.vae.VAE(_contamination =0.1_, _preprocessing =True_, _lr =0.001_, _epoch_num =30_, _batch_size =32_, _optimizer_name ='adam'_, _device =None_, _random_state =42_, _use_compile =False_, _compile_mode ='default'_, _verbose =1_, _optimizer_params :[dict](https://docs.python.org/3/library/stdtypes.html#dict "\(in Python v3.13\)")={'weight_decay': 1e-05}_, _beta =1.0_, _capacity =0.0_, _encoder_neuron_list =[128, 64, 32]_, _decoder_neuron_list =[32, 64, 128]_, _latent_dim =2_, _hidden_activation_name ='relu'_, _output_activation_name ='sigmoid'_, _batch_norm =False_, _dropout_rate =0.2_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html#VAE)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE "Link to this definition") 
    
Bases: `BaseDeepLearningDetector`
Variational auto encoder Encoder maps X onto a latent space Z Decoder samples Z from N(0,1) VAE_loss = Reconstruction_loss + KL_loss
Reference See [[BKW13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1144 "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.")] Kingma, Diederik, Welling ‚ÄòAuto-Encodeing Variational Bayes‚Äô <https://arxiv.org/abs/1312.6114> for details.
beta VAE In Loss, the emphasis is on KL_loss and capacity of a bottleneck: VAE_loss = Reconstruction_loss + beta * KL_loss
Reference See [[BBHP+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1146 "Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. arXiv preprint arXiv:1804.03599, 2018.")] Burges et al ‚ÄòUnderstanding disentangling in beta-VAE‚Äô <https://arxiv.org/pdf/1804.03599.pdf> for details.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1053 "Link to this heading") 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

preprocessingbool, optional (default=True) 
    
If True, apply the preprocessing procedure before training models. 

lrfloat, optional (default=1e-3) 
    
The initial learning rate for the optimizer. 

epoch_numint, optional (default=30) 
    
The number of epochs for training. 

batch_sizeint, optional (default=32) 
    
The batch size for training. 

optimizer_namestr, optional (default=‚Äôadam‚Äô) 
    
The name of theoptimizer used to train the model. 

devicestr, optional (default=None) 
    
The device to use for the model. If None, it will be decided automatically. If you want to use MPS, set it to ‚Äòmps‚Äô. 

random_stateint, optional (default=42) 
    
The random seed for reproducibility. 

use_compilebool, optional (default=False) 
    
Whether to compile the model. If True, the model will be compiled before training. This is only available for PyTorch version >= 2.0.0. and Python < 3.12. 

compile_modestr, optional (default=‚Äôdefault‚Äô) 
    
The mode to compile the model. Can be either ‚Äúdefault‚Äù, ‚Äúreduce-overhead‚Äù, ‚Äúmax-autotune‚Äù or ‚Äúmax-autotune-no-cudagraphs‚Äù. See <https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile> for details. 

verboseint, optional (default=1) 
    
Verbosity mode. - 0 = silent - 1 = progress bar - 2 = one line per epoch. 

optimizer_paramsdict, optional (default={‚Äòweight_decay‚Äô: 1e-5}) 
    
Additional parameters for the optimizer. For example, optimizer_params={‚Äòweight_decay‚Äô: 1e-5}. 

betafloat, optional (default=1.0) 
    
Coefficient of beta VAE. The weight of KL divergence. Default is regular VAE. 

capacityfloat, optional (default=0.0) 
    
The maximum capacity of a loss bottleneck. 

encoder_neuron_listlist, optional (default=[128, 64, 32]) 
    
The number of neurons per hidden layers in encoder. So the encoder has the structure as [feature_size, 128, 64, 32, latent_dim]. 

decoder_neuron_listlist, optional (default=[32, 64, 128]) 
    
The number of neurons per hidden layers in decoder. So the decoder has the structure as [latent_dim, 32, 64, 128, feature_size]. 

latent_dimint, optional (default=2) 
    
The dimension of latent space. 

hidden_activation_namestr, optional (default=‚Äôrelu‚Äô) 
    
The activation function used in hidden layers. 

output_activation_namestr, optional (default=‚Äôsigmoid‚Äô) 
    
The activation function used in output layer. 

batch_normboolean, optional (default=False) 
    
Whether to apply Batch Normalization, See <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> 

dropout_ratefloat in (0., 1), optional (default=0.2) 
    
The dropout to be used across all layers.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1054 "Link to this heading") 

modeltorch.nn.Module 
    
The underlying VAE model. 

optimizertorch.optim 
    
The optimizer used to train the model. 

criterionpython function 
    
The loss function used to train the model. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1491)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[threshold_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1493)float 
    
The threshold is based on `contamination`. It is the `n_samples * contamination` most abnormal samples in `decision_scores_`. The threshold is calculated for generating binary outlier labels. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1495)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

build_model()[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html#VAE.build_model)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.build_model "Link to this definition") 
    
Need to define model in this method. self.feature_size is the number of features in the input data. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1055 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1056 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_ , _batch_size =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function "Link to this definition") 
    
Predict raw anomaly score of X using the fitted detector.
The anomaly score of an input sample is computed based on different detector algorithms. For consistency, outliers are assigned with larger anomaly scores. Parameters ‚Äî‚Äî‚Äî- X : numpy array of shape (n_samples, n_features)
> The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. 

batch_sizeint, optional (default=None) 
    
The batch size for processing the input samples. If not specified, the default batch size is used.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1057 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

decision_function_update(_anomaly_scores_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function_update "Link to this definition") 
    
For any additional operations after each decision function call. 

epoch_update()[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.epoch_update "Link to this definition") 
    
For any additional operations after each epoch. 

evaluate(_data_loader_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluate "Link to this definition") 
    
Evaluate the deep learning model.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1058 "Link to this heading") 

data_loadertorch.utils.data.DataLoader 
    
The data loader for evaluating the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1059 "Link to this heading") 

outlier_scoresnumpy array of shape (n_samples,) 
    
The outlier scores of the input samples. 

evaluating_forward(_batch_data_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html#VAE.evaluating_forward)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_forward "Link to this definition") 
    
Forward pass for evaluating the model. Abstract method to be implemented.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1060 "Link to this heading") 

batch_datatuple 
    
The batch data for evaluating the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1061 "Link to this heading") 

outputnumpy array 
    
The output of the model. 

evaluating_prepare()[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_prepare "Link to this definition") 


fit(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit "Link to this definition") 
    
Fit detector. y is ignored in unsupervised methods.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1062 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

ynumpy array of shape (n_samples,), optional (default=None) 
    
The ground truth of input samples. Not used in unsupervised methods. 

fit_predict(_X_ , _y =None_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1063 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1064 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1065 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1066 "Link to this heading")
score : float
Deprecated since version 0.6.9: fit_predict_score will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. Scoring could be done by calling an evaluation method, e.g., AUC ROC. 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1067 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1068 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

_classmethod_ load(_path_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.load "Link to this definition") 
    
Load the model from the specified path.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1069 "Link to this heading") 

pathstr 
    
The path to load the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1070 "Link to this heading") 

modelBaseDeepLearningDetector 
    
The loaded model. 

predict(_X_ , _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1071 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1072 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

confidencenumpy array of shape (n_samples,). 
    
Only if return_confidence is set to True. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1074 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1075 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_ , _method ='linear'_, _return_confidence =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Two approaches are possible:
  1. simply use Min-max conversion to linearly transform the outlier scores into the range of [0,1]. The model must be fitted first.
  2. use unifying scores, see [[BKKSZ11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1114 "Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In Proceedings of the 2011 SIAM International Conference on Data Mining, 13‚Äì24. SIAM, 2011.")].


############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1077 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

methodstr, optional (default=‚Äôlinear‚Äô) 
    
probability conversion method. It must be one of ‚Äòlinear‚Äô or ‚Äòunify‚Äô. 

return_confidenceboolean, optional(default=False) 
    
If True, also return the confidence of prediction.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1078 "Link to this heading") 

outlier_probabilitynumpy array of shape (n_samples, n_classes) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. Note it depends on the number of classes, which is by default 2 classes ([proba of normal, proba of outliers]). 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1079 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1080 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

save(_path_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.save "Link to this definition") 
    
Save the model to the specified path.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1081 "Link to this heading") 

pathstr 
    
The path to save the model. 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1082 "Link to this heading")
self : object 

train(_train_loader_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.train "Link to this definition") 
    
Train the deep learning model.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1083 "Link to this heading") 

train_loadertorch.utils.data.DataLoader 
    
The data loader for training the model. 

training_forward(_batch_data_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html#VAE.training_forward)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_forward "Link to this definition") 
    
Forward pass for training the model. Abstract method to be implemented.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1084 "Link to this heading") 

batch_datatuple 
    
The batch data for training the model.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1085 "Link to this heading") 

lossfloat or tuple of float 
    
The loss.item of the model, or a tuple of loss.item if there are multiple losses. 

training_prepare()[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_prepare "Link to this definition") 

###### pyod.models.xgbod module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.xgbod "Link to this heading")
XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning. A semi-supervised outlier detection framework. 

_class_ pyod.models.xgbod.XGBOD(_estimator_list =None_, _standardization_flag_list =None_, _max_depth =3_, _learning_rate =0.1_, _n_estimators =100_, _silent =True_, _objective ='binary:logistic'_, _booster ='gbtree'_, _n_jobs =1_, _nthread =None_, _gamma =0_, _min_child_weight =1_, _max_delta_step =0_, _subsample =1_, _colsample_bytree =1_, _colsample_bylevel =1_, _reg_alpha =0_, _reg_lambda =1_, _scale_pos_weight =1_, _base_score =0.5_, _random_state =0_, _** kwargs_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD "Link to this definition") 
    
Bases: [`BaseDetector`](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector "pyod.models.base.BaseDetector")
XGBOD class for outlier detection. It first uses the passed in unsupervised outlier detectors to extract richer representation of the data and then concatenates the newly generated features to the original feature for constructing the augmented feature space. An XGBoost classifier is then applied on this augmented feature space. Read more in the [[BZH18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1124 "Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In International Joint Conference on Neural Networks \(IJCNN\). IEEE, 2018.")].
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1087 "Link to this heading") 

estimator_listlist, optional (default=None) 
    
The list of pyod detectors passed in for unsupervised learning 

standardization_flag_listlist, optional (default=None) 
    
The list of boolean flags for indicating whether to perform standardization for each detector. 

max_depthint 
    
Maximum tree depth for base learners. 

learning_ratefloat 
    
Boosting learning rate (xgb‚Äôs ‚Äúeta‚Äù) 

n_estimatorsint 
    
Number of boosted trees to fit. 

silentbool 
    
Whether to print messages while running boosting. 

objectivestring or callable 
    
Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below). 

boosterstring 
    
Specify which booster to use: gbtree, gblinear or dart. 

n_jobsint 
    
Number of parallel threads used to run xgboost. (replaces `nthread`) 

gammafloat 
    
Minimum loss reduction required to make a further partition on a leaf node of the tree. 

min_child_weightint 
    
Minimum sum of instance weight(hessian) needed in a child. 

max_delta_stepint 
    
Maximum delta step we allow each tree‚Äôs weight estimation to be. 

subsamplefloat 
    
Subsample ratio of the training instance. 

colsample_bytreefloat 
    
Subsample ratio of columns when constructing each tree. 

colsample_bylevelfloat 
    
Subsample ratio of columns for each split, in each level. 

reg_alphafloat (xgb‚Äôs alpha) 
    
L1 regularization term on weights. 

reg_lambdafloat (xgb‚Äôs lambda) 
    
L2 regularization term on weights. 

scale_pos_weightfloat 
    
Balancing of positive and negative weights. 

base_score:
    
The initial prediction score of all instances, global bias. 

random_stateint 
    
Random number seed. (replaces seed)
### missing : float, optional ### Value in the data which needs to be present as a missing value. If ### None, defaults to np.nan. 

importance_type: string, default ‚Äúgain‚Äù
    
The feature importance type for the `feature_importances_` property: either ‚Äúgain‚Äù, ‚Äúweight‚Äù, ‚Äúcover‚Äù, ‚Äútotal_gain‚Äù or ‚Äútotal_cover‚Äù. 

**kwargsdict, optional 
    
Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: <https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst>. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError.
Note: **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn.
######### Attributes[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1088 "Link to this heading") 

[n_detector_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1497)int 
    
The number of unsupervised of detectors used. 

[clf_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1499)object 
    
The XGBoost classifier. 

[decision_scores_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1501)numpy array of shape (n_samples,) 
    
The outlier scores of the training data. The higher, the more abnormal. Outliers tend to have higher scores. This value is available once the detector is fitted. 

[labels_](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1503)int, either 0 or 1 
    
The binary labels of the training data. 0 stands for inliers and 1 for outliers/anomalies. It is generated by applying `threshold_` on `decision_scores_`. 

compute_rejection_stats(_T =32_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_, _verbose =False_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.compute_rejection_stats "Link to this definition") 
     

Add reject option into the unsupervised detector. 
    
This comes with guarantees: an estimate of the expected rejection rate (return_rejectrate=True), an upper bound of the rejection rate (return_ub_rejectrate= True), and an upper bound on the cost (return_ub_cost=True).
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1089 "Link to this heading") 

T: int, optional(default=32)
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive),
    
optional (default = [1,1, contamination]) costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r). 

verbose: bool, optional (default = False)
    
If true, it prints the expected rejection rate, the upper bound rejection rate, and the upper bound of the cost.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1090 "Link to this heading")
expected_rejection_rate: float, the expected rejection rate; upperbound_rejection_rate: float, the upper bound for the rejection rate
> satisfied with probability 1-delta;
upperbound_cost: float, the upper bound for the cost; 

decision_function(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.decision_function)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.decision_function "Link to this definition") 
    
Predict raw anomaly scores of X using the fitted detector.
The anomaly score of an input sample is computed based on the fitted detector. For consistency, outliers are assigned with higher anomaly scores.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1091 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. Sparse matrices are accepted only if they are supported by the base estimator.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1092 "Link to this heading") 

anomaly_scoresnumpy array of shape (n_samples,) 
    
The anomaly score of the input samples. 

fit(_X_ , _y_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.fit)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit "Link to this definition") 
    
Fit the model using X and y as training data.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1093 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
Training data. 

ynumpy array of shape (n_samples,) 
    
The ground truth (binary label)
  * 0 : inliers
  * 1 : outliers


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1094 "Link to this heading")
self : object 

fit_predict(_X_ , _y_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.fit_predict)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict "Link to this definition") 
    
Fit detector first and then predict whether a particular sample is an outlier or not. y is ignored in unsupervised models.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1095 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1096 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers.
Deprecated since version 0.6.9: fit_predict will be removed in pyod 0.8.0.; it will be replaced by calling fit function first and then accessing labels_ attribute for consistency. 

fit_predict_score(_X_ , _y_ , _scoring ='roc_auc_score'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.fit_predict_score)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict_score "Link to this definition") 
    
Fit the detector, predict on samples, and evaluate the model by predefined metrics, e.g., ROC.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1097 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

yIgnored 
    
Not used, present for API consistency by convention. 

scoringstr, optional (default=‚Äôroc_auc_score‚Äô) 
    
Evaluation metric:
  * ‚Äòroc_auc_score‚Äô: ROC score
  * ‚Äòprc_n_score‚Äô: Precision @ rank n score


############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1098 "Link to this heading")
score : float 

get_params(_deep =True_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.get_params "Link to this definition") 
    
Get parameters for this estimator.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1099 "Link to this heading") 

deepbool, optional (default=True) 
    
If True, will return the parameters for this estimator and contained subobjects that are estimators.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1100 "Link to this heading") 

paramsmapping of string to any 
    
Parameter names mapped to their values. 

predict(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.predict)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict "Link to this definition") 
    
Predict if a particular sample is an outlier or not. Calling xgboost predict function.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1101 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1102 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. 0 stands for inliers and 1 for outliers. 

predict_confidence(_X_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_confidence "Link to this definition") 
    
Predict the model‚Äôs confidence in making the same prediction under slightly different training sets. See [[BPVD20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1153 "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 227‚Äì243. Springer, 2020.")].
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1104 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1105 "Link to this heading") 

confidencenumpy array of shape (n_samples,) 
    
For each observation, tells how consistently the model would make the same prediction if the training set was perturbed. Return a probability, ranging in [0,1]. 

predict_proba(_X_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#XGBOD.predict_proba)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_proba "Link to this definition") 
    
Predict the probability of a sample being outlier. Calling xgboost predict_proba function.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1106 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1107 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. 

predict_with_rejection(_X_ , _T =32_, _return_stats =False_, _delta =0.1_, _c_fp =1_, _c_fn =1_, _c_r =-1_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_with_rejection "Link to this definition") 
     

Predict if a particular sample is an outlier or not, 
    
allowing the detector to reject (i.e., output = -2) low confidence predictions.
############ Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1108 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples. 

Tint, optional(default=32) 
    
It allows to set the rejection threshold to 1-2exp(-T). The higher the value of T, the more rejections are made. 

return_stats: bool, optional (default = False)
    
If true, it returns also three additional float values: the estimated rejection rate, the upper bound rejection rate, and the upper bound of the cost. 

delta: float, optional (default = 0.1)
    
The upper bound rejection rate holds with probability 1-delta. 

c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
    
costs for false positive predictions (c_fp), false negative predictions (c_fn) and rejections (c_r).
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1109 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, it tells whether it should be considered as an outlier according to the fitted model. 0 stands for inliers, 1 for outliers and -2 for rejection.
expected_rejection_rate: float, if return_stats is True; upperbound_rejection_rate: float, if return_stats is True; upperbound_cost: float, if return_stats is True; 

set_params(_** params_)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.set_params "Link to this definition") 
    
Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form `<component>__<parameter>` so that it‚Äôs possible to update each component of a nested object.
See <http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html> and sklearn/base.py for more information.
############ Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1110 "Link to this heading")
self : object
###### Module contents[¬∂](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models "Link to this heading")
References
[BAgg15] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id89),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id354),[3](https://pyod.readthedocs.io/en/latest/pyod.models.html#id792))
Charu C Aggarwal. Outlier analysis. In _Data mining_ , 75‚Äì79. Springer, 2015.
[BAS15] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id162),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id173))
Charu C Aggarwal and Saket Sathe. Theoretical foundations and algorithms for outlier ensembles. _ACM SIGKDD Explorations Newsletter_ , 17(1):24‚Äì47, 2015.
[[BABC20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id871)]
Yahya Almardeny, Noureddine Boujnah, and Frances Cleary. A novel outlier detection method for multivariate data. _IEEE Transactions on Knowledge and Data Engineering_ , 2020.
[[BAP02](https://pyod.readthedocs.io/en/latest/pyod.models.html#id480)]
Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In _European Conference on Principles of Data Mining and Knowledge Discovery_ , 15‚Äì27. Springer, 2002.
[[BAAR96](https://pyod.readthedocs.io/en/latest/pyod.models.html#id532)]
Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. A linear method for deviation detection in large databases. In _KDD_ , volume 1141, 972‚Äì981. 1996.
[[BBTA+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id428)]
Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. Isolation-based anomaly detection using nearest-neighbor ensembles. _Computational Intelligence_ , 34(4):968‚Äì998, 2018.
[BBirgeR06] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id378),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id559))
Lucien Birg√© and Yves Rozenholc. How many bins should be put in a regular histogram. _ESAIM: Probability and Statistics_ , 10:24‚Äì45, 2006.
[[BBKNS00](https://pyod.readthedocs.io/en/latest/pyod.models.html#id585)]
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying density-based local outliers. In _ACM sigmod record_ , volume 29, 93‚Äì104. ACM, 2000.
[BBHP+18] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1050),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1052))
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in betvae. _arXiv preprint arXiv:1804.03599_ , 2018.
[[BCoo77](https://pyod.readthedocs.io/en/latest/pyod.models.html#id176)]
R Dennis Cook. Detection of influential observation in linear regression. _Technometrics_ , 19(1):15‚Äì18, 1977.
[[BFM01](https://pyod.readthedocs.io/en/latest/pyod.models.html#id818)]
Kai-Tai Fang and Chang-Xing Ma. Wrap-around l2-discrepancy of random sampling, latin hypercube and uniform designs. _Journal of complexity_ , 17(4):608‚Äì624, 2001.
[[BGD12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id377)]
Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): a fast unsupervised anomaly detection algorithm. _KI-2012: Poster and Demo Track_ , pages 59‚Äì63, 2012.
[[BGHNN22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id637)]
Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. Lunar: unifying local outlier detection methods via graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_ , volume 36, 6737‚Äì6745. 2022.
[[BHR04](https://pyod.readthedocs.io/en/latest/pyod.models.html#id714)]
Johanna Hardin and David M Rocke. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. _Computational Statistics & Data Analysis_, 44(4):625‚Äì638, 2004.
[[BHGPRR19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1035)]
Navid Hashemi, Eduardo Verdugo German, Jonatan Pena Ramirez, and Justin Ruths. Filtering approaches for dealing with noise in anomaly detection. In _2019 IEEE 58th Conference on Decision and Control (CDC)_ , 5356‚Äì5361. IEEE, December 2019. URL: <http://dx.doi.org/10.1109/CDC40024.2019.9029258>, [doi:10.1109/cdc40024.2019.9029258](https://doi.org/10.1109/cdc40024.2019.9029258).
[[BHXD03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id116)]
Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. _Pattern Recognition Letters_ , 24(9-10):1641‚Äì1650, 2003.
[[BHof07](https://pyod.readthedocs.io/en/latest/pyod.models.html#id506)]
Heiko Hoffmann. Kernel pca for novelty detection. _Pattern recognition_ , 40(3):863‚Äì874, 2007.
[[BIH93](https://pyod.readthedocs.io/en/latest/pyod.models.html#id688)]
Boris Iglewicz and David Caster Hoaglin. _How to detect and handle outliers_. Volume 16. Asq Press, 1993.
[[BJHuszarPvdH12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id975)]
JHM Janssens, Ferenc Husz√°r, EO Postma, and HJ van den Herik. Stochastic outlier selection. Technical Report, Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands, 2012.
[BKW13] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1049),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1051))
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ , 2013.
[BKKSZ11] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id16),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id37),[3](https://pyod.readthedocs.io/en/latest/pyod.models.html#id58),[4](https://pyod.readthedocs.io/en/latest/pyod.models.html#id83),[5](https://pyod.readthedocs.io/en/latest/pyod.models.html#id107),[6](https://pyod.readthedocs.io/en/latest/pyod.models.html#id134),[7](https://pyod.readthedocs.io/en/latest/pyod.models.html#id157),[8](https://pyod.readthedocs.io/en/latest/pyod.models.html#id194),[9](https://pyod.readthedocs.io/en/latest/pyod.models.html#id220),[10](https://pyod.readthedocs.io/en/latest/pyod.models.html#id246),[11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id269),[12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id295),[13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id322),[14](https://pyod.readthedocs.io/en/latest/pyod.models.html#id348),[15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id372),[16](https://pyod.readthedocs.io/en/latest/pyod.models.html#id398),[17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id423),[18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id448),[19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id474),[20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id500),[21](https://pyod.readthedocs.io/en/latest/pyod.models.html#id526),[22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id552),[23](https://pyod.readthedocs.io/en/latest/pyod.models.html#id579),[24](https://pyod.readthedocs.io/en/latest/pyod.models.html#id605),[25](https://pyod.readthedocs.io/en/latest/pyod.models.html#id631),[26](https://pyod.readthedocs.io/en/latest/pyod.models.html#id655),[27](https://pyod.readthedocs.io/en/latest/pyod.models.html#id682),[28](https://pyod.readthedocs.io/en/latest/pyod.models.html#id708),[29](https://pyod.readthedocs.io/en/latest/pyod.models.html#id734),[30](https://pyod.readthedocs.io/en/latest/pyod.models.html#id760),[31](https://pyod.readthedocs.io/en/latest/pyod.models.html#id786),[32](https://pyod.readthedocs.io/en/latest/pyod.models.html#id812),[33](https://pyod.readthedocs.io/en/latest/pyod.models.html#id837),[34](https://pyod.readthedocs.io/en/latest/pyod.models.html#id865),[35](https://pyod.readthedocs.io/en/latest/pyod.models.html#id891),[36](https://pyod.readthedocs.io/en/latest/pyod.models.html#id917),[37](https://pyod.readthedocs.io/en/latest/pyod.models.html#id943),[38](https://pyod.readthedocs.io/en/latest/pyod.models.html#id969),[39](https://pyod.readthedocs.io/en/latest/pyod.models.html#id996),[40](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1022),[41](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1076))
Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Interpreting and unifying outlier scores. In _Proceedings of the 2011 SIAM International Conference on Data Mining_ , 13‚Äì24. SIAM, 2011.
[[BKKrogerSZ09](https://pyod.readthedocs.io/en/latest/pyod.models.html#id923)]
Hans-Peter Kriegel, Peer Kr√∂ger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 831‚Äì838. Springer, 2009.
[[BKZ+08](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1)]
Hans-Peter Kriegel, Arthur Zimek, and others. Angle-based outlier detection in high-dimensional data. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_ , 444‚Äì452. ACM, 2008.
[[BLLP07](https://pyod.readthedocs.io/en/latest/pyod.models.html#id454)]
Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In _International Workshop on Machine Learning and Data Mining in Pattern Recognition_ , 61‚Äì75. Springer, 2007.
[[BLK05](https://pyod.readthedocs.io/en/latest/pyod.models.html#id328)]
Aleksandar Lazarevic and Vipin Kumar. Feature bagging for outlier detection. In _Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining_ , 157‚Äì166. ACM, 2005.
[[BLZB+20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id199)]
Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. COPOD: copula-based outlier detection. In _IEEE International Conference on Data Mining (ICDM)_. IEEE, 2020.
[[BLZH+22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id301)]
Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and H. George Chen. Ecod: unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_ , 2022.
[[BLTZ08](https://pyod.readthedocs.io/en/latest/pyod.models.html#id404)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on_ , 413‚Äì422. IEEE, 2008.
[[BLTZ12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id404)]
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. _ACM Transactions on Knowledge Discovery from Data (TKDD)_ , 6(1):3, 2012.
[BLLZ+19] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id740),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id949))
Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. Generative adversarial active learning for unsupervised outlier detection. _IEEE Transactions on Knowledge and Data Engineering_ , 2019.
[[BPKGF03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id611)]
Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B Gibbons, and Christos Faloutsos. Loci: fast outlier detection using the local correlation integral. In _Data Engineering, 2003. Proceedings. 19th International Conference on_ , 315‚Äì326. IEEE, 2003.
[BPVD20] ([1](https://pyod.readthedocs.io/en/latest/pyod.models.html#id13),[2](https://pyod.readthedocs.io/en/latest/pyod.models.html#id34),[3](https://pyod.readthedocs.io/en/latest/pyod.models.html#id55),[4](https://pyod.readthedocs.io/en/latest/pyod.models.html#id80),[5](https://pyod.readthedocs.io/en/latest/pyod.models.html#id104),[6](https://pyod.readthedocs.io/en/latest/pyod.models.html#id131),[7](https://pyod.readthedocs.io/en/latest/pyod.models.html#id154),[8](https://pyod.readthedocs.io/en/latest/pyod.models.html#id191),[9](https://pyod.readthedocs.io/en/latest/pyod.models.html#id217),[10](https://pyod.readthedocs.io/en/latest/pyod.models.html#id243),[11](https://pyod.readthedocs.io/en/latest/pyod.models.html#id266),[12](https://pyod.readthedocs.io/en/latest/pyod.models.html#id292),[13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id319),[14](https://pyod.readthedocs.io/en/latest/pyod.models.html#id345),[15](https://pyod.readthedocs.io/en/latest/pyod.models.html#id369),[16](https://pyod.readthedocs.io/en/latest/pyod.models.html#id395),[17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id420),[18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id445),[19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id471),[20](https://pyod.readthedocs.io/en/latest/pyod.models.html#id497),[21](https://pyod.readthedocs.io/en/latest/pyod.models.html#id523),[22](https://pyod.readthedocs.io/en/latest/pyod.models.html#id549),[23](https://pyod.readthedocs.io/en/latest/pyod.models.html#id576),[24](https://pyod.readthedocs.io/en/latest/pyod.models.html#id602),[25](https://pyod.readthedocs.io/en/latest/pyod.models.html#id628),[26](https://pyod.readthedocs.io/en/latest/pyod.models.html#id652),[27](https://pyod.readthedocs.io/en/latest/pyod.models.html#id679),[28](https://pyod.readthedocs.io/en/latest/pyod.models.html#id705),[29](https://pyod.readthedocs.io/en/latest/pyod.models.html#id731),[30](https://pyod.readthedocs.io/en/latest/pyod.models.html#id757),[31](https://pyod.readthedocs.io/en/latest/pyod.models.html#id783),[32](https://pyod.readthedocs.io/en/latest/pyod.models.html#id809),[33](https://pyod.readthedocs.io/en/latest/pyod.models.html#id834),[34](https://pyod.readthedocs.io/en/latest/pyod.models.html#id862),[35](https://pyod.readthedocs.io/en/latest/pyod.models.html#id888),[36](https://pyod.readthedocs.io/en/latest/pyod.models.html#id914),[37](https://pyod.readthedocs.io/en/latest/pyod.models.html#id940),[38](https://pyod.readthedocs.io/en/latest/pyod.models.html#id966),[39](https://pyod.readthedocs.io/en/latest/pyod.models.html#id993),[40](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1019),[41](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1073),[42](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1103))
Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in their example-wise predictions. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_ , 227‚Äì243. Springer, 2020.
[[BPevny16](https://pyod.readthedocs.io/en/latest/pyod.models.html#id558)]
Tom√°≈° Pevn\\`y. Loda: lightweight on-line detector of anomalies. _Machine Learning_ , 102(2):275‚Äì304, 2016.
[[BRRS00](https://pyod.readthedocs.io/en/latest/pyod.models.html#id480)]
Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In _ACM Sigmod Record_ , volume 29, 427‚Äì438. ACM, 2000.
[[BRD99](https://pyod.readthedocs.io/en/latest/pyod.models.html#id714)]
Peter J Rousseeuw and Katrien Van Driessen. A fast algorithm for the minimum covariance determinant estimator. _Technometrics_ , 41(3):212‚Äì223, 1999.
[[BRVG+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id226)]
Lukas Ruff, Robert Vandermeulen, Nico G√∂rnitz, Lucas Deecke, Shoaib Siddiqui, Alexander Binder, Emmanuel M√ºller, and Marius Kloft. Deep one-class classification. _International conference on machine learning_ , 2018.
[[BSSeebockW+17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id63)]
Thomas Schlegl, Philipp Seeb√∂ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In _International conference on information processing in medical imaging_ , 146‚Äì157. Springer, 2017.
[[BScholkopfPST+01](https://pyod.readthedocs.io/en/latest/pyod.models.html#id766)]
Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_ , 13(7):1443‚Äì1471, 2001.
[[BSCSC03](https://pyod.readthedocs.io/en/latest/pyod.models.html#id792)]
Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly detection scheme based on principal component classifier. Technical Report, MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, 2003.
[[BSB13](https://pyod.readthedocs.io/en/latest/pyod.models.html#id897)]
Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling. _Advances in neural information processing systems_ , 2013.
[[BTCFC02](https://pyod.readthedocs.io/en/latest/pyod.models.html#id139)]
Jian Tang, Zhixiang Chen, Ada Wai-Chee Fu, and David W Cheung. Enhancing effectiveness of outlier detections for low density patterns. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_ , 535‚Äì548. Springer, 2002.
[[BXPWW23](https://pyod.readthedocs.io/en/latest/pyod.models.html#id275)]
Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_ , ():1‚Äì14, 2023. [doi:10.1109/TKDE.2023.3270293](https://doi.org/10.1109/TKDE.2023.3270293).
[[BYRV17](https://pyod.readthedocs.io/en/latest/pyod.models.html#id843)]
Chong You, Daniel P Robinson, and Ren√© Vidal. Provable self-representation based outlier detection in a union of subspaces. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ , 3395‚Äì3404. 2017.
[[BZRF+18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id42)]
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. Adversarially learned anomaly detection. In _2018 IEEE International conference on data mining (ICDM)_ , 727‚Äì736. IEEE, 2018.
[[BZH18](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1086)]
Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised representation learning. In _International Joint Conference on Neural Networks (IJCNN)_. IEEE, 2018.
[[BZHC+21](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1002)]
Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang, Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng Sun, and Leman Akoglu. Suod: accelerating large-scale unsupervised heterogeneous outlier detection. _Proceedings of Machine Learning and Systems_ , 2021.
[[BZNHL19](https://pyod.readthedocs.io/en/latest/pyod.models.html#id661)]
Yue Zhao, Zain Nasrullah, Maciej K Hryniewicki, and Zheng Li. LSCP: locally selective combination in parallel outlier ensembles. In _Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019_ , 585‚Äì593. Calgary, Canada, May 2019. SIAM. URL: <https://doi.org/10.1137/1.9781611975673.66>, [doi:10.1137/1.9781611975673.66](https://doi.org/10.1137/1.9781611975673.66).
[ Next Utility Functions ](https://pyod.readthedocs.io/en/latest/pyod.utils.html) [ Previous API Reference ](https://pyod.readthedocs.io/en/latest/pyod.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [pyod.models.abod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod)
      * [`ABOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD)
        * [`ABOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.compute_rejection_stats)
        * [`ABOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.decision_function)
        * [`ABOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit)
        * [`ABOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict)
        * [`ABOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit_predict_score)
        * [`ABOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict)
        * [`ABOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_confidence)
        * [`ABOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_proba)
        * [`ABOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.predict_with_rejection)
    * [pyod.models.ae1svm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ae1svm)
      * [`AE1SVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM)
        * [`AE1SVM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.compute_rejection_stats)
        * [`AE1SVM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.decision_function)
        * [`AE1SVM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit)
        * [`AE1SVM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict)
        * [`AE1SVM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit_predict_score)
        * [`AE1SVM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict)
        * [`AE1SVM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_confidence)
        * [`AE1SVM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_proba)
        * [`AE1SVM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.predict_with_rejection)
    * [pyod.models.alad module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.alad)
      * [`ALAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD)
        * [`ALAD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.compute_rejection_stats)
        * [`ALAD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.decision_function)
        * [`ALAD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit)
        * [`ALAD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict)
        * [`ALAD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit_predict_score)
        * [`ALAD.plot_learning_curves()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.plot_learning_curves)
        * [`ALAD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict)
        * [`ALAD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_confidence)
        * [`ALAD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_proba)
        * [`ALAD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.predict_with_rejection)
    * [pyod.models.anogan module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.anogan)
      * [`AnoGAN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN)
        * [`AnoGAN.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.compute_rejection_stats)
        * [`AnoGAN.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.decision_function)
        * [`AnoGAN.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit)
        * [`AnoGAN.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict)
        * [`AnoGAN.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit_predict_score)
        * [`AnoGAN.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.get_params)
        * [`AnoGAN.plot_learning_curves()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.plot_learning_curves)
        * [`AnoGAN.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict)
        * [`AnoGAN.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_confidence)
        * [`AnoGAN.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_proba)
        * [`AnoGAN.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.predict_with_rejection)
        * [`AnoGAN.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.set_params)
    * [pyod.models.auto_encoder module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.auto_encoder)
      * [`AutoEncoder`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder)
        * [`AutoEncoder.build_model()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.build_model)
        * [`AutoEncoder.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.compute_rejection_stats)
        * [`AutoEncoder.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.decision_function)
        * [`AutoEncoder.evaluate()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.evaluate)
        * [`AutoEncoder.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit)
        * [`AutoEncoder.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict)
        * [`AutoEncoder.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.fit_predict_score)
        * [`AutoEncoder.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict)
        * [`AutoEncoder.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_confidence)
        * [`AutoEncoder.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_proba)
        * [`AutoEncoder.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.predict_with_rejection)
        * [`AutoEncoder.save()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.save)
        * [`AutoEncoder.train()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.train)
        * [`AutoEncoder.training_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_forward)
        * [`AutoEncoder.training_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_prepare)
    * [pyod.models.auto_encoder_torch module](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-auto-encoder-torch-module)
    * [pyod.models.cblof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)
      * [`CBLOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF)
        * [`CBLOF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.compute_rejection_stats)
        * [`CBLOF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.decision_function)
        * [`CBLOF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit)
        * [`CBLOF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict)
        * [`CBLOF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit_predict_score)
        * [`CBLOF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict)
        * [`CBLOF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_confidence)
        * [`CBLOF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_proba)
        * [`CBLOF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.predict_with_rejection)
    * [pyod.models.cof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof)
      * [`COF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF)
        * [`COF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.compute_rejection_stats)
        * [`COF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.decision_function)
        * [`COF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit)
        * [`COF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict)
        * [`COF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit_predict_score)
        * [`COF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict)
        * [`COF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_confidence)
        * [`COF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_proba)
        * [`COF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.predict_with_rejection)
    * [pyod.models.combination module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.combination)
      * [`aom()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom)
      * [`average()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average)
      * [`majority_vote()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote)
      * [`maximization()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization)
      * [`median()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median)
      * [`moa()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa)
    * [pyod.models.cd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd)
      * [`CD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD)
        * [`CD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.compute_rejection_stats)
        * [`CD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.decision_function)
        * [`CD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit)
        * [`CD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict)
        * [`CD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit_predict_score)
        * [`CD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict)
        * [`CD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_confidence)
        * [`CD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_proba)
        * [`CD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.predict_with_rejection)
    * [pyod.models.copod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod)
      * [`COPOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD)
        * [`COPOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.compute_rejection_stats)
        * [`COPOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.decision_function)
        * [`COPOD.explain_outlier()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.explain_outlier)
        * [`COPOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit)
        * [`COPOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict)
        * [`COPOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit_predict_score)
        * [`COPOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.get_params)
        * [`COPOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict)
        * [`COPOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_confidence)
        * [`COPOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_proba)
        * [`COPOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.predict_with_rejection)
        * [`COPOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.set_params)
      * [`skew()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.skew)
    * [pyod.models.deep_svdd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.deep_svdd)
      * [`DeepSVDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD)
        * [`DeepSVDD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.compute_rejection_stats)
        * [`DeepSVDD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.decision_function)
        * [`DeepSVDD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit)
        * [`DeepSVDD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict)
        * [`DeepSVDD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit_predict_score)
        * [`DeepSVDD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.get_params)
        * [`DeepSVDD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict)
        * [`DeepSVDD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_confidence)
        * [`DeepSVDD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_proba)
        * [`DeepSVDD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.predict_with_rejection)
        * [`DeepSVDD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.set_params)
    * [pyod.models.devnet module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.devnet)
      * [`DevNet`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet)
        * [`DevNet.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.compute_rejection_stats)
        * [`DevNet.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.decision_function)
        * [`DevNet.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit)
        * [`DevNet.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict)
        * [`DevNet.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict_score)
        * [`DevNet.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.get_params)
        * [`DevNet.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict)
        * [`DevNet.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_confidence)
        * [`DevNet.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_proba)
        * [`DevNet.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.predict_with_rejection)
        * [`DevNet.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.set_params)
    * [pyod.models.dif module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.dif)
      * [`DIF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF)
        * [`DIF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.compute_rejection_stats)
        * [`DIF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.decision_function)
        * [`DIF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit)
        * [`DIF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict)
        * [`DIF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit_predict_score)
        * [`DIF.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.get_params)
        * [`DIF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict)
        * [`DIF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_confidence)
        * [`DIF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_proba)
        * [`DIF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.predict_with_rejection)
        * [`DIF.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.set_params)
    * [pyod.models.ecod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod)
      * [`ECOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD)
        * [`ECOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.compute_rejection_stats)
        * [`ECOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.decision_function)
        * [`ECOD.explain_outlier()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.explain_outlier)
        * [`ECOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit)
        * [`ECOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict)
        * [`ECOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit_predict_score)
        * [`ECOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.get_params)
        * [`ECOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict)
        * [`ECOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_confidence)
        * [`ECOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_proba)
        * [`ECOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.predict_with_rejection)
        * [`ECOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.set_params)
      * [`skew()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.skew)
    * [pyod.models.feature_bagging module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.feature_bagging)
      * [`FeatureBagging`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging)
        * [`FeatureBagging.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.compute_rejection_stats)
        * [`FeatureBagging.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.decision_function)
        * [`FeatureBagging.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit)
        * [`FeatureBagging.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict)
        * [`FeatureBagging.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit_predict_score)
        * [`FeatureBagging.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.get_params)
        * [`FeatureBagging.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict)
        * [`FeatureBagging.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_confidence)
        * [`FeatureBagging.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_proba)
        * [`FeatureBagging.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.predict_with_rejection)
        * [`FeatureBagging.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.set_params)
    * [pyod.models.gmm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm)
      * [`GMM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM)
        * [`GMM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.compute_rejection_stats)
        * [`GMM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.decision_function)
        * [`GMM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit)
        * [`GMM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict)
        * [`GMM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit_predict_score)
        * [`GMM.precisions_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_)
        * [`GMM.precisions_cholesky_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.precisions_cholesky_)
        * [`GMM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict)
        * [`GMM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_confidence)
        * [`GMM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_proba)
        * [`GMM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.predict_with_rejection)
    * [pyod.models.hbos module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos)
      * [`HBOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS)
        * [`HBOS.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.compute_rejection_stats)
        * [`HBOS.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.decision_function)
        * [`HBOS.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit)
        * [`HBOS.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict)
        * [`HBOS.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit_predict_score)
        * [`HBOS.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.get_params)
        * [`HBOS.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict)
        * [`HBOS.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_confidence)
        * [`HBOS.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_proba)
        * [`HBOS.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.predict_with_rejection)
        * [`HBOS.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.set_params)
    * [pyod.models.iforest module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest)
      * [`IForest`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest)
        * [`IForest.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.compute_rejection_stats)
        * [`IForest.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.decision_function)
        * [`IForest.feature_importances_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.feature_importances_)
        * [`IForest.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit)
        * [`IForest.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict)
        * [`IForest.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit_predict_score)
        * [`IForest.max_samples_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.max_samples_)
        * [`IForest.n_features_in_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.n_features_in_)
        * [`IForest.offset_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.offset_)
        * [`IForest.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict)
        * [`IForest.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_confidence)
        * [`IForest.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_proba)
        * [`IForest.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.predict_with_rejection)
    * [pyod.models.inne module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne)
      * [`INNE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE)
        * [`INNE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.compute_rejection_stats)
        * [`INNE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.decision_function)
        * [`INNE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit)
        * [`INNE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict)
        * [`INNE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit_predict_score)
        * [`INNE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.get_params)
        * [`INNE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict)
        * [`INNE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_confidence)
        * [`INNE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_proba)
        * [`INNE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.predict_with_rejection)
        * [`INNE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.set_params)
    * [pyod.models.kde module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde)
      * [`KDE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE)
        * [`KDE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.compute_rejection_stats)
        * [`KDE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.decision_function)
        * [`KDE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit)
        * [`KDE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict)
        * [`KDE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit_predict_score)
        * [`KDE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.get_params)
        * [`KDE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict)
        * [`KDE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_confidence)
        * [`KDE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_proba)
        * [`KDE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.predict_with_rejection)
        * [`KDE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.set_params)
    * [pyod.models.knn module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn)
      * [`KNN`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN)
        * [`KNN.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.compute_rejection_stats)
        * [`KNN.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.decision_function)
        * [`KNN.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit)
        * [`KNN.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict)
        * [`KNN.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit_predict_score)
        * [`KNN.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.get_params)
        * [`KNN.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict)
        * [`KNN.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_confidence)
        * [`KNN.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_proba)
        * [`KNN.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.predict_with_rejection)
        * [`KNN.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.set_params)
    * [pyod.models.kpca module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kpca)
      * [`KPCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA)
        * [`KPCA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.compute_rejection_stats)
        * [`KPCA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.decision_function)
        * [`KPCA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit)
        * [`KPCA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict)
        * [`KPCA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit_predict_score)
        * [`KPCA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.get_params)
        * [`KPCA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict)
        * [`KPCA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_confidence)
        * [`KPCA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_proba)
        * [`KPCA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.predict_with_rejection)
        * [`KPCA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.set_params)
    * [pyod.models.lmdd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd)
      * [`LMDD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD)
        * [`LMDD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.compute_rejection_stats)
        * [`LMDD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.decision_function)
        * [`LMDD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit)
        * [`LMDD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict)
        * [`LMDD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit_predict_score)
        * [`LMDD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.get_params)
        * [`LMDD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict)
        * [`LMDD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_confidence)
        * [`LMDD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_proba)
        * [`LMDD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.predict_with_rejection)
        * [`LMDD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.set_params)
    * [pyod.models.loda module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda)
      * [`LODA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA)
        * [`LODA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.compute_rejection_stats)
        * [`LODA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.decision_function)
        * [`LODA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit)
        * [`LODA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict)
        * [`LODA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit_predict_score)
        * [`LODA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.get_params)
        * [`LODA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict)
        * [`LODA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_confidence)
        * [`LODA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_proba)
        * [`LODA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.predict_with_rejection)
        * [`LODA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.set_params)
    * [pyod.models.lof module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)
      * [`LOF`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF)
        * [`LOF.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.compute_rejection_stats)
        * [`LOF.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.decision_function)
        * [`LOF.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit)
        * [`LOF.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict)
        * [`LOF.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit_predict_score)
        * [`LOF.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.get_params)
        * [`LOF.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict)
        * [`LOF.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_confidence)
        * [`LOF.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_proba)
        * [`LOF.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.predict_with_rejection)
        * [`LOF.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.set_params)
    * [pyod.models.loci module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci)
      * [`LOCI`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI)
        * [`LOCI.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.compute_rejection_stats)
        * [`LOCI.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.decision_function)
        * [`LOCI.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit)
        * [`LOCI.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict)
        * [`LOCI.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit_predict_score)
        * [`LOCI.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.get_params)
        * [`LOCI.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict)
        * [`LOCI.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_confidence)
        * [`LOCI.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_proba)
        * [`LOCI.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.predict_with_rejection)
        * [`LOCI.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.set_params)
    * [pyod.models.lunar module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lunar)
      * [`LUNAR`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR)
        * [`LUNAR.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.compute_rejection_stats)
        * [`LUNAR.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.decision_function)
        * [`LUNAR.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit)
        * [`LUNAR.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict)
        * [`LUNAR.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit_predict_score)
        * [`LUNAR.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.get_params)
        * [`LUNAR.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict)
        * [`LUNAR.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_confidence)
        * [`LUNAR.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_proba)
        * [`LUNAR.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.predict_with_rejection)
        * [`LUNAR.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.set_params)
    * [pyod.models.lscp module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lscp)
      * [`LSCP`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP)
        * [`LSCP.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.compute_rejection_stats)
        * [`LSCP.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.decision_function)
        * [`LSCP.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit)
        * [`LSCP.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict)
        * [`LSCP.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit_predict_score)
        * [`LSCP.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.get_params)
        * [`LSCP.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict)
        * [`LSCP.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_confidence)
        * [`LSCP.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_proba)
        * [`LSCP.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.predict_with_rejection)
        * [`LSCP.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.set_params)
    * [pyod.models.mad module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mad)
      * [`MAD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD)
        * [`MAD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.compute_rejection_stats)
        * [`MAD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.decision_function)
        * [`MAD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit)
        * [`MAD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict)
        * [`MAD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit_predict_score)
        * [`MAD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.get_params)
        * [`MAD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict)
        * [`MAD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_confidence)
        * [`MAD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_proba)
        * [`MAD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.predict_with_rejection)
        * [`MAD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.set_params)
    * [pyod.models.mcd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd)
      * [`MCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD)
        * [`MCD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.compute_rejection_stats)
        * [`MCD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.decision_function)
        * [`MCD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit)
        * [`MCD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict)
        * [`MCD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit_predict_score)
        * [`MCD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.get_params)
        * [`MCD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict)
        * [`MCD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_confidence)
        * [`MCD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_proba)
        * [`MCD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.predict_with_rejection)
        * [`MCD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.set_params)
    * [pyod.models.mo_gaal module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mo_gaal)
      * [`MO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL)
        * [`MO_GAAL.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.compute_rejection_stats)
        * [`MO_GAAL.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.decision_function)
        * [`MO_GAAL.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit)
        * [`MO_GAAL.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict)
        * [`MO_GAAL.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit_predict_score)
        * [`MO_GAAL.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.get_params)
        * [`MO_GAAL.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict)
        * [`MO_GAAL.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_confidence)
        * [`MO_GAAL.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_proba)
        * [`MO_GAAL.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.predict_with_rejection)
        * [`MO_GAAL.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.set_params)
    * [pyod.models.ocsvm module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm)
      * [`OCSVM`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM)
        * [`OCSVM.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.compute_rejection_stats)
        * [`OCSVM.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.decision_function)
        * [`OCSVM.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit)
        * [`OCSVM.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict)
        * [`OCSVM.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit_predict_score)
        * [`OCSVM.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.get_params)
        * [`OCSVM.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict)
        * [`OCSVM.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_confidence)
        * [`OCSVM.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_proba)
        * [`OCSVM.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.predict_with_rejection)
        * [`OCSVM.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.set_params)
    * [pyod.models.pca module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca)
      * [`PCA`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA)
        * [`PCA.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.compute_rejection_stats)
        * [`PCA.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.decision_function)
        * [`PCA.explained_variance_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.explained_variance_)
        * [`PCA.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit)
        * [`PCA.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict)
        * [`PCA.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit_predict_score)
        * [`PCA.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.get_params)
        * [`PCA.noise_variance_`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.noise_variance_)
        * [`PCA.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict)
        * [`PCA.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_confidence)
        * [`PCA.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_proba)
        * [`PCA.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.predict_with_rejection)
        * [`PCA.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.set_params)
    * [pyod.models.qmcd module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.qmcd)
      * [`QMCD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD)
        * [`QMCD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.compute_rejection_stats)
        * [`QMCD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.decision_function)
        * [`QMCD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit)
        * [`QMCD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict)
        * [`QMCD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit_predict_score)
        * [`QMCD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.get_params)
        * [`QMCD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict)
        * [`QMCD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_confidence)
        * [`QMCD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_proba)
        * [`QMCD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.predict_with_rejection)
        * [`QMCD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.set_params)
    * [pyod.models.rgraph module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rgraph)
      * [`RGraph`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph)
        * [`RGraph.active_support_elastic_net()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.active_support_elastic_net)
        * [`RGraph.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.compute_rejection_stats)
        * [`RGraph.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.decision_function)
        * [`RGraph.elastic_net_subspace_clustering()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering)
        * [`RGraph.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit)
        * [`RGraph.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict)
        * [`RGraph.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit_predict_score)
        * [`RGraph.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.get_params)
        * [`RGraph.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict)
        * [`RGraph.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_confidence)
        * [`RGraph.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_proba)
        * [`RGraph.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.predict_with_rejection)
        * [`RGraph.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.set_params)
    * [pyod.models.rod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod)
      * [`ROD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD)
        * [`ROD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.compute_rejection_stats)
        * [`ROD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.decision_function)
        * [`ROD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit)
        * [`ROD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict)
        * [`ROD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit_predict_score)
        * [`ROD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.get_params)
        * [`ROD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict)
        * [`ROD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_confidence)
        * [`ROD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_proba)
        * [`ROD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.predict_with_rejection)
        * [`ROD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.set_params)
    * [pyod.models.sampling module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sampling)
      * [`Sampling`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling)
        * [`Sampling.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.compute_rejection_stats)
        * [`Sampling.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.decision_function)
        * [`Sampling.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit)
        * [`Sampling.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict)
        * [`Sampling.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit_predict_score)
        * [`Sampling.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.get_params)
        * [`Sampling.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict)
        * [`Sampling.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_confidence)
        * [`Sampling.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_proba)
        * [`Sampling.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.predict_with_rejection)
        * [`Sampling.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.set_params)
    * [pyod.models.sod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)
      * [`SOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD)
        * [`SOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.compute_rejection_stats)
        * [`SOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.decision_function)
        * [`SOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit)
        * [`SOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict)
        * [`SOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit_predict_score)
        * [`SOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.get_params)
        * [`SOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict)
        * [`SOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_confidence)
        * [`SOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_proba)
        * [`SOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.predict_with_rejection)
        * [`SOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.set_params)
    * [pyod.models.so_gaal module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.so_gaal)
      * [`SO_GAAL`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL)
        * [`SO_GAAL.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.compute_rejection_stats)
        * [`SO_GAAL.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.decision_function)
        * [`SO_GAAL.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit)
        * [`SO_GAAL.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict)
        * [`SO_GAAL.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit_predict_score)
        * [`SO_GAAL.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.get_params)
        * [`SO_GAAL.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict)
        * [`SO_GAAL.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_confidence)
        * [`SO_GAAL.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_proba)
        * [`SO_GAAL.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.predict_with_rejection)
        * [`SO_GAAL.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.set_params)
    * [pyod.models.sos module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos)
      * [`SOS`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS)
        * [`SOS.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.compute_rejection_stats)
        * [`SOS.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.decision_function)
        * [`SOS.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit)
        * [`SOS.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict)
        * [`SOS.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit_predict_score)
        * [`SOS.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.get_params)
        * [`SOS.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict)
        * [`SOS.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_confidence)
        * [`SOS.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_proba)
        * [`SOS.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.predict_with_rejection)
        * [`SOS.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.set_params)
    * [pyod.models.suod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.suod)
      * [`SUOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD)
        * [`SUOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.compute_rejection_stats)
        * [`SUOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.decision_function)
        * [`SUOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit)
        * [`SUOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict)
        * [`SUOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit_predict_score)
        * [`SUOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.get_params)
        * [`SUOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict)
        * [`SUOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_confidence)
        * [`SUOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_proba)
        * [`SUOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.predict_with_rejection)
        * [`SUOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.set_params)
    * [pyod.models.thresholds module](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-thresholds-module)
      * [`AUCP()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.AUCP)
      * [`BOOT()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.BOOT)
      * [`CHAU()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CHAU)
      * [`CLF()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLF)
      * [`CLUST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLUST)
      * [`CPD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CPD)
      * [`DECOMP()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DECOMP)
      * [`DSN()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DSN)
      * [`EB()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.EB)
      * [`FGD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FGD)
      * [`FILTER()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FILTER)
      * [`FWFM()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FWFM)
      * [`GESD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.GESD)
      * [`HIST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.HIST)
      * [`IQR()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.IQR)
      * [`KARCH()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.KARCH)
      * [`MAD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MAD)
      * [`MCST()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MCST)
      * [`META()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.META)
      * [`MOLL()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MOLL)
      * [`MTT()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MTT)
      * [`OCSVM()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.OCSVM)
      * [`QMCD()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.QMCD)
      * [`REGR()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.REGR)
      * [`VAE()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.VAE)
      * [`WIND()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.WIND)
      * [`YJ()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.YJ)
      * [`ZSCORE()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.ZSCORE)
    * [pyod.models.vae module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.vae)
      * [`VAE`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE)
        * [`VAE.build_model()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.build_model)
        * [`VAE.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.compute_rejection_stats)
        * [`VAE.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function)
        * [`VAE.decision_function_update()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.decision_function_update)
        * [`VAE.epoch_update()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.epoch_update)
        * [`VAE.evaluate()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluate)
        * [`VAE.evaluating_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_forward)
        * [`VAE.evaluating_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_prepare)
        * [`VAE.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit)
        * [`VAE.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict)
        * [`VAE.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.fit_predict_score)
        * [`VAE.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.get_params)
        * [`VAE.load()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.load)
        * [`VAE.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict)
        * [`VAE.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_confidence)
        * [`VAE.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_proba)
        * [`VAE.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.predict_with_rejection)
        * [`VAE.save()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.save)
        * [`VAE.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.set_params)
        * [`VAE.train()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.train)
        * [`VAE.training_forward()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_forward)
        * [`VAE.training_prepare()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_prepare)
    * [pyod.models.xgbod module](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.xgbod)
      * [`XGBOD`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD)
        * [`XGBOD.compute_rejection_stats()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.compute_rejection_stats)
        * [`XGBOD.decision_function()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.decision_function)
        * [`XGBOD.fit()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit)
        * [`XGBOD.fit_predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict)
        * [`XGBOD.fit_predict_score()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict_score)
        * [`XGBOD.get_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.get_params)
        * [`XGBOD.predict()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict)
        * [`XGBOD.predict_confidence()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_confidence)
        * [`XGBOD.predict_proba()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_proba)
        * [`XGBOD.predict_with_rejection()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_with_rejection)
        * [`XGBOD.set_params()`](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.set_params)
    * [Module contents](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models)



---

## 15. Utility Functions - pyod 2.0.5 documentation {#15-1}

**URL:** https://pyod.readthedocs.io/en/latest/pyod.utils.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:15:59

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/pyod.utils.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/pyod.utils.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/pyod.utils.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Utility Functions[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#utility-functions "Link to this heading")
###### pyod.utils.data module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.data "Link to this heading")
Utility functions for manipulating data 

pyod.utils.data.check_consistent_shape(_X_train_ , _y_train_ , _X_test_ , _y_test_ , _y_train_pred_ , _y_test_pred_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#check_consistent_shape)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.check_consistent_shape "Link to this definition") 
    
Internal shape to check input data shapes are consistent.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#parameters "Link to this heading") 

X_trainnumpy array of shape (n_samples, n_features) 
    
The training samples. 

y_trainlist or array of shape (n_samples,) 
    
The ground truth of training samples. 

X_testnumpy array of shape (n_samples, n_features) 
    
The test samples. 

y_testlist or array of shape (n_samples,) 
    
The ground truth of test samples. 

y_train_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the training samples. 

y_test_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the test samples.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#returns "Link to this heading") 

X_trainnumpy array of shape (n_samples, n_features) 
    
The training samples. 

y_trainlist or array of shape (n_samples,) 
    
The ground truth of training samples. 

X_testnumpy array of shape (n_samples, n_features) 
    
The test samples. 

y_testlist or array of shape (n_samples,) 
    
The ground truth of test samples. 

y_train_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the training samples. 

y_test_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the test samples. 

pyod.utils.data.evaluate_print(_clf_name_ , _y_ , _y_pred_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#evaluate_print)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.evaluate_print "Link to this definition") 
    
Utility function for evaluating and printing the results for examples. Default metrics include ROC and Precision @ n
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id1 "Link to this heading") 

clf_namestr 
    
The name of the detector. 

ylist or numpy array of shape (n_samples,) 
    
The ground truth. Binary (0: inliers, 1: outliers). 

y_predlist or numpy array of shape (n_samples,) 
    
The raw outlier scores as returned by a fitted model. 

pyod.utils.data.generate_data(_n_train =1000_, _n_test =500_, _n_features =2_, _contamination =0.1_, _train_only =False_, _offset =10_, _behaviour ='new'_, _random_state =None_, _n_nan =0_, _n_inf =0_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#generate_data)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data "Link to this definition") 
    
Utility function to generate synthesized data. Normal data is generated by a multivariate Gaussian distribution and outliers are generated by a uniform distribution. ‚ÄúX_train, X_test, y_train, y_test‚Äù are returned.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id2 "Link to this heading") 

n_trainint, (default=1000) 
    
The number of training points to generate. 

n_testint, (default=500) 
    
The number of test points to generate. 

n_featuresint, optional (default=2) 
    
The number of features (dimensions). 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. 

train_onlybool, optional (default=False) 
    
If true, generate train data only. 

offsetint, optional (default=10) 
    
Adjust the value range of Gaussian and Uniform. 

behaviourstr, default=‚Äônew‚Äô 
    
Behaviour of the returned datasets which can be either ‚Äòold‚Äô or ‚Äònew‚Äô. Passing `behaviour='new'` returns ‚ÄúX_train, X_test, y_train, y_test‚Äù, while passing `behaviour='old'` returns ‚ÄúX_train, y_train, X_test, y_test‚Äù. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

n_nanint 
    
The number of values that are missing (np.nan). Defaults to zero. 

n_infint 
    
The number of values that are infinite. (np.inf). Defaults to zero.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id3 "Link to this heading") 

X_trainnumpy array of shape (n_train, n_features) 
    
Training data. 

X_testnumpy array of shape (n_test, n_features) 
    
Test data. 

y_trainnumpy array of shape (n_train,) 
    
Training ground truth. 

y_testnumpy array of shape (n_test,) 
    
Test ground truth. 

pyod.utils.data.generate_data_categorical(_n_train =1000_, _n_test =500_, _n_features =2_, _n_informative =2_, _n_category_in =2_, _n_category_out =2_, _contamination =0.1_, _shuffle =True_, _random_state =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#generate_data_categorical)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_categorical "Link to this definition") 
    
Utility function to generate synthesized categorical data.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id4 "Link to this heading") 

n_trainint, (default=1000) 
    
The number of training points to generate. 

n_testint, (default=500) 
    
The number of test points to generate. 

n_featuresint, optional (default=2) 
    
The number of features for each sample. 

n_informativeint in (1, n_features), optional (default=2) 
    
The number of informative features in the outlier points. The higher the easier the outlier detection should be. Note that n_informative should not be less than or equal n_features. 

n_category_inint in (1, n_inliers), optional (default=2) 
    
The number of categories in the inlier points. 

n_category_outint in (1, n_outliers), optional (default=2) 
    
The number of categories in the outlier points. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. 

shuffle: bool, optional(default=True)
    
If True, inliers will be shuffled which makes more noisy distribution. 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id5 "Link to this heading") 

X_trainnumpy array of shape (n_train, n_features) 
    
Training data. 

y_trainnumpy array of shape (n_train,) 
    
Training ground truth. 

X_testnumpy array of shape (n_test, n_features) 
    
Test data. 

y_testnumpy array of shape (n_test,) 
    
Test ground truth. 

pyod.utils.data.generate_data_clusters(_n_train =1000_, _n_test =500_, _n_clusters =2_, _n_features =2_, _contamination =0.1_, _size ='same'_, _density ='same'_, _dist =0.25_, _random_state =None_, _return_in_clusters =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#generate_data_clusters)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters "Link to this definition") 
     

Utility function to generate synthesized data in clusters.
    
Generated data can involve the low density pattern problem and global outliers which are considered as difficult tasks for outliers detection algorithms.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id6 "Link to this heading") 

n_trainint, (default=1000) 
    
The number of training points to generate. 

n_testint, (default=500) 
    
The number of test points to generate. 

n_clustersint, optional (default=2) 
    
The number of centers (i.e. clusters) to generate. 

n_featuresint, optional (default=2) 
    
The number of features for each sample. 

contaminationfloat in (0., 0.5), optional (default=0.1) 
    
The amount of contamination of the data set, i.e. the proportion of outliers in the data set. 

sizestr, optional (default=‚Äôsame‚Äô) 
    
Size of each cluster: ‚Äòsame‚Äô generates clusters with same size, ‚Äòdifferent‚Äô generate clusters with different sizes. 

densitystr, optional (default=‚Äôsame‚Äô) 
    
Density of each cluster: ‚Äòsame‚Äô generates clusters with same density, ‚Äòdifferent‚Äô generate clusters with different densities. 

dist: float, optional (default=0.25)
    
Distance between clusters. Should be between 0. and 1.0 It is used to avoid clusters overlapping as much as possible. However, if number of samples and number of clusters are too high, it is unlikely to separate them fully even if `dist` set to 1.0 

random_stateint, RandomState instance or None, optional (default=None) 
    
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. 

return_in_clustersbool, optional (default=False) 
    
If True, the function returns x_train, y_train, x_test, y_test each as a list of numpy arrays where each index represents a cluster. If False, it returns x_train, y_train, x_test, y_test each as numpy array after joining the sequence of clusters arrays,
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id7 "Link to this heading") 

X_trainnumpy array of shape (n_train, n_features) 
    
Training data. 

y_trainnumpy array of shape (n_train,) 
    
Training ground truth. 

X_testnumpy array of shape (n_test, n_features) 
    
Test data. 

y_testnumpy array of shape (n_test,) 
    
Test ground truth. 

pyod.utils.data.get_outliers_inliers(_X_ , _y_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#get_outliers_inliers)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.get_outliers_inliers "Link to this definition") 
    
Internal method to separate inliers from outliers.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id8 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The input samples 

ylist or array of shape (n_samples,) 
    
The ground truth of input samples.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id9 "Link to this heading") 

X_outliersnumpy array of shape (n_samples, n_features) 
    
Outliers. 

X_inliersnumpy array of shape (n_samples, n_features) 
    
Inliers.
###### pyod.utils.example module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.example "Link to this heading")
Utility functions for running examples 

pyod.utils.example.data_visualize(_X_train_ , _y_train_ , _show_figure =True_, _save_figure =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/example.html#data_visualize)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.data_visualize "Link to this definition") 
    
Utility function for visualizing the synthetic samples generated by generate_data_cluster function.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id10 "Link to this heading") 

X_trainnumpy array of shape (n_samples, n_features) 
    
The training samples. 

y_trainlist or array of shape (n_samples,) 
    
The ground truth of training samples. 

show_figurebool, optional (default=True) 
    
If set to True, show the figure. 

save_figurebool, optional (default=False) 
    
If set to True, save the figure to the local. 

pyod.utils.example.visualize(_clf_name_ , _X_train_ , _y_train_ , _X_test_ , _y_test_ , _y_train_pred_ , _y_test_pred_ , _show_figure =True_, _save_figure =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/example.html#visualize)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.visualize "Link to this definition") 
    
Utility function for visualizing the results in examples. Internal use only.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id11 "Link to this heading") 

clf_namestr 
    
The name of the detector. 

X_trainnumpy array of shape (n_samples, n_features) 
    
The training samples. 

y_trainlist or array of shape (n_samples,) 
    
The ground truth of training samples. 

X_testnumpy array of shape (n_samples, n_features) 
    
The test samples. 

y_testlist or array of shape (n_samples,) 
    
The ground truth of test samples. 

y_train_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the training samples. 

y_test_prednumpy array of shape (n_samples, n_features) 
    
The predicted binary labels of the test samples. 

show_figurebool, optional (default=True) 
    
If set to True, show the figure. 

save_figurebool, optional (default=False) 
    
If set to True, save the figure to the local.
###### pyod.utils.stat_models module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.stat_models "Link to this heading")
A collection of statistical models 

pyod.utils.stat_models.column_ecdf(_matrix :ndarray_) ‚Üí ndarray[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#column_ecdf)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.column_ecdf "Link to this definition") 
    
Utility function to compute the column wise empirical cumulative distribution of a 2D feature matrix, where the rows are samples and the columns are features per sample. The accumulation is done in the positive direction of the sample axis.
E.G. p(1) = 0.2, p(0) = 0.3, p(2) = 0.1, p(6) = 0.4 ECDF E(5) = p(x <= 5) ECDF E would be E(-1) = 0, E(0) = 0.3, E(1) = 0.5, E(2) = 0.6, E(3) = 0.6, E(4) = 0.6, E(5) = 0.6, E(6) = 1
Similar to and tested against: <https://www.statsmodels.org/stable/generated/statsmodels.distributions.empirical_distribution.ECDF.html>
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id12 "Link to this heading") 

pyod.utils.stat_models.ecdf_terminate_equals_inplace(_matrix :ndarray_, _probabilities :ndarray_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#ecdf_terminate_equals_inplace)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.ecdf_terminate_equals_inplace "Link to this definition") 
    
This is a helper function for computing the ecdf of an array. It has been outsourced from the original function in order to be able to use the njit compiler of numpy for increased speeds, as it unfortunately needs a loop over all rows and columns of a matrix. It acts in place on the probabilities‚Äô matrix.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id13 "Link to this heading")
matrix : a feature matrix where the rows are samples and each column is a feature !(expected to be sorted)! 

probabilitiesa probability matrix that will be used building the ecdf. It has values between 0 and 1 and 
    
is also sorted.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id14 "Link to this heading") 

pyod.utils.stat_models.pairwise_distances_no_broadcast(_X_ , _Y_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#pairwise_distances_no_broadcast)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pairwise_distances_no_broadcast "Link to this definition") 
    
Utility function to calculate row-wise euclidean distance of two matrix. Different from pair-wise calculation, this function would not broadcast.
For instance, X and Y are both (4,3) matrices, the function would return a distance vector with shape (4,), instead of (4,4).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id15 "Link to this heading") 

Xarray of shape (n_samples, n_features) 
    
First input samples 

Yarray of shape (n_samples, n_features) 
    
Second input samples
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id16 "Link to this heading") 

distancearray of shape (n_samples,) 
    
Row-wise euclidean distance of X and Y 

pyod.utils.stat_models.pearsonr_mat(_mat_ , _w =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#pearsonr_mat)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pearsonr_mat "Link to this definition") 
    
Utility function to calculate pearson matrix (row-wise).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id17 "Link to this heading") 

matnumpy array of shape (n_samples, n_features) 
    
Input matrix. 

wnumpy array of shape (n_features,) 
    
Weights.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id18 "Link to this heading") 

pear_matnumpy array of shape (n_samples, n_samples) 
    
Row-wise pearson score matrix. 

pyod.utils.stat_models.wpearsonr(_x_ , _y_ , _w =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#wpearsonr)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr "Link to this definition") 
    
Utility function to calculate the weighted Pearson correlation of two samples.
See <https://stats.stackexchange.com/questions/221246/such-thing-as-a-weighted-correlation> for more information
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id19 "Link to this heading") 

xarray, shape (n,) 
    
Input x. 

yarray, shape (n,) 
    
Input y. 

warray, shape (n,) 
    
Weights w.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id20 "Link to this heading") 

scoresfloat in range of [-1,1] 
    
Weighted Pearson Correlation between x and y.
###### pyod.utils.utility module[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility "Link to this heading")
A set of utility functions to support outlier detection. 

pyod.utils.utility.argmaxn(_value_list_ , _n_ , _order ='desc'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#argmaxn)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.argmaxn "Link to this definition") 
    
Return the index of top n elements in the list if order is set to ‚Äòdesc‚Äô, otherwise return the index of n smallest ones.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id21 "Link to this heading") 

value_listlist, array, numpy array of shape (n_samples,) 
    
A list containing all values. 

nint 
    
The number of elements to select. 

orderstr, optional (default=‚Äôdesc‚Äô) 
    
The order to sort {‚Äòdesc‚Äô, ‚Äòasc‚Äô}:
  * ‚Äòdesc‚Äô: descending
  * ‚Äòasc‚Äô: ascending


######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id22 "Link to this heading") 

index_listnumpy array of shape (n,) 
    
The index of the top n elements. 

pyod.utils.utility.check_detector(_detector_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#check_detector)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_detector "Link to this definition") 
    
Checks if fit and decision_function methods exist for given detector
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id23 "Link to this heading") 

detectorpyod.models 
    
Detector instance for which the check is performed. 

pyod.utils.utility.check_parameter(_param_ , _low =-2147483647_, _high =2147483647_, _param_name =''_, _include_left =False_, _include_right =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#check_parameter)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_parameter "Link to this definition") 
    
Check if an input is within the defined range.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id24 "Link to this heading") 

paramint, float 
    
The input parameter to check. 

lowint, float 
    
The lower bound of the range. 

highint, float 
    
The higher bound of the range. 

param_namestr, optional (default=‚Äô‚Äô) 
    
The name of the parameter. 

include_leftbool, optional (default=False) 
    
Whether includes the lower bound (lower bound <=). 

include_rightbool, optional (default=False) 
    
Whether includes the higher bound (<= higher bound).
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id25 "Link to this heading") 

within_rangebool or raise errors 
    
Whether the parameter is within the range of (low, high) 

pyod.utils.utility.generate_bagging_indices(_random_state_ , _bootstrap_features_ , _n_features_ , _min_features_ , _max_features_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#generate_bagging_indices)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_bagging_indices "Link to this definition") 
    
Randomly draw feature indices. Internal use only.
Modified from sklearn/ensemble/bagging.py
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id26 "Link to this heading") 

random_stateRandomState 
    
A random number generator instance to define the state of the random permutations generator. 

bootstrap_featuresbool 
    
Specifies whether to bootstrap indice generation 

n_featuresint 
    
Specifies the population size when generating indices 

min_featuresint 
    
Lower limit for number of features to randomly sample 

max_featuresint 
    
Upper limit for number of features to randomly sample
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id27 "Link to this heading") 

feature_indicesnumpy array, shape (n_samples,) 
    
Indices for features to bag 

pyod.utils.utility.generate_indices(_random_state_ , _bootstrap_ , _n_population_ , _n_samples_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#generate_indices)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_indices "Link to this definition") 
    
Draw randomly sampled indices. Internal use only.
See sklearn/ensemble/bagging.py
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id28 "Link to this heading") 

random_stateRandomState 
    
A random number generator instance to define the state of the random permutations generator. 

bootstrapbool 
    
Specifies whether to bootstrap indice generation 

n_populationint 
    
Specifies the population size when generating indices 

n_samplesint 
    
Specifies number of samples to draw
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id29 "Link to this heading") 

indicesnumpy array, shape (n_samples,) 
    
randomly drawn indices 

pyod.utils.utility.get_diff_elements(_li1_ , _li2_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#get_diff_elements)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_diff_elements "Link to this definition") 
    
get the elements in li1 but not li2, and vice versa
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id30 "Link to this heading") 

li1list or numpy array 
    
Input list 1. 

li2list or numpy array 
    
Input list 2.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id31 "Link to this heading") 

differencelist 
    
The difference between li1 and li2. 

pyod.utils.utility.get_intersection(_lst1_ , _lst2_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#get_intersection)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_intersection "Link to this definition") 
    
get the overlapping between two lists
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id32 "Link to this heading") 

li1list or numpy array 
    
Input list 1. 

li2list or numpy array 
    
Input list 2.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id33 "Link to this heading") 

differencelist 
    
The overlapping between li1 and li2. 

pyod.utils.utility.get_label_n(_y_ , _y_pred_ , _n =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#get_label_n)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n "Link to this definition") 
    
Function to turn raw outlier scores into binary labels by assign 1 to top n outlier scores.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id34 "Link to this heading") 

ylist or numpy array of shape (n_samples,) 
    
The ground truth. Binary (0: inliers, 1: outliers). 

y_predlist or numpy array of shape (n_samples,) 
    
The raw outlier scores as returned by a fitted model. 

nint, optional (default=None) 
    
The number of outliers. if not defined, infer using ground truth.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id35 "Link to this heading") 

labelsnumpy array of shape (n_samples,) 
    
binary labels 0: normal points and 1: outliers
######### Examples[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#examples "Link to this heading")
```
>>> frompyod.utils.utilityimport get_label_n
>>> y = [0, 1, 1, 0, 0]
>>> y_pred = [0.1, 0.5, 0.3, 0.2, 0.7]
>>> get_label_n(y, y_pred)
array([0, 1, 0, 0, 1])

```


pyod.utils.utility.get_list_diff(_li1_ , _li2_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#get_list_diff)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_list_diff "Link to this definition") 
    
get the elements in li1 but not li2. li1-li2
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id36 "Link to this heading") 

li1list or numpy array 
    
Input list 1. 

li2list or numpy array 
    
Input list 2.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id37 "Link to this heading") 

differencelist 
    
The difference between li1 and li2. 

pyod.utils.utility.get_optimal_n_bins(_X_ , _upper_bound =None_, _epsilon =1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#get_optimal_n_bins)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_optimal_n_bins "Link to this definition") 
    
Determine optimal number of bins for a histogram using the Birge Rozenblac method (see [[BBirgeR06](https://pyod.readthedocs.io/en/latest/pyod.models.html#id1152 "Lucien Birg√© and Yves Rozenholc. How many bins should be put in a regular histogram. ESAIM: Probability and Statistics, 10:24‚Äì45, 2006.")] for details.)
See <https://doi.org/10.1051/ps:2006001>
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id39 "Link to this heading") 

Xarray-like of shape (n_samples, n_features)  
    
The samples to determine the optimal number of bins for. 

upper_boundint, default=None  
    
The maximum value of n_bins to be considered. If set to None, np.sqrt(X.shape[0]) will be used as upper bound. 

epsilonfloat, default = 1  
    
A stabilizing term added to the logarithm to prevent division by zero.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id40 "Link to this heading") 

optimal_n_binsint  
    
The optimal value of n_bins according to the Birge Rozenblac method 

pyod.utils.utility.invert_order(_scores_ , _method ='multiplication'_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#invert_order)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.invert_order "Link to this definition") 
    
Invert the order of a list of values. The smallest value becomes the largest in the inverted list. This is useful while combining multiple detectors since their score order could be different.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id41 "Link to this heading") 

scoreslist, array or numpy array with shape (n_samples,) 
    
The list of values to be inverted 

methodstr, optional (default=‚Äômultiplication‚Äô) 
    
Methods used for order inversion. Valid methods are:
  * ‚Äòmultiplication‚Äô: multiply by -1
  * ‚Äòsubtraction‚Äô: max(scores) - scores


######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id42 "Link to this heading") 

inverted_scoresnumpy array of shape (n_samples,) 
    
The inverted list
######### Examples[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id43 "Link to this heading")
```
>>> scores1 = [0.1, 0.3, 0.5, 0.7, 0.2, 0.1]
>>> invert_order(scores1)
array([-0.1, -0.3, -0.5, -0.7, -0.2, -0.1])
>>> invert_order(scores1, method='subtraction')
array([0.6, 0.4, 0.2, 0. , 0.5, 0.6])

```


pyod.utils.utility.precision_n_scores(_y_ , _y_pred_ , _n =None_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#precision_n_scores)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores "Link to this definition") 
    
Utility function to calculate precision @ rank n.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id44 "Link to this heading") 

ylist or numpy array of shape (n_samples,) 
    
The ground truth. Binary (0: inliers, 1: outliers). 

y_predlist or numpy array of shape (n_samples,) 
    
The raw outlier scores as returned by a fitted model. 

nint, optional (default=None) 
    
The number of outliers. if not defined, infer using ground truth.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id45 "Link to this heading") 

precision_at_rank_nfloat 
    
Precision at rank n score. 

pyod.utils.utility.score_to_label(_pred_scores_ , _outliers_fraction =0.1_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#score_to_label)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.score_to_label "Link to this definition") 
    
Turn raw outlier outlier scores to binary labels (0 or 1).
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id46 "Link to this heading") 

pred_scoreslist or numpy array of shape (n_samples,) 
    
Raw outlier scores. Outliers are assumed have larger values. 

outliers_fractionfloat in (0,1) 
    
Percentage of outliers.
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id47 "Link to this heading") 

outlier_labelsnumpy array of shape (n_samples,) 
    
For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. 

pyod.utils.utility.standardizer(_X_ , _X_t =None_, _keep_scalar =False_)[[source]](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#standardizer)[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.standardizer "Link to this definition") 
    
Conduct Z-normalization on data to turn input samples become zero-mean and unit variance.
######### Parameters[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id48 "Link to this heading") 

Xnumpy array of shape (n_samples, n_features) 
    
The training samples 

X_tnumpy array of shape (n_samples_new, n_features), optional (default=None) 
    
The data to be converted 

keep_scalarbool, optional (default=False) 
    
The flag to indicate whether to return the scalar
######### Returns[¬∂](https://pyod.readthedocs.io/en/latest/pyod.utils.html#id49 "Link to this heading") 

X_normnumpy array of shape (n_samples, n_features) 
    
X after the Z-score normalization 

X_t_normnumpy array of shape (n_samples, n_features) 
    
X_t after the Z-score normalization 

scalarsklearn scalar object 
    
The scalar used in conversion
[ Next Known Issues & Warnings ](https://pyod.readthedocs.io/en/latest/issues.html) [ Previous All Models ](https://pyod.readthedocs.io/en/latest/pyod.models.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)
On this page 
  * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)
    * [pyod.utils.data module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.data)
      * [`check_consistent_shape()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.check_consistent_shape)
      * [`evaluate_print()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.evaluate_print)
      * [`generate_data()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data)
      * [`generate_data_categorical()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_categorical)
      * [`generate_data_clusters()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters)
      * [`get_outliers_inliers()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.get_outliers_inliers)
    * [pyod.utils.example module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.example)
      * [`data_visualize()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.data_visualize)
      * [`visualize()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.visualize)
    * [pyod.utils.stat_models module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.stat_models)
      * [`column_ecdf()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.column_ecdf)
      * [`ecdf_terminate_equals_inplace()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.ecdf_terminate_equals_inplace)
      * [`pairwise_distances_no_broadcast()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pairwise_distances_no_broadcast)
      * [`pearsonr_mat()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pearsonr_mat)
      * [`wpearsonr()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr)
    * [pyod.utils.utility module](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility)
      * [`argmaxn()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.argmaxn)
      * [`check_detector()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_detector)
      * [`check_parameter()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_parameter)
      * [`generate_bagging_indices()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_bagging_indices)
      * [`generate_indices()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_indices)
      * [`get_diff_elements()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_diff_elements)
      * [`get_intersection()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_intersection)
      * [`get_label_n()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n)
      * [`get_list_diff()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_list_diff)
      * [`get_optimal_n_bins()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_optimal_n_bins)
      * [`invert_order()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.invert_order)
      * [`precision_n_scores()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores)
      * [`score_to_label()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.score_to_label)
      * [`standardizer()`](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.standardizer)



---

## 16. Outlier Detection 101 - pyod 2.0.5 documentation {#16-1}

**URL:** https://pyod.readthedocs.io/en/latest/relevant_knowledge.html
**Ê∑±Â∫¶:** 1
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:05

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
[ View this page ](https://pyod.readthedocs.io/en/latest/_sources/relevant_knowledge.rst.txt "View this page")
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Outlier Detection 101[¬∂](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html#outlier-detection-101 "Link to this heading")
Outlier detection broadly refers to the task of identifying observations which may be considered anomalous given the distribution of a sample. Any observation belonging to the distribution is referred to as an inlier and any outlying point is referred to as an outlier.
In the context of machine learning, there are three common approaches for this task:
  1. 

Unsupervised Outlier Detection
    
     * Training data (unlabelled) contains both normal and anomalous observations.
     * The model identifies outliers during the fitting process.
     * This approach is taken when outliers are defined as points that exist in low-density regions in the data.
     * Any new observations that do not belong to high-density regions are considered outliers.
  2. 

Semi-supervised Novelty Detection
    
     * Training data consists only of observations describing normal behavior.
     * The model is fit on training data and then used to evaluate new observations.
     * This approach is taken when outliers are defined as points differing from the distribution of the training data.
     * Any new observations differing from the training data within a threshold, even if they form a high-density region, are considered outliers.
  3. 

Supervised Outlier Classification
    
     * The ground truth label (inlier vs outlier) for every observation is known.
     * The model is fit on imbalanced training data and then used to classify new observations.
     * This approach is taken when ground truth is available and it is assumed that outliers will follow the same distribution as in the training set.
     * Any new observations are classified using the model.


The algorithms found in _PyOD_ focus on the first two approaches which differ in terms of how the training data is defined and how the model‚Äôs outputs are interpreted. If interested in learning more, please refer to our [Anomaly Detection Resources](https://github.com/yzhao062/anomaly-detection-resources) page for relevant related books, papers, videos, and toolboxes.
[ Next Citations & Achievements ](https://pyod.readthedocs.io/en/latest/pubs.html) [ Previous Known Issues & Warnings ](https://pyod.readthedocs.io/en/latest/issues.html)
Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 17. pyod.models.abod - pyod 2.0.5 documentation {#17-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:20

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/abod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.abod
```
### -*- coding: utf-8 -*-
"""Angle-based Outlier Detector (ABOD)
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importwarnings
fromitertoolsimport combinations

importnumpyasnp
fromnumbaimport njit
fromsklearn.neighborsimport KDTree
fromsklearn.neighborsimport NearestNeighbors
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport check_parameter


@njit
def_wcos(curr_pt, a, b):  ### pragma: no cover
"""Internal function to calculate weighted cosine using optimized
    numba code.

    Parameters
    ----------
    curr_pt : numpy array of shape (n_samples, n_features)
        Current sample to be calculated.

    a : numpy array of shape (n_samples, n_features)
        Training sample a.

    b : numpy array of shape (n_samples, n_features)
        Training sample b.

    Returns
    -------
    wcos : float in range [-1, 1]
        Cosine similarity between a-curr_pt and b-curr_pt.

    """

    a_curr = a - curr_pt
    b_curr = b - curr_pt

    ### wcos = (<a_curr, b_curr>/((|a_curr|*|b_curr|)^2)
    wcos = np.dot(a_curr, b_curr) / (
            np.linalg.norm(a_curr, 2) ** 2) / (
                   np.linalg.norm(b_curr, 2) ** 2)
    return wcos


def_calculate_wocs(curr_pt, X, X_ind):
"""Calculated the variance of weighted cosine of a point.
    wcos = (<a_curr, b_curr>/((|a_curr|*|b_curr|)^2)

    Parameters
    ----------
    curr_pt : numpy array, shape (1, n_features)
        The sample to be calculated.

    X : numpy array of shape (n_samples, n_features)
        The training dataset.

    X_ind : list
        The valid index of the training data.

    Returns
    -------
    cos_angle_var : float
        The variance of cosine angle

    """
    wcos_list = []
    curr_pair_inds = list(combinations(X_ind, 2))
    for j, (a_ind, b_ind) in enumerate(curr_pair_inds):
        a = X[a_ind, :]
        b = X[b_ind, :]

        ### skip if no angle can be formed
        if np.array_equal(a, curr_pt) or np.array_equal(b, curr_pt):
            continue
        ### add the weighted cosine to the list
        wcos_list.append(_wcos(curr_pt, a, b))
    return np.var(wcos_list)


### noinspection PyPep8Naming


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD)
classABOD(BaseDetector):
"""ABOD class for Angle-base Outlier Detection.
    For an observation, the variance of its weighted cosine scores to all
    neighbors could be viewed as the outlying score.
    See :cite:`kriegel2008angle` for details.

    Two version of ABOD are supported:

    - Fast ABOD: use k nearest neighbors to approximate.
    - Original ABOD: consider all training points with high time complexity at
      O(n^3).

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_neighbors : int, optional (default=10)
        Number of neighbors to use by default for k neighbors queries.

    method: str, optional (default='fast')
        Valid values for metric are:

        - 'fast': fast ABOD. Only consider n_neighbors of training points
        - 'default': original ABOD with all training points, which could be
          slow

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_neighbors=5, method='fast'):
        super(ABOD, self).__init__(contamination=contamination)
        self.method = method
        self.n_neighbors = n_neighbors



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.X_train_ = X
        self.n_train_ = X.shape[0]
        self.decision_scores_ = np.zeros([self.n_train_, 1])

        if self.method == 'fast':
            self._fit_fast()
        elif self.method == 'default':
            self._fit_default()
        else:
            raise ValueError(self.method, "is not a valid method")

        ### flip the scores
        self.decision_scores_ = self.decision_scores_.ravel() * -1
        self._process_decision_scores()
        return self




    def_fit_default(self):
"""Default ABOD method. Use all training points with high complexity
        O(n^3). For internal use only.
        """
        for i in range(self.n_train_):
            curr_pt = self.X_train_[i, :]

            ### get the index pairs of the neighbors, remove itself from index
            X_ind = list(range(0, self.n_train_))
            X_ind.remove(i)

            self.decision_scores_[i, 0] = _calculate_wocs(curr_pt,
                                                          self.X_train_,
                                                          X_ind)
        return self

    def_fit_fast(self):
"""Fast ABOD method. Only use n_neighbors for angle calculation.
        Internal use only
        """

        ### make sure the n_neighbors is in the range
        if self.n_neighbors >= self.n_train_:
            self.n_neighbors = self.n_train_ - 1
            warnings.warn("n_neighbors is set to the number of "
                          "training points minus 1: {0}".format(self.n_train_))

            check_parameter(self.n_neighbors, 1, self.n_train_,
                            include_left=True, include_right=True)

        self.tree_ = KDTree(self.X_train_)

        neigh = NearestNeighbors(n_neighbors=self.n_neighbors)
        neigh.fit(self.X_train_)
        ind_arr = neigh.kneighbors(n_neighbors=self.n_neighbors,
                                   return_distance=False)

        for i in range(self.n_train_):
            curr_pt = self.X_train_[i, :]
            X_ind = ind_arr[i, :]
            self.decision_scores_[i, 0] = _calculate_wocs(curr_pt,
                                                          self.X_train_,
                                                          X_ind)
        return self

    ### noinspection PyPep8Naming


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.abod.ABOD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        check_is_fitted(self, ['X_train_', 'n_train_', 'decision_scores_',
                               'threshold_', 'labels_'])
        X = check_array(X)

        if self.method == 'fast':  ### fast ABOD
            ### outliers have higher outlier scores
            return self._decision_function_fast(X) * -1
        else:  ### default ABOD
            return self._decision_function_default(X) * -1




    def_decision_function_default(self, X):
"""Internal method for predicting outlier scores using default ABOD.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        pred_score : array, shape (n_samples,)
            The anomaly score of the input samples.

        """
        ### initialize the output score
        pred_score = np.zeros([X.shape[0], 1])

        for i in range(X.shape[0]):
            curr_pt = X[i, :]
            ### get the index pairs of the neighbors
            X_ind = list(range(0, self.n_train_))
            pred_score[i, :] = _calculate_wocs(curr_pt, self.X_train_, X_ind)

        return pred_score.ravel()

    def_decision_function_fast(self, X):
"""Internal method for predicting outlier scores using Fast ABOD.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        pred_score : array, shape (n_samples,)
            The anomaly score of the input samples.

        """

        check_is_fitted(self, ['tree_'])
        ### initialize the output score
        pred_score = np.zeros([X.shape[0], 1])

        ### get the indexes of the X's k nearest training points
        _, ind_arr = self.tree_.query(X, k=self.n_neighbors)

        for i in range(X.shape[0]):
            curr_pt = X[i, :]
            X_ind = ind_arr[i, :]
            pred_score[i, :] = _calculate_wocs(curr_pt, self.X_train_, X_ind)

        return pred_score.ravel()




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 18. pyod.models.ae1svm - pyod 2.0.5 documentation {#18-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:49

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ae1svm.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.ae1svm
```
### -*- coding: utf-8 -*-
"""Using AE-1SVM with Outlier Detection (PyTorch)
   Source: https://arxiv.org/pdf/1804.04888
   There is another implementation of this model by Minh Nghia: https://github.com/minh-nghia/AE-1SVM (Tensorflow)
"""
### Author: Zhuo Xiao <zhuoxiao@usc.edu>

importnumpyasnp

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
fromtorchimport nn

fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.stat_modelsimport pairwise_distances_no_broadcast
from..utils.torch_utilityimport get_activation_by_name, TorchDataset


classInnerAE1SVM(nn.Module):
"""Internal model combining an Autoencoder and One-class SVM.

    Parameters
    ----------
    n_features : int
        Number of features in the input data.

    encoding_dim : int
        Dimension of the encoded representation.

    rff_dim : int
        Dimension of the random Fourier features.

    sigma : float, optional (default=1.0)
        Scaling factor for the random Fourier features.

    hidden_neurons : tuple of int, optional (default=(128, 64))
        Number of neurons in the hidden layers.

    dropout_rate : float, optional (default=0.2)
        Dropout rate for regularization.

    batch_norm : bool, optional (default=True)
        Whether to use batch normalization.

    hidden_activation : str, optional (default='relu')
        Activation function for hidden layers.
    """

    def__init__(self, n_features, encoding_dim, rff_dim, sigma=1.0,
                 hidden_neurons=(128, 64),
                 dropout_rate=0.2, batch_norm=True, hidden_activation='relu'):
        super(InnerAE1SVM, self).__init__()

        ### Encoder: Sequential model consisting of linear, batch norm,
        ### activation, and dropout layers.
        self.encoder = nn.Sequential()

        ### Decoder: Sequential model to reconstruct the input from the
        ### encoded representation.
        self.decoder = nn.Sequential()

        ### Random Fourier Features layer for approximating the kernel function.
        self.rff = RandomFourierFeatures(encoding_dim, rff_dim, sigma)

        ### Parameters for the SVM.
        self.svm_weights = nn.Parameter(torch.randn(rff_dim))
        self.svm_bias = nn.Parameter(torch.randn(1))

        ### Activation function
        activation = get_activation_by_name(hidden_activation)
        layers_neurons_encoder = [n_features, *hidden_neurons, encoding_dim]

        ### Build encoder
        for idx in range(len(layers_neurons_encoder) - 1):
            self.encoder.add_module(f"linear{idx}",
                                    nn.Linear(layers_neurons_encoder[idx],
                                              layers_neurons_encoder[idx + 1]))
            if batch_norm:
                self.encoder.add_module(f"batch_norm{idx}", nn.BatchNorm1d(
                    layers_neurons_encoder[idx + 1]))
            self.encoder.add_module(f"activation{idx}", activation)
            self.encoder.add_module(f"dropout{idx}", nn.Dropout(dropout_rate))

        layers_neurons_decoder = layers_neurons_encoder[::-1]

        ### Build decoder
        for idx in range(len(layers_neurons_decoder) - 1):
            self.decoder.add_module(f"linear{idx}",
                                    nn.Linear(layers_neurons_decoder[idx],
                                              layers_neurons_decoder[idx + 1]))
            if batch_norm and idx < len(layers_neurons_decoder) - 2:
                self.decoder.add_module(f"batch_norm{idx}", nn.BatchNorm1d(
                    layers_neurons_decoder[idx + 1]))
            self.decoder.add_module(f"activation{idx}", activation)
            if idx < len(layers_neurons_decoder) - 2:
                self.decoder.add_module(f"dropout{idx}",
                                        nn.Dropout(dropout_rate))

    defforward(self, x):
"""Forward pass through the model.

        Parameters
        ----------
        x : torch.Tensor
            Input data.

        Returns
        -------
        tuple of torch. Tensor
            Reconstructed input and random Fourier features.
        """
        x = self.encoder(x)
        rff_features = self.rff(x)
        x = self.decoder(x)
        return x, rff_features

    defsvm_decision_function(self, rff_features):
"""Compute the SVM decision function.

        Parameters
        ----------
        rff_features : torch.Tensor
            Random Fourier features.

        Returns
        -------
        torch.Tensor
            SVM decision scores.
        """
        return torch.matmul(rff_features, self.svm_weights) + self.svm_bias


classRandomFourierFeatures(nn.Module):
"""Layer for computing random Fourier features.

    Parameters
    ----------
    input_dim : int
        Dimension of the input data.

    output_dim : int
        Dimension of the output features.

    sigma : float, optional (default=1.0)
        Scaling factor for the random Fourier features.
    """

    def__init__(self, input_dim, output_dim, sigma=1.0):
        super(RandomFourierFeatures, self).__init__()
        self.weights = nn.Parameter(torch.randn(input_dim, output_dim) * sigma)
        self.bias = nn.Parameter(torch.randn(output_dim) * 2 * np.pi)

    defforward(self, x):
"""Forward pass to compute random Fourier features.

        Parameters
        ----------
        x : torch.Tensor
            Input data.

        Returns
        -------
        torch.Tensor
            Random Fourier features.
        """
        x = torch.matmul(x, self.weights) + self.bias
        return torch.cos(x)




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM)
classAE1SVM(BaseDetector):
"""Auto Encoder with One-class SVM for anomaly detection.

    Note: self.device is needed or all tensors may not be on the same device
    (if device w/ GPU running)

    Parameters
    ----------
    hidden_neurons : list, optional (default=[64, 32])
        Number of neurons in each hidden layer.

    hidden_activation : str, optional (default='relu')
        Activation function for the hidden layers.

    batch_norm : bool, optional (default=True)
        Whether to use batch normalization.

    learning_rate : float, optional (default=1e-3)
        Learning rate for training the model.

    epochs : int, optional (default=50)
        Number of training epochs.

    batch_size : int, optional (default=32)
        Size of each training batch.

    dropout_rate : float, optional (default=0.2)
        Dropout rate for regularization.

    weight_decay : float, optional (default=1e-5)
        Weight decay (L2 penalty) for the optimizer.

    preprocessing : bool, optional (default=True)
        Whether to apply standard scaling to the input data.

    loss_fn : callable, optional (default=torch.nn.MSELoss)
        Loss function to use for reconstruction loss.

    contamination : float, optional (default=0.1)
        Proportion of outliers in the data.

    alpha : float, optional (default=1.0)
        Weight for the reconstruction loss in the final loss computation.

    sigma : float, optional (default=1.0)
        Scaling factor for the random Fourier features.

    nu : float, optional (default=0.1)
        Parameter for the SVM loss.

    kernel_approx_features : int, optional (default=1000)
        Number of random Fourier features to approximate the kernel.
    """

    def__init__(self, hidden_neurons=None, hidden_activation='relu',
                 batch_norm=True, learning_rate=1e-3, epochs=50, batch_size=32,
                 dropout_rate=0.2, weight_decay=1e-5, preprocessing=True,
                 loss_fn=None, contamination=0.1, alpha=1.0, sigma=1.0, nu=0.1,
                 kernel_approx_features=1000):
        super(AE1SVM, self).__init__(contamination=contamination)

        self.model = None
        self.decision_scores_ = None
        self.std = None
        self.mean = None
        self.hidden_neurons = hidden_neurons
        self.hidden_activation = hidden_activation
        self.batch_norm = batch_norm
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.batch_size = batch_size
        self.dropout_rate = dropout_rate
        self.weight_decay = weight_decay
        self.preprocessing = preprocessing
        self.loss_fn = loss_fn or torch.nn.MSELoss()
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")
        self.hidden_neurons = hidden_neurons or [64, 32]
        self.alpha = alpha
        self.sigma = sigma
        self.nu = nu
        self.kernel_approx_features = kernel_approx_features



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.fit)
    deffit(self, X, y=None):
"""Fit the model to the data.

        Parameters
        ----------
        X : numpy.ndarray
            Input data.

        y : None
            Ignored, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)

        n_samples, n_features = X.shape
        if self.preprocessing:
            self.mean, self.std = np.mean(X, axis=0), np.std(X, axis=0)
            self.std[self.std == 0] = 1e-6  ### Avoid division by zero
            train_set = TorchDataset(X=X, mean=self.mean, std=self.std,
                                     return_idx=True)
        else:
            train_set = TorchDataset(X=X, return_idx=True)

        train_loader = torch.utils.data.DataLoader(train_set,
                                                   batch_size=self.batch_size,
                                                   shuffle=True)
        self.model = InnerAE1SVM(n_features=n_features, encoding_dim=32,
                                 rff_dim=self.kernel_approx_features,
                                 sigma=self.sigma,
                                 hidden_neurons=self.hidden_neurons,
                                 dropout_rate=self.dropout_rate,
                                 batch_norm=self.batch_norm,
                                 hidden_activation=self.hidden_activation)
        self.model = self.model.to(self.device)
        self._train_autoencoder(train_loader)

        if self.best_model_dict is not None:
            self.model.load_state_dict(self.best_model_dict)
        else:
            raise ValueError('Training failed, no valid model state found')

        if not isinstance(X, np.ndarray):
            X = np.array(X)

        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()
        return self




    def_train_autoencoder(self, train_loader):
"""Train the autoencoder.

        Parameters
        ----------
        train_loader : torch.utils.data.DataLoader
            DataLoader for the training data.
        """
        optimizer = torch.optim.Adam(self.model.parameters(),
                                     lr=self.learning_rate,
                                     weight_decay=self.weight_decay)
        self.best_loss = float('inf')
        self.best_model_dict = None

        for epoch in range(self.epochs):
            overall_loss = []
            for data, data_idx in train_loader:
                data = data.to(self.device).float()
                reconstructions, rff_features = self.model(data)
                recon_loss = self.loss_fn(data, reconstructions)
                svm_scores = self.model.svm_decision_function(rff_features)
                svm_loss = torch.mean(torch.clamp(1 - svm_scores, min=0))

                loss = self.alpha * recon_loss + svm_loss
                self.model.zero_grad()
                loss.backward()
                optimizer.step()
                overall_loss.append(loss.item())
            if (epoch + 1) % 10 == 0:
                print(
                    f'Epoch {epoch+1}/{self.epochs}, Loss: {np.mean(overall_loss)}')

            if np.mean(overall_loss) < self.best_loss:
                self.best_loss = np.mean(overall_loss)
                self.best_model_dict = self.model.state_dict()



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ae1svm.AE1SVM.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        Parameters
        ----------
        X : numpy.ndarray
            The input samples.

        Returns
        -------
        numpy.ndarray
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['model', 'best_model_dict'])
        X = check_array(X)
        dataset = TorchDataset(X=X, mean=self.mean,
                               std=self.std, return_idx=True) \
            if self.preprocessing else (TorchDataset(X=X, return_idx=True))
        dataloader = torch.utils.data.DataLoader(dataset,
                                                 batch_size=self.batch_size,
                                                 shuffle=False)
        self.model.eval()

        outlier_scores = np.zeros([X.shape[0], ])
        with torch.no_grad():
            for data, data_idx in dataloader:
                data = data.to(self.device).float()
                reconstructions, rff_features = self.model(data)
                scores = pairwise_distances_no_broadcast(data.cpu().numpy(),
                                                         reconstructions.cpu().numpy())
                outlier_scores[data_idx] = scores
        return outlier_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 19. pyod.models.alad - pyod 2.0.5 documentation {#19-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:13

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/alad.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.alad
```
### -*- coding: utf-8 -*-
"""Using Adversarially Learned Anomaly Detection
"""
### Author: Michiel Bongaerts (but not author of the ALAD method)
### Pytorch version Author: Jiaqi Li <jli77629@usc.edu>


importnumpyasnp
importpandasaspd

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.optimasoptim
frommatplotlibimport pyplot as plt
fromsklearn.preprocessingimport StandardScaler
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport check_parameter




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD)
classALAD(BaseDetector):
"""Adversarially Learned Anomaly Detection (ALAD). 
    Paper: https://arxiv.org/pdf/1812.02288.pdf

    See :cite:`zenati2018adversarially` for details.
    Parameters
    ----------
    output_activation : str, optional (default=None)
        Activation function to use for output layers for encoder and dector.

    activation_hidden_disc : str, optional (default='tanh')
        Activation function to use for hidden layers in discrimators.

    activation_hidden_gen : str, optional (default='tanh')
        Activation function to use for hidden layers in encoder and decoder
        (i.e. generator).

    epochs : int, optional (default=500)
        Number of epochs to train the model.

    batch_size : int, optional (default=32)
        Number of samples per gradient update.

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    dec_layers : list, optional (default=[5,10,25])
        List that indicates the number of nodes per hidden layer for the d
        ecoder network.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    enc_layers : list, optional (default=[25,10,5])
        List that indicates the number of nodes per hidden layer for the
        encoder network.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    disc_xx_layers : list, optional (default=[25,10,5])
        List that indicates the number of nodes per hidden layer for
        discriminator_xx.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    disc_zz_layers : list, optional (default=[25,10,5])
        List that indicates the number of nodes per hidden layer for
        discriminator_zz.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    disc_xz_layers : list, optional (default=[25,10,5])
        List that indicates the number of nodes per hidden layer for
        discriminator_xz.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    learning_rate_gen: float in (0., 1), optional (default=0.001)
        learning rate of training the encoder and decoder

    learning_rate_disc: float in (0., 1), optional (default=0.001)
        learning rate of training the discriminators

    add_recon_loss: bool optional (default=False)
        add an extra loss for encoder and decoder based on the reconstruction
        error

    lambda_recon_loss: float in (0., 1), optional (default=0.1)
        if ``add_recon_loss= True``, the reconstruction loss gets multiplied
        by ``lambda_recon_loss`` and added to the total loss for the generator
         (i.e. encoder and decoder).

    preprocessing : bool, optional (default=True)
        If True, apply standardization on the data.

    verbose : int, optional (default=1)
        Verbosity mode.
        - 0 = silent
        - 1 = progress bar

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. When fitting this is used
        to define the threshold on the decision function.

    device : str or None, optional (default=None)
        The device to use for computation. If None, the default device will be used.
        Possible values include 'cpu' or 'gpu'. This parameter allows the user
        to specify the preferred device for running the model.
    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data [0,1].
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, activation_hidden_gen='tanh',
                 activation_hidden_disc='tanh',
                 output_activation=None,
                 dropout_rate=0.2,
                 latent_dim=2,
                 dec_layers=[5, 10, 25],
                 enc_layers=[25, 10, 5],
                 disc_xx_layers=[25, 10, 5],
                 disc_zz_layers=[25, 10, 5],
                 disc_xz_layers=[25, 10, 5],
                 learning_rate_gen=0.0001, learning_rate_disc=0.0001,
                 add_recon_loss=False, lambda_recon_loss=0.1,
                 epochs=200,
                 verbose=0,
                 preprocessing=False,
                 add_disc_zz_loss=True, spectral_normalization=False,
                 batch_size=32, contamination=0.1, device=None):
        super(ALAD, self).__init__(contamination=contamination)

        self.device = device if device else torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")
        self.activation_hidden_disc = activation_hidden_disc
        self.activation_hidden_gen = activation_hidden_gen
        self.output_activation = output_activation
        self.dropout_rate = dropout_rate
        self.latent_dim = latent_dim
        self.dec_layers = dec_layers
        self.enc_layers = enc_layers

        self.disc_xx_layers = disc_xx_layers
        self.disc_zz_layers = disc_zz_layers
        self.disc_xz_layers = disc_xz_layers

        self.add_recon_loss = add_recon_loss
        self.lambda_recon_loss = lambda_recon_loss
        self.add_disc_zz_loss = add_disc_zz_loss

        self.contamination = contamination
        self.epochs = epochs
        self.learning_rate_gen = learning_rate_gen
        self.learning_rate_disc = learning_rate_disc
        self.preprocessing = preprocessing
        self.batch_size = batch_size
        self.verbose = verbose
        self.spectral_normalization = spectral_normalization

        if self.spectral_normalization:
            try:
                importtorch.nn.utils.spectral_normasspectral_norm
                self.spectral_norm = spectral_norm
            except ImportError:
                print('Spectral normalization not available. '
                      'Install torch>=1.0.0.')
                self.spectral_normalization = False

        check_parameter(dropout_rate, 0, 1, param_name='dropout_rate',
                        include_left=True)

    def_build_model(self):
        defget_activation(name):
            if name == 'tanh':
                return nn.Tanh()
            elif name == 'sigmoid':
                return nn.Sigmoid()
            elif name == 'relu':
                return nn.ReLU()
            else:
                raise ValueError(
                    "Unsupported activation function: {}".format(name))

        ### Create the decoder
        dec_layers = []
        input_dim = self.latent_dim
        for l_dim in self.dec_layers:
            dec_layers.append(nn.Linear(input_dim, l_dim))
            dec_layers.append(nn.Dropout(self.dropout_rate))
            dec_layers.append(get_activation(self.activation_hidden_gen))
            input_dim = l_dim
        dec_layers.append(nn.Linear(input_dim, self.n_features_))
        if self.output_activation:
            dec_layers.append(get_activation(self.output_activation))
        self.dec = nn.Sequential(*dec_layers).to(self.device)

        ### Create the encoder
        enc_layers = []
        input_dim = self.n_features_
        for l_dim in self.enc_layers:
            enc_layers.append(nn.Linear(input_dim, l_dim))
            enc_layers.append(nn.Dropout(self.dropout_rate))
            enc_layers.append(get_activation(self.activation_hidden_gen))
            input_dim = l_dim
        enc_layers.append(nn.Linear(input_dim, self.latent_dim))
        if self.output_activation:
            enc_layers.append(get_activation(self.output_activation))
        self.enc = nn.Sequential(*enc_layers).to(self.device)

        ### Create the discriminators
        defcreate_discriminator(layers, input_dim):
            disc_layers = []
            for l_dim in layers:
                disc_layers.append(nn.Linear(input_dim, l_dim))
                if self.spectral_normalization:
                    disc_layers[-1] = nn.utils.spectral_norm(disc_layers[-1])
                disc_layers.append(nn.Dropout(self.dropout_rate))
                disc_layers.append(get_activation(self.activation_hidden_disc))
                input_dim = l_dim
            disc_layers.append(nn.Linear(input_dim, 1))
            disc_layers.append(nn.Sigmoid())
            return nn.Sequential(*disc_layers).to(self.device)

        self.disc_xx = create_discriminator(self.disc_xx_layers,
                                            2 * self.n_features_)
        self.disc_zz = create_discriminator(self.disc_zz_layers,
                                            2 * self.latent_dim)
        self.disc_xz = create_discriminator(self.disc_xz_layers,
                                            self.n_features_ + self.latent_dim)

        ### Optimizers
        self.opt_gen = optim.Adam(
            list(self.enc.parameters()) + list(self.dec.parameters()),
            lr=self.learning_rate_gen)
        self.opt_disc = optim.Adam(list(self.disc_xx.parameters()) + list(
            self.disc_xz.parameters()) + list(self.disc_zz.parameters()),
                                   lr=self.learning_rate_disc)

        self.hist_loss_disc = []
        self.hist_loss_gen = []

    deftrain_step(self, data):
        x_real, z_real = data

        x_real = torch.FloatTensor(x_real).to(self.device)
        z_real = torch.FloatTensor(z_real).to(self.device)

        self.opt_disc.zero_grad()
        x_gen = self.dec(z_real)
        z_gen = self.enc(x_real)

        out_true_xz = self.disc_xz(torch.cat((x_real, z_gen), dim=1))
        out_fake_xz = self.disc_xz(torch.cat((x_gen, z_real), dim=1))

        out_true_xx = self.disc_xx(torch.cat((x_real, x_real), dim=1))
        out_fake_xx = self.disc_xx(torch.cat((x_real, x_gen), dim=1))

        loss_dxz = nn.BCELoss()(out_true_xz,
                                torch.ones_like(out_true_xz)) + nn.BCELoss()(
            out_fake_xz, torch.zeros_like(out_fake_xz))
        loss_dxx = nn.BCELoss()(out_true_xx,
                                torch.ones_like(out_true_xx)) + nn.BCELoss()(
            out_fake_xx, torch.zeros_like(out_fake_xx))

        if self.add_disc_zz_loss:
            out_true_zz = self.disc_zz(torch.cat((z_real, z_real), dim=1))
            out_fake_zz = self.disc_zz(torch.cat((z_real, z_gen), dim=1))
            loss_dzz = nn.BCELoss()(out_true_zz, torch.ones_like(
                out_true_zz)) + nn.BCELoss()(out_fake_zz,
                                             torch.zeros_like(out_fake_zz))
            loss_disc = loss_dxz + loss_dzz + loss_dxx
        else:
            loss_disc = loss_dxz + loss_dxx

        loss_disc.backward()
        self.opt_disc.step()

        self.opt_gen.zero_grad()
        x_gen = self.dec(z_real)
        z_gen = self.enc(x_real)

        out_true_xz = self.disc_xz(torch.cat((x_real, z_gen), dim=1))
        out_fake_xz = self.disc_xz(torch.cat((x_gen, z_real), dim=1))

        out_true_xx = self.disc_xx(torch.cat((x_real, x_real), dim=1))
        out_fake_xx = self.disc_xx(torch.cat((x_real, x_gen), dim=1))

        loss_gexz = nn.BCELoss()(out_fake_xz,
                                 torch.ones_like(out_fake_xz)) + nn.BCELoss()(
            out_true_xz, torch.zeros_like(out_true_xz))
        loss_gexx = nn.BCELoss()(out_fake_xx,
                                 torch.ones_like(out_fake_xx)) + nn.BCELoss()(
            out_true_xx, torch.zeros_like(out_true_xx))

        if self.add_disc_zz_loss:
            out_true_zz = self.disc_zz(torch.cat((z_real, z_real), dim=1))
            out_fake_zz = self.disc_zz(torch.cat((z_real, z_gen), dim=1))
            loss_gezz = nn.BCELoss()(out_fake_zz, torch.ones_like(
                out_fake_zz)) + nn.BCELoss()(out_true_zz,
                                             torch.zeros_like(out_true_zz))
            cycle_consistency = loss_gezz + loss_gexx
            loss_gen = loss_gexz + cycle_consistency
        else:
            cycle_consistency = loss_gexx
            loss_gen = loss_gexz + cycle_consistency

        if self.add_recon_loss:
            x_recon = self.dec(self.enc(x_real))
            loss_recon = torch.mean((x_real - x_recon) ** 2)
            loss_gen += loss_recon * self.lambda_recon_loss

        loss_gen.backward()
        self.opt_gen.step()

        self.hist_loss_disc.append(loss_disc.item())
        self.hist_loss_gen.append(loss_gen.item())



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.fit)
    deffit(self, X, y=None, noise_std=0.1):
"""Fit detector. y is ignored in unsupervised methods.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : Ignored
            Not used, present for API consistency by convention.
        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### Get number of sampels and features from train set
        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]
        self._build_model()

        ### Apply data scaling or not
        if self.preprocessing:
            self.scaler_ = StandardScaler()
            X_norm = self.scaler_.fit_transform(X)
        else:
            X_norm = np.copy(X)

        for n in range(self.epochs):
            if n % 50 == 0 and n != 0 and self.verbose == 1:
                print(f'Train iter: {n}')

            ### Shuffle train 
            np.random.shuffle(X_norm)

            X_train_sel = X_norm[
                          :min(self.batch_size, self.n_samples_)].astype(
                np.float32)
            latent_noise = np.random.normal(0, 1, (
                X_train_sel.shape[0], self.latent_dim))
            X_train_sel += np.random.normal(0, noise_std,
                                            size=X_train_sel.shape)
            self.train_step((X_train_sel, latent_noise))

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        pred_scores = self.get_outlier_scores(X_norm)
        self.decision_scores_ = pred_scores
        self._process_decision_scores()
        return self




    deftrain_more(self, X, epochs=100, noise_std=0.1):
"""This function allows the researcher to perform extra training
        instead of the fixed number determined
        by the fit() function.
        """
        ### fit() should have been called first
        check_is_fitted(self, ['decision_scores_'])

        ### Apply data scaling or not
        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        for n in range(epochs):
            if n % 50 == 0 and n != 0 and self.verbose == 1:
                print(f'Train iter: {n}')

            ### Shuffle train 
            np.random.shuffle(X_norm)

            X_train_sel = X_norm[
                          :min(self.batch_size, self.n_samples_)].astype(
                np.float32)
            latent_noise = np.random.normal(0, 1, (
                X_train_sel.shape[0], self.latent_dim))
            X_train_sel += np.random.normal(0, noise_std,
                                            size=X_train_sel.shape)
            self.train_step((X_train_sel, latent_noise))

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        pred_scores = self.get_outlier_scores(X_norm)
        self.decision_scores_ = pred_scores
        self._process_decision_scores()
        return self

    defget_outlier_scores(self, X_norm):
        X_norm = torch.FloatTensor(X_norm).to(self.device)
        X_enc = self.enc(X_norm).detach().cpu().numpy()
        X_enc_gen = self.dec(
            torch.FloatTensor(X_enc).to(self.device)).detach().cpu().numpy()

        out_true_xx = self.disc_xx(
            torch.cat((X_norm, X_norm), dim=1)).detach().cpu().numpy()
        out_fake_xx = self.disc_xx(
            torch.cat((X_norm, torch.FloatTensor(X_enc_gen).to(self.device)),
                      dim=1)).detach().cpu().numpy()

        outlier_scores = np.mean(np.abs((out_true_xx - out_fake_xx) ** 2),
                                 axis=1)
        return outlier_scores



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_'])
        X = check_array(X)

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        X_norm = torch.FloatTensor(X_norm).to(self.device)
        pred_scores = self.get_outlier_scores(X_norm.cpu().numpy())
        return pred_scores






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.alad.ALAD.plot_learning_curves)
    defplot_learning_curves(self, start_ind=0, window_smoothening=10):
        fig = plt.figure(figsize=(12, 5))

        l_gen = pd.Series(self.hist_loss_gen[start_ind:]).rolling(
            window=window_smoothening).mean()
        l_disc = pd.Series(self.hist_loss_disc[start_ind:]).rolling(
            window=window_smoothening).mean()

        ax = fig.add_subplot(1, 2, 1)
        ax.plot(range(len(l_gen)), l_gen)
        ax.set_title('Generator')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')

        ax = fig.add_subplot(1, 2, 2)
        ax.plot(range(len(l_disc)), l_disc)
        ax.set_title('Discriminator(s)')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')

        plt.show()






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 20. pyod.models.anogan - pyod 2.0.5 documentation {#20-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:31

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/anogan.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.anogan
```
### -*- coding: utf-8 -*-
"""Anomaly Detection with Generative Adversarial Networks  (AnoGAN)
 Paper: https://arxiv.org/pdf/1703.05921.pdf
 Note, that this is another implementation of AnoGAN as the one from https://github.com/fuchami/ANOGAN
"""
### Author: Michiel Bongaerts (but not author of the AnoGAN method)
### License: BSD 2 clause

importmatplotlib.pyplotasplt
importnumpyasnp
importpandasaspd

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.optimasoptim
fromsklearn.preprocessingimport StandardScaler
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted
fromtorch.utils.dataimport DataLoader, TensorDataset

from.baseimport BaseDetector
from..utils.torch_utilityimport get_activation_by_name
from..utils.utilityimport check_parameter


classGenerator(nn.Module):
    def__init__(self, latent_dim_G, n_features, G_layers, dropout_rate,
                 activation_hidden, output_activation):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim_G
        self.n_features = n_features
        self.layers = G_layers
        self.dropout_rate = dropout_rate
        self.activation_hidden = activation_hidden
        self.output_activation = output_activation

        self.model = self._build_generator()

    def_build_generator(self):
        layers = [nn.Dropout(self.dropout_rate), nn.Linear(
            self.latent_dim, self.layers[0]),
                  get_activation_by_name(self.activation_hidden)]
        for i in range(1, len(self.layers)):
            layers.extend([
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.layers[i - 1], self.layers[i]),
                get_activation_by_name(self.activation_hidden)
            ])
        layers.append(nn.Linear(self.layers[-1], self.n_features))
        if self.output_activation:
            layers.append(get_activation_by_name(self.output_activation))
        return nn.Sequential(*layers)

    defforward(self, x):
        return self.model(x)


classDiscriminator(nn.Module):
    def__init__(self, n_features, D_layers, dropout_rate, activation_hidden):
        super(Discriminator, self).__init__()
        self.n_features = n_features
        self.layers = D_layers
        self.dropout_rate = dropout_rate
        self.activation_hidden = activation_hidden

        self.model = self._build_discriminator()

    def_build_discriminator(self):
        layers = [nn.Dropout(self.dropout_rate), nn.Linear(
            self.n_features, self.layers[0]),
                  get_activation_by_name(self.activation_hidden)]
        for i in range(1, len(self.layers)):
            layers.extend([
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.layers[i - 1], self.layers[i]),
                get_activation_by_name(self.activation_hidden)
            ])
        layers.extend([nn.Linear(self.layers[-1], 1), nn.Sigmoid()])
        return nn.Sequential(*layers)

    defforward(self, x):
        return self.model(x)


classQueryModel(nn.Module):
    def__init__(self, generator, discriminator, latent_dim_G):
        super(QueryModel, self).__init__()
        self.generator = generator
        self.discriminator = discriminator
        self.z_gamma_layer = nn.Linear(latent_dim_G, latent_dim_G)

    defforward(self, query_sample):
        z_gamma = self.z_gamma_layer(query_sample)
        sample_gen = self.generator(z_gamma)
        sample_disc_latent = self.discriminator(sample_gen)
        return z_gamma, sample_gen, sample_disc_latent




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN)
classAnoGAN(BaseDetector):
"""Anomaly Detection with Generative Adversarial Networks  (AnoGAN).
    See the original paper "Unsupervised anomaly detection with generative
    adversarial networks to guide marker discovery".

    See :cite:`schlegl2017unsupervised` for details.

    Parameters
    ----------

    output_activation : str, optional (default=None)
        Activation function to use for output layer.


    activation_hidden : str, optional (default='tanh')
        Activation function to use for output layer.

    epochs : int, optional (default=500)
        Number of epochs to train the model.

    batch_size : int, optional (default=32)
        Number of samples per gradient update.

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    G_layers : list, optional (default=[20,10,3,10,20])
        List that indicates the number of nodes per hidden layer for the
        generator. Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    D_layers : list, optional (default=[20,10,5])
        List that indicates the number of nodes per hidden layer for the
        discriminator. Thus, [10,10] indicates 2 hidden layers having each 10
        nodes.


    learning_rate: float in (0., 1), optional (default=0.001)
        learning rate of training the network

    index_D_layer_for_recon_error: int, optional (default = 1)
        This is the index of the hidden layer in the discriminator for which
        the reconstruction error will be determined between query sample and
        the sample created from the latent space.

    learning_rate_query: float in (0., 1), optional (default=0.001)
        learning rate for the backpropagation steps needed to find a point in
        the latent space of the generator that approximate the query sample


    epochs_query: int, optional (default=20) 
        Number of epochs to approximate the query sample in the latent space
        of the generator

    preprocessing : bool, optional (default=True)
        If True, apply standardization on the data.

    verbose : int, optional (default=1)
        Verbosity mode.
        - 0 = silent
        - 1 = progress bar

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. When fitting this is used
        to define the threshold on the decision function.

    Attributes
    ----------

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data [0,1].
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, activation_hidden='tanh', dropout_rate=0.2,
                 latent_dim_G=2,
                 G_layers=[20, 10, 3, 10, 20], verbose=0, D_layers=[20, 10, 5],
                 index_D_layer_for_recon_error=1, epochs=500,
                 preprocessing=False,
                 learning_rate=0.001, learning_rate_query=0.01,
                 epochs_query=20,
                 batch_size=32, output_activation=None, contamination=0.1,
                 device=None):
        super(AnoGAN, self).__init__(contamination=contamination)

        self.activation_hidden = activation_hidden
        self.dropout_rate = dropout_rate
        self.latent_dim_G = latent_dim_G
        self.G_layers = G_layers
        self.D_layers = D_layers
        self.index_D_layer_for_recon_error = index_D_layer_for_recon_error
        self.output_activation = output_activation
        self.contamination = contamination
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.learning_rate_query = learning_rate_query
        self.epochs_query = epochs_query
        self.preprocessing = preprocessing
        self.batch_size = batch_size
        self.verbose = verbose

        self.hist_loss_generator = []
        self.hist_loss_discriminator = []

        self.device = device

        check_parameter(dropout_rate, 0, 1,
                        param_name='dropout_rate', include_left=True)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.plot_learning_curves)
    defplot_learning_curves(self, start_ind=0,
                             window_smoothening=10):  ### pragma: no cover
        fig = plt.figure(figsize=(12, 5))
        l_gen = pd.Series(self.hist_loss_generator[start_ind:]).rolling(
            window_smoothening).mean()
        l_disc = pd.Series(self.hist_loss_discriminator[start_ind:]).rolling(
            window_smoothening).mean()

        ax = fig.add_subplot(1, 2, 1)
        ax.plot(range(len(l_gen)), l_gen)
        ax.set_title('Generator')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')

        ax = fig.add_subplot(1, 2, 2)
        ax.plot(range(len(l_disc)), l_disc)
        ax.set_title('Discriminator')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')

        plt.show()






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### Verify and construct the hidden units
        self.n_samples_, self.n_features_ = X.shape

        ### Standardize data for better performance
        if self.preprocessing:
            self.scaler_ = StandardScaler()
            X_norm = self.scaler_.fit_transform(X)
        else:
            X_norm = np.copy(X)
        X_norm = torch.tensor(X_norm, dtype=torch.float32)
        ### train the discriminator and generator
        self.generator = Generator(latent_dim_G=self.latent_dim_G,
                                   n_features=self.n_features_,
                                   G_layers=self.G_layers,
                                   dropout_rate=self.dropout_rate,
                                   activation_hidden=self.activation_hidden,
                                   output_activation=self.output_activation)
        self.discriminator = Discriminator(n_features=self.n_features_,
                                           D_layers=self.D_layers,
                                           dropout_rate=self.dropout_rate,
                                           activation_hidden=self.activation_hidden)

        self.generator.to(self.device)
        self.discriminator.to(self.device)

        optimizer_g = optim.Adam(
            self.generator.parameters(), lr=self.learning_rate)
        optimizer_d = optim.Adam(
            self.discriminator.parameters(), lr=self.learning_rate)

        dataset = TensorDataset(X_norm)
        dataloader = DataLoader(
            dataset, batch_size=self.batch_size, shuffle=True)

        for n in range(self.epochs):
            if n % 100 == 0 and n != 0 and self.verbose == 1:
                print(f'Train iter: {n}')

            self.generator.train()
            self.discriminator.train()
            for X_train_ in dataloader:
                X_train_sel = X_train_[0].to(self.device)
                latent_noise = torch.rand(X_train_sel.size(
                    0), self.latent_dim_G, dtype=torch.float32).to(self.device)

                generated_data = self.generator(latent_noise)
                real_output = self.discriminator(X_train_sel)
                fake_output = self.discriminator(generated_data.detach())

                loss_D_real = nn.BCELoss()(real_output, torch.ones_like(
                    real_output) * 0.9).to(self.device)
                loss_D_fake = nn.BCELoss()(fake_output,
                                           torch.zeros_like(fake_output)).to(
                    self.device)
                loss_D = loss_D_real + loss_D_fake
                optimizer_d.zero_grad()
                loss_D.backward()
                optimizer_d.step()

                fake_output = self.discriminator(generated_data)
                loss_G = nn.BCELoss()(fake_output,
                                      torch.ones_like(fake_output)).to(
                    self.device)
                optimizer_g.zero_grad()
                loss_G.backward()
                optimizer_g.step()

                self.hist_loss_discriminator.append(loss_D.item())
                self.hist_loss_generator.append(loss_G.item())

        ### Instantiate and train the query model
        self.generator.eval()
        self.discriminator.eval()
        self.query_model = QueryModel(
            self.generator, self.discriminator, self.latent_dim_G).to(
            self.device)
        optimizer_query = optim.Adam(
            self.query_model.parameters(), lr=self.learning_rate_query)
        scores = []
        ### For each sample, use a few backpropagation steps to obtain a point in the latent space that best resembles the query sample
        self.query_model.train()
        for i in range(X_norm.shape[0]):
            if self.verbose == 1:
                print('query sample {} / {}'.format(i + 1, X_norm.shape[0]))

            query_sample = X_norm[[i],].to(self.device)
            assert (query_sample.shape[0] == 1)
            assert (query_sample.shape[1] == self.n_features_)

            ### Make pseudo input (just zeros)
            zeros = torch.zeros((1, self.latent_dim_G)).to(self.device)

            ### build model for back-propagating a approximate latent space where
            ### reconstruction with query sample is optimal
            for i in range(self.epochs_query):
                if i % 25 == 0 and self.verbose == 1:
                    print('iter:', i)

                z, sample_gen, sample_disc_latent = self.query_model(zeros)
                with torch.no_grad():
                    sample_disc_latent_original = self.discriminator(
                        query_sample)
                ### Reconstruction loss generator
                loss_recon_gen = torch.mean(torch.mean(
                    torch.abs(query_sample - sample_gen), axis=-1))
                ### Reconstruction loss latent space of discrimator
                loss_recon_disc = torch.mean(torch.mean(
                    torch.abs(
                        sample_disc_latent_original - sample_disc_latent),
                    axis=-1))
                total_loss = loss_recon_gen + loss_recon_disc

                optimizer_query.zero_grad()
                total_loss.backward()
                optimizer_query.step()
            ### Predict on X itself and calculate the reconstruction error as
            ### the outlier scores.
            scores.append(total_loss.item())

        self.decision_scores_ = np.array(scores)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.anogan.AnoGAN.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_'])
        X = check_array(X)

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        X_norm = torch.tensor(X_norm, dtype=torch.float32)

        ### Predict on X
        pred_scores = []

        self.query_model.eval()
        with torch.no_grad():
            for i in range(X_norm.shape[0]):
                if self.verbose == 1:
                    print(
                        'query sample {} / {}'.format(i + 1, X_norm.shape[0]))

                query_sample = X_norm[[i],].to(self.device)

                zeros = torch.zeros((1, self.latent_dim_G)).to(self.device)
                z, sample_gen, sample_disc_latent = self.query_model(zeros)
                sample_disc_latent_original = self.discriminator(query_sample)

                loss_recon_gen = torch.mean(torch.mean(
                    torch.abs(query_sample - sample_gen), axis=-1))
                loss_recon_disc = torch.mean(torch.mean(
                    torch.abs(
                        sample_disc_latent_original - sample_disc_latent),
                    axis=-1))
                total_loss = loss_recon_gen + loss_recon_disc
                pred_scores.append(total_loss.item())

        return np.array(pred_scores)






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 21. pyod.models.auto_encoder - pyod 2.0.5 documentation {#21-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:46

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/auto_encoder.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.auto_encoder
```
### -*- coding: utf-8 -*-
"""Using AutoEncoder with Outlier Detection
"""
### Author: Tiankai Yang <tiankaiy@usc.edu>
### License: BSD 2 clause


try:
    importtorch
except ImportError:
    print('please install torch first')

fromtorchimport nn

frompyod.models.base_dlimport BaseDeepLearningDetector
frompyod.utils.stat_modelsimport pairwise_distances_no_broadcast
frompyod.utils.torch_utilityimport LinearBlock




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder)
classAutoEncoder(BaseDeepLearningDetector):
"""
    Auto Encoder (AE) is a type of neural networks for learning useful data
    representations in an unsupervised manner. Similar to PCA, AE could be used
    to detect outlying objects in the data by calculating the reconstruction
    errors. See :cite:`aggarwal2015outlier` Chapter 3 for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, 
        i.e. the proportion of outliers in the data set. 
        Used when fitting to define the threshold on the decision function.

    preprocessing : bool, optional (default=True)
        If True, apply the preprocessing procedure before training models.

    lr : float, optional (default=1e-3)
        The initial learning rate for the optimizer.

    epoch_num : int, optional (default=10)
        The number of epochs for training.

    batch_size : int, optional (default=32)
        The batch size for training.

    optimizer_name : str, optional (default='adam')
        The name of theoptimizer used to train the model.

    device : str, optional (default=None)
        The device to use for the model. If None, it will be decided
        automatically. If you want to use MPS, set it to 'mps'.

    random_state : int, optional (default=42)
        The random seed for reproducibility.

    use_compile : bool, optional (default=False)
        Whether to compile the model.
        If True, the model will be compiled before training.
        This is only available for
        PyTorch version >= 2.0.0. and Python < 3.12.

    compile_mode : str, optional (default='default')
        The mode to compile the model.
        Can be either ‚Äúdefault‚Äù, ‚Äúreduce-overhead‚Äù,
        ‚Äúmax-autotune‚Äù or ‚Äúmax-autotune-no-cudagraphs‚Äù.
        See https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile for details.

    verbose : int, optional (default=1)
        Verbosity mode.
        - 0 = silent
        - 1 = progress bar
        - 2 = one line per epoch.

    optimizer_params : dict, optional (default={'weight_decay': 1e-5})
        Additional parameters for the optimizer.
        For example, `optimizer_params={'weight_decay': 1e-5}`.

    hidden_neuron_list : list, optional (default=[64, 32])
        The number of neurons per hidden layers. 
        So the network has the structure as [feature_size, 64, 32, 32, 64, feature_size].

    hidden_activation_name : str, optional (default='relu')
        The activation function used in hidden layers.

    batch_norm : boolean, optional (default=True)
        Whether to apply Batch Normalization,
        See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    Attributes
    ----------
    model : torch.nn.Module
        The underlying AutoEncoder model.

    optimizer : torch.optim
        The optimizer used to train the model.

    criterion : torch.nn.modules
        The loss function used to train the model.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self,
                 contamination=0.1, preprocessing=True,
                 lr=1e-3, epoch_num=10, batch_size=32,
                 optimizer_name='adam',
                 device=None, random_state=42,
                 use_compile=False, compile_mode='default',
                 verbose=1,
                 optimizer_params: dict = {'weight_decay': 1e-5},
                 hidden_neuron_list=[64, 32],
                 hidden_activation_name='relu',
                 batch_norm=True, dropout_rate=0.2):
        super(AutoEncoder, self).__init__(contamination=contamination,
                                          preprocessing=preprocessing,
                                          lr=lr, epoch_num=epoch_num,
                                          batch_size=batch_size,
                                          optimizer_name=optimizer_name,
                                          criterion_name='mse',
                                          device=device,
                                          random_state=random_state,
                                          use_compile=use_compile,
                                          compile_mode=compile_mode,
                                          verbose=verbose,
                                          optimizer_params=optimizer_params)
        self.hidden_neuron_list = hidden_neuron_list
        self.hidden_activation_name = hidden_activation_name
        self.batch_norm = batch_norm
        self.dropout_rate = dropout_rate



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.build_model)
    defbuild_model(self):
        self.model = AutoEncoderModel(
            self.feature_size,
            hidden_neuron_list=self.hidden_neuron_list,
            hidden_activation_name=self.hidden_activation_name,
            batch_norm=self.batch_norm,
            dropout_rate=self.dropout_rate)






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.auto_encoder.AutoEncoder.training_forward)
    deftraining_forward(self, batch_data):
        x = batch_data
        x = x.to(self.device)
        self.optimizer.zero_grad()
        x_recon = self.model(x)
        loss = self.criterion(x_recon, x)
        loss.backward()
        self.optimizer.step()
        return loss.item()




    defevaluating_forward(self, batch_data):
        x = batch_data
        x_gpu = x.to(self.device)
        x_recon = self.model(x_gpu)
        score = pairwise_distances_no_broadcast(x.numpy(),
                                                x_recon.cpu().numpy())
        return score





classAutoEncoderModel(nn.Module):
    def__init__(self,
                 feature_size,
                 hidden_neuron_list=[64, 32],
                 hidden_activation_name='relu',
                 batch_norm=True, dropout_rate=0.2):
        super(AutoEncoderModel, self).__init__()

        self.feature_size = feature_size
        self.hidden_neuron_list = hidden_neuron_list
        self.hidden_activation_name = hidden_activation_name
        self.batch_norm = batch_norm
        self.dropout_rate = dropout_rate

        self.encoder = self._build_encoder()
        self.decoder = self._build_decoder()

    def_build_encoder(self):
        encoder_layers = []
        last_neuron_size = self.feature_size
        for neuron_size in self.hidden_neuron_list:
            encoder_layers.append(LinearBlock(
                last_neuron_size, neuron_size,
                activation_name=self.hidden_activation_name,
                batch_norm=self.batch_norm,
                dropout_rate=self.dropout_rate))
            last_neuron_size = neuron_size
        return nn.Sequential(*encoder_layers)

    def_build_decoder(self):
        decoder_layers = []
        last_neuron_size = self.hidden_neuron_list[-1]
        for neuron_size in reversed(self.hidden_neuron_list[:-1]):
            decoder_layers.append(LinearBlock(
                last_neuron_size, neuron_size,
                activation_name=self.hidden_activation_name,
                batch_norm=self.batch_norm,
                dropout_rate=self.dropout_rate))
            last_neuron_size = neuron_size
        decoder_layers.append(nn.Linear(last_neuron_size, self.feature_size))
        return nn.Sequential(*decoder_layers)

    defforward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 22. pyod.models.base - pyod 2.0.5 documentation {#22-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:20

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/base.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.base
```
### -*- coding: utf-8 -*-
"""Base class for all outlier detector models
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importabc
importwarnings
fromcollectionsimport defaultdict
frominspectimport signature

importnumpyasnp
fromnumpyimport percentile
fromscipy.specialimport erf
fromscipy.statsimport binom
fromsklearn.metricsimport roc_auc_score
fromsklearn.preprocessingimport MinMaxScaler
fromsklearn.utilsimport deprecated
fromsklearn.utils.multiclassimport check_classification_targets
fromsklearn.utils.validationimport check_is_fitted
fromscipy.optimizeimport root_scalar

from.sklearn_baseimport _pprint
from..utils.utilityimport precision_n_scores




[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector)
classBaseDetector(metaclass=abc.ABCMeta):
"""Abstract class for all outlier detection algorithms.


    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    @abc.abstractmethod
    def__init__(self, contamination=0.1):

        if (isinstance(contamination, (float, int))):

            if not (0. < contamination <= 0.5):
                raise ValueError("contamination must be in (0, 0.5], "
                                 "got: %f" % contamination)

        ### allow arbitrary input such as PyThreshld object
        self.contamination = contamination

    ### noinspection PyIncorrectDocstring


[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit)
    @abc.abstractmethod
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        pass






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.decision_function)
    @abc.abstractmethod
    defdecision_function(self, X):
"""Predict raw anomaly scores of X using the fitted detector.

        The anomaly score of an input sample is computed based on the fitted
        detector. For consistency, outliers are assigned with
        higher anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        pass






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict)
    @deprecated()
    deffit_predict(self, X, y=None):
"""Fit detector first and then predict whether a particular sample
        is an outlier or not. y is ignored in unsupervised models.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.

        .. deprecated:: 0.6.9
          `fit_predict` will be removed in pyod 0.8.0.; it will be
          replaced by calling `fit` function first and then accessing
          `labels_` attribute for consistency.
        """

        self.fit(X, y)
        return self.labels_






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict)
    defpredict(self, X, return_confidence=False):
"""Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        return_confidence : boolean, optional(default=False)
            If True, also return the confidence of prediction.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.
        confidence : numpy array of shape (n_samples,).
            Only if return_confidence is set to True.
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        pred_score = self.decision_function(X)

        if isinstance(self.contamination, (float, int)):
            prediction = (pred_score > self.threshold_).astype('int').ravel()

        ### if this is a PyThresh object
        else:
            prediction = self.contamination.eval(pred_score)

        if return_confidence:
            confidence = self.predict_confidence(X)
            return prediction, confidence

        return prediction






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_proba)
    defpredict_proba(self, X, method='linear', return_confidence=False):
"""Predict the probability of a sample being outlier. Two approaches
        are possible:

        1. simply use Min-max conversion to linearly transform the outlier
           scores into the range of [0,1]. The model must be
           fitted first.
        2. use unifying scores, see :cite:`kriegel2011interpreting`.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        method : str, optional (default='linear')
            probability conversion method. It must be one of
            'linear' or 'unify'.

        return_confidence : boolean, optional(default=False)
            If True, also return the confidence of prediction.


        Returns
        -------
        outlier_probability : numpy array of shape (n_samples, n_classes)
            For each observation, tells whether or not
            it should be considered as an outlier according to the
            fitted model. Return the outlier probability, ranging
            in [0,1]. Note it depends on the number of classes, which is by
            default 2 classes ([proba of normal, proba of outliers]).
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        train_scores = self.decision_scores_

        test_scores = self.decision_function(X)

        probs = np.zeros([X.shape[0], int(self._classes)])
        if method == 'linear':
            scaler = MinMaxScaler().fit(train_scores.reshape(-1, 1))
            probs[:, 1] = scaler.transform(
                test_scores.reshape(-1, 1)).ravel().clip(0, 1)
            probs[:, 0] = 1 - probs[:, 1]

            if return_confidence:
                confidence = self.predict_confidence(X)
                return probs, confidence

            return probs

        elif method == 'unify':
            ### turn output into probability
            pre_erf_score = (test_scores - self._mu) / (
                    self._sigma * np.sqrt(2))
            erf_score = erf(pre_erf_score)
            probs[:, 1] = erf_score.clip(0, 1).ravel()
            probs[:, 0] = 1 - probs[:, 1]

            if return_confidence:
                confidence = self.predict_confidence(X)
                return probs, confidence

            return probs
        else:
            raise ValueError(method,
                             'is not a valid probability conversion method')






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_confidence)
    defpredict_confidence(self, X):
"""Predict the model's confidence in making the same prediction
        under slightly different training sets.
        See :cite:`perini2020quantifying`.

        Parameters
        -------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        confidence : numpy array of shape (n_samples,)
            For each observation, tells how consistently the model would
            make the same prediction if the training set was perturbed.
            Return a probability, ranging in [0,1].

        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])

        n = len(self.decision_scores_)

        ### todo: this has an optimization opportunity since the scores may
        ### already be available
        test_scores = self.decision_function(X)

        count_instances = np.vectorize(
            lambda x: np.count_nonzero(self.decision_scores_ <= x))
        n_instances = count_instances(test_scores)

        ### Derive the outlier probability using Bayesian approach
        posterior_prob = np.vectorize(lambda x: (1 + x) / (2 + n))(n_instances)

        if not isinstance(self.contamination, (float, int)):
            contam = np.sum(self.labels_) / n
        ### if this is a PyThresh object
        else:
            contam = self.contamination

        ### Transform the outlier probability into a confidence value
        confidence = np.vectorize(
            lambda p: 1 - binom.cdf(n - int(n * contam), n, p))(
            posterior_prob)

        if isinstance(self.contamination, (float, int)):
            prediction = (test_scores > self.threshold_).astype('int').ravel()
        ### if this is a PyThresh object
        else:
            prediction = self.contamination.eval(test_scores)
        np.place(confidence, prediction == 0, 1 - confidence[prediction == 0])

        return confidence






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.predict_with_rejection)
    defpredict_with_rejection(self, X, T=32, return_stats=False,
                               delta=0.1, c_fp=1, c_fn=1, c_r=-1):
"""Predict if a particular sample is an outlier or not, 
           allowing the detector to reject (i.e., output = -2) 
           low confidence predictions.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        T : int, optional(default=32)
            It allows to set the rejection threshold to 1-2exp(-T).
            The higher the value of T, the more rejections are made.
        return_stats: bool, optional (default = False)
                      If true, it returns also three additional float values:
                      the estimated rejection rate, the upper bound rejection
                      rate, and the upper bound of the cost.
        delta: float, optional (default = 0.1)
               The upper bound rejection rate holds with probability 1-delta.
        c_fp, c_fn, c_r: floats (positive), optional (default = [1,1, contamination])
                         costs for false positive predictions (c_fp), false negative
                         predictions (c_fn) and rejections (c_r).
        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
                         For each observation, it tells whether it should be
                         considered as an outlier according to the fitted
                         model. 0 stands for inliers, 1 for outliers and
                         -2 for rejection.
        expected_rejection_rate:   float, if return_stats is True;
        upperbound_rejection_rate: float, if return_stats is True;
        upperbound_cost:           float, if return_stats is True;

        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        if c_r < 0:
            warnings.warn(
                "The cost of rejection must be positive. "
                "It has been set to the contamination rate.")
            c_r = self.contamination

        if delta <= 0 or delta >= 1:
            warnings.warn(
                "delta must belong to (0,1). It's value has been set to 0.1")
            delta = 0.1

        self.rejection_threshold_ = 1 - 2 * np.exp(-T)
        prediction = self.predict(X)
        confidence = self.predict_confidence(X)
        np.place(confidence, prediction == 0, 1 - confidence[prediction == 0])
        confidence = 2 * abs(confidence - .5)
        prediction[np.where(confidence <= self.rejection_threshold_)[0]] = -2

        if return_stats:
            expected_rejrate, ub_rejrate, ub_cost = self.compute_rejection_stats(
                T=T, delta=delta,
                c_fp=c_fp, c_fn=c_fn, c_r=c_r)
            return prediction, [expected_rejrate, ub_rejrate, ub_cost]

        return prediction






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.compute_rejection_stats)
    defcompute_rejection_stats(self, T=32, delta=0.1, c_fp=1, c_fn=1, c_r=-1,
                                verbose=False):
"""Add reject option into the unsupervised detector. 
           This comes with guarantees: an estimate of the expected
           rejection rate (return_rejectrate=True), an upper
           bound of the rejection rate (return_ub_rejectrate= True),
           and an upper bound on the cost (return_ub_cost=True).
        Parameters
        ----------
        T: int, optional(default=32)
           It allows to set the rejection threshold to 1-2exp(-T).
           The higher the value of T, the more rejections are made.
        delta: float, optional (default = 0.1)
               The upper bound rejection rate holds with probability 1-delta.
        c_fp, c_fn, c_r: floats (positive),
                         optional (default = [1,1, contamination])
                         costs for false positive predictions (c_fp),
                         false negative predictions (c_fn) and rejections (c_r).
        verbose: bool, optional (default = False)
                 If true, it prints the expected rejection rate, the upper
                 bound rejection rate, and the upper bound of the cost.

        Returns
        -------
        expected_rejection_rate:   float, the expected rejection rate;
        upperbound_rejection_rate: float, the upper bound for the rejection rate
                                   satisfied with probability 1-delta;
        upperbound_cost:           float, the upper bound for the cost;
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])

        if c_r < 0:
            c_r = self.contamination

        if delta <= 0 or delta >= 1:
            delta = 0.1

        ### Computing the expected rejection rate
        n = len(self.decision_scores_)
        n_gamma_minus1 = int(n * self.contamination) - 1
        argsmin = (n_gamma_minus1, n, 1 - np.exp(-T))
        argsmax = (n_gamma_minus1, n, np.exp(-T))
        q1 = root_scalar(lambda p, k, n, C: binom.cdf(k, n, p) - C,
                         bracket=[0, 1], method='brentq', args=argsmin).root
        q2 = root_scalar(lambda p, k, n, C: binom.cdf(k, n, p) - C,
                         bracket=[0, 1], method='brentq', args=argsmax).root
        expected_reject_rate = q2 - q1

        ### Computing the upper bound for the rejection rate
        right_mar = (-self.contamination * (n + 2) + n + 1) / n + (
                    T * (n + 2)) / (np.sqrt(2 * n ** 3 * T))
        right_mar = min(1, right_mar)
        left_mar = (
                (2 + n * (1 - self.contamination) * (n + 1)) / n ** 2
                - np.sqrt(
            0.5 * n ** 5 * (
                    2 * n * (
                    -3 * self.contamination ** 2
                    - 2 * n * (1 - self.contamination) ** 2
                    + 4 * self.contamination - 3
            )
                    + T * (n + 2) ** 2 - 8
            )
        ) / n ** 4
        )
        left_mar = max(0, left_mar)
        add_term = 2 * np.sqrt(np.log(2 / delta) / (2 * n))
        upperbound_rejectrate = right_mar - left_mar + add_term

        ### Computing the upper bound for the cost function
        n_gamma_minus1 = int(n * self.contamination) - 1
        argsmin = (n_gamma_minus1, n, 1 - np.exp(-T))
        argsmax = (n_gamma_minus1, n, np.exp(-T))
        q1 = root_scalar(lambda p, k, n, C: binom.cdf(k, n, p) - C,
                         bracket=[0, 1], method='brentq', args=argsmin).root
        q2 = root_scalar(lambda p, k, n, C: binom.cdf(k, n, p) - C,
                         bracket=[0, 1], method='brentq', args=argsmax).root
        upperbound_cost = np.min([self.contamination, q1]) * c_fp + np.min(
            [1 - q2, self.contamination]) * c_fn + (q2 - q1) * c_r

        if verbose:
            print("Expected rejection rate: ",
                  np.round(expected_reject_rate, 4), '%')
            print("Upper bound rejection rate: ",
                  np.round(upperbound_rejectrate, 4), '%')
            print("Upper bound cost: ", np.round(upperbound_cost, 4))

        return expected_reject_rate, upperbound_rejectrate, upperbound_cost




    def_predict_rank(self, X, normalized=False):
"""Predict the outlyingness rank of a sample by a fitted model. The
        method is for outlier detector score combination.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        normalized : bool, optional (default=False)
            If set to True, all ranks are normalized to [0,1].

        Returns
        -------
        ranks : array, shape (n_samples,)
            Outlying rank of a sample according to the training data.

        """

        check_is_fitted(self, ['decision_scores_'])

        test_scores = self.decision_function(X)
        train_scores = self.decision_scores_

        sorted_train_scores = np.sort(train_scores)
        ranks = np.searchsorted(sorted_train_scores, test_scores)

        if normalized:
            ### return normalized ranks
            ranks = ranks / ranks.max()
        return ranks



[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.fit_predict_score)
    @deprecated()
    deffit_predict_score(self, X, y, scoring='roc_auc_score'):
"""Fit the detector, predict on samples, and evaluate the model by
        predefined metrics, e.g., ROC.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        scoring : str, optional (default='roc_auc_score')
            Evaluation metric:

            - 'roc_auc_score': ROC score
            - 'prc_n_score': Precision @ rank n score

        Returns
        -------
        score : float

        .. deprecated:: 0.6.9
          `fit_predict_score` will be removed in pyod 0.8.0.; it will be
          replaced by calling `fit` function first and then accessing
          `labels_` attribute for consistency. Scoring could be done by
          calling an evaluation method, e.g., AUC ROC.
        """

        self.fit(X)

        if scoring == 'roc_auc_score':
            score = roc_auc_score(y, self.decision_scores_)
        elif scoring == 'prc_n_score':
            score = precision_n_scores(y, self.decision_scores_)
        else:
            raise NotImplementedError('PyOD built-in scoring only supports '
                                      'ROC and Precision @ rank n')

        print("{metric}: {score}".format(metric=scoring, score=score))

        return score





    def_set_n_classes(self, y):
"""Set the number of classes if `y` is presented, which is not
        expected. It could be useful for multi-class outlier detection.

        Parameters
        ----------
        y : numpy array of shape (n_samples,)
            Ground truth.

        Returns
        -------
        self
        """

        self._classes = 2  ### default as binary classification
        if y is not None:
            check_classification_targets(y)
            self._classes = len(np.unique(y))
            warnings.warn(
                "y should not be presented in unsupervised learning.")
        return self

    def_process_decision_scores(self):
"""Internal function to calculate key attributes:

        - threshold_: used to decide the binary label
        - labels_: binary labels of training data

        Returns
        -------
        self
        """

        if isinstance(self.contamination, (float, int)):
            self.threshold_ = percentile(self.decision_scores_,
                                         100 * (1 - self.contamination))
            self.labels_ = (self.decision_scores_ > self.threshold_).astype(
                'int').ravel()

        ### if this is a PyThresh object
        else:
            self.labels_ = self.contamination.eval(self.decision_scores_)
            self.threshold_ = self.contamination.thresh_
            if not self.threshold_:
                self.threshold_ = np.sum(self.labels_) / len(self.labels_)

        ### calculate for predict_proba()

        self._mu = np.mean(self.decision_scores_)
        self._sigma = np.std(self.decision_scores_)

        return self

    ### noinspection PyMethodParameters
    def_get_param_names(cls):
        ### noinspection PyPep8
"""Get parameter names for the estimator

        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html
        and sklearn/base.py for more information.
        """

        ### fetch the constructor or the original constructor before
        ### deprecation wrapping if any
        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
        if init is object.__init__:
            ### No explicit constructor to introspect
            return []

        ### introspect the constructor arguments to find the model parameters
        ### to represent
        init_signature = signature(init)
        ### Consider the constructor parameters excluding 'self'
        parameters = [p for p in init_signature.parameters.values()
                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]
        for p in parameters:
            if p.kind == p.VAR_POSITIONAL:
                raise RuntimeError("scikit-learn estimators should always "
                                   "specify their parameters in the signature"
                                   " of their __init__ (no varargs)."
                                   " %s with constructor %s doesn't "
                                   " follow this convention."
                                   % (cls, init_signature))
        ### Extract and sort argument names excluding 'self'
        return sorted([p.name for p in parameters])

    ### noinspection PyPep8


[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.get_params)
    defget_params(self, deep=True):
"""Get parameters for this estimator.

        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html
        and sklearn/base.py for more information.

        Parameters
        ----------
        deep : bool, optional (default=True)
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.

        Returns
        -------
        params : mapping of string to any
            Parameter names mapped to their values.
        """

        out = dict()
        for key in self._get_param_names():
            ### We need deprecation warnings to always be on in order to
            ### catch deprecated param values.
            ### This is set in utils/__init__.py but it gets overwritten
            ### when running under python3 somehow.
            warnings.simplefilter("always", DeprecationWarning)
            try:
                with warnings.catch_warnings(record=True) as w:
                    value = getattr(self, key, None)
                if len(w) and w[0].category == DeprecationWarning:
                    ### if the parameter is deprecated, don't show it
                    continue
            finally:
                warnings.filters.pop(0)

            ### XXX: should we rather test if instance of estimator?
            if deep and hasattr(value, 'get_params'):
                deep_items = value.get_params().items()
                out.update((key + '__' + k, val) for k, val in deep_items)
            out[key] = value
        return out






[docs][](https://pyod.readthedocs.io/en/latest/api_cc.html#pyod.models.base.BaseDetector.set_params)
    defset_params(self, **params):
        ### noinspection PyPep8
"""Set the parameters of this estimator.
        The method works on simple estimators as well as on nested objects
        (such as pipelines). The latter have parameters of the form
        ``<component>__<parameter>`` so that it's possible to update each
        component of a nested object.

        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html
        and sklearn/base.py for more information.

        Returns
        -------
        self : object
        """

        if not params:
            ### Simple optimization to gain speed (inspect is slow)
            return self
        valid_params = self.get_params(deep=True)

        nested_params = defaultdict(dict)  ### grouped by prefix
        for key, value in params.items():
            key, delim, sub_key = key.partition('__')
            if key not in valid_params:
                raise ValueError('Invalid parameter %s for estimator %s. '
                                 'Check the list of available parameters '
                                 'with `estimator.get_params().keys()`.' %
                                 (key, self))

            if delim:
                nested_params[key][sub_key] = value
            else:
                setattr(self, key, value)

        for key, sub_params in nested_params.items():
            valid_params[key].set_params(**sub_params)

        return self




    def__repr__(self):
        ### noinspection PyPep8
"""
        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html
        and sklearn/base.py for more information.
        """

        class_name = self.__class__.__name__
        return '%s(%s)' % (class_name, _pprint(self.get_params(deep=False),
                                               offset=len(class_name), ),)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 23. pyod.models.cblof - pyod 2.0.5 documentation {#23-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:17

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cblof.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.cblof
```
### -*- coding: utf-8 -*-
"""Clustering Based Local Outlier Factor (CBLOF)
"""
### Author: Yue Zhao <yzhao062@gmail.com>
###         Shangwen Huang <https://github.com/shangwen777>
### License: BSD 2 clause


importwarnings

importnumpyasnp
fromscipy.spatial.distanceimport cdist
fromsklearn.clusterimport KMeans
fromsklearn.utils.estimator_checksimport check_estimator
fromsklearn.utils.validationimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.stat_modelsimport pairwise_distances_no_broadcast
from..utils.utilityimport check_parameter

__all__ = ['CBLOF']




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF)
classCBLOF(BaseDetector):
r"""The CBLOF operator calculates the outlier score based on cluster-based
    local outlier factor.

    CBLOF takes as an input the data set and the cluster model that was
    generated by a clustering algorithm. It classifies the clusters into small
    clusters and large clusters using the parameters alpha and beta.
    The anomaly score is then calculated based on the size of the cluster the
    point belongs to as well as the distance to the nearest large cluster.

    Use weighting for outlier factor based on the sizes of the clusters as
    proposed in the original publication. Since this might lead to unexpected
    behavior (outliers close to small clusters are not found), it is disabled
    by default.Outliers scores are solely computed based on their distance to
    the closest large cluster center.

    By default, kMeans is used for clustering algorithm instead of
    Squeezer algorithm mentioned in the original paper for multiple reasons.

    See :cite:`he2003discovering` for details.

    Parameters
    ----------
    n_clusters : int, optional (default=8)
        The number of clusters to form as well as the number of
        centroids to generate.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    clustering_estimator : Estimator, optional (default=None)
        The base clustering algorithm for performing data clustering.
        A valid clustering algorithm should be passed in. The estimator should
        have standard sklearn APIs, fit() and predict(). The estimator should
        have attributes ``labels_`` and ``cluster_centers_``.
        If ``cluster_centers_`` is not in the attributes once the model is fit,
        it is calculated as the mean of the samples in a cluster.

        If not set, CBLOF uses KMeans for scalability. See
        https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

    alpha : float in (0.5, 1), optional (default=0.9)
        Coefficient for deciding small and large clusters. The ratio
        of the number of samples in large clusters to the number of samples in
        small clusters.

    beta : int or float in (1,), optional (default=5).
        Coefficient for deciding small and large clusters. For a list
        sorted clusters by size `|C1|, \|C2|, ..., |Cn|, beta = |Ck|/|Ck-1|`

    use_weights : bool, optional (default=False)
        If set to True, the size of clusters are used as weights in
        outlier score calculation.

    check_estimator : bool, optional (default=False)
        If set to True, check whether the base estimator is consistent with
        sklearn standard.

        .. warning::
            check_estimator may throw errors with scikit-learn 0.20 above.

    random_state : int, RandomState or None, optional (default=None)
        If int, random_state is the seed used by the random
        number generator; If RandomState instance, random_state is the random
        number generator; If None, the random number generator is the
        RandomState instance used by `np.random`.


    Attributes
    ----------
    clustering_estimator_ : Estimator, sklearn instance
        Base estimator for clustering.

    cluster_labels_ : list of shape (n_samples,)
        Cluster assignment for the training samples.

    n_clusters_ : int
        Actual number of clusters (possibly different from n_clusters).

    cluster_sizes_ : list of shape (n_clusters_,)
        The size of each cluster once fitted with the training data.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher scores.
        This value is available once the detector is fitted.

    cluster_centers_ : numpy array of shape (n_clusters_, n_features)
        The center of each cluster.

    small_cluster_labels_ : list of clusters numbers
        The cluster assignments belonging to small clusters.

    large_cluster_labels_ : list of clusters numbers
        The cluster assignments belonging to large clusters.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, n_clusters=8, contamination=0.1,
                 clustering_estimator=None, alpha=0.9, beta=5,
                 use_weights=False, check_estimator=False, random_state=None,
                 n_jobs=1):
        super(CBLOF, self).__init__(contamination=contamination)
        self.n_clusters = n_clusters
        self.clustering_estimator = clustering_estimator
        self.alpha = alpha
        self.beta = beta
        self.use_weights = use_weights
        self.check_estimator = check_estimator
        self.random_state = random_state

    ### noinspection PyIncorrectDocstring


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)
        n_samples, n_features = X.shape

        ### check parameters
        ### number of clusters are default to 8
        self._validate_estimator(default=KMeans(
            n_clusters=self.n_clusters,
            random_state=self.random_state))

        self.clustering_estimator_.fit(X=X, y=y)
        ### Get the labels of the clustering results
        ### labels_ is consistent across sklearn clustering algorithms
        self.cluster_labels_ = self.clustering_estimator_.labels_
        self.cluster_sizes_ = np.bincount(self.cluster_labels_)

        ### Get the actual number of clusters
        self.n_clusters_ = self.cluster_sizes_.shape[0]

        if self.n_clusters_ != self.n_clusters:
            warnings.warn("The chosen clustering for CBLOF forms {0} clusters"
                          "which is inconsistent with n_clusters ({1}).".
                          format(self.n_clusters_, self.n_clusters))

        self._set_cluster_centers(X, n_features)
        self._set_small_large_clusters(n_samples)

        self.decision_scores_ = self._decision_function(X,
                                                        self.cluster_labels_)

        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cblof.CBLOF.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        X = check_array(X)
        labels = self.clustering_estimator_.predict(X)
        return self._decision_function(X, labels)




    def_validate_estimator(self, default=None):
"""Check the value of alpha and beta and clustering algorithm.
        """
        check_parameter(self.alpha, low=0, high=1, param_name='alpha',
                        include_left=False, include_right=False)

        check_parameter(self.beta, low=1, param_name='beta',
                        include_left=False)

        if self.clustering_estimator is not None:
            self.clustering_estimator_ = self.clustering_estimator
        else:
            self.clustering_estimator_ = default

        ### make sure the base clustering algorithm is valid
        if self.clustering_estimator_ is None:
            raise ValueError("clustering algorithm cannot be None")

        if self.check_estimator:
            check_estimator(self.clustering_estimator_)

    def_set_cluster_centers(self, X, n_features):
        ### Noted not all clustering algorithms have cluster_centers_
        if hasattr(self.clustering_estimator_, 'cluster_centers_'):
            self.cluster_centers_ = self.clustering_estimator_.cluster_centers_
        else:
            ### Set the cluster center as the mean of all the samples within
            ### the cluster
            warnings.warn("The chosen clustering for CBLOF does not have"
                          "the center of clusters. Calculate the center"
                          "as the mean of the clusters.")
            self.cluster_centers_ = np.zeros([self.n_clusters_, n_features])
            for i in range(self.n_clusters_):
                self.cluster_centers_[i, :] = np.mean(
                    X[np.where(self.cluster_labels_ == i)], axis=0)

    def_set_small_large_clusters(self, n_samples):
        ### Sort the index of clusters by the number of samples belonging to it
        size_clusters = np.bincount(self.cluster_labels_)

        ### Sort the order from the largest to the smallest
        sorted_cluster_indices = np.argsort(size_clusters * -1)

        ### Initialize the lists of index that fulfill the requirements by
        ### either alpha or beta
        alpha_list = []
        beta_list = []

        for i in range(1, self.n_clusters_):
            temp_sum = np.sum(size_clusters[sorted_cluster_indices[:i]])
            if temp_sum >= n_samples * self.alpha:
                alpha_list.append(i)

            if size_clusters[sorted_cluster_indices[i - 1]] / size_clusters[
                sorted_cluster_indices[i]] >= self.beta:
                beta_list.append(i)

            ### Find the separation index fulfills both alpha and beta
        intersection = np.intersect1d(alpha_list, beta_list)

        if len(intersection) > 0:
            self._clustering_threshold = intersection[0]
        elif len(alpha_list) > 0:
            self._clustering_threshold = alpha_list[0]
        elif len(beta_list) > 0:
            self._clustering_threshold = beta_list[0]
        else:
            raise ValueError("Could not form valid cluster separation. Please "
                             "change n_clusters or change clustering method")

        self.small_cluster_labels_ = sorted_cluster_indices[
                                     self._clustering_threshold:]
        self.large_cluster_labels_ = sorted_cluster_indices[
                                     0:self._clustering_threshold]

        ### No need to calculate small cluster center
        ### self.small_cluster_centers_ = self.cluster_centers_[
        ###     self.small_cluster_labels_]

        self._large_cluster_centers = self.cluster_centers_[
            self.large_cluster_labels_]

    def_decision_function(self, X, labels):
        ### Initialize the score array
        scores = np.zeros([X.shape[0], ])

        small_indices = np.where(
            np.isin(labels, self.small_cluster_labels_))[0]
        large_indices = np.where(
            np.isin(labels, self.large_cluster_labels_))[0]

        if small_indices.shape[0] != 0:
            ### Calculate the outlier factor for the samples in small clusters
            dist_to_large_center = cdist(X[small_indices, :],
                                         self._large_cluster_centers)

            scores[small_indices] = np.min(dist_to_large_center, axis=1)

        if large_indices.shape[0] != 0:
            ### Calculate the outlier factor for the samples in large clusters
            large_centers = self.cluster_centers_[labels[large_indices]]

            scores[large_indices] = pairwise_distances_no_broadcast(
                X[large_indices, :], large_centers)

        if self.use_weights:
            ### Weights are calculated as the number of elements in the cluster
            scores = scores * self.cluster_sizes_[labels]

        return scores.ravel()




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 24. pyod.models.cd - pyod 2.0.5 documentation {#24-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:03

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cd.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.cd
```
### -*- coding: utf-8 -*-
"""Cook's distance outlier detection (CD)
"""

### Author: D Kulik
### License: BSD 2 clause


importnumpyasnp
fromsklearn.linear_modelimport LinearRegression
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


def_Cooks_dist(X, y, model):
"""Calculated the Cook's distance

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The training dataset.

    y : numpy array of shape (n_samples)
        The training datset

    model : object
        Regression model used to calculate the Cook's distance

    Returns
    -------
    distances_ : numpy array of shape (n_samples)
        Cook's distance
    """

    ### Leverage is computed as the diagonal of the projection matrix of X
    leverage = (X * np.linalg.pinv(X).T).sum(1)

    ### Compute the rank and the degrees of freedom of the model
    rank = np.linalg.matrix_rank(X)
    df = X.shape[0] - rank

    ### Compute the MSE from the residuals
    residuals = y - model.predict(X)
    mse = np.dot(residuals, residuals) / df

    ### Compute Cook's distance
    if (mse != 0) or (mse != np.nan):
        residuals_studentized = residuals / np.sqrt(mse) / np.sqrt(
            1 - leverage)
        distance_ = residuals_studentized ** 2 / X.shape[1]
        distance_ *= leverage / (1 - leverage)
        distance_ = ((distance_ - distance_.min())
                     / (distance_.max() - distance_.min()))

    else:
        distance_ = np.ones(len(y)) * np.nan

    return distance_


def_process_distances(X, model):
"""Calculated the mean Cook's distances for
    each feature

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The training dataset.

    model : object
        Regression model used to calculate the Cook's distance

    Returns
    -------
    distances_ : numpy array of shape (n_samples)
        mean Cook's distance
    """

    distances_ = []
    for i in range(X.shape[1]):
        mod = model

        ### Extract new X and y inputs
        exp = np.delete(X.copy(), i, axis=1)
        resp = X[:, i]

        exp = exp.reshape(-1, 1) if exp.ndim == 1 else exp

        ### Fit the model
        mod.fit(exp, resp)

        ### Get Cook's Distance
        distance_ = _Cooks_dist(exp, resp, mod)

        distances_.append(distance_)

    distances_ = np.nanmean(distances_, axis=0)

    return distances_




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD)
classCD(BaseDetector):
"""Cook's distance can be used to identify points that negatively
       affect a regression model. A combination of each observation‚Äôs
       leverage and residual values are used in the measurement. Higher
       leverage and residuals relate to  higher Cook‚Äôs distances. Note
       that this method is unsupervised and requires at least two 
       features for X with which to calculate the mean Cook's distance
       for each datapoint. Read more in the :cite:`cook1977detection`.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.
    model : object, optional (default=LinearRegression())
        Regression model used to calculate the Cook's distance
    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
       The modified z-score to use as a threshold. Observations with
       a modified z-score (based on the median absolute deviation) greater
       than this value will be classified as outliers.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
        """

    def__init__(self, contamination=0.1, model=LinearRegression()):
        super(CD, self).__init__(contamination=contamination)
        self.model = model



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.fit)
    deffit(self, X, y=None):
""""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### Validate inputs X and y
        X = check_array(X)

        self._set_n_classes(y)

        ### Get Cook's distance
        distances_ = _process_distances(X, self.model)

        self.decision_scores_ = distances_

        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cd.CD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        For consistency, outliers are assigned with larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])

        ### Validate input X
        X = check_array(X)

        ### Get Cook's distance
        distances_ = _process_distances(X, self.model)

        return distances_






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 25. pyod.models.cof - pyod 2.0.5 documentation {#25-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:46

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/cof.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.cof
```
### -*- coding: utf-8 -*-
"""Connectivity-Based Outlier Factor (COF) Algorithm
"""
### Author: Yahya Almardeny <almardeny@gmail.com>, Roel Bouman <roel.bouman@ru.nl> (memory efficient COF)
### License: BSD 2 clause


importwarnings
fromoperatorimport itemgetter

importnumpyasnp
fromscipy.spatialimport distance_matrix
fromscipy.spatialimport minkowski_distance
fromsklearn.utilsimport check_array

from.baseimport BaseDetector
from..utils.utilityimport check_parameter




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF)
classCOF(BaseDetector):
"""Connectivity-Based Outlier Factor (COF) COF uses the ratio of average
    chaining distance of data point and the average of average chaining
    distance of k nearest neighbor of the data point, as the outlier score
    for observations.

    See :cite:`tang2002enhancing` for details.
    Two version of COF are supported:

    - Fast COF: computes the entire pairwise distance matrix at the cost of a
      O(n^2) memory requirement.
    - Memory efficient COF: calculates pairwise distances incrementally.
      Use this implementation when it is not feasible to fit the n-by-n 
      distance in memory. This leads to a linear overhead because many 
      distances will have to be recalculated.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_neighbors : int, optional (default=20)
        Number of neighbors to use by default for k neighbors queries.
        Note that n_neighbors should be less than the number of samples.
        If n_neighbors is larger than the number of samples provided,
        all samples will be used.
    method : string, optional (default='fast')
        Valid values for method are:
        - 'fast' Fast COF, computes the full pairwise distance matrix up front.
        - 'memory' Memory-efficient COF, computes pairwise distances only when
          needed at the cost of computational speed.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    n_neighbors_: int
        Number of neighbors to use by default for k neighbors queries.
    """

    def__init__(self, contamination=0.1, n_neighbors=20, method="fast"):
        super(COF, self).__init__(contamination=contamination)
        if isinstance(n_neighbors, int):
            check_parameter(n_neighbors, low=1, param_name='n_neighbors')
        else:
            raise TypeError(
                "n_neighbors should be int. Got %s" % type(n_neighbors))
        self.n_neighbors = n_neighbors
        self.method = method



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self.n_train_ = X.shape[0]
        self.n_neighbors_ = self.n_neighbors

        if self.n_neighbors_ >= self.n_train_:
            self.n_neighbors_ = self.n_train_ - 1
            warnings.warn(
                "n_neighbors is set to the number of training points "
                "minus 1: {0}".format(self.n_neighbors_))

            check_parameter(self.n_neighbors_, 1, self.n_train_,
                            include_left=True, include_right=True)

        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.cof.COF.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        if self.method.lower() == "fast":
            return self._cof_fast(X)
        elif self.method.lower() == "memory":
            return self._cof_memory(X)
        else:
            raise ValueError(
                "method should be set to either \'fast\' or \'memory\'. Got %s" % self.method)




    def_cof_memory(self, X):
"""
        Connectivity-Based Outlier Factor (COF) Algorithm
        This function is called internally to calculate the
        Connectivity-Based Outlier Factor (COF) as an outlier
        score for observations.
        This function uses a memory efficient implementation at the cost of 
        speed.
        :return: numpy array containing COF scores for observations.
                 The greater the COF, the greater the outlierness.
        """
        ### dist_matrix = np.array(distance_matrix(X, X))
        sbn_path_index = np.zeros((X.shape[0], self.n_neighbors_),
                                  dtype=np.int64)
        ac_dist, cof_ = np.zeros((X.shape[0])), np.zeros((X.shape[0]))
        for i in range(X.shape[0]):
            ### sbn_path = np.argsort(dist_matrix[i])
            sbn_path = np.argsort(minkowski_distance(X[i, :], X, p=2))
            sbn_path_index[i, :] = sbn_path[1: self.n_neighbors_ + 1]
            cost_desc = np.zeros((self.n_neighbors_))
            for j in range(self.n_neighbors_):
                ### cost_desc.append(
                ###    np.min(dist_matrix[sbn_path[j + 1]][sbn_path][:j + 1]))
                cost_desc[j] = np.min(
                    minkowski_distance(X[sbn_path[j + 1]], X, p=2)[sbn_path][
                    :j + 1])
            acd = np.zeros((self.n_neighbors_))
            for _h, cost_ in enumerate(cost_desc):
                neighbor_add1 = self.n_neighbors_ + 1
                acd[_h] = ((2. * (neighbor_add1 - (_h + 1))) / (
                        neighbor_add1 * self.n_neighbors_)) * cost_
            ac_dist[i] = np.sum(acd)
        for _g in range(X.shape[0]):
            cof_[_g] = (ac_dist[_g] * self.n_neighbors_) / np.sum(
                ac_dist[sbn_path_index[_g]])
        return np.nan_to_num(cof_)

    def_cof_fast(self, X):
"""
        Connectivity-Based Outlier Factor (COF) Algorithm
        This function is called internally to calculate the
        Connectivity-Based Outlier Factor (COF) as an outlier
        score for observations.
        This function uses a fast implementation at the cost of memory.
        :return: numpy array containing COF scores for observations.
                 The greater the COF, the greater the outlierness.
        """
        dist_matrix = np.array(distance_matrix(X, X))
        sbn_path_index, ac_dist, cof_ = [], [], []
        for i in range(X.shape[0]):
            sbn_path = np.argsort(dist_matrix[i])
            sbn_path_index.append(sbn_path[1: self.n_neighbors_ + 1])
            cost_desc = []
            for j in range(self.n_neighbors_):
                cost_desc.append(
                    np.min(dist_matrix[sbn_path[j + 1]][sbn_path][:j + 1]))
            acd = []
            for _h, cost_ in enumerate(cost_desc):
                neighbor_add1 = self.n_neighbors_ + 1
                acd.append(((2. * (neighbor_add1 - (_h + 1))) / (
                        neighbor_add1 * self.n_neighbors_)) * cost_)
            ac_dist.append(np.sum(acd))
        for _g in range(X.shape[0]):
            cof_.append((ac_dist[_g] * self.n_neighbors_) /
                        np.sum(itemgetter(*sbn_path_index[_g])(ac_dist)))
        return np.nan_to_num(cof_)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 26. pyod.models.combination - pyod 2.0.5 documentation {#26-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:31

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/combination.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.combination
```
### -*- coding: utf-8 -*-
"""A collection of model combination functionalities.
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


try:
    importcombo
except ImportError:
    print('please install combo first for combination by `pip install combo`')

fromcombo.models.score_combimport aom as combo_aom
fromcombo.models.score_combimport average as combo_average
fromcombo.models.score_combimport majority_vote as combo_majority_vote
fromcombo.models.score_combimport maximization as combo_maximization
fromcombo.models.score_combimport median as combo_median
fromcombo.models.score_combimport moa as combo_moa




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.aom)
defaom(scores, n_buckets=5, method='static', bootstrap_estimators=False,
        random_state=None):
"""Average of Maximum - An ensemble method for combining multiple
    estimators. See :cite:`aggarwal2015theoretical` for details.

    First dividing estimators into subgroups, take the maximum score as the
    subgroup score. Finally, take the average of all subgroup outlier scores.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        The score matrix outputted from various estimators

    n_buckets : int, optional (default=5)
        The number of subgroups to build

    method : str, optional (default='static')
        {'static', 'dynamic'}, if 'dynamic', build subgroups
        randomly with dynamic bucket size.

    bootstrap_estimators : bool, optional (default=False)
        Whether estimators are drawn with replacement.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the
        random number generator; If RandomState instance, random_state is
        the random number generator; If None, the random number generator
        is the RandomState instance used by `np.random`.

    Returns
    -------
    combined_scores : Numpy array of shape (n_samples,)
        The combined outlier scores.

    """

    return combo_aom(scores, n_buckets, method, bootstrap_estimators,
                     random_state)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.moa)
defmoa(scores, n_buckets=5, method='static', bootstrap_estimators=False,
        random_state=None):
"""Maximization of Average - An ensemble method for combining multiple
    estimators. See :cite:`aggarwal2015theoretical` for details.

    First dividing estimators into subgroups, take the average score as the
    subgroup score. Finally, take the maximization of all subgroup outlier
    scores.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        The score matrix outputted from various estimators

    n_buckets : int, optional (default=5)
        The number of subgroups to build

    method : str, optional (default='static')
        {'static', 'dynamic'}, if 'dynamic', build subgroups
        randomly with dynamic bucket size.

    bootstrap_estimators : bool, optional (default=False)
        Whether estimators are drawn with replacement.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the
        random number generator; If RandomState instance, random_state is
        the random number generator; If None, the random number generator
        is the RandomState instance used by `np.random`.

    Returns
    -------
    combined_scores : Numpy array of shape (n_samples,)
        The combined outlier scores.

    """
    return combo_moa(scores, n_buckets, method, bootstrap_estimators,
                     random_state)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.average)
defaverage(scores, estimator_weights=None):
"""Combination method to merge the outlier scores from multiple estimators
    by taking the average.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        Score matrix from multiple estimators on the same samples.

    estimator_weights : list of shape (1, n_estimators)
        If specified, using weighted average

    Returns
    -------
    combined_scores : numpy array of shape (n_samples, )
        The combined outlier scores.

    """
    return combo_average(scores, estimator_weights)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.maximization)
defmaximization(scores):
"""Combination method to merge the outlier scores from multiple estimators
    by taking the maximum.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        Score matrix from multiple estimators on the same samples.

    Returns
    -------
    combined_scores : numpy array of shape (n_samples, )
        The combined outlier scores.

    """
    return combo_maximization(scores)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.majority_vote)
defmajority_vote(scores, weights=None):
"""Combination method to merge the scores from multiple estimators
    by majority vote.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        Score matrix from multiple estimators on the same samples.


    weights : numpy array of shape (1, n_estimators)
        If specified, using weighted majority weight.

    Returns
    -------
    combined_scores : numpy array of shape (n_samples, )
        The combined scores.

    """
    return combo_majority_vote(scores, n_classes=2, weights=weights)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.combination.median)
defmedian(scores):
"""Combination method to merge the scores from multiple estimators
    by taking the median.

    Parameters
    ----------
    scores : numpy array of shape (n_samples, n_estimators)
        Score matrix from multiple estimators on the same samples.

    Returns
    -------
    combined_scores : numpy array of shape (n_samples, )
        The combined scores.

    """
    return combo_median(scores)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 27. pyod.models.copod - pyod 2.0.5 documentation {#27-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:07

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/copod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.copod
```
"""Copula Based Outlier Detector (COPOD)
"""
### Author: Zheng Li <jk_zhengli@hotmail.com>
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importwarnings

importmatplotlib.pyplotasplt
importnumpyasnp
fromjoblibimport Parallel, delayed
fromscipy.statsimport skew as skew_sp
fromsklearn.utilsimport check_array

from.baseimport BaseDetector
from.sklearn_baseimport _partition_estimators
from..utils.stat_modelsimport column_ecdf




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.skew)
defskew(X, axis=0):
    return np.nan_to_num(skew_sp(X, axis=axis))





def_parallel_ecdf(n_dims, X):
"""Private method to calculate ecdf in parallel.    
    Parameters
    ----------
    n_dims : int
        The number of dimensions of the current input matrix

    X : numpy array
        The subarray for building the ECDF

    Returns
    -------
    U_l_mat : numpy array
        ECDF subarray.

    U_r_mat : numpy array
        ECDF subarray.
    """
    U_l_mat = np.zeros([X.shape[0], n_dims])
    U_r_mat = np.zeros([X.shape[0], n_dims])

    for i in range(n_dims):
        U_l_mat[:, i: i + 1] = column_ecdf(X[:, i: i + 1])
        U_r_mat[:, i: i + 1] = column_ecdf(X[:, i: i + 1] * -1)
    return U_l_mat, U_r_mat




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD)
classCOPOD(BaseDetector):
"""COPOD class for Copula Based Outlier Detector.
    COPOD is a parameter-free, highly interpretable outlier detection algorithm
    based on empirical copula models.
    See :cite:`li2020copod` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.
    n_jobs : optional (default=1)
        The number of jobs to run in parallel for both `fit` and
        `predict`. If -1, then the number of jobs is set to the
        number of cores.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.
    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.
    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_jobs=1):
        super(COPOD, self).__init__(contamination=contamination)
        self.n_jobs = n_jobs



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : Ignored
            Not used, present for API consistency by convention.
        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self.X_train = X
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
         For consistency, outliers are assigned with larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        ### use multi-thread execution
        if self.n_jobs != 1:
            return self._decision_function_parallel(X)
        if hasattr(self, 'X_train'):
            original_size = X.shape[0]
            X = np.concatenate((self.X_train, X), axis=0)
        self.U_l = -1 * np.log(column_ecdf(X))
        self.U_r = -1 * np.log(column_ecdf(-X))

        skewness = np.sign(skew(X, axis=0))
        self.U_skew = self.U_l * -1 * np.sign(
            skewness - 1) + self.U_r * np.sign(skewness + 1)
        self.O = np.maximum(self.U_skew, np.add(self.U_l, self.U_r) / 2)
        if hasattr(self, 'X_train'):
            decision_scores_ = self.O.sum(axis=1)[-original_size:]
        else:
            decision_scores_ = self.O.sum(axis=1)
        return decision_scores_.ravel()




    def_decision_function_parallel(self, X):
"""Predict raw anomaly score of X using the fitted detector.
         For consistency, outliers are assigned with larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        if hasattr(self, 'X_train'):
            original_size = X.shape[0]
            X = np.concatenate((self.X_train, X), axis=0)

        n_samples, n_features = X.shape[0], X.shape[1]

        if n_features < 2:
            raise ValueError(
                'n_jobs should not be used on one dimensional dataset')

        if n_features <= self.n_jobs:
            self.n_jobs = n_features
            warnings.warn("n_features <= n_jobs; setting them equal instead.")

        n_jobs, n_dims_list, starts = _partition_estimators(n_features,
                                                            self.n_jobs)

        all_results = Parallel(n_jobs=n_jobs, max_nbytes=None,
                               verbose=True)(
            delayed(_parallel_ecdf)(
                n_dims_list[i],
                X[:, starts[i]:starts[i + 1]],
            )
            for i in range(n_jobs))

        ### recover the results
        self.U_l = np.zeros([n_samples, n_features])
        self.U_r = np.zeros([n_samples, n_features])

        for i in range(n_jobs):
            self.U_l[:, starts[i]:starts[i + 1]] = all_results[i][0]
            self.U_r[:, starts[i]:starts[i + 1]] = all_results[i][1]

        self.U_l = -1 * np.log(self.U_l)
        self.U_r = -1 * np.log(self.U_r)

        skewness = np.sign(skew(X, axis=0))
        self.U_skew = self.U_l * -1 * np.sign(
            skewness - 1) + self.U_r * np.sign(skewness + 1)
        self.O = np.maximum(self.U_skew, np.add(self.U_l, self.U_r) / 2)
        if hasattr(self, 'X_train'):
            decision_scores_ = self.O.sum(axis=1)[-original_size:]
        else:
            decision_scores_ = self.O.sum(axis=1)
        return decision_scores_.ravel()



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.copod.COPOD.explain_outlier)
    defexplain_outlier(self, ind, columns=None, cutoffs=None,
                        feature_names=None, file_name=None,
                        file_type=None):  ### pragma: no cover
"""Plot dimensional outlier graph for a given data point within
        the dataset.

        Parameters
        ----------
        ind : int
            The index of the data point one wishes to obtain
            a dimensional outlier graph for.

        columns : list
            Specify a list of features/dimensions for plotting. If not 
            specified, use all features.
        cutoffs : list of floats in (0., 1), optional (default=[0.95, 0.99])
            The significance cutoff bands of the dimensional outlier graph.
        feature_names : list of strings
            The display names of all columns of the dataset,
            to show on the x-axis of the plot.

        file_name : string
            The name to save the figure

        file_type : string
            The file type to save the figure

        Returns
        -------
        Plot : matplotlib plot
            The dimensional outlier graph for data point with index ind.
        """
        if columns is None:
            columns = list(range(self.O.shape[1]))
            column_range = range(1, self.O.shape[1] + 1)
        else:
            column_range = range(1, len(columns) + 1)

        cutoffs = [1 - self.contamination,
                   0.99] if cutoffs is None else cutoffs

        ### plot outlier scores
        plt.scatter(column_range, self.O[ind, columns], marker='^', c='black',
                    label='Outlier Score')

        for i in cutoffs:
            plt.plot(column_range,
                     np.quantile(self.O[:, columns], q=i, axis=0),
                     '--',
                     label='{percentile} Cutoff Band'.format(percentile=i))
        plt.xlim([1, max(column_range)])
        plt.ylim([0, int(self.O[:, columns].max().max()) + 1])
        plt.ylabel('Dimensional Outlier Score')
        plt.xlabel('Dimension')

        ticks = list(column_range)
        if feature_names is not None:
            assert len(feature_names) == len(ticks), \
                "Length of feature_names does not match dataset dimensions."
            plt.xticks(ticks, labels=feature_names)
        else:
            plt.xticks(ticks)

        plt.yticks(range(0, int(self.O[:, columns].max().max()) + 1))
        plt.xlim(0.95, ticks[-1] + 0.05)
        label = 'Outlier' if self.labels_[ind] == 1 else 'Inlier'
        plt.title(
            'Outlier score breakdown for sample #{index} ({label})'.format(
                index=ind + 1, label=label))
        plt.legend()
        plt.tight_layout()

        ### save the file if specified
        if file_name is not None:
            if file_type is not None:
                plt.savefig(file_name + '.' + file_type, dpi=300)
            ### if not specified, save as png
            else:
                plt.savefig(file_name + '.' + 'png', dpi=300)
        plt.show()






        ### todo: consider returning results
        ### return self.O[ind, columns], self.O[:, columns].quantile(q=cutoffs[0], axis=0), self.O[:, columns].quantile(q=cutoffs[1], axis=0)

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 28. pyod.models.deep_svdd - pyod 2.0.5 documentation {#28-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:24

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/deep_svdd.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.deep_svdd
```
### -*- coding: utf-8 -*-
"""Deep One-Class Classification for outlier detection
"""
### Author: Rafal Bodziony <bodziony.rafal@gmail.com> for the TensorFlow version
### Author: Yuehan Qin <yuehanqi@usc.edu> for the PyTorch version
### License: BSD 2 clause


importnumpyasnp

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.optimasoptim

fromsklearn.preprocessingimport StandardScaler
fromsklearn.utilsimport check_array
fromtorch.utils.dataimport DataLoader, TensorDataset

from.baseimport BaseDetector
from..utils.torch_utilityimport get_activation_by_name
from..utils.utilityimport check_parameter

optimizer_dict = {
    'sgd': optim.SGD,
    'adam': optim.Adam,
    'rmsprop': optim.RMSprop,
    'adagrad': optim.Adagrad,
    'adadelta': optim.Adadelta,
    'adamw': optim.AdamW,
    'nadam': optim.NAdam,
    'sparseadam': optim.SparseAdam,
    'asgd': optim.ASGD,
    'lbfgs': optim.LBFGS
}


classInnerDeepSVDD(nn.Module):
"""Inner class for DeepSVDD model.

    Parameters
    ----------
    n_features:
        Number of features in the input data.

    use_ae: bool, optional (default=False)
        The AutoEncoder type of DeepSVDD it reverse neurons from hidden_neurons
        if set to True.

    hidden_neurons : list, optional (default=[64, 32])
        The number of neurons per hidden layers. if use_ae is True, neurons
        will be reversed eg. [64, 32] -> [64, 32, 32, 64, n_features]

    hidden_activation : str, optional (default='relu')
        Activation function to use for hidden layers.
        All hidden layers are forced to use the same type of activation.

    output_activation : str, optional (default='sigmoid')
        Activation function to use for output layer.

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    l2_regularizer : float in (0., 1), optional (default=0.1)
        The regularization strength of activity_regularizer
        applied on each layer. By default, l2 regularizer is used. See
    """

    def__init__(self, n_features, use_ae,
                 hidden_neurons, hidden_activation,
                 output_activation,
                 dropout_rate, l2_regularizer):
        super(InnerDeepSVDD, self).__init__()
        self.n_features = n_features
        self.use_ae = use_ae
        self.hidden_neurons = hidden_neurons or [64, 32]
        self.hidden_activation = hidden_activation
        self.output_activation = output_activation
        self.dropout_rate = dropout_rate
        self.l2_regularizer = l2_regularizer
        self.model = self._build_model()

    def_init_c(self, X_norm, eps=0.1):
        intermediate_output = {}
        hook_handle = self.model._modules.get(
            'net_output').register_forward_hook(
            lambda module, input, output: intermediate_output.update(
                {'net_output': output})
        )
        output = self.model(X_norm)

        out = intermediate_output['net_output']
        hook_handle.remove()

        self.c = torch.mean(out, dim=0)
        self.c[(torch.abs(self.c) < eps) & (self.c < 0)] = -eps
        self.c[(torch.abs(self.c) < eps) & (self.c > 0)] = eps

    def_build_model(self):
        layers = nn.Sequential()
        layers.add_module('input_layer',
                          nn.Linear(self.n_features, self.hidden_neurons[0],
                                    bias=False))
        layers.add_module('hidden_activation_e0',
                          get_activation_by_name(self.hidden_activation))
        for i in range(1, len(self.hidden_neurons) - 1):
            layers.add_module(f'hidden_layer_e{i}',
                              nn.Linear(self.hidden_neurons[i - 1],
                                        self.hidden_neurons[i], bias=False))
            layers.add_module(f'hidden_activation_e{i}',
                              get_activation_by_name(self.hidden_activation))
            layers.add_module(f'hidden_dropout_e{i}',
                              nn.Dropout(self.dropout_rate))
        layers.add_module(f'net_output', nn.Linear(self.hidden_neurons[-2],
                                                   self.hidden_neurons[-1],
                                                   bias=False))
        layers.add_module(f'hidden_activation_e{len(self.hidden_neurons)}',
                          get_activation_by_name(self.hidden_activation))

        if self.use_ae:
            for j in range(len(self.hidden_neurons) - 1, 0, -1):
                layers.add_module(f'hidden_layer_d{j}',
                                  nn.Linear(self.hidden_neurons[j],
                                            self.hidden_neurons[j - 1],
                                            bias=False))
                layers.add_module(f'hidden_activation_d{j}',
                                  get_activation_by_name(
                                      self.hidden_activation))
                layers.add_module(f'hidden_dropout_d{j}',
                                  nn.Dropout(self.dropout_rate))
            layers.add_module(f'output_layer',
                              nn.Linear(self.hidden_neurons[0],
                                        self.n_features, bias=False))
            layers.add_module(f'output_activation',
                              get_activation_by_name(self.output_activation))
        return layers

    defforward(self, x):
        return self.model(x)




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD)
classDeepSVDD(BaseDetector):
"""Deep One-Class Classifier with AutoEncoder (AE) is a type of neural
        networks for learning useful data representations in an unsupervised way.
        DeepSVDD trains a neural network while minimizing the volume of a
        hypersphere that encloses the network representations of the data,
        forcing the network to extract the common factors of variation.
        Similar to PCA, DeepSVDD could be used to detect outlying objects in the
        data by calculating the distance from center
        See :cite:`ruff2018deepsvdd` for details.

        Parameters
        ----------
        n_features: int, 
            Number of features in the input data.

        c: float, optional (default='forwad_nn_pass')
            Deep SVDD center, the default will be calculated based on network
            initialization first forward pass. To get repeated results set
            random_state if c is set to None.

        use_ae: bool, optional (default=False)
            The AutoEncoder type of DeepSVDD it reverse neurons from hidden_neurons
            if set to True.

        hidden_neurons : list, optional (default=[64, 32])
            The number of neurons per hidden layers. if use_ae is True, neurons
            will be reversed eg. [64, 32] -> [64, 32, 32, 64, n_features]

        hidden_activation : str, optional (default='relu')
            Activation function to use for hidden layers.
            All hidden layers are forced to use the same type of activation.
            See https://keras.io/activations/

        output_activation : str, optional (default='sigmoid')
            Activation function to use for output layer.
            See https://keras.io/activations/

        optimizer : str, optional (default='adam')
            String (name of optimizer) or optimizer instance.
            See https://keras.io/optimizers/

        epochs : int, optional (default=100)
            Number of epochs to train the model.

        batch_size : int, optional (default=32)
            Number of samples per gradient update.

        dropout_rate : float in (0., 1), optional (default=0.2)
            The dropout to be used across all layers.

        l2_regularizer : float in (0., 1), optional (default=0.1)
            The regularization strength of activity_regularizer
            applied on each layer. By default, l2 regularizer is used. See
            https://keras.io/regularizers/

        validation_size : float in (0., 1), optional (default=0.1)
            The percentage of data to be used for validation.

        preprocessing : bool, optional (default=True)
            If True, apply standardization on the data.

        random_state : random_state: int, RandomState instance or None, optional
            (default=None)
            If int, random_state is the seed used by the random
            number generator; If RandomState instance, random_state is the random
            number generator; If None, the random number generator is the
            RandomState instance used by `np.random`.

        contamination : float in (0., 0.5), optional (default=0.1)
            The amount of contamination of the data set, i.e.
            the proportion of outliers in the data set. When fitting this is used
            to define the threshold on the decision function.

        Attributes
        ----------
        decision_scores_ : numpy array of shape (n_samples,)
            The outlier scores of the training data.
            The higher, the more abnormal. Outliers tend to have higher
            scores. This value is available once the detector is
            fitted.

        threshold_ : float
            The threshold is based on ``contamination``. It is the
            ``n_samples * contamination`` most abnormal samples in
            ``decision_scores_``. The threshold is calculated for generating
            binary outlier labels.

        labels_ : int, either 0 or 1
            The binary labels of the training data. 0 stands for inliers
            and 1 for outliers/anomalies. It is generated by applying
            ``threshold_`` on ``decision_scores_``.
        """

    def__init__(self, n_features, c=None, use_ae=False, hidden_neurons=None,
                 hidden_activation='relu',
                 output_activation='sigmoid', optimizer='adam', epochs=100,
                 batch_size=32,
                 dropout_rate=0.2, l2_regularizer=0.1, validation_size=0.1,
                 preprocessing=True,
                 verbose=1, random_state=None, contamination=0.1):
        super(DeepSVDD, self).__init__(contamination=contamination)

        self.n_features = n_features
        self.c = c
        self.use_ae = use_ae
        self.hidden_neurons = hidden_neurons or [64, 32]
        self.hidden_activation = hidden_activation
        self.output_activation = output_activation
        self.optimizer = optimizer
        self.epochs = epochs
        self.batch_size = batch_size
        self.dropout_rate = dropout_rate
        self.l2_regularizer = l2_regularizer
        self.validation_size = validation_size
        self.preprocessing = preprocessing
        self.verbose = verbose
        self.random_state = random_state
        self.model_ = None
        self.best_model_dict = None

        if self.random_state is not None:
            torch.manual_seed(self.random_state)
        check_parameter(dropout_rate, 0, 1, param_name='dropout_rate',
                        include_left=True)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### Verify and construct the hidden units
        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]

        ### Standardize data for better performance
        if self.preprocessing:
            self.scaler_ = StandardScaler()
            X_norm = self.scaler_.fit_transform(X)
        else:
            X_norm = np.copy(X)

        ### Shuffle the data for validation as Keras do not shuffling for
        ### Validation Split
        np.random.shuffle(X_norm)

        ### Validate and complete the number of hidden neurons
        if np.min(self.hidden_neurons) > self.n_features_ and self.use_ae:
            raise ValueError("The number of neurons should not exceed "
                             "the number of features")

        ### Build DeepSVDD model & fit with X
        self.model_ = InnerDeepSVDD(self.n_features, use_ae=self.use_ae,
                                    hidden_neurons=self.hidden_neurons,
                                    hidden_activation=self.hidden_activation,
                                    output_activation=self.output_activation,
                                    dropout_rate=self.dropout_rate,
                                    l2_regularizer=self.l2_regularizer)
        X_norm = torch.tensor(X_norm, dtype=torch.float32)
        if self.c is None:
            self.c = 0.0
            self.model_._init_c(X_norm)

        ### Predict on X itself and calculate the reconstruction error as
        ### the outlier scores. Noted X_norm was shuffled has to recreate
        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        X_norm = torch.tensor(X_norm, dtype=torch.float32)
        dataset = TensorDataset(X_norm, X_norm)
        dataloader = DataLoader(dataset, batch_size=self.batch_size,
                                shuffle=True)

        best_loss = float('inf')
        best_model_dict = None

        optimizer = optimizer_dict[self.optimizer](self.model_.parameters(),
                                                   weight_decay=self.l2_regularizer)
        w_d = 1e-6 * sum(
            [torch.linalg.norm(w) for w in self.model_.parameters()])

        for epoch in range(self.epochs):
            self.model_.train()
            epoch_loss = 0
            for batch_x, _ in dataloader:
                optimizer.zero_grad()
                outputs = self.model_(batch_x)
                dist = torch.sum((outputs - self.c) ** 2, dim=-1)
                if self.use_ae:
                    loss = torch.mean(dist) + w_d + torch.mean(
                        torch.square(outputs - batch_x))
                else:
                    loss = torch.mean(dist) + w_d

                ### loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
                if epoch_loss < best_loss:
                    best_loss = epoch_loss
                    best_model_dict = self.model_.state_dict()
            print(f"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss}")
        self.best_model_dict = best_model_dict

        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.deep_svdd.DeepSVDD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        ### check_is_fitted(self, ['model_', 'history_'])
        X = check_array(X)

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)
        X_norm = torch.tensor(X_norm, dtype=torch.float32)
        self.model_.eval()
        with torch.no_grad():
            outputs = self.model_(X_norm)
            dist = torch.sum((outputs - self.c) ** 2, dim=-1)
        anomaly_scores = dist.numpy()
        return anomaly_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 29. pyod.models.devnet - pyod 2.0.5 documentation {#29-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:35

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/devnet.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.devnet
```
### -*- coding: utf-8 -*-

"""Deep anomaly detection with deviation networks
Part of the codes are adapted from
https://github.com/GuansongPang/deviation-network
"""
### Author: Sihan Chen <schen976@usc.edu>
### License: BSD 2 clause


### Import necessary libraries
importnumpyasnp
importtorch
importtorch.nnasnn
importtorch.optimasoptim
fromsklearn.utilsimport check_array
fromtorch.utils.dataimport Dataset, DataLoader

from.baseimport BaseDetector
from..utils.torch_utilityimport TorchDataset

MAX_INT = np.iinfo(np.int32).max
data_format = 0

### Set random seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)


### Define the network architectures
classDevNetD(nn.Module):
    def__init__(self, input_shape):
        super(DevNetD, self).__init__()
        self.fc1 = nn.Linear(input_shape, 1000)
        self.fc2 = nn.Linear(1000, 250)
        self.fc3 = nn.Linear(250, 20)
        self.score = nn.Linear(20, 1)

    defforward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.score(x)
        return x


classDevNetS(nn.Module):
    def__init__(self, input_shape):
        super(DevNetS, self).__init__()
        self.fc1 = nn.Linear(input_shape, 1000)
        self.score = nn.Linear(1000, 1)

    defforward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.score(x)
        return x


classDevNetLinear(nn.Module):
    def__init__(self, input_shape):
        super(DevNetLinear, self).__init__()
        self.score = nn.Linear(input_shape, 1)

    defforward(self, x):
        x = self.score(x)
        return x


defdeviation_loss(y_true, y_pred):
'''
    Z-score-based deviation loss translated to PyTorch.
    '''
    confidence_margin = 5.0
    ### size=5000 is the setting of l in algorithm 1 in the paper
    ref = torch.randn(5000, device=y_pred.device,
                      dtype=torch.float32)  ### Generate normal distributed ref values
    dev = (y_pred - ref.mean()) / ref.std()
    inlier_loss = torch.abs(dev)
    outlier_loss = torch.abs(torch.clamp(confidence_margin - dev, min=0))

    ### Compute the mean of the weighted sum of inlier and outlier losses
    return torch.mean((1 - y_true) * inlier_loss + y_true * outlier_loss)


### Define the training and testing process
deftrain_and_test(model, train_loader, test_loader, epochs, device):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.train()

    for epoch in range(epochs):
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

    model.eval()
    with torch.no_grad():
        total_loss = 0
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            total_loss += criterion(outputs, labels).item()
        print('Test Loss:', total_loss / len(test_loader))


### Main function to run the model
defdeviation_network(input_shape, network_depth):
'''
    Construct the deviation network-based detection model in PyTorch style
    '''
    ### Select the model based on the network depth
    if network_depth == 4:
        model = DevNetD(input_shape)
    elif network_depth == 2:
        model = DevNetS(input_shape)
    elif network_depth == 1:
        model = DevNetLinear(input_shape)
    else:
        raise ValueError(
            "The network depth is not set properly")  ### Use exception instead of sys.exit

    ### Initialize the optimizer
    optimizer = optim.RMSprop(model.parameters(), lr=0.001,
                              weight_decay=1e-6)  ### Set clipnorm equivalent in PyTorch
    return model, optimizer


classSupDataset(Dataset):
    def__init__(self, x, outlier_indices, inlier_indices, rng):
        self.x = x
        self.outlier_indices = outlier_indices
        self.inlier_indices = inlier_indices
        self.rng = np.random.RandomState(
            42)  ### Ensure rng is seeded outside or fixed

    def__len__(self):
        return len(self.outlier_indices) + len(
            self.inlier_indices)  ### or any other appropriate length

    def__getitem__(self, idx):
        if idx < len(self.inlier_indices):
            ### Assuming inliers are processed first
            label = 0  ### Assuming inlier label
            index = self.inlier_indices[idx]
        else:
            ### Processing outliers
            label = 1  ### Assuming outlier label
            index = self.outlier_indices[idx - len(self.inlier_indices)]

        return self.x[index], label


definput_batch_generation_sup_sparse(x_train, outlier_indices, inlier_indices,
                                      batch_size, rng):
'''
    Batch generation for samples, alternating between positive and negative.
    Adjusted for use with PyTorch, handling data in tensors.
    '''
    training_data = []
    training_labels = []
    n_inliers = len(inlier_indices)
    n_outliers = len(outlier_indices)

    for i in range(batch_size):
        if i % 2 == 0:
            sid = rng.choice(n_inliers, 1)
            training_data.append(x_train[inlier_indices[sid.item()]])
            training_labels.append(0)
        else:
            sid = rng.choice(n_outliers, 1)
            training_data.append(x_train[outlier_indices[sid.item()]])
            training_labels.append(1)

    ### Convert lists to tensors
    training_data = torch.stack(training_data)
    training_labels = torch.tensor(training_labels, dtype=torch.long)

    return training_data, training_labels


defload_model_weight_predict(model, x_test):
    ### Ensure x_test is a PyTorch tensor and also ensure it's on the same device as the model
    x_test = torch.tensor(x_test, dtype=torch.float32)

    ### Assuming data_format variable should be defined somewhere in the context or as a parameter
    data_format = 0  ### Assuming it's set correctly according to your use-case

    if data_format == 0:
        scores = model(x_test)
    else:
        data_size = x_test.shape[0]
        scores = torch.zeros([data_size, ])
        batch_size = 512
        for i in range(0, data_size, batch_size):
            end = min(i + batch_size, data_size)
            subset = x_test[i:end]
            scores[i:end] = model(subset)

    ### Make sure the output is flattened before returning
    scores = scores.flatten()  ### Flatten the tensor to ensure it's one-dimensional

    return scores.detach().cpu().numpy()  ### Convert to numpy array if needed




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet)
classDevNet(BaseDetector):
    def__init__(self,
                 network_depth=2,
                 batch_size=512,
                 epochs=50,
                 nb_batch=20,
                 known_outliers=30,
                 cont_rate=0.02,
                 data_format=0,  ### Assuming '0' for CSV
                 random_seed=42,
                 device=None,
                 contamination=0.1):
        super(DevNet, self).__init__(contamination=contamination)
        self._classes = 2
        self.network_depth = network_depth
        self.batch_size = batch_size
        self.epochs = epochs
        self.nb_batch = nb_batch
        self.known_outliers = known_outliers
        self.cont_rate = cont_rate
        self.data_format = data_format
        self.random_seed = random_seed
        self.device = device
        if self.device is None:
            self.device = torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu")



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit)
    deffit(self, X, y):
        outlier_indices = np.where(y == 1)[0]
        inlier_indices = np.where(y == 0)[0]
        n_outliers = len(outlier_indices)
        print("Original training size: %d, No. outliers: %d" % (
            X.shape[0], n_outliers))
        n_noise = len(np.where(y == 0)[0]) * self.contamination / (
                1. - self.contamination)
        n_noise = int(n_noise)
        outlier_indices = np.where(y == 1)[0]
        inlier_indices = np.where(y == 0)[0]
        print(y.shape[0], outlier_indices.shape[0], inlier_indices.shape[0],
              n_noise)
        ### Data manipulation part can be adjusted as needed.
        self.model, optimizer = deviation_network(X.shape[1],
                                                  self.network_depth)
        rng = np.random.RandomState(42)
        train_dataset = SupDataset(X, outlier_indices, inlier_indices, rng)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size,
                                  shuffle=True)

        deftrain_model(model, data_loader, epochs):
            model.train()
            for epoch in range(epochs):
                for data, labels in data_loader:
                    data, labels = data.to(torch.float32), labels.to(
                        torch.float32)  ### Ensure data types
                    optimizer.zero_grad()
                    outputs = model(data)
                    loss = deviation_loss(outputs, labels)
                    loss.backward()
                    optimizer.step()
                print(f'Epoch {epoch+1}, Loss: {loss.item()}')

        ### Training the model
        train_model(self.model, train_loader, epochs=self.epochs)
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.decision_function)
    defdecision_function(self, X):
        X = check_array(X)

        dataset = TorchDataset(X=X, return_idx=True)

        dataloader = torch.utils.data.DataLoader(dataset,
                                                 batch_size=self.batch_size,
                                                 shuffle=False)
        ### enable the evaluation mode
        self.model.eval()

        ### construct the vector for holding the reconstruction error
        outlier_scores = np.zeros([X.shape[0], ])
        with torch.no_grad():
            for data, data_idx in dataloader:
                data_cuda = data.to(self.device).float()
                ### this is the outlier score
                outlier_scores[data_idx] = load_model_weight_predict(
                    self.model, data)
        return outlier_scores






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.devnet.DevNet.fit_predict_score)
    deffit_predict_score(self, X, y, scoring='roc_auc_score'):
"""
        Fit the detector with labels, predict on samples, and evaluate the model by predefined metrics.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : numpy array of shape (n_samples,)
            The labels or target values corresponding to X.
        scoring : str, optional (default='roc_auc_score')
            Evaluation metric:
            - 'roc_auc_score': ROC score
            - 'prc_n_score': Precision @ rank n score

        Returns
        -------
        score : float
        """

        ### Fit the model with both X and y
        self.fit(X, y)

        ### Prediction and scoring
        if scoring == 'roc_auc_score':
            fromsklearn.metricsimport roc_auc_score
            score = roc_auc_score(y, self.decision_scores_)
        elif scoring == 'prc_n_score':
            fromsklearn.metricsimport precision_recall_curve
            precision, _, _ = precision_recall_curve(y, self.decision_scores_)
            score = precision[
                1]  ### Assuming this is how you'd compute Precision @ rank n
        else:
            raise NotImplementedError('PyOD built-in scoring only supports '
                                      'ROC and Precision @ rank n')

        print("{metric}: {score}".format(metric=scoring, score=score))

        return score






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 30. pyod.models.dif - pyod 2.0.5 documentation {#30-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:10

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/dif.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.dif
```
### -*- coding: utf-8 -*-
"""Deep Isolation Forest for Anomaly Detection (DIF)
"""
### Author: Hongzuo Xu <hongzuoxu@126.edu>
### License: BSD 2 clause


importnumpyasnp

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
fromtorch.utils.dataimport DataLoader

fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted
fromsklearn.ensembleimport IsolationForest
fromsklearn.preprocessingimport StandardScaler, MinMaxScaler

from.baseimport BaseDetector
from..utils.torch_utilityimport get_activation_by_name




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF)
classDIF(BaseDetector):
"""Deep Isolation Forest (DIF) is an extension of iForest. It uses deep
    representation ensemble to achieve non-linear isolation on original data
    space. See :cite:`xu2023dif` for details.

    Parameters
    ----------
    batch_size : int, optional (default=1000)
        Number of samples per gradient update.

    representation_dim, int, optional (default=20)
        Dimensionality of the representation space.

    hidden_neurons, list, optional (default=[64, 32])
        The number of neurons per hidden layers. So the network has the
        structure as [n_features, hidden_neurons[0], hidden_neurons[1], ..., representation_dim]

    hidden_activation, str, optional (default='tanh')
        Activation function to use for hidden layers.
        All hidden layers are forced to use the same type of activation.
        See https://pytorch.org/docs/stable/nn.html for details.
        Currently only
        'relu': nn.ReLU()
        'sigmoid': nn.Sigmoid()
        'tanh': nn.Tanh()
        are supported. See pyod/utils/torch_utility.py for details.

    skip_connection, boolean, optional (default=False)
        If True, apply skip-connection in the neural network structure.

    n_ensemble, int, optional (default=50)
        The number of deep representation ensemble members.

    n_estimators, int, optional (default=6)
        The number of isolation forest of each representation.

    max_samples, int, optional (default=256)
        The number of samples to draw from X to train each base isolation tree.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random
        number generator;
        If None, the random number generator is the
        RandomState instance used by `np.random`.

    device, 'cuda', 'cpu', or None, optional (default=None)
        if 'cuda', use GPU acceleration in torch
        if 'cpu', use cpu in torch
        if None, automatically determine whether GPU is available


    Attributes
    ----------
    net_lst : list of torch.Module
        The list of representation neural networks.

    iForest_lst : list of iForest
        The list of instantiated iForest model.

    x_reduced_lst: list of numpy array
        The list of training data representations

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self,
                 batch_size=1000,
                 representation_dim=20,
                 hidden_neurons=None,
                 hidden_activation='tanh',
                 skip_connection=False,
                 n_ensemble=50,
                 n_estimators=6,
                 max_samples=256,
                 contamination=0.1,
                 random_state=None,
                 device=None):
        super(DIF, self).__init__(contamination=contamination)
        self.batch_size = batch_size
        self.representation_dim = representation_dim
        self.hidden_activation = hidden_activation
        self.skip_connection = skip_connection
        self.hidden_neurons = hidden_neurons

        self.n_ensemble = n_ensemble
        self.n_estimators = n_estimators
        self.max_samples = max_samples

        self.random_state = random_state
        self.device = device

        self.minmax_scaler = None

        ### create default calculation device (support GPU if available)
        if self.device is None:
            self.device = torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu")

        ### set random seed
        if self.random_state is not None:
            torch.manual_seed(self.random_state)
            torch.cuda.manual_seed(self.random_state)
            torch.cuda.manual_seed_all(self.random_state)
            np.random.seed(self.random_state)

        ### default values for the amount of hidden neurons
        if self.hidden_neurons is None:
            self.hidden_neurons = [500, 100]



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        n_samples, n_features = X.shape[0], X.shape[1]

        ### conduct min-max normalization before feeding into neural networks
        self.minmax_scaler = MinMaxScaler()
        self.minmax_scaler.fit(X)
        X = self.minmax_scaler.transform(X)

        ### prepare neural network parameters
        network_params = {
            'n_features': n_features,
            'n_hidden': self.hidden_neurons,
            'n_output': self.representation_dim,
            'activation': self.hidden_activation,
            'skip_connection': self.skip_connection
        }

        ### iteration
        self.net_lst = []
        self.iForest_lst = []
        self.x_reduced_lst = []
        ensemble_seeds = np.random.randint(0, 100000, self.n_ensemble)
        for i in range(self.n_ensemble):
            ### instantiate network class and seed random seed
            net = MLPnet(**network_params).to(self.device)
            torch.manual_seed(ensemble_seeds[i])

            ### initialize network parameters
            for name, param in net.named_parameters():
                if name.endswith('weight'):
                    torch.nn.init.normal_(param, mean=0., std=1.)

            x_reduced = self._deep_representation(net, X)

            ### save network and representations
            self.x_reduced_lst.append(x_reduced)
            self.net_lst.append(net)

            ### perform iForest upon representations
            self.iForest_lst.append(
                IsolationForest(n_estimators=self.n_estimators,
                                max_samples=self.max_samples,
                                random_state=ensemble_seeds[i])
            )
            self.iForest_lst[i].fit(x_reduced)

        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.dif.DIF.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['net_lst', 'iForest_lst', 'x_reduced_lst'])
        X = check_array(X)

        ### conduct min-max normalization before feeding into neural networks
        X = self.minmax_scaler.transform(X)

        testing_n_samples = X.shape[0]
        score_lst = np.zeros([self.n_ensemble, testing_n_samples])

        ### iteration
        for i in range(self.n_ensemble):
            ### transform testing data to representation
            x_reduced = self._deep_representation(self.net_lst[i], X)

            ### calculate outlier scores
            scores = _cal_score(x_reduced, self.iForest_lst[i])
            score_lst[i] = scores

        final_scores = np.average(score_lst, axis=0)
        return final_scores




    def_deep_representation(self, net, X):
        x_reduced = []

        with torch.no_grad():
            loader = DataLoader(X, batch_size=self.batch_size,
                                drop_last=False, pin_memory=True,
                                shuffle=False)
            for batch_x in loader:
                batch_x = batch_x.float().to(self.device)
                batch_x_reduced = net(batch_x)
                x_reduced.append(batch_x_reduced)

        x_reduced = torch.cat(x_reduced).data.cpu().numpy()
        x_reduced = StandardScaler().fit_transform(x_reduced)
        x_reduced = np.tanh(x_reduced)
        return x_reduced





classMLPnet(torch.nn.Module):
    def__init__(self, n_features, n_hidden=[500, 100], n_output=20,
                 activation='ReLU', bias=False, batch_norm=False,
                 skip_connection=False):
        super(MLPnet, self).__init__()
        self.skip_connection = skip_connection
        self.n_output = n_output

        num_layers = len(n_hidden)

        if type(activation) == str:
            activation = [activation] * num_layers
            activation.append(None)

        assert len(activation) == len(
            n_hidden) + 1, 'activation and n_hidden are not matched'

        self.layers = []
        for i in range(num_layers + 1):
            in_channels, out_channels = \
                self.get_in_out_channels(i, num_layers, n_features,
                                         n_hidden, n_output, skip_connection)
            self.layers += [
                LinearBlock(in_channels, out_channels,
                            bias=bias, batch_norm=batch_norm,
                            activation=activation[i],
                            skip_connection=skip_connection if i != num_layers else False)
            ]
        self.network = torch.nn.Sequential(*self.layers)

    defforward(self, x):
        x = self.network(x)
        return x

    @staticmethod
    defget_in_out_channels(i, num_layers, n_features, n_hidden, n_output,
                            skip_connection):
        if skip_connection is False:
            in_channels = n_features if i == 0 else n_hidden[i - 1]
            out_channels = n_output if i == num_layers else n_hidden[i]
        else:
            in_channels = n_features if i == 0 else np.sum(
                n_hidden[:i]) + n_features
            out_channels = n_output if i == num_layers else n_hidden[i]
        return in_channels, out_channels


classLinearBlock(torch.nn.Module):
    def__init__(self, in_channels, out_channels,
                 activation='Tanh', bias=False, batch_norm=False,
                 skip_connection=False):
        super(LinearBlock, self).__init__()

        self.skip_connection = skip_connection

        self.linear = torch.nn.Linear(in_channels, out_channels, bias=bias)

        if activation is not None:
            ### self.act_layer = _instantiate_class("torch.nn.modules.activation", activation)
            self.act_layer = get_activation_by_name(activation)
        else:
            self.act_layer = torch.nn.Identity()

        self.batch_norm = batch_norm
        if batch_norm is True:
            dim = out_channels
            self.bn_layer = torch.nn.BatchNorm1d(dim, affine=bias)

    defforward(self, x):
        x1 = self.linear(x)
        x1 = self.act_layer(x1)

        if self.batch_norm is True:
            x1 = self.bn_layer(x1)

        if self.skip_connection:
            x1 = torch.cat([x, x1], axis=1)

        return x1


def_cal_score(xx, clf):
    depths = np.zeros((xx.shape[0], len(clf.estimators_)))
    depth_sum = np.zeros(xx.shape[0])
    deviations = np.zeros((xx.shape[0], len(clf.estimators_)))
    leaf_samples = np.zeros((xx.shape[0], len(clf.estimators_)))

    for ii, estimator_tree in enumerate(clf.estimators_):
        tree = estimator_tree.tree_
        n_node = tree.node_count

        if n_node == 1:
            continue

        ### get feature and threshold of each node in the iTree
        ### in feature_lst, -2 indicates the leaf node
        feature_lst, threshold_lst = tree.feature.copy(), tree.threshold.copy()

        ### compute depth and score
        leaves_index = estimator_tree.apply(xx)
        node_indicator = estimator_tree.decision_path(xx)

        ### The number of training samples in each test sample leaf
        n_node_samples = estimator_tree.tree_.n_node_samples

        ### node_indicator is a sparse matrix with shape (n_samples, n_nodes),
        ### indicating the path of input data samples
        ### each layer would result in a non-zero element in this matrix,
        ### and then the row-wise summation is the depth of data sample
        n_samples_leaf = estimator_tree.tree_.n_node_samples[leaves_index]
        d = (np.ravel(node_indicator.sum(axis=1)) + _average_path_length(
            n_samples_leaf) - 1.0)
        depths[:, ii] = d
        depth_sum += d

        ### decision path of data matrix XX
        node_indicator = np.array(node_indicator.todense())

        ### set a matrix with shape [n_sample, n_node],
        ### representing the feature value of each sample on each node
        ### set the leaf node as -2
        value_mat = np.array([xx[i][feature_lst] for i in range(xx.shape[0])])
        value_mat[:, np.where(feature_lst == -2)[0]] = -2
        th_mat = np.array([threshold_lst for _ in range(xx.shape[0])])

        mat = np.abs(value_mat - th_mat) * node_indicator

        exist = (mat != 0)
        dev = mat.sum(axis=1) / (exist.sum(axis=1) + 1e-6)
        deviations[:, ii] = dev

    scores = 2 ** (-depth_sum / (len(clf.estimators_) * _average_path_length(
        [clf.max_samples_])))
    deviation = np.mean(deviations, axis=1)
    leaf_sample = (clf.max_samples_ - np.mean(leaf_samples,
                                              axis=1)) / clf.max_samples_

    scores = scores * deviation
    ### scores = scores * deviation * leaf_sample
    return scores


def_average_path_length(n_samples_leaf):
"""
    The average path length in a n_samples iTree, which is equal to
    the average path length of an unsuccessful BST search since the
    latter has the same structure as an isolation tree.
    Parameters
    ----------
    n_samples_leaf : array-like of shape (n_samples,)
        The number of training samples in each test sample leaf, for
        each estimators.

    Returns
    -------
    average_path_length : ndarray of shape (n_samples,)
    """

    n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)

    n_samples_leaf_shape = n_samples_leaf.shape
    n_samples_leaf = n_samples_leaf.reshape((1, -1))
    average_path_length = np.zeros(n_samples_leaf.shape)

    mask_1 = n_samples_leaf <= 1
    mask_2 = n_samples_leaf == 2
    not_mask = ~np.logical_or(mask_1, mask_2)

    average_path_length[mask_1] = 0.
    average_path_length[mask_2] = 1.
    average_path_length[not_mask] = (
            2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
            - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
    )

    return average_path_length.reshape(n_samples_leaf_shape)

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 31. pyod.models.ecod - pyod 2.0.5 documentation {#31-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:07

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ecod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.ecod
```
"""Unsupervised Outlier Detection Using
Empirical Cumulative Distribution Functions (ECOD)
"""
### Author: Zheng Li <jk_zhengli@hotmail.com>
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importwarnings

importmatplotlib.pyplotasplt
importnumpyasnp
fromjoblibimport Parallel, delayed
fromscipy.statsimport skew as skew_sp
fromsklearn.utilsimport check_array

from.baseimport BaseDetector
from.sklearn_baseimport _partition_estimators
from..utils.stat_modelsimport column_ecdf




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.skew)
defskew(X, axis=0):
    return np.nan_to_num(skew_sp(X, axis=axis))





def_parallel_ecdf(n_dims, X):
"""Private method to calculate ecdf in parallel.
    Parameters
    ----------
    n_dims : int
        The number of dimensions of the current input matrix

    X : numpy array
        The subarray for building the ECDF

    Returns
    -------
    U_l_mat : numpy array
        ECDF subarray.

    U_r_mat : numpy array
        ECDF subarray.
    """
    U_l_mat = np.zeros([X.shape[0], n_dims])
    U_r_mat = np.zeros([X.shape[0], n_dims])

    for i in range(n_dims):
        U_l_mat[:, i: i + 1] = column_ecdf(X[:, i: i + 1])
        U_r_mat[:, i: i + 1] = column_ecdf(X[:, i: i + 1] * -1)
    return U_l_mat, U_r_mat




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD)
classECOD(BaseDetector):
"""ECOD class for Unsupervised Outlier Detection Using Empirical
    Cumulative Distribution Functions (ECOD)
    ECOD is a parameter-free, highly interpretable outlier detection algorithm
    based on empirical CDF functions.
    See :cite:`li2021ecod` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_jobs : optional (default=1)
        The number of jobs to run in parallel for both `fit` and
        `predict`. If -1, then the number of jobs is set to the
        number of cores.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.
    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.
    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_jobs=1):
        super(ECOD, self).__init__(contamination=contamination)
        self.n_jobs = n_jobs



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : Ignored
            Not used, present for API consistency by convention.
        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self.X_train = X
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
         For consistency, outliers are assigned with larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        ### use multi-thread execution
        if self.n_jobs != 1:
            return self._decision_function_parallel(X)
        if hasattr(self, 'X_train'):
            original_size = X.shape[0]
            X = np.concatenate((self.X_train, X), axis=0)
        self.U_l = -1 * np.log(column_ecdf(X))
        self.U_r = -1 * np.log(column_ecdf(-X))

        skewness = np.sign(skew(X, axis=0))
        self.U_skew = self.U_l * -1 * np.sign(
            skewness - 1) + self.U_r * np.sign(skewness + 1)

        self.O = np.maximum(self.U_l, self.U_r)
        self.O = np.maximum(self.U_skew, self.O)

        if hasattr(self, 'X_train'):
            decision_scores_ = self.O.sum(axis=1)[-original_size:]
        else:
            decision_scores_ = self.O.sum(axis=1)
        return decision_scores_.ravel()




    def_decision_function_parallel(self, X):
"""Predict raw anomaly score of X using the fitted detector.
         For consistency, outliers are assigned with larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        if hasattr(self, 'X_train'):
            original_size = X.shape[0]
            X = np.concatenate((self.X_train, X), axis=0)

        n_samples, n_features = X.shape[0], X.shape[1]

        if n_features < 2:
            raise ValueError(
                'n_jobs should not be used on one dimensional dataset')

        if n_features <= self.n_jobs:
            self.n_jobs = n_features
            warnings.warn("n_features <= n_jobs; setting them equal instead.")

        n_jobs, n_dims_list, starts = _partition_estimators(n_features,
                                                            self.n_jobs)

        all_results = Parallel(n_jobs=n_jobs, max_nbytes=None,
                               verbose=True)(
            delayed(_parallel_ecdf)(
                n_dims_list[i],
                X[:, starts[i]:starts[i + 1]],
            )
            for i in range(n_jobs))

        ### recover the results
        self.U_l = np.zeros([n_samples, n_features])
        self.U_r = np.zeros([n_samples, n_features])

        for i in range(n_jobs):
            self.U_l[:, starts[i]:starts[i + 1]] = all_results[i][0]
            self.U_r[:, starts[i]:starts[i + 1]] = all_results[i][1]

        self.U_l = -1 * np.log(self.U_l)
        self.U_r = -1 * np.log(self.U_r)

        skewness = np.sign(skew(X, axis=0))
        self.U_skew = self.U_l * -1 * np.sign(
            skewness - 1) + self.U_r * np.sign(skewness + 1)

        self.O = np.maximum(self.U_l, self.U_r)
        self.O = np.maximum(self.U_skew, self.O)

        if hasattr(self, 'X_train'):
            decision_scores_ = self.O.sum(axis=1)[-original_size:]
        else:
            decision_scores_ = self.O.sum(axis=1)
        return decision_scores_.ravel()



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD.explain_outlier)
    defexplain_outlier(self, ind, columns=None, cutoffs=None,
                        feature_names=None, file_name=None,
                        file_type=None):  ### pragma: no cover
"""Plot dimensional outlier graph for a given data point within
        the dataset.

        Parameters
        ----------
        ind : int
            The index of the data point one wishes to obtain
            a dimensional outlier graph for.

        columns : list
            Specify a list of features/dimensions for plotting. If not
            specified, use all features.

        cutoffs : list of floats in (0., 1), optional (default=[0.95, 0.99])
            The significance cutoff bands of the dimensional outlier graph.

        feature_names : list of strings
            The display names of all columns of the dataset,
            to show on the x-axis of the plot.

        file_name : string
            The name to save the figure

        file_type : string
            The file type to save the figure

        Returns
        -------
        Plot : matplotlib plot
            The dimensional outlier graph for data point with index ind.
        """
        if columns is None:
            columns = list(range(self.O.shape[1]))
            column_range = range(1, self.O.shape[1] + 1)
        else:
            column_range = range(1, len(columns) + 1)

        cutoffs = [1 - self.contamination,
                   0.99] if cutoffs is None else cutoffs

        ### plot outlier scores
        plt.scatter(column_range, self.O[ind, columns], marker='^', c='black',
                    label='Outlier Score')

        for i in cutoffs:
            plt.plot(column_range,
                     np.quantile(self.O[:, columns], q=i, axis=0),
                     '--',
                     label='{percentile} Cutoff Band'.format(percentile=i))
        plt.xlim([1, max(column_range)])
        plt.ylim([0, int(self.O[:, columns].max().max()) + 1])
        plt.ylabel('Dimensional Outlier Score')
        plt.xlabel('Dimension')

        ticks = list(column_range)
        if feature_names is not None:
            assert len(feature_names) == len(ticks), \
                "Length of feature_names does not match dataset dimensions."
            plt.xticks(ticks, labels=feature_names)
        else:
            plt.xticks(ticks)

        plt.yticks(range(0, int(self.O[:, columns].max().max()) + 1))
        plt.xlim(0.95, ticks[-1] + 0.05)
        label = 'Outlier' if self.labels_[ind] == 1 else 'Inlier'
        plt.title(
            'Outlier score breakdown for sample #{index} ({label})'.format(
                index=ind + 1, label=label))
        plt.legend()
        plt.tight_layout()

        ### save the file if specified
        if file_name is not None:
            if file_type is not None:
                plt.savefig(file_name + '.' + file_type, dpi=300)
            ### if not specified, save as png
            else:
                plt.savefig(file_name + '.' + 'png', dpi=300)
        plt.show()






        ### todo: consider returning results
        ### return self.O[ind, columns], self.O[:, columns].quantile(q=cutoffs[0], axis=0), self.O[:, columns].quantile(q=cutoffs[1], axis=0)

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 32. pyod.models.feature_bagging - pyod 2.0.5 documentation {#32-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:00

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/feature_bagging.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.feature_bagging
```
### -*- coding: utf-8 -*-
"""Feature bagging detector
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumbers

importnumpyasnp
fromsklearn.baseimport clone
fromsklearn.utilsimport check_array
fromsklearn.utilsimport check_random_state
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from.combinationimport average, maximization
from.lofimport LOF
from..utils.utilityimport check_detector
from..utils.utilityimport check_parameter
from..utils.utilityimport generate_bagging_indices

MAX_INT = np.iinfo(np.int32).max


def_set_random_states(estimator, random_state=None):
"""Sets fixed random_state parameters for an estimator. Internal use only.
    Modified from sklearn/base.py

    Finds all parameters ending ``random_state`` and sets them to integers
    derived from ``random_state``.

    Parameters
    ----------
    estimator : estimator supporting get/set_params
        Estimator with potential randomness managed by random_state
        parameters.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Notes
    -----
    This does not necessarily set *all* ``random_state`` attributes that
    control an estimator's randomness, only those accessible through
    ``estimator.get_params()``.  ``random_state``s not controlled include
    those belonging to:

        * cross-validation splitters
        * ``scipy.stats`` rvs
    """
    random_state = check_random_state(random_state)
    to_set = {}
    for key in sorted(estimator.get_params(deep=True)):
        if key == 'random_state' or key.endswith('__random_state'):
            to_set[key] = random_state.randint(MAX_INT)

    if to_set:
        estimator.set_params(**to_set)


### def _parallel_decision_function(estimators, estimators_features, X):
###     n_samples = X.shape[0]
###     scores = np.zeros((n_samples, len(estimators)))
#
###     for i, (estimator, features) in enumerate(
###             zip(estimators, estimators_features)):
###         if hasattr(estimator, 'decision_function'):
###             estimator_score = estimator.decision_function(
###                 X[:, features])
###             scores[:, i] = estimator_score
###         else:
###             raise NotImplementedError(
###                 'current base detector has no decision_function')
###     return scores


### TODO: should support parallelization at the model level
### TODO: detector score combination through BFS should be implemented
### See https://github.com/yzhao062/pyod/issues/59


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging)
classFeatureBagging(BaseDetector):
""" A feature bagging detector is a meta estimator that fits a number of
    base detectors on various sub-samples of the dataset and use averaging
    or other combination methods to improve the predictive accuracy and
    control over-fitting.

    The sub-sample size is always the same as the original input sample size
    but the features are randomly sampled from half of the features to all
    features.

    By default, LOF is used as the base estimator. However, any estimator
    could be used as the base estimator, such as kNN and ABOD.

    Feature bagging first construct n subsamples by random selecting a subset
    of features, which induces the diversity of base estimators.

    Finally, the prediction score is generated by averaging/taking the maximum
    of all base detectors. See :cite:`lazarevic2005feature` for details.

    Parameters
    ----------
    base_estimator : object or None, optional (default=None)
        The base estimator to fit on random subsets of the dataset.
        If None, then the base estimator is a LOF detector.

    n_estimators : int, optional (default=10)
        The number of base estimators in the ensemble.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    max_features : int or float, optional (default=1.0)
        The number of features to draw from X to train each base estimator.

        - If int, then draw `max_features` features.
        - If float, then draw `max_features * X.shape[1]` features.

    bootstrap_features : bool, optional (default=False)
        Whether features are drawn with replacement.

    check_detector : bool, optional (default=True)
        If set to True, check whether the base estimator is consistent with
        pyod standard.

    check_estimator : bool, optional (default=False)
        If set to True, check whether the base estimator is consistent with
        sklearn standard.

        .. deprecated:: 0.6.9
          `check_estimator` will be removed in pyod 0.8.0.; it will be
          replaced by `check_detector`.

    n_jobs : optional (default=1)
        The number of jobs to run in parallel for both `fit` and
        `predict`. If -1, then the number of jobs is set to the
        number of cores.

    random_state : int, RandomState or None, optional (default=None)
        If int, random_state is the seed used by the random
        number generator; If RandomState instance, random_state is the random
        number generator; If None, the random number generator is the
        RandomState instance used by `np.random`.

    combination : str, optional (default='average')
        The method of combination:

        - if 'average': take the average of all detectors
        - if 'max': take the maximum scores of all detectors

    verbose : int, optional (default=0)
        Controls the verbosity of the building process.

    estimator_params : dict, optional (default=None)
        The list of attributes to use as parameters
        when instantiating a new base estimator. If none are given,
        default parameters are used.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    """

    def__init__(self, base_estimator=None, n_estimators=10, contamination=0.1,
                 max_features=1.0, bootstrap_features=False,
                 check_detector=True, check_estimator=False, n_jobs=1,
                 random_state=None, combination='average', verbose=0,
                 estimator_params=None):

        super(FeatureBagging, self).__init__(contamination=contamination)
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.max_features = max_features
        self.bootstrap_features = bootstrap_features
        self.check_detector = check_detector
        self.check_estimator = check_estimator
        self.combination = combination
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.verbose = verbose
        if estimator_params is not None:
            self.estimator_params = estimator_params
        else:
            self.estimator_params = {}



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        random_state = check_random_state(self.random_state)

        X = check_array(X)
        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]

        self._set_n_classes(y)

        ### expect at least 2 features, does not make sense if only have
        ### 1 feature
        check_parameter(self.n_features_, low=2, include_left=True,
                        param_name='n_features')

        ### check parameters
        self._validate_estimator(default=LOF(n_jobs=self.n_jobs))

        ### use at least half of the features
        self.min_features_ = int(0.5 * self.n_features_)

        ### Validate max_features
        if isinstance(self.max_features, (numbers.Integral, np.integer)):
            self.max_features_ = self.max_features
        else:  ### float
            self.max_features_ = int(self.max_features * self.n_features_)

        ### min_features and max_features could equal
        check_parameter(self.max_features_, low=self.min_features_,
                        param_name='max_features', high=self.n_features_,
                        include_left=True, include_right=True)

        self.estimators_ = []
        self.estimators_features_ = []

        n_more_estimators = self.n_estimators - len(self.estimators_)

        if n_more_estimators < 0:
            raise ValueError('n_estimators=%d must be larger or equal to '
                             'len(estimators_)=%d when warm_start==True'
                             % (self.n_estimators, len(self.estimators_)))

        seeds = random_state.randint(MAX_INT, size=n_more_estimators)
        self._seeds = seeds

        for i in range(self.n_estimators):
            random_state = np.random.RandomState(seeds[i])

            ### max_features is incremented by one since random
            ### function is [min_features, max_features)
            features = generate_bagging_indices(random_state,
                                                self.bootstrap_features,
                                                self.n_features_,
                                                self.min_features_,
                                                self.max_features_ + 1)
            ### initialize and append estimators
            estimator = self._make_estimator(append=False,
                                             random_state=random_state)
            estimator.fit(X[:, features])

            self.estimators_.append(estimator)
            self.estimators_features_.append(features)

        ### decision score matrix from all estimators
        all_decision_scores = self._get_decision_scores()

        if self.combination == 'average':
            self.decision_scores_ = average(all_decision_scores)
        else:
            self.decision_scores_ = maximization(all_decision_scores)

        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.feature_bagging.FeatureBagging.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['estimators_', 'estimators_features_',
                               'decision_scores_', 'threshold_', 'labels_'])
        X = check_array(X)

        if self.n_features_ != X.shape[1]:
            raise ValueError("Number of features of the model must "
                             "match the input. Model n_features is {0} and "
                             "input n_features is {1}."
                             "".format(self.n_features_, X.shape[1]))

        ### Parallel loop
        ### n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,
        ###                                                      self.n_jobs)
        ### all_pred_scores = Parallel(n_jobs=n_jobs, verbose=self.verbose)(
        ###     delayed(_parallel_decision_function)(
        ###         self.estimators_[starts[i]:starts[i + 1]],
        ###         self.estimators_features_[starts[i]:starts[i + 1]],
        ###         X)
        ###     for i in range(n_jobs))
        #
        ### ### Reduce
        ### all_pred_scores = np.concatenate(all_pred_scores, axis=1)
        all_pred_scores = self._predict_decision_scores(X)

        if self.combination == 'average':
            return average(all_pred_scores)
        else:
            return maximization(all_pred_scores)




    def_predict_decision_scores(self, X):
        all_pred_scores = np.zeros([X.shape[0], self.n_estimators])
        for i in range(self.n_estimators):
            features = self.estimators_features_[i]
            all_pred_scores[:, i] = self.estimators_[i].decision_function(
                X[:, features])
        return all_pred_scores

    def_get_decision_scores(self):
        all_decision_scores = np.zeros([self.n_samples_, self.n_estimators])
        for i in range(self.n_estimators):
            all_decision_scores[:, i] = self.estimators_[i].decision_scores_
        return all_decision_scores

    def_validate_estimator(self, default=None):
"""Check the estimator and the n_estimator attribute, set the
        `base_estimator_` attribute."""
        if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))

        if self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))

        if self.base_estimator is not None:
            self.base_estimator_ = self.base_estimator
        else:
            self.base_estimator_ = default

        if self.base_estimator_ is None:
            raise ValueError("base_estimator cannot be None")

        ### make sure estimator is consistent with sklearn
        if self.check_detector:
            check_detector(self.base_estimator_)

    def_make_estimator(self, append=True, random_state=None):
"""Make and configure a copy of the `base_estimator_` attribute.

        sklearn/base.py

        Warning: This method should be used to properly instantiate new
        sub-estimators.
        """

        ### TODO: add a check for estimator_param
        estimator = clone(self.base_estimator_)
        estimator.set_params(**self.estimator_params)

        if random_state is not None:
            _set_random_states(estimator, random_state)

        if append:
            self.estimators_.append(estimator)

        return estimator

    def__len__(self):
"""Returns the number of estimators in the ensemble."""
        return len(self.estimators_)

    def__getitem__(self, index):
"""Returns the index'th estimator in the ensemble."""
        return self.estimators_[index]

    def__iter__(self):
"""Returns iterator over estimators in the ensemble."""
        return iter(self.estimators_)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 33. pyod.models.gmm - pyod 2.0.5 documentation {#33-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:03

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/gmm.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.gmm
```
### -*- coding: utf-8 -*-
"""
Outlier detection based on Gaussian Mixture Model (GMM).
"""
### Author: Akira Tamamori <tamamori5917@gmail.com>
### License: BSD 2 clause


fromsklearn.mixtureimport GaussianMixture
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

frompyod.models.baseimport BaseDetector
frompyod.utils.utilityimport invert_order




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM)
classGMM(BaseDetector):
"""Wrapper of scikit-learn Gaussian Mixture Model with more functionalities.
    Unsupervised Outlier Detection.

    See :cite:`aggarwal2015outlier` Chapter 2 for details.

    Parameters
    ----------
    n_components : int, default=1
        The number of mixture components.

    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'
        String describing the type of covariance parameters to use.

    tol : float, default=1e-3
        The convergence threshold. EM iterations will stop when the
        lower bound average gain is below this threshold.

    reg_covar : float, default=1e-6
        Non-negative regularization added to the diagonal of covariance.
        Allows to assure that the covariance matrices are all positive.

    max_iter : int, default=100
        The number of EM iterations to perform.

    n_init : int, default=1
        The number of initializations to perform. The best results are kept.

    init_params : {'kmeans', 'random'}, default='kmeans'
        The method used to initialize the weights, the means and the
        precisions.

    weights_init : array-like of shape (n_components, ), default=None
        The user-provided initial weights.
        If it is None, weights are initialized using the `init_params` method.

    means_init : array-like of shape (n_components, n_features), default=None
        The user-provided initial means,
        If it is None, means are initialized using the `init_params` method.

    precisions_init : array-like, default=None
        The user-provided initial precisions (inverse of the covariance
        matrices).
        If it is None, precisions are initialized using the 'init_params'
        method.

    random_state : int, RandomState instance or None, default=None
        Controls the random seed given to the method chosen to initialize the
        parameters.

    warm_start : bool, default=False
        If 'warm_start' is True, the solution of the last fitting is used as
        initialization for the next call of fit().

    verbose : int, default=0
        Enable verbose output.

    verbose_interval : int, default=10
        Number of iteration done before the next print.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set.

    Attributes
    ----------
    weights_ : array-like of shape (n_components,)
        The weights of each mixture components.

    means_ : array-like of shape (n_components, n_features)
        The mean of each mixture component.

    covariances_ : array-like
        The covariance of each mixture component.

    precisions_ : array-like
        The precision matrices for each component in the mixture.

    precisions_cholesky_ : array-like
        The cholesky decomposition of the precision matrices of each mixture
        component.

    converged_ : bool
        True when convergence was reached in fit(), False otherwise.

    n_iter_ : int
        Number of step used by the best fit of EM to reach the convergence.

    lower_bound_ : float
        Lower bound value on the log-likelihood (of the training data with
        respect to the model) of the best fit of EM.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(
            self,
            n_components=1,
            covariance_type="full",
            tol=1e-3,
            reg_covar=1e-6,
            max_iter=100,
            n_init=1,
            init_params="kmeans",
            weights_init=None,
            means_init=None,
            precisions_init=None,
            random_state=None,
            warm_start=False,
            contamination=0.1,
    ):
        super().__init__(contamination=contamination)
        self.n_components = n_components
        self.covariance_type = covariance_type
        self.tol = tol
        self.reg_covar = reg_covar
        self.max_iter = max_iter
        self.n_init = n_init
        self.init_params = init_params
        self.weights_init = weights_init
        self.means_init = means_init
        self.precisions_init = precisions_init
        self.random_state = random_state
        self.warm_start = warm_start

        self.detector_ = None
        self.decision_scores_ = None



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        sample_weight : array-like, shape (n_samples,)
            Per-sample weights. Rescale C per sample. Higher weights
            force the classifier to put more emphasis on these points.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.detector_ = GaussianMixture(
            n_components=self.n_components,
            covariance_type=self.covariance_type,
            tol=self.tol,
            reg_covar=self.reg_covar,
            max_iter=self.max_iter,
            n_init=self.n_init,
            init_params=self.init_params,
            weights_init=self.weights_init,
            means_init=self.means_init,
            precisions_init=self.precisions_init,
            random_state=self.random_state,
            warm_start=self.warm_start,
        )

        self.detector_.fit(X=X, y=y)

        ### invert decision_scores_. Outliers comes with higher outlier scores
        self.decision_scores_ = invert_order(self.detector_.score_samples(X))
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.gmm.GMM.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])

        ### Invert outlier scores. Outliers come with higher outlier scores
        return invert_order(self.detector_.score_samples(X))




    @property
    defweights_(self):
"""The weights of each mixture components.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.weights_

    @property
    defmeans_(self):
"""The mean of each mixture component.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.means_

    @property
    defcovariances_(self):
"""The covariance of each mixture component.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.covariances_

    @property
    defprecisions_(self):
"""The precision matrices for each component in the mixture.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.precisions_

    @property
    defprecisions_cholesky_(self):
"""The cholesky decomposition of the precision matrices
           of each mixture component.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.precisions_cholesky_

    @property
    defconverged_(self):
"""True when convergence was reached in fit(), False otherwise.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.converged_

    @property
    defn_iter_(self):
"""Number of step used by the best fit of EM to reach the convergence.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.n_iter_

    @property
    deflower_bound_(self):
"""Lower bound value on the log-likelihood of the best fit of EM.
        Decorator for scikit-learn Gaussian Mixture Model attributes.
        """
        return self.detector_.lower_bound_




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 34. pyod.models.hbos - pyod 2.0.5 documentation {#34-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:06

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/hbos.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.hbos
```
### -*- coding: utf-8 -*-
"""Histogram-based Outlier Detection (HBOS)
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromnumbaimport njit
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport check_parameter
from..utils.utilityimport get_optimal_n_bins
from..utils.utilityimport invert_order




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS)
classHBOS(BaseDetector):
"""Histogram- based outlier detection (HBOS) is an efficient unsupervised
    method. It assumes the feature independence and calculates the degree
    of outlyingness by building histograms. See :cite:`goldstein2012histogram`
    for details.    

    Two versions of HBOS are supported:        
    - Static number of bins: uses a static number of bins for all features.
    - Automatic number of bins: every feature uses a number of bins deemed to 
      be optimal according to the Birge-Rozenblac method
      (:cite:`birge2006many`).
    Parameters
    ----------
    n_bins : int or string, optional (default=10)
        The number of bins. "auto" uses the birge-rozenblac method for
        automatic selection of the optimal number of bins for each feature.

    alpha : float in (0, 1), optional (default=0.1)
        The regularizer for preventing overflow.

    tol : float in (0, 1), optional (default=0.5)
        The parameter to decide the flexibility while dealing
        the samples falling outside the bins.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    Attributes
    ----------
    bin_edges_ : numpy array of shape (n_bins + 1, n_features )
        The edges of the bins.

    hist_ : numpy array of shape (n_bins, n_features)
        The density of each histogram.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, n_bins=10, alpha=0.1, tol=0.5, contamination=0.1):
        super(HBOS, self).__init__(contamination=contamination)
        self.n_bins = n_bins
        self.alpha = alpha
        self.tol = tol

        check_parameter(alpha, 0, 1, param_name='alpha')
        check_parameter(tol, 0, 1, param_name='tol')



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        _, n_features = X.shape[0], X.shape[1]

        if isinstance(self.n_bins, str) and self.n_bins.lower() == "auto":
            ### Uses the birge rozenblac method for automatic histogram size per feature
            self.hist_ = []
            self.bin_edges_ = []

            ### build the histograms for all dimensions
            for i in range(n_features):
                n_bins = get_optimal_n_bins(X[:, i])
                hist, bin_edges = np.histogram(X[:, i], bins=n_bins,
                                               density=True)
                self.hist_.append(hist)
                self.bin_edges_.append(bin_edges)
                ### the sum of (width * height) should equal to 1
                assert (np.isclose(1, np.sum(
                    hist * np.diff(bin_edges)), atol=0.1))

            outlier_scores = _calculate_outlier_scores_auto(X, self.bin_edges_,
                                                            self.hist_,
                                                            self.alpha,
                                                            self.tol)

        elif check_parameter(self.n_bins, low=2, high=np.inf):
            self.hist_ = np.zeros([self.n_bins, n_features])
            self.bin_edges_ = np.zeros([self.n_bins + 1, n_features])

            ### build the histograms for all dimensions
            for i in range(n_features):
                self.hist_[:, i], self.bin_edges_[:, i] = \
                    np.histogram(X[:, i], bins=self.n_bins, density=True)
                ### the sum of (width * height) should equal to 1
                assert (np.isclose(1, np.sum(
                    self.hist_[:, i] * np.diff(self.bin_edges_[:, i])),
                                   atol=0.1))

            outlier_scores = _calculate_outlier_scores(X, self.bin_edges_,
                                                       self.hist_,
                                                       self.n_bins,
                                                       self.alpha, self.tol)

        ### invert decision_scores_. Outliers comes with higher outlier scores
        self.decision_scores_ = invert_order(np.sum(outlier_scores, axis=1))
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['hist_', 'bin_edges_'])
        X = check_array(X)

        if isinstance(self.n_bins, str) and self.n_bins.lower() == "auto":
            outlier_scores = _calculate_outlier_scores_auto(X, self.bin_edges_,
                                                            self.hist_,
                                                            self.alpha,
                                                            self.tol)
        elif check_parameter(self.n_bins, low=2, high=np.inf):
            outlier_scores = _calculate_outlier_scores(X, self.bin_edges_,
                                                       self.hist_,
                                                       self.n_bins,
                                                       self.alpha, self.tol)
        return invert_order(np.sum(outlier_scores, axis=1))







### @njit #due to variable size of histograms, can no longer naively use numba for jit
def_calculate_outlier_scores_auto(X, bin_edges, hist, alpha,
                                   tol):  ### pragma: no cover
"""The internal function to calculate the outlier scores based on
    the bins and histograms constructed with the training data. The program
    is optimized through numba. It is excluded from coverage test for
    eliminating the redundancy.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features
        The input samples.

    bin_edges : list of length n_features containing numpy arrays
        The edges of the bins.

    hist : =list of length n_features containing numpy arrays
        The density of each histogram.

    alpha : float in (0, 1)
        The regularizer for preventing overflow.

    tol : float in (0, 1)
        The parameter to decide the flexibility while dealing
        the samples falling outside the bins.

    Returns
    -------
    outlier_scores : numpy array of shape (n_samples, n_features)
        Outlier scores on all features (dimensions).
    """

    n_samples, n_features = X.shape[0], X.shape[1]
    outlier_scores = np.zeros(shape=(n_samples, n_features))

    for i in range(n_features):

        ### Find the indices of the bins to which each value belongs.
        ### See documentation for np.digitize since it is tricky
        ### >>> x = np.array([0.2, 6.4, 3.0, 1.6, -1, 100, 10])
        ### >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])
        ### >>> np.digitize(x, bins, right=True)
        ### array([1, 4, 3, 2, 0, 5, 4], dtype=int64)

        bin_inds = np.digitize(X[:, i], bin_edges[i], right=True)

        ### Calculate the outlying scores on dimension i
        ### Add a regularizer for preventing overflow
        out_score_i = np.log2(hist[i] + alpha)

        optimal_n_bins = get_optimal_n_bins(X[:, i])

        for j in range(n_samples):

            ### If the sample does not belong to any bins
            ### bin_ind == 0 (fall outside since it is too small)
            if bin_inds[j] == 0:
                dist = bin_edges[i][0] - X[j, i]
                bin_width = bin_edges[i][1] - bin_edges[i][0]

                ### If it is only slightly lower than the smallest bin edge
                ### assign it to bin 1
                if dist <= bin_width * tol:
                    outlier_scores[j, i] = out_score_i[0]
                else:
                    outlier_scores[j, i] = np.min(out_score_i)

            ### If the sample does not belong to any bins
            ### bin_ind == optimal_n_bins+1 (fall outside since it is too large)
            elif bin_inds[j] == optimal_n_bins + 1:
                dist = X[j, i] - bin_edges[i][-1]
                bin_width = bin_edges[i][-1] - bin_edges[i][-2]

                ### If it is only slightly larger than the largest bin edge
                ### assign it to the last bin
                if dist <= bin_width * tol:
                    outlier_scores[j, i] = out_score_i[optimal_n_bins - 1]
                else:
                    outlier_scores[j, i] = np.min(out_score_i)
            else:
                outlier_scores[j, i] = out_score_i[bin_inds[j] - 1]

    return outlier_scores


@njit
def_calculate_outlier_scores(X, bin_edges, hist, n_bins, alpha,
                              tol):  ### pragma: no cover
"""The internal function to calculate the outlier scores based on
    the bins and histograms constructed with the training data. The program
    is optimized through numba. It is excluded from coverage test for
    eliminating the redundancy.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input samples.

    bin_edges : numpy array of shape (n_bins + 1, n_features )
        The edges of the bins.

    hist : numpy array of shape (n_bins, n_features)
        The density of each histogram.

    n_bins : int
        The number of bins. 

    alpha : float in (0, 1)
        The regularizer for preventing overflow.

    tol : float in (0, 1)
        The parameter to decide the flexibility while dealing
        the samples falling outside the bins.

    Returns
    -------
    outlier_scores : numpy array of shape (n_samples, n_features)
        Outlier scores on all features (dimensions).
    """

    n_samples, n_features = X.shape[0], X.shape[1]
    outlier_scores = np.zeros(shape=(n_samples, n_features))

    for i in range(n_features):

        ### Find the indices of the bins to which each value belongs.
        ### See documentation for np.digitize since it is tricky
        ### >>> x = np.array([0.2, 6.4, 3.0, 1.6, -1, 100, 10])
        ### >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])
        ### >>> np.digitize(x, bins, right=True)
        ### array([1, 4, 3, 2, 0, 5, 4], dtype=int64)

        bin_inds = np.digitize(X[:, i], bin_edges[:, i], right=True)

        ### Calculate the outlying scores on dimension i
        ### Add a regularizer for preventing overflow
        out_score_i = np.log2(hist[:, i] + alpha)

        for j in range(n_samples):

            ### If the sample does not belong to any bins
            ### bin_ind == 0 (fall outside since it is too small)
            if bin_inds[j] == 0:
                dist = bin_edges[0, i] - X[j, i]
                bin_width = bin_edges[1, i] - bin_edges[0, i]

                ### If it is only slightly lower than the smallest bin edge
                ### assign it to bin 1
                if dist <= bin_width * tol:
                    outlier_scores[j, i] = out_score_i[0]
                else:
                    outlier_scores[j, i] = np.min(out_score_i)

            ### If the sample does not belong to any bins
            ### bin_ind == n_bins+1 (fall outside since it is too large)
            elif bin_inds[j] == n_bins + 1:
                dist = X[j, i] - bin_edges[-1, i]
                bin_width = bin_edges[-1, i] - bin_edges[-2, i]

                ### If it is only slightly larger than the largest bin edge
                ### assign it to the last bin
                if dist <= bin_width * tol:
                    outlier_scores[j, i] = out_score_i[n_bins - 1]
                else:
                    outlier_scores[j, i] = np.min(out_score_i)
            else:
                outlier_scores[j, i] = out_score_i[bin_inds[j] - 1]

    return outlier_scores

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 35. pyod.models.iforest - pyod 2.0.5 documentation {#35-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:28

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.iforest
```
### -*- coding: utf-8 -*-
"""IsolationForest Outlier Detector. Implemented on scikit-learn library.
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromjoblibimport Parallel
fromjoblib.parallelimport delayed
fromsklearn.ensembleimport IsolationForest
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
### noinspection PyProtectedMember
from..utils.utilityimport invert_order


### TODO: behavior of Isolation Forest will change in sklearn 0.22. See below.
### in 0.22, scikit learn will start adjust decision_function values by
### offset to make the values below zero as outliers. In other words, it is
### an absolute shift, which SHOULD NOT affect the result of PyOD at all as
### the order is still preserved.

### Behaviour of the decision_function which can be either ‚Äòold‚Äô or ‚Äònew‚Äô.
### Passing behaviour='new' makes the decision_function change to match other
### anomaly detection algorithm API which will be the default behaviour in the
### future. As explained in details in the offset_ attribute documentation,
### the decision_function becomes dependent on the contamination parameter,
### in such a way that 0 becomes its natural threshold to detect outliers.

### offset_ : float
### Offset used to define the decision function from the raw scores.
### We have the relation: decision_function = score_samples - offset_.
### Assuming behaviour == ‚Äònew‚Äô, offset_ is defined as follows.
### When the contamination parameter is set to ‚Äúauto‚Äù,
### the offset is equal to -0.5 as the scores of inliers are close to 0 and the
### scores of outliers are close to -1. When a contamination parameter different
### than ‚Äúauto‚Äù is provided, the offset is defined in such a way we obtain the
### expected number of outliers (samples with decision function < 0) in training.
### Assuming the behaviour parameter is set to ‚Äòold‚Äô,
### we always have offset_ = -0.5, making the decision function independent from
### the contamination parameter.

### check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html for more information




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest)
classIForest(BaseDetector):
"""Wrapper of scikit-learn Isolation Forest with more functionalities.

    The IsolationForest 'isolates' observations by randomly selecting a
    feature and then randomly selecting a split value between the maximum and
    minimum values of the selected feature.
    See :cite:`liu2008isolation,liu2012isolation` for details.

    Since recursive partitioning can be represented by a tree structure, the
    number of splittings required to isolate a sample is equivalent to the path
    length from the root node to the terminating node.

    This path length, averaged over a forest of such random trees, is a
    measure of normality and our decision function.

    Random partitioning produces noticeably shorter paths for anomalies.
    Hence, when a forest of random trees collectively produce shorter path
    lengths for particular samples, they are highly likely to be anomalies.

    Parameters
    ----------
    n_estimators : int, optional (default=100)
        The number of base estimators in the ensemble.

    max_samples : int or float, optional (default="auto")
        The number of samples to draw from X to train each base estimator.

            - If int, then draw `max_samples` samples.
            - If float, then draw `max_samples * X.shape[0]` samples.
            - If "auto", then `max_samples=min(256, n_samples)`.

        If max_samples is larger than the number of samples provided,
        all samples will be used for all trees (no sampling).

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e. the proportion
        of outliers in the data set. Used when fitting to define the threshold
        on the decision function.

    max_features : int or float, optional (default=1.0)
        The number of features to draw from X to train each base estimator.

            - If int, then draw `max_features` features.
            - If float, then draw `max_features * X.shape[1]` features.

    bootstrap : bool, optional (default=False)
        If True, individual trees are fit on random subsets of the training
        data sampled with replacement. If False, sampling without replacement
        is performed.

    n_jobs : integer, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.

    behaviour : str, default='old'
        Behaviour of the ``decision_function`` which can be either 'old' or
        'new'. Passing ``behaviour='new'`` makes the ``decision_function``
        change to match other anomaly detection algorithm API which will be
        the default behaviour in the future. As explained in details in the
        ``offset_`` attribute documentation, the ``decision_function`` becomes
        dependent on the contamination parameter, in such a way that 0 becomes
        its natural threshold to detect outliers.

        .. versionadded:: 0.7.0
           ``behaviour`` is added in 0.7.0 for back-compatibility purpose.

        .. deprecated:: 0.20
           ``behaviour='old'`` is deprecated in sklearn 0.20 and will not be
           possible in 0.22.

        .. deprecated:: 0.22
           ``behaviour`` parameter will be deprecated in sklearn 0.22 and
           removed in 0.24.

        .. warning::
            Only applicable for sklearn 0.20 above.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    verbose : int, optional (default=0)
        Controls the verbosity of the tree building process.

    Attributes
    ----------
    estimators_ : list of DecisionTreeClassifier
        The collection of fitted sub-estimators.

    estimators_samples_ : list of arrays
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator.

    max_samples_ : integer
        The actual number of samples

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, n_estimators=100,
                 max_samples="auto",
                 contamination=0.1,
                 max_features=1.,
                 bootstrap=False,
                 n_jobs=1,
                 behaviour='old',
                 random_state=None,
                 verbose=0):
        super(IForest, self).__init__(contamination=contamination)
        self.n_estimators = n_estimators
        self.max_samples = max_samples
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.n_jobs = n_jobs
        self.behaviour = behaviour
        self.random_state = random_state
        self.verbose = verbose



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### In sklearn 0.20+ new behaviour is added (arg behaviour={'new','old'})
        ### to IsolationForest that shifts the location of the anomaly scores
        ### noinspection PyProtectedMember

        self.detector_ = IsolationForest(n_estimators=self.n_estimators,
                                         max_samples=self.max_samples,
                                         contamination=self.contamination,
                                         max_features=self.max_features,
                                         bootstrap=self.bootstrap,
                                         n_jobs=self.n_jobs,
                                         random_state=self.random_state,
                                         verbose=self.verbose)

        self.detector_.fit(X=X, y=None, sample_weight=None)

        ### invert decision_scores_. Outliers comes with higher outlier scores.
        self.decision_scores_ = invert_order(
            self.detector_.decision_function(X))
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.iforest.IForest.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        ### invert outlier scores. Outliers comes with higher outlier scores
        return invert_order(self.detector_.decision_function(X))




    @property
    defestimators_(self):
"""The collection of fitted sub-estimators.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.estimators_

    @property
    defestimators_samples_(self):
"""The subset of drawn samples (i.e., the in-bag samples) for
        each base estimator.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.estimators_samples_

    @property
    defmax_samples_(self):
"""The actual number of samples.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.max_samples_

    @property
    defestimators_features_(self):
"""The indeces of the subset of features used to train the estimators.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.estimators_features_

    @property
    defn_features_in_(self):
"""The number of features seen during the fit.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.n_features_in_

    @property
    defoffset_(self):
"""Offset used to define the decision function from the raw scores.
        Decorator for scikit-learn Isolation Forest attributes.
        """
        return self.detector_.offset_

    @property
    deffeature_importances_(self):
"""The impurity-based feature importance. The higher, the more
        important the feature. The importance of a feature is computed as the
        (normalized) total reduction of the criterion brought by that feature.
        It is also known as the Gini importance.

        .. warning::
        impurity-based feature importance can be misleading for
        high cardinality features (many unique values). See
        https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html
        as an alternative.

        Returns
        -------
        feature_importances_ : ndarray of shape (n_features,)
            The values of this array sum to 1, unless all trees are single node
            trees consisting of only the root node, in which case it will be an
            array of zeros.
        """
        check_is_fitted(self)
        all_importances = Parallel(
            n_jobs=self.n_jobs)(
            delayed(getattr)(tree, "feature_importances_")
            for tree in self.detector_.estimators_
            if tree.tree_.node_count > 1
        )

        if not all_importances:
            return np.zeros(self.n_features_in_, dtype=np.float64)

        all_importances = np.mean(all_importances, axis=0, dtype=np.float64)
        return all_importances / np.sum(all_importances)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 36. pyod.models.inne - pyod 2.0.5 documentation {#36-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:21

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/inne.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.inne
```
### -*- coding: utf-8 -*-
"""Isolation-based anomaly detection using nearest-neighbor ensembles.
Part of the codes are adapted from https://github.com/xhan97/inne
"""
### Author: Xin Han <xinhan197@gmail.com>
### License: BSD 2 clause


importnumbers
fromwarningsimport warn

importnumpyasnp
fromsklearn.metricsimport euclidean_distances
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted, check_random_state

from.baseimport BaseDetector
from..utils.utilityimport MAX_INT, invert_order

MIN_FLOAT = np.finfo(float).eps




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE)
classINNE(BaseDetector):
""" Isolation-based anomaly detection using nearest-neighbor ensembles.

    The INNE algorithm uses the nearest neighbour ensemble to isolate
    anomalies. It partitions the data space into regions using a subsample and
    determines an isolation score for each region. As each region adapts to
    local distribution, the calculated isolation score is a local measure that
    is relative to the local neighbourhood, enabling it to detect both global
    and local anomalies. INNE has linear time complexity to efficiently handle
    large and high-dimensional datasets with complex distributions.

    See :cite:`bandaragoda2018isolation` for details.

    Parameters
    ----------
    n_estimators : int, default=200
        The number of base estimators in the ensemble.

    max_samples : int or float, optional (default="auto")
        The number of samples to draw from X to train each base estimator.

            - If int, then draw `max_samples` samples.
            - If float, then draw `max_samples` * X.shape[0]` samples.
            - If "auto", then `max_samples=min(8, n_samples)`.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e. the proportion
        of outliers in the data set. Used when fitting to define the threshold
        on the decision function.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Attributes
    ----------
    max_samples_ : integer
        The actual number of samples

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self,
                 n_estimators=200,
                 max_samples="auto",
                 contamination=0.1,
                 random_state=None):
        self.n_estimators = n_estimators
        self.max_samples = max_samples
        self.random_state = random_state
        self.contamination = contamination



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)

        ### Check data
        X = check_array(X, accept_sparse=False)
        self._set_n_classes(y)

        n_samples = X.shape[0]
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(8, n_samples)
            else:
                raise ValueError(
                    "max_samples (%s) is not supported."
                    'Valid choices are: "auto", int or'
                    "float"
                    % self.max_samples
                )

        elif isinstance(self.max_samples, numbers.Integral):
            if self.max_samples > n_samples:
                warn(
                    "max_samples (%s) is greater than the "
                    "total number of samples (%s). max_samples "
                    "will be set to n_samples for estimation."
                    % (self.max_samples, n_samples)
                )
                max_samples = n_samples
            else:
                max_samples = self.max_samples
        else:  ### float
            if not 0.0 < self.max_samples <= 1.0:
                raise ValueError(
                    "max_samples must be in (0, 1], got %r" % self.max_samples
                )
            max_samples = int(self.max_samples * X.shape[0])
        self.max_samples_ = max_samples

        self._fit(X)
        self.decision_scores_ = invert_order(self._score_samples(X))
        self._process_decision_scores()
        return self




    def_fit(self, X):
""" Build n_estimators sets of hyperspheres. 

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. 

        Returns
        -------
        self : object
        """

        n_samples, n_features = X.shape
        self._centroids = np.empty(
            [self.n_estimators, self.max_samples_, n_features])
        self._ratio = np.empty([self.n_estimators, self.max_samples_])
        self._centroids_radius = np.empty(
            [self.n_estimators, self.max_samples_])

        random_state = check_random_state(self.random_state)
        self._seeds = random_state.randint(MAX_INT, size=self.n_estimators)

        for i in range(self.n_estimators):
            rnd = check_random_state(self._seeds[i])
            ### randomly selected subsamples of size max_samples_ as centroids.
            center_index = rnd.choice(
                n_samples, self.max_samples_, replace=False)

            self._centroids[i] = X[center_index]
            center_dist = euclidean_distances(
                self._centroids[i], self._centroids[i], squared=True)
            np.fill_diagonal(center_dist, np.inf)
            ### radius of each hypersphere is the Nearest Neighbors
            ### distance of centroid.
            self._centroids_radius[i] = np.amin(center_dist, axis=1)
            ### Nearest Neighbors of centroids
            cnn_index = np.argmin(center_dist, axis=1)
            cnn_radius = self._centroids_radius[i][cnn_index]

            self._ratio[i] = 1 - (cnn_radius + MIN_FLOAT) / \
                             (self._centroids_radius[i] + MIN_FLOAT)
        return self



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.inne.INNE.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. 

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        ### invert outlier scores. Outliers comes with higher outlier scores
        return invert_order(self._score_samples(X))




    def_score_samples(self, X):
"""
        Opposite of the anomaly score defined in the original paper.
        The anomaly score of an input sample is computed as
        the mean anomaly score over all set of hyperspheres.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal.
        """

        ### check data
        X = check_array(X, accept_sparse=False)
        isolation_scores = np.ones([self.n_estimators, X.shape[0]])

        ### each test instance is evaluated against n_estimators sets of
        ### hyperspheres
        for i in range(self.n_estimators):
            x_dists = euclidean_distances(X, self._centroids[i], squared=True)
            ### find instances that are covered by at least one hypersphere.
            cover_radius = np.where(
                x_dists <= self._centroids_radius[i],
                self._centroids_radius[i], np.nan)
            x_covered = np.where(~np.isnan(cover_radius).all(axis=1))
            ### the centroid of the hypersphere covering x and having the
            ### smallest radius
            cnn_x = np.nanargmin(cover_radius[x_covered], axis=1)
            isolation_scores[i][x_covered] = self._ratio[i][cnn_x]
        ### the isolation scores are averaged to produce the anomaly score
        scores = np.mean(isolation_scores, axis=0)
        return -scores




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 37. pyod.models.kde - pyod 2.0.5 documentation {#37-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:17

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kde.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.kde
```
### -*- coding: utf-8 -*-
"""Kernel Density Estimation (KDE) for Unsupervised Outlier Detection.
"""
### Author: Akira Tamamori <tamamori5917@gmail.com>
### License: BSD 2 clause


fromwarningsimport warn

fromsklearn.neighborsimport KernelDensity
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport invert_order




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE)
classKDE(BaseDetector):
"""KDE class for outlier detection.

    For an observation, its negative log probability density could be viewed
    as the outlying score.

    See :cite:`latecki2007outlier` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    bandwidth : float, optional (default=1.0)
        The bandwidth of the kernel.

    algorithm : {'auto', 'ball_tree', 'kd_tree'}, optional
        Algorithm used to compute the kernel density estimator:

        - 'ball_tree' will use BallTree
        - 'kd_tree' will use KDTree
        - 'auto' will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

    leaf_size : int, optional (default = 30)
        Leaf size passed to BallTree. This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree.  The optimal value depends on the
        nature of the problem.

    metric : string or callable, default 'minkowski'
        metric to use for distance computation. Any metric from scikit-learn
        or scipy.spatial.distance can be used.

        If metric is a callable function, it is called on each
        pair of instances (rows) and the resulting value recorded. The callable
        should take two arrays as input and return one value indicating the
        distance between them. This works for Scipy's metrics, but is less
        efficient than passing the metric name as a string.

        Distance matrices are not supported.

        Valid values for metric are:

        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
          'manhattan']

        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',
          'sqeuclidean', 'yule']

        See the documentation for scipy.spatial.distance for details on these
        metrics.

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(
            self,
            contamination=0.1,
            bandwidth=1.0,
            algorithm="auto",
            leaf_size=30,
            metric="minkowski",
            metric_params=None,
    ):
        super().__init__(contamination=contamination)
        self.bandwidth = bandwidth
        self.algorithm = algorithm
        self.leaf_size = leaf_size
        self.metric = metric
        self.metric_params = metric_params

        if self.algorithm != "auto" and self.algorithm != "ball_tree":
            warn(
                "algorithm parameter is deprecated and will be removed "
                "in version 0.7.6. By default, ball_tree will be used.",
                FutureWarning,
            )

        self.kde_ = KernelDensity(
            bandwidth=self.bandwidth,
            algorithm=self.algorithm,
            leaf_size=self.leaf_size,
            metric=self.metric,
            metric_params=self.metric_params,
        )

        self.decision_scores_ = None



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.kde_.fit(X)

        ### invert decision_scores_. Outliers comes with higher outlier scores.
        self.decision_scores_ = invert_order(self.kde_.score_samples(X))
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kde.KDE.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])

        X = check_array(X)

        ### invert outlier scores. Outliers comes with higher outlier scores.
        return invert_order(self.kde_.score_samples(X))






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 38. pyod.models.knn - pyod 2.0.5 documentation {#38-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:27

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/knn.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.knn
```
### -*- coding: utf-8 -*-
"""k-Nearest Neighbors Detector (kNN)
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


fromwarningsimport warn

importnumpyasnp
fromsklearn.neighborsimport BallTree
fromsklearn.neighborsimport NearestNeighbors
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


### TODO: algorithm parameter is deprecated and will be removed in 0.7.6.
### Warning has been turned on.
### TODO: since Ball_tree is used by default, may introduce its parameters.



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN)
classKNN(BaseDetector):
    ### noinspection PyPep8
"""kNN class for outlier detection.
    For an observation, its distance to its kth nearest neighbor could be
    viewed as the outlying score. It could be viewed as a way to measure
    the density. See :cite:`ramaswamy2000efficient,angiulli2002fast` for
    details.

    Three kNN detectors are supported:
    largest: use the distance to the kth neighbor as the outlier score
    mean: use the average of all k neighbors as the outlier score
    median: use the median of the distance to k neighbors as the outlier score

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_neighbors : int, optional (default = 5)
        Number of neighbors to use by default for k neighbors queries.

    method : str, optional (default='largest')
        {'largest', 'mean', 'median'}

        - 'largest': use the distance to the kth neighbor as the outlier score
        - 'mean': use the average of all k neighbors as the outlier score
        - 'median': use the median of the distance to k neighbors as the
          outlier score

    radius : float, optional (default = 1.0)
        Range of parameter space to use by default for `radius_neighbors`
        queries.

    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
        Algorithm used to compute the nearest neighbors:

        - 'ball_tree' will use BallTree
        - 'kd_tree' will use KDTree
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.

        .. deprecated:: 0.74
           ``algorithm`` is deprecated in PyOD 0.7.4 and will not be
           possible in 0.7.6. It has to use BallTree for consistency.

    leaf_size : int, optional (default = 30)
        Leaf size passed to BallTree. This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree.  The optimal value depends on the
        nature of the problem.

    metric : string or callable, default 'minkowski'
        metric to use for distance computation. Any metric from scikit-learn
        or scipy.spatial.distance can be used.

        If metric is a callable function, it is called on each
        pair of instances (rows) and the resulting value recorded. The callable
        should take two arrays as input and return one value indicating the
        distance between them. This works for Scipy's metrics, but is less
        efficient than passing the metric name as a string.

        Distance matrices are not supported.

        Valid values for metric are:

        - from scikit-learn: ['cityblock', 'euclidean', 'l1', 'l2',
          'manhattan']

        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',
          'sqeuclidean', 'yule']

        See the documentation for scipy.spatial.distance for details on these
        metrics.

    p : integer, optional (default = 2)
        Parameter for the Minkowski metric from
        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
        equivalent to using manhattan_distance (l1), and euclidean_distance
        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
        See http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    n_jobs : int, optional (default = 1)
        The number of parallel jobs to run for neighbors search.
        If ``-1``, then the number of jobs is set to the number of CPU cores.
        Affects only kneighbors and kneighbors_graph methods.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_neighbors=5, method='largest',
                 radius=1.0, algorithm='auto', leaf_size=30,
                 metric='minkowski', p=2, metric_params=None, n_jobs=1,
                 **kwargs):
        super(KNN, self).__init__(contamination=contamination)
        self.n_neighbors = n_neighbors
        self.method = method
        self.radius = radius
        self.algorithm = algorithm
        self.leaf_size = leaf_size
        self.metric = metric
        self.p = p
        self.metric_params = metric_params
        self.n_jobs = n_jobs

        if self.algorithm != 'auto' and self.algorithm != 'ball_tree':
            warn('algorithm parameter is deprecated and will be removed '
                 'in version 0.7.6. By default, ball_tree will be used.',
                 FutureWarning)

        self.neigh_ = NearestNeighbors(n_neighbors=self.n_neighbors,
                                       radius=self.radius,
                                       algorithm=self.algorithm,
                                       leaf_size=self.leaf_size,
                                       metric=self.metric,
                                       p=self.p,
                                       metric_params=self.metric_params,
                                       n_jobs=self.n_jobs,
                                       **kwargs)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.neigh_.fit(X)

        ### In certain cases, _tree does not exist for NearestNeighbors
        ### See Issue #158 (https://github.com/yzhao062/pyod/issues/158)
        ### n_neighbors = 100
        if self.neigh_._tree is not None:
            self.tree_ = self.neigh_._tree

        else:
            if self.metric_params is not None:
                self.tree_ = BallTree(X, leaf_size=self.leaf_size,
                                      metric=self.metric,
                                      **self.metric_params)
            else:
                self.tree_ = BallTree(X, leaf_size=self.leaf_size,
                                      metric=self.metric)

        dist_arr, _ = self.neigh_.kneighbors(n_neighbors=self.n_neighbors,
                                             return_distance=True)
        dist = self._get_dist_by_method(dist_arr)

        self.decision_scores_ = dist.ravel()
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.knn.KNN.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['tree_', 'decision_scores_',
                               'threshold_', 'labels_'])

        X = check_array(X)

        ### initialize the output score
        pred_scores = np.zeros([X.shape[0], 1])

        for i in range(X.shape[0]):
            x_i = X[i, :]
            x_i = np.asarray(x_i).reshape(1, x_i.shape[0])

            ### get the distance of the current point
            dist_arr, _ = self.tree_.query(x_i, k=self.n_neighbors)
            dist = self._get_dist_by_method(dist_arr)
            pred_score_i = dist[-1]

            ### record the current item
            pred_scores[i, :] = pred_score_i

        return pred_scores.ravel()




    def_get_dist_by_method(self, dist_arr):
"""Internal function to decide how to process passed in distance array

        Parameters
        ----------
        dist_arr : numpy array of shape (n_samples, n_neighbors)
            Distance matrix.

        Returns
        -------
        dist : numpy array of shape (n_samples,)
            The outlier scores by distance.
        """

        if self.method == 'largest':
            return dist_arr[:, -1]
        elif self.method == 'mean':
            return np.mean(dist_arr, axis=1)
        elif self.method == 'median':
            return np.median(dist_arr, axis=1)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 39. pyod.models.kpca - pyod 2.0.5 documentation {#39-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:35

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/kpca.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.kpca
```
### -*- coding: utf-8 -*-
"""Kernel Principal Component Analysis (KPCA) Outlier Detector
"""
### Author: Akira Tamamori <tamamori5917@gmail.com>
### License: BSD 2 clause

importnumpyasnp
fromsklearn.decompositionimport KernelPCA
fromsklearn.utilsimport check_array, check_random_state
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport check_parameter


classPyODKernelPCA(KernelPCA):
"""A wrapper class for KernelPCA class of scikit-learn."""

    def__init__(
            self,
            n_components=None,
            kernel="rbf",
            gamma=None,
            degree=3,
            coef0=1,
            kernel_params=None,
            alpha=1.0,
            fit_inverse_transform=False,
            eigen_solver="auto",
            tol=0,
            max_iter=None,
            remove_zero_eig=False,
            copy_X=True,
            n_jobs=None,
            random_state=None,
    ):
        super().__init__(
            kernel=kernel,
            gamma=gamma,
            degree=degree,
            coef0=coef0,
            kernel_params=kernel_params,
            alpha=alpha,
            fit_inverse_transform=fit_inverse_transform,
            eigen_solver=eigen_solver,
            tol=tol,
            max_iter=max_iter,
            remove_zero_eig=remove_zero_eig,
            n_jobs=n_jobs,
            copy_X=copy_X,
            random_state=check_random_state(random_state),
        )

    @property
    defget_centerer(self):
"""Return a protected member _centerer."""
        return self._centerer

    @property
    defget_kernel(self):
"""Return a protected member _get_kernel."""
        return self._get_kernel




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA)
classKPCA(BaseDetector):
"""KPCA class for outlier detection.

    PCA is performed on the feature space uniquely determined by the kernel,
    and the reconstruction error on the feature space is used as the anomaly score.

    See :cite:`hoffmann2007kernel`
    Heiko Hoffmann, "Kernel PCA for novelty detection,"
    Pattern Recognition, vol.40, no.3, pp. 863-874, 2007.
    https://www.sciencedirect.com/science/article/pii/S0031320306003414
    for details.

    Parameters
    ----------
    n_components : int, optional (default=None)
        Number of components. If None, all non-zero components are kept.

    n_selected_components : int, optional (default=None)
        Number of selected principal components
        for calculating the outlier scores. It is not necessarily equal to
        the total number of the principal components. If not set, use
        all principal components.

    kernel : string {'linear', 'poly', 'rbf', 'sigmoid',
                     'cosine', 'precomputed'}, optional (default='rbf')
        Kernel used for PCA.

    gamma : float, optional (default=None)
        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.

    degree : int, optional (default=3)
        Degree for poly kernels. Ignored by other kernels.

    coef0 : float, optional (default=1)
        Independent term in poly and sigmoid kernels.
        Ignored by other kernels.

    kernel_params : dict, optional (default=None)
        Parameters (keyword arguments) and
        values for kernel passed as callable object.
        Ignored by other kernels.

    alpha : float, optional (default=1.0)
        Hyperparameter of the ridge regression that learns the
        inverse transform (when inverse_transform=True).

    eigen_solver : string, {'auto', 'dense', 'arpack', 'randomized'}, \
            default='auto'
        Select eigensolver to use. If `n_components` is much
        less than the number of training samples, randomized (or arpack to a
        smaller extend) may be more efficient than the dense eigensolver.
        Randomized SVD is performed according to the method of Halko et al.

        auto :
            the solver is selected by a default policy based on n_samples
            (the number of training samples) and `n_components`:
            if the number of components to extract is less than 10 (strict) and
            the number of samples is more than 200 (strict), the 'arpack'
            method is enabled. Otherwise the exact full eigenvalue
            decomposition is computed and optionally truncated afterwards
            ('dense' method).
        dense :
            run exact full eigenvalue decomposition calling the standard
            LAPACK solver via `scipy.linalg.eigh`, and select the components
            by postprocessing.
        arpack :
            run SVD truncated to n_components calling ARPACK solver using
            `scipy.sparse.linalg.eigsh`. It requires strictly
            0 < n_components < n_samples
        randomized :
            run randomized SVD.
            implementation selects eigenvalues based on their module; therefore
            using this method can lead to unexpected results if the kernel is
            not positive semi-definite.

    tol : float, optional (default=0)
        Convergence tolerance for arpack.
        If 0, optimal value will be chosen by arpack.

    max_iter : int, optional (default=None)
        Maximum number of iterations for arpack.
        If None, optimal value will be chosen by arpack.

    remove_zero_eig : bool, optional (default=False)
        If True, then all components with zero eigenvalues are removed, so
        that the number of components in the output may be < n_components
        (and sometimes even zero due to numerical instability).
        When n_components is None, this parameter is ignored and components
        with zero eigenvalues are removed regardless.

    copy_X : bool, optional (default=True)
        If True, input X is copied and stored by the model in the `X_fit_`
        attribute. If no further changes will be done to X, setting
        `copy_X=False` saves memory by storing a reference.

    n_jobs : int, optional (default=None)
        The number of parallel jobs to run.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors.

    sampling : bool, optional (default=False)
        If True, sampling subset from the dataset is performed only once,
        in order to reduce time complexity while keeping detection performance.

    subset_size : float in (0., 1.0) or int (0, n_samples), optional (default=20)
        If sampling is True, the size of subset is specified.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance
        used by np.random.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(
            self,
            contamination=0.1,
            n_components=None,
            n_selected_components=None,
            kernel="rbf",
            gamma=None,
            degree=3,
            coef0=1,
            kernel_params=None,
            alpha=1.0,
            eigen_solver="auto",
            tol=0,
            max_iter=None,
            remove_zero_eig=False,
            copy_X=True,
            n_jobs=None,
            sampling=False,
            subset_size=20,
            random_state=None,
    ):
        super().__init__(contamination=contamination)
        self.n_components = n_components
        self.n_selected_components = n_selected_components
        self.kernel = kernel
        self.gamma = gamma
        self.degree = degree
        self.coef0 = coef0
        self.kernel_params = kernel_params
        self.alpha = alpha
        self.eigen_solver = eigen_solver
        self.tol = tol
        self.max_iter = max_iter
        self.remove_zero_eig = remove_zero_eig
        self.copy_X = copy_X
        self.n_jobs = n_jobs
        self.sampling = sampling
        self.subset_size = subset_size
        self.random_state = check_random_state(random_state)
        self.decision_scores_ = None
        self.n_selected_components_ = None

    def_check_subset_size(self, array):
"""Check subset size."""
        n_samples, _ = array.shape
        if isinstance(self.subset_size, int) is True:
            if 0 < self.subset_size <= n_samples:
                subset_size = self.subset_size
            else:
                raise ValueError(
                    f"subset_size={self.subset_size} "
                    f"must be between 0 and n_samples={n_samples}."
                )

        if isinstance(self.subset_size, float) is True:
            if 0.0 < self.subset_size <= 1.0:
                subset_size = int(self.subset_size * n_samples)
            else:
                raise ValueError("subset_size=%r must be between 0.0 and 1.0")

        return subset_size



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X, copy=self.copy_X)
        self._set_n_classes(y)

        ### perform subsampling to reduce time complexity
        if self.sampling is True:
            subset_size = self._check_subset_size(X)
            random_indices = self.random_state.choice(
                X.shape[0],
                size=subset_size,
                replace=False,
            )
            X = X[random_indices, :]

        ### copy the attributes from the sklearn Kernel PCA object
        if self.n_components is None:
            n_components = X.shape[0]  ### use all dimensions
        else:
            if self.n_components < 1:
                raise ValueError(
                    f"`n_components` should be >= 1, got: {self.n_components}"
                )
            n_components = min(X.shape[0], self.n_components)

        ### validate the number of components to be used for outlier detection
        if self.n_selected_components is None:
            self.n_selected_components_ = n_components
        else:
            self.n_selected_components_ = self.n_selected_components
        check_parameter(
            self.n_selected_components_,
            1,
            n_components,
            include_left=True,
            include_right=True,
            param_name="n_selected_components",
        )

        self.kpca = PyODKernelPCA(
            n_components=self.n_components,
            kernel=self.kernel,
            gamma=self.gamma,
            degree=self.degree,
            coef0=self.coef0,
            kernel_params=self.kernel_params,
            alpha=self.alpha,
            fit_inverse_transform=False,
            eigen_solver=self.eigen_solver,
            tol=self.tol,
            max_iter=self.max_iter,
            remove_zero_eig=self.remove_zero_eig,
            copy_X=self.copy_X,
            n_jobs=self.n_jobs,
            random_state=self.random_state,
        )
        x_transformed = self.kpca.fit_transform(X)
        x_transformed = x_transformed[:, : self.n_selected_components_]

        centerer = self.kpca.get_centerer
        kernel = self.kpca.get_kernel

        potential = []
        for i in range(X.shape[0]):
            sample = X[i, :].reshape(1, -1)
            potential.append(kernel(sample))
        potential = np.array(potential).squeeze()
        potential = potential - 2 * centerer.K_fit_rows_ + centerer.K_fit_all_

        ### reconstruction error
        self.decision_scores_ = potential - np.sum(np.square(x_transformed),
                                                   axis=1)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.kpca.KPCA.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])
        X = check_array(X)

        ### Compute centered gram matrix between X and training data X_fit_
        centerer = self.kpca.get_centerer
        kernel = self.kpca.get_kernel
        gram_matrix = kernel(X, self.kpca.X_fit_)

        x_transformed = self.kpca.transform(X)
        x_transformed = x_transformed[:, : self.n_selected_components_]

        potential = []
        for i in range(X.shape[0]):
            sample = X[i, :].reshape(1, -1)
            potential.append(kernel(sample))
        potential = np.array(potential).squeeze()
        gram_fit_rows = np.sum(gram_matrix, axis=1) / gram_matrix.shape[1]
        potential = potential - 2 * gram_fit_rows + centerer.K_fit_all_

        ### reconstruction error
        anomaly_scores = potential - np.sum(np.square(x_transformed), axis=1)

        return anomaly_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 40. pyod.models.lmdd - pyod 2.0.5 documentation {#40-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:49

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lmdd.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.lmdd
```
### -*- coding: utf-8 -*-
"""Linear Model Deviation-base outlier detection (LMDD).
"""
### Author: Yahya Almardeny <almardeny@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromnumbaimport njit
fromscipyimport stats
fromsklearn.utilsimport check_array, check_random_state

frompyod.utilsimport check_parameter
from.baseimport BaseDetector


@njit
def_aad(X):
"""Internal Function to Calculate Average Absolute Deviation
    (a.k.a Mean Absolute Deviation)
    """
    return np.mean(np.absolute(X - np.mean(X)))


def_check_params(n_iter, dis_measure, random_state):
"""Internal function to check for and validate class parameters.
    Also, to return random state instance and the appropriate dissimilarity
    measure if valid.
    """
    if isinstance(n_iter, int):
        check_parameter(n_iter, low=1, param_name='n_iter')
    else:
        raise TypeError("n_iter should be int, got %s" % n_iter)

    if isinstance(dis_measure, str):
        if dis_measure not in ('aad', 'var', 'iqr'):
            raise ValueError("Unknown dissimilarity measure type, "
                             "dis_measure should be in "
                             "(\'aad\', \'var\', \'iqr\'), "
                             "got %s" % dis_measure)
        ### TO-DO: 'mad': Median Absolute Deviation to be added
        ### once Scipy stats version 1.3.0 is released
    else:
        raise TypeError("dis_measure should be str, got %s" % dis_measure)

    return check_random_state(random_state), _aad if dis_measure == 'aad' \
        else (np.var if dis_measure == 'var'
              else (stats.iqr if dis_measure == 'iqr' else None))




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD)
classLMDD(BaseDetector):
"""Linear Method for Deviation-based Outlier Detection.

    LMDD employs the concept of the smoothing factor which
    indicates how much the dissimilarity can be reduced by
    removing a subset of elements from the data-set.
    Read more in the :cite:`arning1996linear`.

    Note: this implementation has minor modification to make it output scores
    instead of labels.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_iter : int, optional (default=50)
        Number of iterations where in each iteration,
        the process is repeated after randomizing the order of the input.
        Note that n_iter is a very important factor that affects the accuracy.
        The higher the better the accuracy and the longer the execution.

    dis_measure: str, optional (default='aad')
        Dissimilarity measure to be used in calculating the smoothing factor
        for points, options available:

        - 'aad': Average Absolute Deviation
        - 'var': Variance
        - 'iqr': Interquartile Range

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_iter=50, dis_measure='aad',
                 random_state=None):
        super(LMDD, self).__init__(contamination=contamination)
        self.n_iter, self.n_iter_ = n_iter, n_iter
        self.dis_measure, self.dis_measure_ = dis_measure, dis_measure

        ### add this assignment to prevent clone error; not being used.
        self.random_state = random_state
        self.random_state_, self.dis_measure_ = _check_params(n_iter,
                                                              dis_measure,
                                                              random_state)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        return self.__sf(X)




    def__dis(self, X):
"""
        Internal function to calculate for
        dissimilarity in a sequence of sets.
        """
        res_ = np.zeros(shape=(X.shape[0],))
        var_max, j = -np.inf, 0
        ### this can be vectorized but just for comforting memory
        for i in range(1, X.shape[0]):
            _var = self.dis_measure_(X[:i + 1]) - self.dis_measure_(X[:i])
            if _var > var_max:
                var_max = _var
                j = i

        if var_max > res_[j]:
            res_[j] = var_max

            for k in range(j + 1, X.shape[0]):
                dk_diff = self.dis_measure_(np.vstack((X[:j], X[k]))) \
                          - self.dis_measure_(np.vstack((X[:j + 1], X[k])))
                if dk_diff >= 0:
                    res_[k] = dk_diff + var_max

        return res_

    def__sf(self, X):
"""Internal function to calculate for Smoothing Factors of data points
        Repeated n_iter_ of times in randomized mode.
        """
        dis_ = np.zeros(shape=(X.shape[0],))
        card_ = np.zeros(shape=(X.shape[0],))
        ### perform one process with the original input order
        itr_res = self.__dis(X)
        np.put(card_, X.shape[0] - sum([i > 0. for i in itr_res]),
               np.where(itr_res > 0.))

        ### create a copy of random state to preserve original state for
        ### future fits (if any)
        random_state = np.random.RandomState(
            seed=self.random_state_.get_state()[1][0])
        indices = np.arange(X.shape[0])
        for _ in range(self.n_iter_):
            ind_ = indices
            random_state.shuffle(ind_)
            _x = X[indices]
            ### get dissimilarity of this iteration and restore original order
            itr_res = self.__dis(_x)[np.argsort(ind_)]
            current_card = X.shape[0] - sum([i > 0. for i in itr_res])
            ### compare with previous iteration to get the maximal dissimilarity
            for i, j in enumerate(itr_res):
                if j > dis_[i]:
                    dis_[i] = j
                    card_[i] = current_card
            ### Increase random state seed by one to reorder input next iteration
            random_state.seed(random_state.get_state()[1][0] + 1)

        return np.multiply(dis_, card_)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 41. pyod.models.loci - pyod 2.0.5 documentation {#41-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:56

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loci.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.loci
```
### -*- coding: utf-8 -*-
"""Local Correlation Integral (LOCI).
Part of the codes are adapted from https://github.com/Cloudy10/loci
"""
### Author: Winston Li <jk_zhengli@hotmail.com>
### License: BSD 2 clause


importnumpyasnp
fromnumbaimport njit
fromscipy.spatial.distanceimport pdist, squareform
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


@njit
def_get_critical_values(dist_matrix, alpha, p_ix, r_max,
                         r_min=0):  ### pragma: no cover
"""Computes the critical values of a given distance matrix.

    Parameters
    ----------
    dist_matrix : array-like, shape (n_samples, n_features)
        The distance matrix w.r.t. to the training samples.

    p_ix : int
        Subsetting index

    alpha : int, default = 0.5
        The neighbourhood parameter measures how large of a neighbourhood
        should be considered "local".

    r_max : int
        Maximum neighbourhood radius

    r_min : int, default = 0
        Minimum neighbourhood radius

    Returns
    -------
    cv : array, shape (n_critical_val, )
        Returns a list of critical values.
    """

    distances = dist_matrix[p_ix, :]
    mask = (r_min < distances) & (distances <= r_max)
    cv = np.sort(
        np.concatenate((distances[mask], distances[mask] / alpha)))
    return cv


@njit
def_get_sampling_N(dist_matrix, p_ix, r):  ### pragma: no cover
"""Computes the set of r-neighbours.

    Parameters
    ----------
    dist_matrix : array-like, shape (n_samples, n_features)
        The distance matrix w.r.t. to the training samples.

    p_ix : int
        Subsetting index

    r : int
        Neighbourhood radius


    Returns
    -------
    sample : array, shape (n_sample, )
        Returns a list of neighbourhood data points.
    """

    p_distances = dist_matrix[p_ix, :]
    sample = np.nonzero(p_distances <= r)[0]
    return sample




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI)
classLOCI(BaseDetector):
"""Local Correlation Integral.
    LOCI is highly effective for detecting outliers and groups of 
    outliers ( a.k.a.micro-clusters), which offers the following advantages 
    and novelties: (a) It provides an automatic, data-dictated cut-off to 
    determine whether a point is an outlier‚Äîin contrast, previous methods 
    force users to pick cut-offs, without any hints as to what cut-off value 
    is best for a given dataset. (b) It can provide a LOCI plot for each 
    point; this plot summarizes a wealth of information about the data in 
    the vicinity of the point, determining clusters, micro-clusters, their 
    diameters and their inter-cluster distances. None of the existing 
    outlier-detection methods can match this feature, because they output 
    only a single number for each point: its outlierness score.(c) It can 
    be computed as quickly as the best previous methods
    Read more in the :cite:`papadimitriou2003loci`.
    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1) 
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.
    alpha : int, default = 0.5
        The neighbourhood parameter measures how large of a neighbourhood
        should be considered "local".
    k: int, default = 3
        An outlier cutoff threshold for determine whether or not a point 
        should be considered an outlier.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    Examples
    --------
    >>> from pyod.models.loci import LOCI
    >>> from pyod.utils.data import generate_data
    >>> n_train = 50
    >>> n_test = 50
    >>> contamination = 0.1
    >>> X_train, y_train, X_test, y_test = generate_data(
    ...     n_train=n_train, n_test=n_test,
    ...     contamination=contamination, random_state=42)
    >>> clf = LOCI()
    >>> clf.fit(X_train)
    LOCI(alpha=0.5, contamination=0.1, k=None)
    """

    def__init__(self, contamination=0.1, alpha=0.5, k=3):
        super(LOCI, self).__init__(contamination=contamination)
        self.alpha = alpha
        self.threshold_ = k

    def_get_alpha_n(self, dist_matrix, indices, r):
"""Computes the alpha neighbourhood points.

        Parameters
        ----------
        dist_matrix : array-like, shape (n_samples, n_features)
            The distance matrix w.r.t. to the training samples.

        indices : int
            Subsetting index

        r : int
            Neighbourhood radius

        Returns
        -------
        alpha_n : array, shape (n_alpha, )
            Returns the alpha neighbourhood points.
        """

        if type(indices) is int:
            alpha_n = np.count_nonzero(
                dist_matrix[indices, :] < (r * self.alpha))
            return alpha_n
        else:
            alpha_n = np.count_nonzero(
                dist_matrix[indices, :] < (r * self.alpha), axis=1)
            return alpha_n

    def_calculate_decision_score(self, X):
"""Computes the outlier scores.
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input data points.
        Returns
        -------
        outlier_scores : list
            Returns the list of outlier scores for input dataset.       
        """
        outlier_scores = [0] * X.shape[0]
        dist_matrix = squareform(pdist(X, metric="euclidean"))
        max_dist = dist_matrix.max()
        r_max = max_dist / self.alpha

        for p_ix in range(X.shape[0]):
            critical_values = _get_critical_values(dist_matrix, self.alpha,
                                                   p_ix, r_max)
            for r in critical_values:
                n_values = self._get_alpha_n(dist_matrix,
                                             _get_sampling_N(dist_matrix,
                                                             p_ix, r), r)
                cur_alpha_n = self._get_alpha_n(dist_matrix, p_ix, r)
                n_hat = np.mean(n_values)
                mdef = 1 - (cur_alpha_n / n_hat)
                sigma_mdef = np.std(n_values) / n_hat
                if n_hat >= 20:
                    outlier_scores[p_ix] = mdef / sigma_mdef
                    if mdef > (self.threshold_ * sigma_mdef):
                        break
        return np.asarray(outlier_scores)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.fit)
    deffit(self, X, y=None):
"""Fit the model using X as training data.
        Parameters
        ----------
        X : array, shape (n_samples, n_features)
            Training data.

        y : Ignored
            Not used, present for API consistency by convention.
        Returns
        -------
        self : object

        """
        X = check_array(X)
        self._set_n_classes(y)
        self.decision_scores_ = self._calculate_decision_score(X)
        self.labels_ = (self.decision_scores_ > self.threshold_).astype(
            'int').ravel()

        ### calculate for predict_proba()

        self._mu = np.mean(self.decision_scores_)
        self._sigma = np.std(self.decision_scores_)
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loci.LOCI.decision_function)
    defdecision_function(self, X):
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        X = check_array(X)
        return self._calculate_decision_score(X)






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 42. pyod.models.loda - pyod 2.0.5 documentation {#42-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:00

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/loda.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.loda
```
### -*- coding: utf-8 -*-
"""Loda: Lightweight on-line detector of anomalies
Adapted from tilitools (https://github.com/nicococo/tilitools) by
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumbers

importnumpyasnp
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport get_optimal_n_bins




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA)
classLODA(BaseDetector):
"""Loda: Lightweight on-line detector of anomalies. See
    :cite:`pevny2016loda` for more information.
    Two versions of LODA are supported:        
    - Static number of bins: uses a static number of bins for all random cuts.
    - Automatic number of bins: every random cut uses a number of bins deemed 
      to be optimal according to the Birge-Rozenblac method
      (:cite:`birge2006many`).

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_bins : int or string, optional (default = 10)
        The number of bins for the histogram. If set to "auto", the 
        Birge-Rozenblac method will be used to automatically determine the 
        optimal number of bins.

    n_random_cuts : int, optional (default = 100)
        The number of random cuts.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_bins=10, n_random_cuts=100):
        super(LODA, self).__init__(contamination=contamination)
        self.n_bins = n_bins
        self.n_random_cuts = n_random_cuts
        self.weights = np.ones(n_random_cuts, dtype=float) / n_random_cuts



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)
        pred_scores = np.zeros([X.shape[0], 1])

        n_components = X.shape[1]
        n_nonzero_components = np.sqrt(n_components)
        n_zero_components = n_components - int(n_nonzero_components)

        self.projections_ = np.random.randn(self.n_random_cuts, n_components)

        ### If set to auto: determine optimal n_bins using Birge Rozenblac method
        if isinstance(self.n_bins, str) and self.n_bins.lower() == "auto":

            self.histograms_ = []
            self.limits_ = []
            self.n_bins_ = []  ### only used when n_bins is determined by method "auto"

            for i in range(self.n_random_cuts):
                rands = np.random.permutation(n_components)[:n_zero_components]
                self.projections_[i, rands] = 0.
                projected_data = self.projections_[i, :].dot(X.T)

                n_bins = get_optimal_n_bins(projected_data)
                self.n_bins_.append(n_bins)

                histogram, limits = np.histogram(
                    projected_data, bins=n_bins, density=False)
                histogram = histogram.astype(np.float64)
                histogram += 1e-12
                histogram /= np.sum(histogram)

                self.histograms_.append(histogram)
                self.limits_.append(limits)

                ### calculate the scores for the training samples
                inds = np.searchsorted(limits[:n_bins - 1],
                                       projected_data, side='left')
                pred_scores[:, 0] += -self.weights[i] * np.log(
                    histogram[inds])

        elif isinstance(self.n_bins, numbers.Integral):

            self.histograms_ = np.zeros((self.n_random_cuts, self.n_bins))
            self.limits_ = np.zeros((self.n_random_cuts, self.n_bins + 1))

            for i in range(self.n_random_cuts):
                rands = np.random.permutation(n_components)[:n_zero_components]
                self.projections_[i, rands] = 0.
                projected_data = self.projections_[i, :].dot(X.T)
                self.histograms_[i, :], self.limits_[i, :] = np.histogram(
                    projected_data, bins=self.n_bins, density=False)
                self.histograms_[i, :] += 1e-12
                self.histograms_[i, :] /= np.sum(self.histograms_[i, :])

                ### calculate the scores for the training samples
                inds = np.searchsorted(self.limits_[i, :self.n_bins - 1],
                                       projected_data, side='left')
                pred_scores[:, 0] += -self.weights[i] * np.log(
                    self.histograms_[i, inds])

        else:
            raise ValueError("n_bins must be an int or \'auto\', "
                             "got: %f" % self.n_bins)

        self.decision_scores_ = (pred_scores / self.n_random_cuts).ravel()
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.loda.LODA.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['projections_', 'decision_scores_',
                               'threshold_', 'labels_'])

        X = check_array(X)
        pred_scores = np.zeros([X.shape[0], 1])

        if isinstance(self.n_bins, str) and self.n_bins.lower() == "auto":

            for i in range(self.n_random_cuts):
                projected_data = self.projections_[i, :].dot(X.T)

                inds = np.searchsorted(self.limits_[i][:self.n_bins_[i] - 1],
                                       projected_data, side='left')
                pred_scores[:, 0] += -self.weights[i] * np.log(
                    self.histograms_[i][inds])

        elif isinstance(self.n_bins, numbers.Integral):

            for i in range(self.n_random_cuts):
                projected_data = self.projections_[i, :].dot(X.T)

                inds = np.searchsorted(self.limits_[i, :self.n_bins - 1],
                                       projected_data, side='left')
                pred_scores[:, 0] += -self.weights[i] * np.log(
                    self.histograms_[i, inds])
        else:
            raise ValueError("n_bins must be an int or \'auto\', "
                             "got: %f" % self.n_bins)

        pred_scores /= self.n_random_cuts
        return pred_scores.ravel()






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 43. pyod.models.lof - pyod 2.0.5 documentation {#43-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:49

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.lof
```
### -*- coding: utf-8 -*-
"""Local Outlier Factor (LOF). Implemented on scikit-learn library.
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


fromsklearn.neighborsimport LocalOutlierFactor
fromsklearn.utils.validationimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport invert_order


### noinspection PyProtectedMember




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF)
classLOF(BaseDetector):
"""Wrapper of scikit-learn LOF Class with more functionalities.
    Unsupervised Outlier Detection using Local Outlier Factor (LOF).

    The anomaly score of each sample is called Local Outlier Factor.
    It measures the local deviation of density of a given sample with
    respect to its neighbors.
    It is local in that the anomaly score depends on how isolated the object
    is with respect to the surrounding neighborhood.
    More precisely, locality is given by k-nearest neighbors, whose distance
    is used to estimate the local density.
    By comparing the local density of a sample to the local densities of
    its neighbors, one can identify samples that have a substantially lower
    density than their neighbors. These are considered outliers.
    See :cite:`breunig2000lof` for details.

    Parameters
    ----------
    n_neighbors : int, optional (default=20)
        Number of neighbors to use by default for `kneighbors` queries.
        If n_neighbors is larger than the number of samples provided,
        all samples will be used.

    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
        Algorithm used to compute the nearest neighbors:

        - 'ball_tree' will use BallTree
        - 'kd_tree' will use KDTree
        - 'brute' will use a brute-force search.
        - 'auto' will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.

    leaf_size : int, optional (default=30)
        Leaf size passed to `BallTree` or `KDTree`. This can
        affect the speed of the construction and query, as well as the memory
        required to store the tree. The optimal value depends on the
        nature of the problem.

    metric : string or callable, default 'minkowski'
        metric used for the distance computation. Any metric from scikit-learn
        or scipy.spatial.distance can be used.

        If 'precomputed', the training input X is expected to be a distance
        matrix.

        If metric is a callable function, it is called on each
        pair of instances (rows) and the resulting value recorded. The callable
        should take two arrays as input and return one value indicating the
        distance between them. This works for Scipy's metrics, but is less
        efficient than passing the metric name as a string.

        Valid values for metric are:

        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
          'manhattan']

        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',
          'sqeuclidean', 'yule']

        See the documentation for scipy.spatial.distance for details on these
        metrics:
        http://docs.scipy.org/doc/scipy/reference/spatial.distance.html

    p : integer, optional (default = 2)
        Parameter for the Minkowski metric from
        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
        equivalent to using manhattan_distance (l1), and euclidean_distance
        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
        See http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e. the proportion
        of outliers in the data set. When fitting this is used to define the
        threshold on the decision function.

    n_jobs : int, optional (default = 1)
        The number of parallel jobs to run for neighbors search.
        If ``-1``, then the number of jobs is set to the number of CPU cores.
        Affects only kneighbors and kneighbors_graph methods.

    novelty : bool (default=False)
        By default, LocalOutlierFactor is only meant to be used for outlier
        detection (novelty=False). Set novelty to True if you want to use
        LocalOutlierFactor for novelty detection. In this case be aware that
        that you should only use predict, decision_function and score_samples
        on new unseen data and not on the training set.

    Attributes
    ----------
    n_neighbors_ : int
        The actual number of neighbors used for `kneighbors` queries.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,
                 metric='minkowski', p=2, metric_params=None,
                 contamination=0.1, n_jobs=1, novelty=True):
        super(LOF, self).__init__(contamination=contamination)
        self.n_neighbors = n_neighbors
        self.algorithm = algorithm
        self.leaf_size = leaf_size
        self.metric = metric
        self.p = p
        self.metric_params = metric_params
        self.n_jobs = n_jobs
        self.novelty = novelty

    ### noinspection PyIncorrectDocstring


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X, accept_sparse=True)
        self._set_n_classes(y)

        self.detector_ = LocalOutlierFactor(n_neighbors=self.n_neighbors,
                                            algorithm=self.algorithm,
                                            leaf_size=self.leaf_size,
                                            metric=self.metric,
                                            p=self.p,
                                            metric_params=self.metric_params,
                                            contamination=self.contamination,
                                            n_jobs=self.n_jobs,
                                            novelty=self.novelty)
        self.detector_.fit(X=X, y=y)

        ### Invert decision_scores_. Outliers comes with higher outlier scores
        self.decision_scores_ = invert_order(
            self.detector_.negative_outlier_factor_)
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lof.LOF.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])

        ### Invert outlier scores. Outliers comes with higher outlier scores
        ### noinspection PyProtectedMember
        try:
            return invert_order(self.detector_._score_samples(X))
        except AttributeError:
            try:
                return invert_order(self.detector_._decision_function(X))
            except AttributeError:
                return invert_order(self.detector_.score_samples(X))




    @property
    defn_neighbors_(self):
"""The actual number of neighbors used for kneighbors queries.
        Decorator for scikit-learn LOF attributes.
        """
        return self.detector_.n_neighbors_




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 44. pyod.models.lscp - pyod 2.0.5 documentation {#44-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:53

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lscp.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.lscp
```
"""Locally Selective Combination of Parallel Outlier Ensembles (LSCP).
Adapted from the original implementation.
"""
### Author: Zain Nasrullah <zain.nasrullah.zn@gmail.com>
### License: BSD 2 clause


importcollections
importwarnings

importnumpyasnp
fromsklearn.neighborsimport KDTree
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted
fromsklearn.utils.validationimport check_random_state

### PyOD imports
from.baseimport BaseDetector
from..utils.stat_modelsimport pearsonr
from..utils.utilityimport argmaxn
from..utils.utilityimport check_detector
from..utils.utilityimport generate_bagging_indices
from..utils.utilityimport standardizer


### TODO: find random state that is causing runtime warning in pearson



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP)
classLSCP(BaseDetector):
""" Locally Selection Combination in Parallel Outlier Ensembles

    LSCP is an unsupervised parallel outlier detection ensemble which selects
    competent detectors in the local region of a test instance. This
    implementation uses an Average of Maximum strategy. First, a heterogeneous
    list of base detectors is fit to the training data and then generates a
    pseudo ground truth for each train instance is generated by
    taking the maximum outlier score.

    For each test instance:
    1) The local region is defined to be the set of nearest training points in
    randomly sampled feature subspaces which occur more frequently than
    a defined threshold over multiple iterations.

    2) Using the local region, a local pseudo ground truth is defined and the
    pearson correlation is calculated between each base detector's training
    outlier scores and the pseudo ground truth.

    3) A histogram is built out of pearson correlation scores; detectors in
    the largest bin are selected as competent base detectors for the given
    test instance.

    4) The average outlier score of the selected competent detectors is taken
    to be the final score.

    See :cite:`zhao2019lscp` for details.

    Parameters
    ----------
    detector_list : List, length must be greater than 1
        Base unsupervised outlier detectors from PyOD. (Note: requires fit and
        decision_function methods)

    local_region_size : int, optional (default=30)
        Number of training points to consider in each iteration of the local
        region generation process (30 by default).

    local_max_features : float in (0.5, 1.), optional (default=1.0)
        Maximum proportion of number of features to consider when defining the
        local region (1.0 by default).

    n_bins : int, optional (default=10)
        Number of bins to use when selecting the local region

    random_state : RandomState, optional (default=None)
        A random number generator instance to define the state of the random
        permutations generator.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function (0.1 by default).

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    Examples
    --------
    >>> from pyod.utils.data import generate_data
    >>> from pyod.utils.utility import standardizer
    >>> from pyod.models.lscp import LSCP
    >>> from pyod.models.lof import LOF
    >>> X_train, y_train, X_test, y_test = generate_data(
    ...     n_train=50, n_test=50,
    ...     contamination=0.1, random_state=42)
    >>> X_train, X_test = standardizer(X_train, X_test)
    >>> detector_list = [LOF(), LOF()]
    >>> clf = LSCP(detector_list)
    >>> clf.fit(X_train)
    LSCP(...)
    """

    def__init__(self, detector_list, local_region_size=30,
                 local_max_features=1.0, n_bins=10,
                 random_state=None, contamination=0.1):
        super(LSCP, self).__init__(contamination=contamination)
        self.detector_list = detector_list
        self.n_clf = len(self.detector_list)
        self.local_region_size = local_region_size
        self.local_region_min = 30
        self.local_region_max = 200
        self.local_max_features = local_max_features
        self.local_min_features = 0.5
        self.local_region_iterations = 20
        self.local_region_threshold = int(self.local_region_iterations / 2)
        self.n_bins = n_bins
        self.n_selected = 1
        self.random_state = random_state



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### check detector_list
        if len(self.detector_list) < 2:
            raise ValueError("The detector list has less than 2 detectors.")

        for detector in self.detector_list:
            check_detector(detector)

        ### check random state and input
        self.random_state = check_random_state(self.random_state)
        X = check_array(X)
        self._set_n_classes(y)
        self.n_features_ = X.shape[1]

        ### normalize input data
        self.X_train_norm_ = X
        train_scores = np.zeros([self.X_train_norm_.shape[0], self.n_clf])

        ### fit each base detector and calculate standardized train scores
        for k, detector in enumerate(self.detector_list):
            detector.fit(self.X_train_norm_)
            train_scores[:, k] = detector.decision_scores_
        self.train_scores_ = train_scores

        ### set decision scores and threshold
        self.decision_scores_ = self._get_decision_scores(X)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lscp.LSCP.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        ### check whether model has been fit
        check_is_fitted(self, ['training_pseudo_label_', 'train_scores_',
                               'X_train_norm_', 'n_features_'])

        ### check input array
        X = check_array(X)
        if self.n_features_ != X.shape[1]:
            raise ValueError("Number of features of the model must "
                             "match the input. Model n_features is {0} and "
                             "input n_features is {1}."
                             "".format(self.n_features_, X.shape[1]))

        ### get decision scores and return
        decision_scores = self._get_decision_scores(X)
        return decision_scores




    def_get_decision_scores(self, X):
""" Helper function for getting outlier scores on test data X (note:
        model must already be fit)

        Parameters
        ----------
        X : numpy array, shape (n_samples, n_features)
            Test data

        Returns
        -------
        pred_scores_ens : numpy array, shape (n_samples,)
            Outlier scores for test samples
        """

        ### raise warning if local region size is outside acceptable limits
        if (self.local_region_size < self.local_region_min) or (
                self.local_region_size > self.local_region_max):
            warnings.warn("Local region size of {} is outside "
                          "recommended range [{}, {}]".format(
                self.local_region_size, self.local_region_min,
                self.local_region_max))

        ### standardize test data and get local region for each test instance
        X_test_norm = X
        test_local_regions = self._get_local_region(X_test_norm)

        ### calculate test scores
        test_scores = np.zeros([X_test_norm.shape[0], self.n_clf])
        for k, detector in enumerate(self.detector_list):
            test_scores[:, k] = detector.decision_function(X_test_norm)

        ### generate standardized scores
        train_scores_norm, test_scores_norm = standardizer(self.train_scores_,
                                                           test_scores)

        ### generate pseudo target for training --> for calculating weights
        self.training_pseudo_label_ = np.max(train_scores_norm,
                                             axis=1).reshape(-1, 1)

        ### placeholder for ensemble predictions
        pred_scores_ens = np.zeros([X_test_norm.shape[0], ])

        ### iterate through test instances (test_local_regions
        ### indices correspond to x_test)
        for i, test_local_region in enumerate(test_local_regions):

            ### get pseudo target and training scores in local region of
            ### test instance
            local_pseudo_ground_truth = self.training_pseudo_label_[
                test_local_region,].ravel()
            local_train_scores = train_scores_norm[test_local_region, :]

            ### calculate pearson correlation between local pseudo ground truth
            ### and local train scores
            pearson_corr_scores = np.zeros([self.n_clf, ])
            for d in range(self.n_clf):
                pearson_corr_scores[d,] = pearsonr(
                    local_pseudo_ground_truth, local_train_scores[:, d])[0]

            ### return best score
            pred_scores_ens[i,] = np.mean(
                test_scores_norm[
                    i, self._get_competent_detectors(pearson_corr_scores)])

        return pred_scores_ens

    def_get_local_region(self, X_test_norm):
""" Get local region for each test instance

        Parameters
        ----------
        X_test_norm : numpy array, shape (n_samples, n_features)
            Normalized test data

        Returns
        -------
        final_local_region_list : List of lists, shape of [n_samples, [local_region]]
            Indices of training samples in the local region of each test sample
        """

        ### Initialize the local region list
        local_region_list = [[]] * X_test_norm.shape[0]

        if self.local_max_features > 1.0:
            warnings.warn(
                "Local max features greater than 1.0, reducing to 1.0")
            self.local_max_features = 1.0

        if self.X_train_norm_.shape[1] * self.local_min_features < 1:
            warnings.warn(
                "Local min features smaller than 1, increasing to 1.0")
            self.local_min_features = 1.0

        ### perform multiple iterations
        for _ in range(self.local_region_iterations):

            ### if min and max are the same, then use all features
            if self.local_max_features == self.local_min_features:
                features = range(0, self.X_train_norm_.shape[1])
                warnings.warn("Local min features equals local max features; "
                              "use all features instead.")

            else:
                ### randomly generate feature subspaces
                features = generate_bagging_indices(
                    self.random_state,
                    bootstrap_features=False,
                    n_features=self.X_train_norm_.shape[1],
                    min_features=int(
                        self.X_train_norm_.shape[1] * self.local_min_features),
                    max_features=int(
                        self.X_train_norm_.shape[1] * self.local_max_features))

            ### build KDTree out of training subspace
            tree = KDTree(self.X_train_norm_[:, features])

            ### Find neighbors of each test instance
            _, ind_arr = tree.query(X_test_norm[:, features],
                                    k=self.local_region_size)

            ### add neighbors to local region list
            for j in range(X_test_norm.shape[0]):
                local_region_list[j] = local_region_list[j] + \
                                       ind_arr[j, :].tolist()

        ### keep nearby points which occur at least local_region_threshold times
        final_local_region_list = [[]] * X_test_norm.shape[0]
        for j in range(X_test_norm.shape[0]):
            tmp = [item for item, count in collections.Counter(
                local_region_list[j]).items() if
                   count > self.local_region_threshold]
            decrease_value = 0
            while len(tmp) < 2:
                decrease_value = decrease_value + 1
                assert decrease_value < self.local_region_threshold
                tmp = [item for item, count in
                       collections.Counter(local_region_list[j]).items() if
                       count > (self.local_region_threshold - decrease_value)]

            final_local_region_list[j] = tmp

        return final_local_region_list

    def_get_competent_detectors(self, scores):
""" Identifies competent base detectors based on correlation scores

        Parameters
        ----------
        scores : numpy array, shape (n_clf,)
            Correlation scores for each classifier (for a specific
            test instance)

        Returns
        -------
        candidates : List
            Indices for competent detectors (for given test instance)
        """

        ### create histogram of correlation scores
        scores = scores.reshape(-1, 1)

        ### TODO: handle when Pearson score is 0
        ### if scores contain nan, change it to 0
        if np.isnan(scores).any():
            scores = np.nan_to_num(scores)

        if self.n_bins > self.n_clf:
            warnings.warn(
                "The number of histogram bins is greater than the number of "
                "classifiers, reducing n_bins to n_clf.")
            self.n_bins = self.n_clf
        hist, bin_edges = np.histogram(scores, bins=self.n_bins)

        ### find n_selected largest bins
        max_bins = argmaxn(hist, n=self.n_selected)
        candidates = []

        ### iterate through bins
        for max_bin in max_bins:
            ### determine which detectors are inside this bin
            selected = np.where((scores >= bin_edges[max_bin])
                                & (scores <= bin_edges[max_bin + 1]))

            ### add to list of candidates
            candidates = candidates + selected[0].tolist()

        return candidates

    def__len__(self):
        return len(self.detector_list)

    def__getitem__(self, index):
        return self.detector_list[index]

    def__iter__(self):
        return iter(self.detector_list)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 45. pyod.models.lunar - pyod 2.0.5 documentation {#45-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:56

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lunar.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.lunar
```
### -*- coding: utf-8 -*-
"""LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks
"""
### Author: Adam Goodge <a.goodge@u.nus.edu>
#

fromcopyimport deepcopy

importnumpyasnp

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.nn.functionalasF
importtorch.optimasoptim

fromsklearn.metricsimport roc_auc_score
fromsklearn.model_selectionimport train_test_split
fromsklearn.neighborsimport NearestNeighbors
fromsklearn.preprocessingimport MinMaxScaler
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


### negative samples for training
defgenerate_negative_samples(x, sample_type, proportion, epsilon):
    n_samples = int(proportion * (len(x)))
    n_dim = x.shape[-1]

    ### uniform samples in range [x.min(),x.max()]
    rand_unif = x.min() + (x.max() - x.min()) * np.random.rand(n_samples,
                                                               n_dim).astype(
        'float32')
    ### subspace perturbation samples
    x_temp = x[np.random.choice(np.arange(len(x)), size=n_samples)]
    randmat = np.random.rand(n_samples, n_dim) < 0.3
    rand_sub = x_temp + randmat * (
            epsilon * np.random.randn(n_samples, n_dim)).astype('float32')

    if sample_type == 'UNIFORM':
        neg_x = rand_unif
    if sample_type == 'SUBSPACE':
        neg_x = rand_sub
    if sample_type == 'MIXED':
        ### randomly sample from uniform and gaussian negative samples
        neg_x = np.concatenate((rand_unif, rand_sub), 0)
        neg_x = neg_x[np.random.choice(np.arange(len(neg_x)), size=n_samples)]

    neg_y = np.ones(len(neg_x))

    return neg_x.astype('float32'), neg_y.astype('float32')


classSCORE_MODEL(nn.Module):
    def__init__(self, k):
        super(SCORE_MODEL, self).__init__()
        self.hidden_size = 256
        self.network = nn.Sequential(
            nn.Linear(k, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, 1),
            nn.Sigmoid()
        )

    defforward(self, x):
        out = self.network(x)
        out = torch.squeeze(out, 1)
        return out


classWEIGHT_MODEL(nn.Module):
    def__init__(self, k):
        super(WEIGHT_MODEL, self).__init__()
        self.hidden_size = 256
        self.network = nn.Sequential(
            nn.Linear(k, self.hidden_size),
            nn.ReLU(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.LayerNorm(self.hidden_size),
            nn.Linear(self.hidden_size, k),
        )
        self.final_norm = nn.BatchNorm1d(1)

    defforward(self, x):
        alpha = self.network(x)
        ### get weights > 0 and sum to 1.0
        alpha = F.softmax(alpha, dim=1)
        ### multiply weights by each distance in input vector
        out = torch.sum(alpha * x, dim=1, keepdim=True)
        ### batch norm
        out = self.final_norm(out)
        out = torch.squeeze(out, 1)
        return out




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR)
classLUNAR(BaseDetector):
"""
    LUNAR class for outlier detection. See https://www.aaai.org/AAAI22Papers/AAAI-51.GoodgeA.pdf for details.
    For an observation, its ordered list of distances to its k nearest neighbours is input to a neural network, 
    with one of the following outputs:

        1) SCORE_MODEL: network directly outputs the anomaly score.
        2) WEIGHT_MODEL: network outputs a set of weights for the k distances, the anomaly score is then the
            sum of weighted distances.

    See :cite:`goodge2022lunar` for details.

    Parameters
    ----------
    model_type: str in ['WEIGHT', 'SCORE'], optional (default = 'WEIGHT')
        Whether to use WEIGHT_MODEL or SCORE_MODEL for anomaly scoring.

    n_neighbors: int, optional (default = 5)
        Number of neighbors to use by default for k neighbors queries.

    negative_sampling: str in ['UNIFORM', 'SUBSPACE', MIXED'], optional (default = 'MIXED)
        Type of negative samples to use between:

        - 'UNIFORM': uniformly distributed samples
        - 'SUBSPACE': subspace perturbation (additive random noise in a subset of features)
        - 'MIXED': a combination of both types of samples

    val_size: float in [0,1], optional (default = 0.1)
        Proportion of samples to be used for model validation

    scaler: object in {StandardScaler(), MinMaxScaler(), optional (default = MinMaxScaler())
        Method of data normalization

    epsilon: float, optional (default = 0.1)
        Hyper-parameter for the generation of negative samples. 
        A smaller epsilon results in negative samples more similar to normal samples.

    proportion: float, optional (default = 1.0)
        Hyper-parameter for the proprotion of negative samples to use relative to the 
        number of normal training samples.

    n_epochs: int, optional (default = 200)
        Number of epochs to train neural network.

    lr: float, optional (default = 0.001)
        Learning rate.

    wd: float, optional (default = 0.1)
        Weight decay.
    verbose: int in {0,1}, optional (default = 0):
        To view or hide training progress

    Attributes
    ----------
    """

    def__init__(self, model_type="WEIGHT", n_neighbours=5,
                 negative_sampling="MIXED",
                 val_size=0.1, scaler=MinMaxScaler(), epsilon=0.1,
                 proportion=1.0,
                 n_epochs=200, lr=0.001, wd=0.1, verbose=0, contamination=0.1):
        super(LUNAR, self).__init__(contamination=contamination)

        self.model_type = model_type
        self.n_neighbours = n_neighbours
        self.negative_sampling = negative_sampling
        self.epsilon = epsilon
        self.proportion = proportion
        self.n_epochs = n_epochs
        self.scaler = scaler
        self.lr = lr
        self.wd = wd
        self.val_size = val_size
        self.verbose = verbose
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu')

        if model_type == "SCORE":
            self.network = SCORE_MODEL(n_neighbours).to(self.device)
        elif model_type == "WEIGHT":
            self.network = WEIGHT_MODEL(n_neighbours).to(self.device)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.fit)
    deffit(self, X, y=None):
"""Fit detector. y is assumed to be 0 for all training samples.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : Ignored
            Overwritten with 0 for all training samples (assumed to be normal).
        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### X = check_array(X)
        self._set_n_classes(y)
        X = X.astype('float32')
        y = np.zeros(len(X))

        ### split training and validation sets
        train_x, val_x, train_y, val_y = train_test_split(X, y,
                                                          test_size=self.val_size)

        ### fit data scaler to the training set if scaler has been passed
        if (self.scaler == None):
            pass
        else:
            self.scaler.fit(train_x)

        ### scale data if scaler has been passed
        if (self.scaler == None):
            pass
        else:
            train_x = self.scaler.transform(train_x)
            val_x = self.scaler.transform(val_x)

        ### generate negative samples for training and validation set seperately 
        neg_train_x, neg_train_y = generate_negative_samples(train_x,
                                                             self.negative_sampling,
                                                             self.proportion,
                                                             self.epsilon)
        neg_val_x, neg_val_y = generate_negative_samples(val_x,
                                                         self.negative_sampling,
                                                         self.proportion,
                                                         self.epsilon)

        train_x = np.vstack((train_x, neg_train_x))
        train_y = np.hstack((train_y, neg_train_y))
        val_x = np.vstack((val_x, neg_val_x))
        val_y = np.hstack((val_y, neg_val_y))

        self.neigh = NearestNeighbors(n_neighbors=self.n_neighbours + 1)
        self.neigh.fit(train_x)

        ### nearest neighbours of training set
        train_dist, _ = self.neigh.kneighbors(train_x[train_y == 0],
                                              n_neighbors=self.n_neighbours + 1)
        neg_train_dist, _ = self.neigh.kneighbors(train_x[train_y == 1],
                                                  n_neighbors=self.n_neighbours)
        ### remove self loops of normal training points
        train_dist = np.vstack((train_dist[:, 1:], neg_train_dist))
        ### nearest neighbours of validation set
        val_dist, _ = self.neigh.kneighbors(val_x,
                                            n_neighbors=self.n_neighbours)

        train_dist = torch.tensor(train_dist, dtype=torch.float32).to(
            self.device)
        train_y = torch.tensor(train_y, dtype=torch.float32).to(self.device)
        val_dist = torch.tensor(val_dist, dtype=torch.float32).to(self.device)
        val_y = torch.tensor(val_y, dtype=torch.float32).to(self.device)
        ### loss function
        criterion = nn.MSELoss(reduction='none')
        ### optimizer
        optimizer = optim.Adam(self.network.parameters(), lr=self.lr,
                               weight_decay=self.wd)
        ### for early stopping
        best_val_score = 0
        ### model training
        for epoch in range(self.n_epochs):

            ### see performance of model before epoch
            with torch.no_grad():

                self.network.eval()

                out = self.network(train_dist)
                train_score = roc_auc_score(train_y.cpu(), out.cpu())

                out = self.network(val_dist)
                val_score = roc_auc_score(val_y.cpu(), out.cpu())

                ### save best model
                if val_score >= best_val_score:
                    best_dict = {'epoch': epoch,
                                 'model_state_dict': deepcopy(
                                     self.network.state_dict()),
                                 'optimizer_state_dict': deepcopy(
                                     optimizer.state_dict()),
                                 'train_score': train_score,
                                 'val_score': val_score,
                                 }

                    ### reset current best score
                    best_val_score = val_score

                if self.verbose == 1:
                    print(
                        f"Epoch {epoch}\t Train Score {np.round(train_score,6)}\t Val Score {np.round(val_score,6)}")

            ### training loop
            self.network.train()
            optimizer.zero_grad()
            out = self.network(train_dist)
            loss = criterion(out, train_y).sum()
            loss.backward()
            optimizer.step()

        ### print best model after training
        if self.verbose == 1:
            print(
                f"Finished training...\nBest Model: Epoch {best_dict['epoch']}\t Train Score {best_dict['train_score']}\t Val Score {best_dict['val_score']}")
        ### load best model after training
        self.network.load_state_dict(best_dict['model_state_dict'])

        ### Determine outlier scores for train set
        ### scale data if scaler has been passed
        if (self.scaler == None):
            X_norm = np.copy(X)
        else:
            X_norm = self.scaler.transform(X)

        ### nearest neighbour search
        dist, _ = self.neigh.kneighbors(X_norm, self.n_neighbours)
        dist = torch.tensor(dist, dtype=torch.float32).to(self.device)
        ### forward pass
        with torch.no_grad():
            self.network.eval()
            anomaly_scores = self.network(dist)

        self.decision_scores_ = anomaly_scores.cpu().detach().numpy().ravel()
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lunar.LUNAR.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        For consistency, outliers are assigned with larger anomaly scores.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        check_is_fitted(self, ['decision_scores_'])
        ### X = check_array(X)
        X = X.astype('float32')

        ### scale data
        if (self.scaler == None):
            pass
        else:
            X = self.scaler.transform(X)

        ### nearest neighbour search
        dist, _ = self.neigh.kneighbors(X, self.n_neighbours)
        dist = torch.tensor(dist, dtype=torch.float32).to(self.device)
        ### forward pass
        with torch.no_grad():
            self.network.eval()
            anomaly_scores = self.network(dist)

        scores = anomaly_scores.cpu().detach().numpy().ravel()

        return scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 46. pyod.models.mad - pyod 2.0.5 documentation {#46-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:00

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mad.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.mad
```
### -*- coding: utf-8 -*-
"""
Median Absolute deviation (MAD) Algorithm.
Strictly for Univariate Data.
"""
### Author: Yahya Almardeny <almardeny@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromsklearn.utilsimport check_array

from.baseimport BaseDetector


def_check_dim(X):
"""
    Internal function to assert univariate data
    """
    if X.shape[1] != 1:
        raise ValueError('MAD algorithm is just for univariate data. '
                         'Got Data with {} Dimensions.'.format(X.shape[1]))




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD)
classMAD(BaseDetector):
"""Median Absolute Deviation: for measuring the distances between
    data points and the median in terms of median distance.
    See :cite:`iglewicz1993detect` for details.

    Parameters
    ----------
    threshold : float, optional (default=3.5)
       The modified z-score to use as a threshold. Observations with
       a modified z-score (based on the median absolute deviation) greater
       than this value will be classified as outliers.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
       The modified z-score to use as a threshold. Observations with
       a modified z-score (based on the median absolute deviation) greater
       than this value will be classified as outliers.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, threshold=3.5, contamination=0.1):
        super(MAD, self).__init__(contamination=contamination)
        if not isinstance(threshold, (float, int)):
            raise TypeError(
                'threshold must be a number. Got {}'.format(type(threshold)))
        self.threshold = threshold



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples. Note that `n_features` must equal 1.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X, ensure_2d=False, force_all_finite=False)
        _check_dim(X)
        self._set_n_classes(y)
        self.threshold_ = self.threshold
        self.median_ = None  ### reset median after each call
        self.median_diff_ = None  ### reset median_diff after each call
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mad.MAD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.
            Note that `n_features` must equal 1.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        X = check_array(X, ensure_2d=False, force_all_finite=False)
        _check_dim(X)
        return self._mad(X)




    def_mad(self, X):
"""
        Apply the robust median absolute deviation (MAD)
        to measure the distances of data points from the median.

        Returns
        -------
        numpy array containing modified Z-scores of the observations.
        The greater the score, the greater the outlierness.
        """
        obs = np.reshape(X, (-1, 1))
        ### `self.median` will be None only before `fit()` is called
        self.median_ = np.nanmedian(
            obs) if self.median_ is None else self.median_
        diff = np.abs(obs - self.median_)
        self.median_diff_ = np.nanmedian(
            diff) if self.median_diff_ is None else self.median_diff_
        return np.nan_to_num(np.ravel(0.6745 * diff / self.median_diff_))

    def_process_decision_scores(self):
"""This overrides PyOD base class function in order to use the
        proper `threshold_` which is quite different in the base class.
        Internal function to calculate key attributes:
        - labels_: binary labels of training data.
        - _mu: mean of decision scores.
        - _sigma: standard deviation of decision scores.

        Returns
        -------
        self
        """
        self.labels_ = (self.decision_scores_ > self.threshold).astype(
            'int').ravel()

        ### calculate for predict_proba()
        self._mu = np.nanmean(self.decision_scores_)
        self._sigma = np.nanstd(self.decision_scores_)

        return self




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 47. pyod.models.mcd - pyod 2.0.5 documentation {#47-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:56

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mcd.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.mcd
```
### -*- coding: utf-8 -*-
"""Outlier Detection with Minimum Covariance Determinant (MCD)
"""
### Author: Yue Zhao <zhaoy@cmu.edu>
### License: BSD 2 clause


fromsklearn.covarianceimport MinCovDet
fromsklearn.utils.validationimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector

__all__ = ['MCD']




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD)
classMCD(BaseDetector):
"""Detecting outliers in a Gaussian distributed dataset using
    Minimum Covariance Determinant (MCD): robust estimator of covariance.

    The Minimum Covariance Determinant covariance estimator is to be applied
    on Gaussian-distributed data, but could still be relevant on data
    drawn from a unimodal, symmetric distribution. It is not meant to be used
    with multi-modal data (the algorithm used to fit a MinCovDet object is
    likely to fail in such a case).
    One should consider projection pursuit methods to deal with multi-modal
    datasets.

    First fit a minimum covariance determinant model and then compute the
    Mahalanobis distance as the outlier degree of the data

    See :cite:`rousseeuw1999fast,hardin2004outlier` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    store_precision : bool
        Specify if the estimated precision is stored.

    assume_centered : bool
        If True, the support of the robust location and the covariance
        estimates is computed, and a covariance estimate is recomputed from
        it, without centering the data.
        Useful to work with data whose mean is significantly equal to
        zero but is not exactly zero.
        If False, the robust location and covariance are directly computed
        with the FastMCD algorithm without additional treatment.

    support_fraction : float, 0 < support_fraction < 1
        The proportion of points to be included in the support of the raw
        MCD estimate. Default is None, which implies that the minimum
        value of support_fraction will be used within the algorithm:
        [n_sample + n_features + 1] / 2

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Attributes
    ----------
    raw_location_ : array-like, shape (n_features,)
        The raw robust estimated location before correction and re-weighting.

    raw_covariance_ : array-like, shape (n_features, n_features)
        The raw robust estimated covariance before correction and re-weighting.

    raw_support_ : array-like, shape (n_samples,)
        A mask of the observations that have been used to compute
        the raw robust estimates of location and shape, before correction
        and re-weighting.

    location_ : array-like, shape (n_features,)
        Estimated robust location

    covariance_ : array-like, shape (n_features, n_features)
        Estimated robust covariance matrix

    precision_ : array-like, shape (n_features, n_features)
        Estimated pseudo inverse matrix.
        (stored only if store_precision is True)

    support_ : array-like, shape (n_samples,)
        A mask of the observations that have been used to compute
        the robust estimates of location and shape.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted. Mahalanobis distances of the training set (on which
        `:meth:`fit` is called) observations.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, store_precision=True,
                 assume_centered=False, support_fraction=None,
                 random_state=None):
        super(MCD, self).__init__(contamination=contamination)
        self.store_precision = store_precision
        self.assume_centered = assume_centered
        self.support_fraction = support_fraction
        self.random_state = random_state

    ### noinspection PyIncorrectDocstring


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### Validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.detector_ = MinCovDet(store_precision=self.store_precision,
                                   assume_centered=self.assume_centered,
                                   support_fraction=self.support_fraction,
                                   random_state=self.random_state)
        self.detector_.fit(X=X, y=y)

        ### Use mahalanabis distance as the outlier score
        self.decision_scores_ = self.detector_.dist_
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mcd.MCD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        X = check_array(X)

        ### Computer mahalanobis distance of the samples
        return self.detector_.mahalanobis(X)




    @property
    defraw_location_(self):
"""The raw robust estimated location before correction and
        re-weighting.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.raw_location_

    @property
    defraw_covariance_(self):
"""The raw robust estimated location before correction and
        re-weighting.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.raw_covariance_

    @property
    defraw_support_(self):
"""A mask of the observations that have been used to compute
        the raw robust estimates of location and shape, before correction
        and re-weighting.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.raw_support_

    @property
    deflocation_(self):
"""Estimated robust location.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.location_

    @property
    defcovariance_(self):
"""Estimated robust covariance matrix.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.covariance_

    @property
    defprecision_(self):
""" Estimated pseudo inverse matrix.
        (stored only if store_precision is True)

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.precision_

    @property
    defsupport_(self):
"""A mask of the observations that have been used to compute
        the robust estimates of location and shape.

        Decorator for scikit-learn MinCovDet attributes.
        """
        return self.detector_.support_




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 48. pyod.models.mo_gaal - pyod 2.0.5 documentation {#48-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:42

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/mo_gaal.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.mo_gaal
```
### -*- coding: utf-8 -*-
"""Multiple-Objective Generative Adversarial Active Learning.
Part of the codes are adapted from
https://github.com/leibinghe/GAAL-based-outlier-detection
"""
### Author: Zhuo Xiao <zhuoxiao@usc.edu>

fromcollectionsimport defaultdict

importnumpyasnp

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.optimasoptim
fromtorch.utils.dataimport DataLoader, TensorDataset

fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from.gaal_baseimport create_discriminator, create_generator




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL)
classMO_GAAL(BaseDetector):
"""Multi-Objective Generative Adversarial Active Learning.

    MO_GAAL directly generates informative potential outliers to assist the
    classifier in describing a boundary that can separate outliers from normal
    data effectively. Moreover, to prevent the generator from falling into the
    mode collapsing problem, the network structure of SO-GAAL is expanded from
    a single generator (SO-GAAL) to multiple generators with different
    objectives (MO-GAAL) to generate a reasonable reference distribution for
    the whole dataset.
    Read more in the :cite:`liu2019generative`.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    k : int, optional (default=10)
        The number of sub generators.

    stop_epochs : int, optional (default=20)
        The number of epochs of training. The number of total epochs equals to three times of stop_epochs.

    lr_d : float, optional (default=0.01)
        The learn rate of the discriminator.

    lr_g : float, optional (default=0.0001)
        The learn rate of the generator.

    momentum : float, optional (default=0.9)
        The momentum parameter for SGD.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, k=10, stop_epochs=20, lr_d=0.01, lr_g=0.0001,
                 momentum=0.9, contamination=0.1):
        super(MO_GAAL, self).__init__(contamination=contamination)
        self.k = k
        self.stop_epochs = stop_epochs
        self.lr_d = lr_d
        self.lr_g = lr_g
        self.momentum = momentum
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")
        self.train_history = defaultdict(list)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        X = check_array(X)
        self._set_n_classes(y)
        self.train_history = defaultdict(list)
        names = locals()
        epochs = self.stop_epochs * 3
        latent_size = X.shape[1]
        data_size = X.shape[0]
        ### Create discriminator
        self.discriminator = create_discriminator(latent_size, data_size).to(
            self.device)
        optimizer_d = optim.SGD(self.discriminator.parameters(), lr=self.lr_d,
                                momentum=self.momentum)
        criterion = nn.BCELoss()

        ### Create k generators
        for i in range(self.k):
            generator_name = 'sub_generator' + str(i)
            generator = create_generator(latent_size).to(self.device)
            names[generator_name] = generator

            ### Define the optimizer for the generator
            optimizer_name = 'optimizer_g' + str(i)
            optimizer_g = optim.SGD(generator.parameters(), lr=self.lr_g,
                                    momentum=self.momentum)
            names[optimizer_name] = optimizer_g

        dataloader = DataLoader(TensorDataset(
            torch.tensor(X, dtype=torch.float32).to(self.device)),
            batch_size=min(500, data_size),
            shuffle=True)

        stop = 0

        ### Start iteration
        for epoch in range(epochs):
            print('Epoch {} of {}'.format(epoch + 1, epochs))
            for batch_idx, data_batch in enumerate(dataloader):
                ### print(f'\nTesting for epoch {epoch + 1} index {batch_idx + 1}:')

                data_batch = data_batch[0].to(self.device)
                batch_size = data_batch.size(0)

                ### Generate noise
                noise = torch.rand(batch_size, latent_size, device=self.device)

                ### Generate potential outliers
                block = ((1 + self.k) * self.k) // 2
                for i in range(self.k):
                    if i != (self.k - 1):
                        noise_start = int(
                            (((self.k + (self.k - i + 1)) * i) / 2) * (
                                    batch_size // block))
                        noise_end = int(
                            (((self.k + (self.k - i)) * (i + 1)) / 2) * (
                                    batch_size // block))
                        names['noise' + str(i)] = noise[noise_start:noise_end]
                    else:
                        noise_start = int(
                            (((self.k + (self.k - i + 1)) * i) / 2) * (
                                    batch_size // block))
                        names['noise' + str(i)] = noise[noise_start:batch_size]

                    names['generated_data' + str(i)] = names[
                        'sub_generator' + str(i)](names['noise' + str(i)])

                ### Concatenate real data to generated data
                all_data = torch.cat(
                    [data_batch] + [names['generated_data' + str(i)] for i in
                                    range(self.k)], dim=0)
                labels = torch.cat(
                    [torch.ones(batch_size, 1, device=self.device),
                     torch.zeros(
                         sum([d.size(0) for d in
                              [names['generated_data' + str(i)] for i in
                               range(self.k)]]), 1, device=self.device)],
                    dim=0)

                ### Ensure outputs and labels are the same size
                assert all_data.size(0) == labels.size(
                    0), "Mismatch between all_data and labels sizes"

                ### Train discriminator
                self.discriminator.train()
                self.discriminator.zero_grad()
                outputs = self.discriminator(all_data)
                outputs = outputs.view(-1,
                                       1)  ### Ensure outputs shape matches labels shape
                discriminator_loss = criterion(outputs, labels)
                discriminator_loss.backward()
                optimizer_d.step()
                self.train_history['discriminator_loss'].append(
                    discriminator_loss.item())

                ### Get the target value of sub-generators
                with torch.no_grad():
                    pred_scores = self.discriminator(
                        torch.tensor(X, dtype=torch.float32,
                                     device=self.device)).cpu().numpy().ravel()

                for i in range(self.k):
                    names['T' + str(i)] = np.percentile(pred_scores,
                                                        i / self.k * 100)
                    names['trick' + str(i)] = torch.tensor(
                        [float(names['T' + str(i)])] * names[
                            'noise' + str(i)].size(0),
                        device=self.device).unsqueeze(1)

                ### Train generators
                if stop == 0:
                    for i in range(self.k):
                        names['optimizer_g' + str(i)].zero_grad()
                        fake_data = names['sub_generator' + str(i)](
                            names['noise' + str(i)])
                        fake_outputs = self.discriminator(fake_data)
                        generator_loss = criterion(fake_outputs,
                                                   names['trick' + str(i)])
                        generator_loss.backward()
                        names['optimizer_g' + str(i)].step()
                        names['sub_generator' + str(
                            i) + '_loss'] = generator_loss.item()
                        self.train_history[f'sub_generator{i}_loss'].append(
                            generator_loss.item())
                else:
                    for i in range(self.k):
                        with torch.no_grad():
                            fake_data = names['sub_generator' + str(i)](
                                names['noise' + str(i)])
                            fake_outputs = self.discriminator(fake_data)
                            generator_loss = criterion(fake_outputs,
                                                       names['trick' + str(i)])
                            names['sub_generator' + str(
                                i) + '_loss'] = generator_loss.item()
                            self.train_history[
                                f'sub_generator{i}_loss'].append(
                                generator_loss.item())

                generator_loss = np.mean(
                    [names['sub_generator' + str(i) + '_loss'] for i in
                     range(self.k)])
                self.train_history['generator_loss'].append(generator_loss)

                if epoch + 1 > self.stop_epochs:
                    stop = 1

        ### Detection result
        decision_scores = self.discriminator(
            torch.tensor(X, dtype=torch.float32,
                         device=self.device)).cpu().detach().numpy()
        self.decision_scores_ = decision_scores.ravel()
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.mo_gaal.MO_GAAL.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['discriminator'])
        X = check_array(X)
        pred_scores = self.discriminator(
            torch.tensor(X, dtype=torch.float32).to(
                self.device)).cpu().detach().numpy().ravel()
        return pred_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 49. pyod.models.ocsvm - pyod 2.0.5 documentation {#49-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:39

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/ocsvm.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.ocsvm
```
### -*- coding: utf-8 -*-
"""One-class SVM detector. Implemented on scikit-learn library.
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


fromsklearn.svmimport OneClassSVM
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport invert_order




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM)
classOCSVM(BaseDetector):
"""Wrapper of scikit-learn one-class SVM Class with more functionalities.
    Unsupervised Outlier Detection.

    Estimate the support of a high-dimensional distribution.

    The implementation is based on libsvm.
    See http://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection
    and :cite:`scholkopf2001estimating`.

    Parameters
    ----------
    kernel : string, optional (default='rbf')
         Specifies the kernel type to be used in the algorithm.
         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or
         a callable.
         If none is given, 'rbf' will be used. If a callable is given it is
         used to precompute the kernel matrix.

    nu : float, optional
        An upper bound on the fraction of training
        errors and a lower bound of the fraction of support
        vectors. Should be in the interval (0, 1]. By default 0.5
        will be taken.

    degree : int, optional (default=3)
        Degree of the polynomial kernel function ('poly').
        Ignored by all other kernels.

    gamma : float, optional (default='auto')
        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
        If gamma is 'auto' then 1/n_features will be used instead.

    coef0 : float, optional (default=0.0)
        Independent term in kernel function.
        It is only significant in 'poly' and 'sigmoid'.

    tol : float, optional
        Tolerance for stopping criterion.

    shrinking : bool, optional
        Whether to use the shrinking heuristic.

    cache_size : float, optional
        Specify the size of the kernel cache (in MB).

    verbose : bool, default: False
        Enable verbose output. Note that this setting takes advantage of a
        per-process runtime setting in libsvm that, if enabled, may not work
        properly in a multithreaded context.

    max_iter : int, optional (default=-1)
        Hard limit on iterations within solver, or -1 for no limit.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.


    Attributes
    ----------
    support_ : array-like, shape = [n_SV]
        Indices of support vectors.

    support_vectors_ : array-like, shape = [nSV, n_features]
        Support vectors.

    dual_coef_ : array, shape = [1, n_SV]
        Coefficients of the support vectors in the decision function.

    coef_ : array, shape = [1, n_features]
        Weights assigned to the features (coefficients in the primal
        problem). This is only available in the case of a linear kernel.

        `coef_` is readonly property derived from `dual_coef_` and
        `support_vectors_`

    intercept_ : array, shape = [1,]
        Constant in the decision function.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0,
                 tol=1e-3, nu=0.5, shrinking=True, cache_size=200,
                 verbose=False, max_iter=-1, contamination=0.1):
        super(OCSVM, self).__init__(contamination=contamination)
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.coef0 = coef0
        self.tol = tol
        self.nu = nu
        self.shrinking = shrinking
        self.cache_size = cache_size
        self.verbose = verbose
        self.max_iter = max_iter



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.fit)
    deffit(self, X, y=None, sample_weight=None, **params):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        sample_weight : array-like, shape (n_samples,)
            Per-sample weights. Rescale C per sample. Higher weights
            force the classifier to put more emphasis on these points.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        self.detector_ = OneClassSVM(kernel=self.kernel,
                                     degree=self.degree,
                                     gamma=self.gamma,
                                     coef0=self.coef0,
                                     tol=self.tol,
                                     nu=self.nu,
                                     shrinking=self.shrinking,
                                     cache_size=self.cache_size,
                                     verbose=self.verbose,
                                     max_iter=self.max_iter)
        self.detector_.fit(X=X, y=y, sample_weight=sample_weight,
                           **params)

        ### invert decision_scores_. Outliers comes with higher outlier scores
        self.decision_scores_ = invert_order(
            self.detector_.decision_function(X))
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ocsvm.OCSVM.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        ### Invert outlier scores. Outliers comes with higher outlier scores
        return invert_order(self.detector_.decision_function(X))




    @property
    defsupport_(self):
"""Indices of support vectors.
        Decorator for scikit-learn One class SVM attributes.
        """
        return self.detector_.support_

    @property
    defsupport_vectors_(self):
"""Support vectors.
        Decorator for scikit-learn One class SVM attributes.
        """
        return self.detector_.support_vectors_

    @property
    defdual_coef_(self):
"""Coefficients of the support vectors in the decision function.
        Decorator for scikit-learn One class SVM attributes.
        """
        return self.detector_.dual_coef_

    @property
    defcoef_(self):
"""Weights assigned to the features (coefficients in the primal
        problem). This is only available in the case of a linear kernel.
        `coef_` is readonly property derived from `dual_coef_` and
        `support_vectors_`
        Decorator for scikit-learn One class SVM attributes.
        """
        return self.detector_.coef_

    @property
    defintercept_(self):
""" Constant in the decision function.
        Decorator for scikit-learn One class SVM attributes.
        """
        return self.detector_.intercept_




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 50. pyod.models.pca - pyod 2.0.5 documentation {#50-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:42

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.pca
```
### -*- coding: utf-8 -*-
"""Principal Component Analysis (PCA) Outlier Detector
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromscipy.spatial.distanceimport cdist
fromsklearn.decompositionimport PCA as sklearn_PCA
fromsklearn.utils.validationimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport check_parameter
from..utils.utilityimport standardizer




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA)
classPCA(BaseDetector):
"""Principal component analysis (PCA) can be used in detecting outliers.
    PCA is a linear dimensionality reduction using Singular Value Decomposition
    of the data to project it to a lower dimensional space.

    In this procedure, covariance matrix of the data can be decomposed to
    orthogonal vectors, called eigenvectors, associated with eigenvalues. The
    eigenvectors with high eigenvalues capture most of the variance in the
    data.

    Therefore, a low dimensional hyperplane constructed by k eigenvectors can
    capture most of the variance in the data. However, outliers are different
    from normal data points, which is more obvious on the hyperplane
    constructed by the eigenvectors with small eigenvalues.

    Therefore, outlier scores can be obtained as the sum of the projected
    distance of a sample on all eigenvectors.
    See :cite:`shyu2003novel,aggarwal2015outlier` for details.

    Score(X) = Sum of weighted euclidean distance between each sample to the
    hyperplane constructed by the selected eigenvectors

    Parameters
    ----------
    n_components : int, float, None or string
        Number of components to keep.
        if n_components is not set all components are kept::

            n_components == min(n_samples, n_features)

        if n_components == 'mle' and svd_solver == 'full', Minka\'s MLE is used
        to guess the dimension
        if ``0 < n_components < 1`` and svd_solver == 'full', select the number
        of components such that the amount of variance that needs to be
        explained is greater than the percentage specified by n_components
        n_components cannot be equal to n_features for svd_solver == 'arpack'.

    n_selected_components : int, optional (default=None)
        Number of selected principal components
        for calculating the outlier scores. It is not necessarily equal to
        the total number of the principal components. If not set, use
        all principal components.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    copy : bool (default True)
        If False, data passed to fit are overwritten and running
        fit(X).transform(X) will not yield the expected results,
        use fit_transform(X) instead.

    whiten : bool, optional (default False)
        When True (False by default) the `components_` vectors are multiplied
        by the square root of n_samples and then divided by the singular values
        to ensure uncorrelated outputs with unit component-wise variances.

        Whitening will remove some information from the transformed signal
        (the relative variance scales of the components) but can sometime
        improve the predictive accuracy of the downstream estimators by
        making their data respect some hard-wired assumptions.

    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}
        auto :
            the solver is selected by a default policy based on `X.shape` and
            `n_components`: if the input data is larger than 500x500 and the
            number of components to extract is lower than 80% of the smallest
            dimension of the data, then the more efficient 'randomized'
            method is enabled. Otherwise the exact full SVD is computed and
            optionally truncated afterwards.
        full :
            run exact full SVD calling the standard LAPACK solver via
            `scipy.linalg.svd` and select the components by postprocessing
        arpack :
            run SVD truncated to n_components calling ARPACK solver via
            `scipy.sparse.linalg.svds`. It requires strictly
            0 < n_components < X.shape[1]
        randomized :
            run randomized SVD by the method of Halko et al.

    tol : float >= 0, optional (default .0)
        Tolerance for singular values computed by svd_solver == 'arpack'.

    iterated_power : int >= 0, or 'auto', (default 'auto')
        Number of iterations for the power method computed by
        svd_solver == 'randomized'.

    random_state : int, RandomState instance or None, optional (default None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.

    weighted : bool, optional (default=True)
        If True, the eigenvalues are used in score computation.
        The eigenvectors with small eigenvalues comes with more importance
        in outlier score calculation.

    standardization : bool, optional (default=True)
        If True, perform standardization first to convert
        data to zero mean and unit variance.
        See http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html

    Attributes
    ----------
    components_ : array, shape (n_components, n_features)
        Principal axes in feature space, representing the directions of
        maximum variance in the data. The components are sorted by
        ``explained_variance_``.

    explained_variance_ : array, shape (n_components,)
        The amount of variance explained by each of the selected components.

        Equal to n_components largest eigenvalues
        of the covariance matrix of X.

    explained_variance_ratio_ : array, shape (n_components,)
        Percentage of variance explained by each of the selected components.

        If ``n_components`` is not set then all components are stored and the
        sum of explained variances is equal to 1.0.

    singular_values_ : array, shape (n_components,)
        The singular values corresponding to each of the selected components.
        The singular values are equal to the 2-norms of the ``n_components``
        variables in the lower-dimensional space.

    mean_ : array, shape (n_features,)
        Per-feature empirical mean, estimated from the training set.

        Equal to `X.mean(axis=0)`.

    n_components_ : int
        The estimated number of components. When n_components is set
        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this
        number is estimated from input data. Otherwise it equals the parameter
        n_components, or n_features if n_components is None.

    noise_variance_ : float
        The estimated noise covariance following the Probabilistic PCA model
        from Tipping and Bishop 1999. See "Pattern Recognition and
        Machine Learning" by C. Bishop, 12.2.1 p. 574 or
        http://www.miketipping.com/papers/met-mppca.pdf. It is required to
        computed the estimated data covariance and score samples.

        Equal to the average of (min(n_features, n_samples) - n_components)
        smallest eigenvalues of the covariance matrix of X.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, n_components=None, n_selected_components=None,
                 contamination=0.1, copy=True, whiten=False, svd_solver='auto',
                 tol=0.0, iterated_power='auto', random_state=None,
                 weighted=True, standardization=True):

        super(PCA, self).__init__(contamination=contamination)
        self.n_components = n_components
        self.n_selected_components = n_selected_components
        self.copy = copy
        self.whiten = whiten
        self.svd_solver = svd_solver
        self.tol = tol
        self.iterated_power = iterated_power
        self.random_state = random_state
        self.weighted = weighted
        self.standardization = standardization

    ### noinspection PyIncorrectDocstring


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### PCA is recommended to use on the standardized data (zero mean and
        ### unit variance).
        if self.standardization:
            X, self.scaler_ = standardizer(X, keep_scalar=True)

        self.detector_ = sklearn_PCA(n_components=self.n_components,
                                     copy=self.copy,
                                     whiten=self.whiten,
                                     svd_solver=self.svd_solver,
                                     tol=self.tol,
                                     iterated_power=self.iterated_power,
                                     random_state=self.random_state)
        self.detector_.fit(X=X, y=y)

        ### copy the attributes from the sklearn PCA object
        self.n_components_ = self.detector_.n_components_
        self.components_ = self.detector_.components_

        ### validate the number of components to be used for outlier detection
        if self.n_selected_components is None:
            self.n_selected_components_ = self.n_components_
        else:
            self.n_selected_components_ = self.n_selected_components
        check_parameter(self.n_selected_components_, 1, self.n_components_,
                        include_left=True, include_right=True,
                        param_name='n_selected_components_')

        ### use eigenvalues as the weights of eigenvectors
        self.w_components_ = np.ones([self.n_components_, ])
        if self.weighted:
            self.w_components_ = self.detector_.explained_variance_ratio_

        ### outlier scores is the sum of the weighted distances between each
        ### sample to the eigenvectors. The eigenvectors with smaller
        ### eigenvalues have more influence
        ### Not all eigenvectors are used, only n_selected_components_ smallest
        ### are used since they better reflect the variance change

        self.selected_components_ = self.components_[
                                    -1 * self.n_selected_components_:, :]
        self.selected_w_components_ = self.w_components_[
                                      -1 * self.n_selected_components_:]

        self.decision_scores_ = np.sum(
            cdist(X, self.selected_components_) / self.selected_w_components_,
            axis=1).ravel()

        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.pca.PCA.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['components_', 'w_components_'])

        X = check_array(X)
        if self.standardization:
            X = self.scaler_.transform(X)

        return np.sum(
            cdist(X, self.selected_components_) / self.selected_w_components_,
            axis=1).ravel()




    @property
    defexplained_variance_(self):
"""The amount of variance explained by each of the selected components.

        Equal to n_components largest eigenvalues
        of the covariance matrix of X.

        Decorator for scikit-learn PCA attributes.
        """
        return self.detector_.explained_variance_

    @property
    defexplained_variance_ratio_(self):
"""Percentage of variance explained by each of the selected components.

        If ``n_components`` is not set then all components are stored and the
        sum of explained variances is equal to 1.0.

        Decorator for scikit-learn PCA attributes.
        """
        return self.detector_.explained_variance_ratio_

    @property
    defsingular_values_(self):
"""The singular values corresponding to each of the selected
        components. The singular values are equal to the 2-norms of the
        ``n_components`` variables in the lower-dimensional space.

        Decorator for scikit-learn PCA attributes.
        """
        return self.detector_.singular_values_

    @property
    defmean_(self):
"""Per-feature empirical mean, estimated from the training set.

        Decorator for scikit-learn PCA attributes.
        """
        return self.detector_.mean_

    @property
    defnoise_variance_(self):
"""The estimated noise covariance following the Probabilistic PCA model
        from Tipping and Bishop 1999. See "Pattern Recognition and
        Machine Learning" by C. Bishop, 12.2.1 p. 574 or
        http://www.miketipping.com/papers/met-mppca.pdf. It is required to
        computed the estimated data covariance and score samples.

        Equal to the average of (min(n_features, n_samples) - n_components)
        smallest eigenvalues of the covariance matrix of X.

        Decorator for scikit-learn PCA attributes.
        """
        return self.detector_.noise_variance_




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 51. pyod.models.qmcd - pyod 2.0.5 documentation {#51-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:31

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/qmcd.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.qmcd
```
### -*- coding: utf-8 -*-
"""Quasi-Monte Carlo Discrepancy outlier detection (QMCD)
"""

### Author: D Kulik
### License: BSD 2 clause


importnumpyasnp
importscipy.statsasstats
fromnumbaimport njit, prange
fromsklearn.preprocessingimport MinMaxScaler
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


@njit(fastmath=True, parallel=True)
def_wrap_around_discrepancy(data, check):
"""Wrap-around Quasi-Monte Carlo discrepancy method"""

    n = data.shape[0]
    d = data.shape[1]
    p = check.shape[0]

    disc = np.zeros(p)

    for i in prange(p):
        dc = 0.0
        for j in prange(n):
            prod = 1.0
            for k in prange(d):
                x_kikj = abs(check[i, k] - data[j, k])
                prod *= 3.0 / 2.0 - x_kikj + x_kikj ** 2

            dc += prod
        disc[i] = dc

    return - (4.0 / 3.0) ** d + 1.0 / (n ** 2) * disc




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD)
classQMCD(BaseDetector):
"""The Wrap-around Quasi-Monte Carlo discrepancy is a uniformity criterion 
       which is used to assess the space filling of a number of samples in a hypercube. 
       It quantifies the distance between the continuous uniform distribution on a 
       hypercube and the discrete uniform distribution on distinct sample points. 
       Therefore, lower discrepancy values for a sample point indicates that it provides 
       better coverage of the parameter space with regard to the rest of the samples. This
       method is kernel based and a higher discrepancy score is relative to the
       rest of the samples, the higher the likelihood of it being an outlier. 
       Read more in the :cite:`fang2001wrap`.
    Parameters
    ----------          

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
       The modified z-score to use as a threshold. Observations with
       a modified z-score (based on the median absolute deviation) greater
       than this value will be classified as outliers.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
        """

    def__init__(self, contamination=0.1):

        super(QMCD, self).__init__(contamination=contamination)



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.fit)
    deffit(self, X, y=None):
"""Fit detector

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        ### Normalize data between 0 and 1
        self._scaler = MinMaxScaler()
        X_norm = self._scaler.fit_transform(X)

        self._fitted_data = X_norm.copy()

        ### Calculate WD QMCD scores
        scores = _wrap_around_discrepancy(X_norm, X_norm)

        ### Get criterion for inverting scores
        self._is_flipped = False
        skew = stats.skew(scores)
        kurt = stats.kurtosis(scores)

        ### Invert score order based on criterion
        if (skew < 0) or ((skew >= 0) & (kurt < 0)):
            scores = scores.max() + scores.min() - scores
            self._is_flipped = True

        self.decision_scores_ = scores

        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.qmcd.QMCD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The independent and dependent/target samples with the target 
            samples being the last column of the numpy array such that
            eg: X = np.append(x, y.reshape(-1,1), axis=1). Sparse matrices are 
            accepted only if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])

        X = check_array(X)

        ### Scale data to fitted data
        X_norm = self._scaler.transform(X)

        ### Calculate WD QMCD scores
        scores = _wrap_around_discrepancy(self._fitted_data, X_norm)

        ### Invert score order based on criterion
        if self._is_flipped:
            scores = self.decision_scores_.max() + self.decision_scores_.min() - scores

        return scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 52. pyod.models.rgraph - pyod 2.0.5 documentation {#52-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:53

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rgraph.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.rgraph
```
### -*- coding: utf-8 -*-
""" R-graph

"""
### Author: Michiel Bongaerts (but not author of the R-graph method)
### License: BSD 2 clause


importwarnings

importnumpyasnp
fromscipyimport sparse
fromsklearn.decompositionimport sparse_encode
fromsklearn.linear_modelimport LinearRegression
fromsklearn.preprocessingimport StandardScaler
fromsklearn.preprocessingimport normalize
fromsklearn.utilsimport check_array

from.baseimport BaseDetector




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph)
classRGraph(BaseDetector):
""" Outlier Detection via R-graph.
    Paper: https://openaccess.thecvf.com/content_cvpr_2017/papers/You_Provable_Self-Representation_Based_CVPR_2017_paper.pdf
    See :cite:`you2017provable` for details.

    Parameters
    ----------
    transition_steps : int, optional (default=20)
        Number of transition steps that are taken in the graph, after which 
        the outlier scores are determined.

    gamma : float

    gamma_nz : boolean, default True
        gamma and gamma_nz together determines the parameter alpha.
        When ``gamma_nz = False``, alpha = gamma.
        When ``gamma_nz = True``, then alpha = gamma * alpha0, where alpha0 is
        the largest number such that the solution to the optimization problem
        with alpha = alpha0 is the zero vector (see Proposition 1 in [1]).
        Therefore, when ``gamma_nz = True``, gamma should be a value greater
        than 1.0. A good choice is typically in the range [5, 500].

    tau : float, default 1.0
        Parameter for elastic net penalty term. 
        When tau = 1.0, the method reduces to sparse subspace clustering with
        basis pursuit (SSC-BP) [2].
        When tau = 0.0, the method reduces to least squares regression (LSR).

    algorithm : string, default ``lasso_lars``
        Algorithm for computing the representation. Either lasso_lars or
        lasso_cd.
        Note: ``lasso_lars`` and ``lasso_cd`` only support tau = 1.
        For cases tau << 1 linear regression is used.


    fit_intercept_LR: bool, optional (default=False)
        For  ``gamma`` > 10000 linear regression is used instead of
        ``lasso_lars`` or ``lasso_cd``. This parameter determines whether the
        intercept for the model is calculated.

    maxiter_lasso : int, default 1000
        The maximum number of iterations for ``lasso_lars`` and ``lasso_cd``.

    n_nonzero : int, default 50
        This is an upper bound on the number of nonzero entries of each
        representation vector.
        If there are more than n_nonzero nonzero entries,
        only the top n_nonzero number of
        entries with largest absolute value are kept.

    active_support: boolean, default True
        Set to True to use the active support algorithm in [1] for solving the
        optimization problem. This should significantly reduce the running time
        when n_samples is large.

    active_support_params: dictionary of string to any, optional
        Parameters (keyword arguments) and values for the active support
        algorithm. It may be used to set the parameters ``support_init``,
        ``support_size`` and ``maxiter``, see
        ``active_support_elastic_net`` for details. 
        Example: active_support_params={'support_size':50, 'maxiter':100}
        Ignored when ``active_support=False``

    preprocessing : bool, optional (default=True)
        If True, apply standardization on the data.


    verbose : int, optional (default=1)
        Verbosity mode.

        - 0 = silent
        - 1 = progress bar
        - 2 = one line per epoch.

        For verbose >= 1, model summary may be printed.

    random_state : random_state: int, RandomState instance or None, optional
        (default=None)
        If int, random_state is the seed used by the random
        number generator; If RandomState instance, random_state is the random
        number generator; If None, the random number generator is the
        RandomState instance used by `np.random`.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. When fitting this is used
        to define the threshold on the decision function.

    blocksize_test_data: int, optional (default=10)
        Test set is splitted into blocks of the size ``blocksize_test_data``
        to at least partially separate test - and train set

    Attributes
    ----------
    transition_matrix_ : numpy array of shape (n_samples,)
        Transition matrix from the last fitted data, this might include 
        training + test data


    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, transition_steps=10, n_nonzero=10, gamma=50.0,
                 gamma_nz=True, algorithm='lasso_lars', tau=1.0,
                 maxiter_lasso=1000, preprocessing=True, contamination=0.1,
                 blocksize_test_data=10,
                 support_init='L2', maxiter=40, support_size=100,
                 active_support=True, fit_intercept_LR=False,
                 verbose=True):

        super(RGraph, self).__init__(contamination=contamination)

        self.transition_steps = transition_steps
        self.n_nonzero = n_nonzero
        self.gamma = gamma
        self.gamma_nz = gamma_nz
        self.algorithm = algorithm
        self.tau = tau
        self.preprocessing = preprocessing
        self.contamination = contamination
        self.maxiter_lasso = maxiter_lasso
        self.support_init = support_init
        self.maxiter = maxiter
        self.support_size = support_size
        self.active_support = active_support
        self.verbose = verbose
        self.blocksize_test_data = blocksize_test_data
        self.fit_intercept_LR = fit_intercept_LR



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.active_support_elastic_net)
    defactive_support_elastic_net(self, X, y, alpha, tau=1.0,
                                   algorithm='lasso_lars', support_init='L2',
                                   support_size=100, maxiter=40,
                                   maxiter_lasso=1000):
"""
        Source: https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py
            An active support based algorithm for solving the elastic net optimization problem
            min_{c} tau ||c||_1 + (1-tau)/2 ||c||_2^2 + alpha / 2 ||y - c X ||_2^2.
        Parameters
        -----------
        X : array-like, shape (n_samples, n_features)

        y : array-like, shape (1, n_features)

        alpha : float

        tau : float, default 1.0

        algorithm : string, default ``spams``
            Algorithm for computing solving the subproblems. Either lasso_lars
            or lasso_cd or spams
            (installation of spams package is required).
            Note: ``lasso_lars`` and ``lasso_cd`` only support tau = 1.

        support_init: string, default ``knn``
            This determines how the active support is initialized.
            It can be either ``knn`` or ``L2``.

        support_size: int, default 100
            This determines the size of the working set.
            A small support_size decreases the runtime per iteration while
            increase the number of iterations.

        maxiter: int default 40
            Termination condition for active support update.
        Returns
        -------
        c : shape n_samples
            The optimal solution to the optimization problem.
        """
        n_samples = X.shape[0]

        if n_samples <= support_size:  ### skip active support search for small scale data
            supp = np.arange(n_samples,
                             dtype=int)  ### this results in the following iteration to converge in 1 iteration
        else:
            if support_init == 'L2':
                L2sol = np.linalg.solve(
                    np.identity(y.shape[1]) * alpha + np.dot(X.T, X), y.T)
                c0 = np.dot(X, L2sol)[:, 0]
                supp = np.argpartition(-np.abs(c0), support_size)[
                       0:support_size]
            elif support_init == 'knn':
                supp = np.argpartition(-np.abs(np.dot(y, X.T)[0]),
                                       support_size)[0:support_size]

        curr_obj = float("inf")
        for _ in range(maxiter):
            Xs = X[supp, :]

            ###### Removed the original option to use 'spams' since this would
            ### require the spams dependency
            ### if algorithm == 'spams':
            ###     cs = spams.lasso(np.asfortranarray(y.T), D=np.asfortranarray(Xs.T), 
            ###                      lambda1=tau*alpha, lambda2=(1.0-tau)*alpha)
            ###     cs = np.asarray(cs.todense()).T
            ### else:
            cs = sparse_encode(y, Xs, algorithm=algorithm, alpha=alpha,
                               max_iter=maxiter_lasso)

            delta = (y - np.dot(cs, Xs)) / alpha

            obj = tau * np.sum(np.abs(cs[0])) + (1.0 - tau) / 2.0 * np.sum(
                np.power(cs[0], 2.0)) + alpha / 2.0 * np.sum(
                np.power(delta, 2.0))
            if curr_obj - obj < 1.0e-10 * curr_obj:
                break
            curr_obj = obj

            coherence = np.abs(np.dot(delta, X.T))[0]
            coherence[supp] = 0
            addedsupp = np.nonzero(coherence > tau + 1.0e-10)[0]

            if addedsupp.size == 0:  ### converged
                break

            ### Find the set of nonzero entries of cs.
            activesupp = supp[np.abs(cs[0]) > 1.0e-10]

            if activesupp.size > 0.8 * support_size:  ### this suggests that support_size is too small and needs to be increased
                support_size = min(
                    [round(max([activesupp.size, support_size]) * 1.1),
                     n_samples])

            if addedsupp.size + activesupp.size > support_size:
                ord = np.argpartition(-coherence[addedsupp],
                                      support_size - activesupp.size)[
                      0:support_size - activesupp.size]
                addedsupp = addedsupp[ord]

            supp = np.concatenate([activesupp, addedsupp])

        c = np.zeros(n_samples)
        c[supp] = cs
        return c






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.elastic_net_subspace_clustering)
    defelastic_net_subspace_clustering(self, X, gamma=50.0, gamma_nz=True,
                                        tau=1.0, algorithm='lasso_lars',
                                        fit_intercept_LR=False,
                                        active_support=True,
                                        active_support_params=None,
                                        n_nonzero=50,
                                        maxiter_lasso=1000):
"""
        Source: https://github.com/ChongYou/subspace-clustering/blob/master/cluster/selfrepresentation.py
        Elastic net subspace clustering (EnSC) [1]. 
        Compute self-representation matrix C from solving the following optimization problem
        min_{c_j} tau ||c_j||_1 + (1-tau)/2 ||c_j||_2^2 + alpha / 2 ||x_j - c_j X ||_2^2 s.t. c_jj = 0,
        where c_j and x_j are the j-th rows of C and X, respectively.
        Parameter ``algorithm`` specifies the algorithm for solving the optimization problem.
        ``lasso_lars`` and ``lasso_cd`` are algorithms implemented in sklearn, 
        ``spams`` refers to the same algorithm as ``lasso_lars`` but is implemented in 
        spams package available at http://spams-devel.gforge.inria.fr/ (installation required)
        In principle, all three algorithms give the same result.    
        For large scale data (e.g. with > 5000 data points), use any of these algorithms in
        conjunction with ``active_support=True``. It adopts an efficient active support 
        strategy that solves the optimization problem by breaking it into a sequence of 
        small scale optimization problems as described in [1].
        If tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2].
        If tau = 0.0, the method reduces to least squares regression (LSR) [3].
        Note: ``lasso_lars`` and ``lasso_cd`` only support tau = 1.
        Parameters
        -----------
        X : array-like, shape (n_samples, n_features)
            Input data to be clustered
        gamma : float
        gamma_nz : boolean, default True
            gamma and gamma_nz together determines the parameter alpha. When ``gamma_nz = False``, 
            alpha = gamma. When ``gamma_nz = True``, then alpha = gamma * alpha0, where alpha0 is 
            the largest number such that the solution to the optimization problem with alpha = alpha0
            is the zero vector (see Proposition 1 in [1]). Therefore, when ``gamma_nz = True``, gamma
            should be a value greater than 1.0. A good choice is typically in the range [5, 500].   
        tau : float, default 1.0
            Parameter for elastic net penalty term. 
            When tau = 1.0, the method reduces to sparse subspace clustering with basis pursuit (SSC-BP) [2].
            When tau = 0.0, the method reduces to least squares regression (LSR) [3].
        algorithm : string, default ``lasso_lars``
            Algorithm for computing the representation. Either lasso_lars or lasso_cd or spams 
            (installation of spams package is required).
            Note: ``lasso_lars`` and ``lasso_cd`` only support tau = 1.
        n_nonzero : int, default 50
            This is an upper bound on the number of nonzero entries of each representation vector. 
            If there are more than n_nonzero nonzero entries,  only the top n_nonzero number of
            entries with largest absolute value are kept.
        active_support: boolean, default True
            Set to True to use the active support algorithm in [1] for solving the optimization problem.
            This should significantly reduce the running time when n_samples is large.
        active_support_params: dictionary of string to any, optional
            Parameters (keyword arguments) and values for the active support algorithm. It may be
            used to set the parameters ``support_init``, ``support_size`` and ``maxiter``, see
            ``active_support_elastic_net`` for details. 
            Example: active_support_params={'support_size':50, 'maxiter':100}
            Ignored when ``active_support=False``
        Returns
        -------
        representation_matrix_ : csr matrix, shape: n_samples by n_samples
            The self-representation matrix.
        References
        ----------- 
        [1] C. You, C.-G. Li, D. Robinson, R. Vidal, Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering, CVPR 2016
        [2] E. Elhaifar, R. Vidal, Sparse Subspace Clustering: Algorithm, Theory, and Applications, TPAMI 2013
        [3] C. Lu, et al. Robust and efficient subspace segmentation via least squares regression, ECCV 2012
        """

        if ((algorithm in ('lasso_lars', 'lasso_cd')) and (
                tau < 1.0 - 1.0e-10)):
            warnings.warn(
                'algorithm {} cannot handle tau smaller than 1. Using tau = 1'.format(
                    algorithm))
            tau = 1.0

        if active_support == True and active_support_params == None:
            active_support_params = {}

        n_samples = X.shape[0]
        rows = np.zeros(n_samples * n_nonzero)
        cols = np.zeros(n_samples * n_nonzero)
        vals = np.zeros(n_samples * n_nonzero)
        curr_pos = 0

        gamma_is_zero_notification = False
        for i in range(n_samples):
            if ((i % 25 == 0) and (self.verbose == 1)):
                print('{}/{}'.format(i, n_samples))

            y = X[i, :].copy().reshape(1, -1)
            X[i, :] = 0

            if algorithm in ('lasso_lars', 'lasso_cd'):
                if gamma_nz == True:
                    coh = np.delete(np.absolute(np.dot(X, y.T)), i)
                    alpha0 = np.amax(
                        coh) / tau  ### value for which the solution is zero
                    alpha = alpha0 / gamma
                else:
                    alpha = 1.0 / gamma

                if (gamma >= 10 ** 4):
                    if (gamma_is_zero_notification == False):
                        warnings.warn(
                            'Set alpha = 0 i.e. LinearRegression() is used')
                        gamma_is_zero_notification = True

                    alpha = 0

                if (alpha == 0):
                    lr = LinearRegression(fit_intercept=fit_intercept_LR)
                    lr.fit(X.T, y[0])
                    c = lr.coef_


                elif active_support == True:
                    c = self.active_support_elastic_net(X, y, alpha, tau,
                                                        algorithm,
                                                        **active_support_params)
                else:

                    ###### Removed the original option to use 'spams' since this would require the spams dependency 
                    ### if algorithm == 'spams':
                    ### c = spams.lasso(np.asfortranarray(y.T), D=np.asfortranarray(X.T),
                    ###                 lambda1=tau * alpha, lambda2=(1.0-tau) * alpha)
                    ### c = np.asarray(c.todense()).T[0]
                    ### else:
                    c = sparse_encode(y, X, algorithm=algorithm, alpha=alpha,
                                      max_iter=maxiter_lasso)[0]

            else:
                warnings.warn("algorithm {} not found".format(algorithm))

            index = np.flatnonzero(c)
            if index.size > n_nonzero:
                ###  warnings.warn("The number of nonzero entries in sparse subspace clustering exceeds n_nonzero")
                index = index[np.argsort(-np.absolute(c[index]))[0:n_nonzero]]
            rows[curr_pos:curr_pos + len(index)] = i
            cols[curr_pos:curr_pos + len(index)] = index
            vals[curr_pos:curr_pos + len(index)] = c[index]
            curr_pos += len(index)

            X[i, :] = y

        return sparse.csr_matrix((vals, (rows, cols)),
                                 shape=(n_samples, n_samples))






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.
        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.
        y : Ignored
            Not used, present for API consistency by convention.
        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### If we "re-fit" the model then we need to make sure previour self.X_train is first deleted
        ### since this parameter is used downstream in the analysis in self.decision_function().
        if hasattr(self, 'X_train'):
            del self.X_train

        X = check_array(X)

        ### Fit scaler on train set
        if self.preprocessing:
            self.scaler_ = StandardScaler()
            self.scaler_.fit(X)

        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self.X_train = X
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rgraph.RGraph.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

        X = check_array(X)

        ### Since we already have a train set, we want to only partially concatenate the test set to the 
        ### train set. This will be done by splitting the test set into different parts and concatenate 
        ### those to the train set.
        if hasattr(self, 'X_train'):
            N = int(X.shape[0] / self.blocksize_test_data) + 1

            scores = []
            for i in range(N):
                if (self.verbose == 1):
                    print("Test block {}/{}".format(i, N))

                X_block_i = np.copy(X[i * self.blocksize_test_data: (
                                                                            i + 1) * self.blocksize_test_data])

                if (X_block_i.shape[0] >= 1):
                    original_size_i = X_block_i.shape[0]

                    ### Concatenate train set with part of the test set
                    X_i = np.concatenate((self.X_train, X_block_i), axis=0)

                    if self.preprocessing:
                        ### Scale concatenated data 
                        X_i_norm = self.scaler_.transform(X_i)
                    else:
                        X_i_norm = np.copy(X_i)

                    scores_i = self._decision_function(X_i_norm)
                    scores_i = scores_i[-original_size_i:]
                    scores.extend(list(scores_i))

            scores = np.array(scores)
            return scores

        else:

            if self.preprocessing:
                ### Scale train set
                X_norm = self.scaler_.transform(X)
            else:
                X_norm = np.copy(X)

            scores = self._decision_function(X_norm)
            return scores




    def_decision_function(self, X_norm):

        A = self.elastic_net_subspace_clustering(
            X_norm, gamma=self.gamma,
            gamma_nz=self.gamma_nz,
            tau=self.tau,
            algorithm=self.algorithm,
            fit_intercept_LR=self.fit_intercept_LR,
            active_support=self.active_support,
            n_nonzero=self.n_nonzero,
            maxiter_lasso=self.maxiter_lasso,
            active_support_params={
                'support_init': self.support_init,
                'support_size': self.support_size,
                'maxiter': self.maxiter}
        )

        self.transition_matrix_ = normalize(np.abs(A.toarray()), norm='l1')

        pi = np.ones((1, len(self.transition_matrix_)), dtype='float64') / len(
            self.transition_matrix_)
        pi_bar = np.zeros((1, len(self.transition_matrix_)), dtype='float64')

        ### Do transition steps
        for _ in range(self.transition_steps):
            pi = pi @ self.transition_matrix_
            pi_bar += pi

        pi_bar /= self.transition_steps
        scores = pi_bar[0]

        ### smaller scores correspond with outliers,
        ### thus we use -1 * score such that
        ### higher scores are associated with outliers
        scores = -1 * scores

        return scores




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 53. pyod.models.rod - pyod 2.0.5 documentation {#53-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:24

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/rod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.rod
```
### -*- coding: utf-8 -*-
"""Rotation-based Outlier Detector (ROD)
"""
### Author: Yahya Almardeny <almardeny@gmail.com>
### License: BSD 2 clause


importmultiprocessing
fromitertoolsimport combinations as com
frommultiprocessingimport Pool

importnumba
importnumpyasnp
fromsklearn.preprocessingimport MinMaxScaler, RobustScaler
fromsklearn.utilsimport check_array

from.baseimport BaseDetector


@numba.njit
defmad(costs, median=None):
"""Apply the robust median absolute deviation (MAD)
    to measure the inconsistency/variability of the
    rotation costs.

    Parameters
    ----------
    costs : list of rotation costs
    median: float (default=None), MAD median

    Returns
    -------
    z : float
        the modified z scores
    """
    costs_ = np.reshape(costs, (-1, 1))
    median = np.nanmedian(costs_) if median is None else median
    diff = np.abs(costs_ - median)
    return np.ravel(0.6745 * diff / np.median(diff)), median


defangle(v1, v2):
"""find the angle between two 3D vectors.

    Parameters
    ----------
    v1 : list, first vector
    v2 : list, second vector

    Returns
    -------
    angle : float, the angle
    """
    return np.arccos(np.dot(v1, v2) /
                     (np.linalg.norm(v1) * np.linalg.norm(v2)))


defgeometric_median(x, eps=1e-5):
"""
    Find the multivariate geometric L1-median by applying
    Vardi and Zhang algorithm.

    Parameters
    ----------
    x : array-like, the data points
    eps: float (default=1e-5), a threshold to indicate when to stop

    Returns
    -------
    gm : array, Geometric L1-median
    """
    points = np.unique(x, axis=0)
    gm_ = np.mean(points, 0)  ### initialize geometric median
    while True:
        D = euclidean(points, gm_, c=True)
        non_zeros = (D != 0)[:, 0]
        Dinv = 1 / D[non_zeros]
        Dinvs = np.sum(Dinv)
        W = Dinv / Dinvs
        T = np.sum(W * points[non_zeros], 0)
        num_zeros = len(points) - np.sum(non_zeros)
        if num_zeros == 0:
            gm1 = T
        elif num_zeros == len(points):
            return gm_
        else:
            R = (T - gm_) * Dinvs
            r = np.linalg.norm(R)
            r_inv = 0 if r == 0 else num_zeros / r
            gm1 = max(0, 1 - r_inv) * T + min(1, r_inv) * gm_

        if euclidean(gm_, gm1) < eps:
            return gm1

        gm_ = gm1


defscale_angles(gammas, scaler1=None, scaler2=None):
"""
    Scale all angles in which angles <= 90
    degree will be scaled within [0 - 54.7] and
    angles > 90 will be scaled within [90 - 126]

    Parameters
    ----------
    gammas : list, angles
    scaler1: obj (default=None), MinMaxScaler of Angles group 1
    scaler2: obj (default=None), MinMaxScaler of Angles group 2

    Returns
    -------
    scaled angles, scaler1, scaler2
    """
    first, second = [], []
    first_ind, second_ind = [], []
    q1 = np.pi / 2.
    for i, g in enumerate(gammas):
        if g <= q1:
            first.append(g)
            first_ind.append(i)
        else:
            second.append(g)
            second_ind.append(i)
    if scaler1 is None:  ### this indicates the `fit()`
        min_f, max_f = 0.001, 0.955
        scaler1 = MinMaxScaler(feature_range=(min_f, max_f))
        ### min_f and max_f are required to be fit by scaler for consistency between train and test sets
        scaler1.fit(np.array(first + [min_f, max_f]).reshape(-1, 1))
        first = scaler1.transform(np.array(first).reshape(-1, 1)).reshape(
            -1) if first else []
    else:
        first = scaler1.transform(np.array(first).reshape(-1, 1)).reshape(
            -1) if first else []
    if scaler2 is None:  ### this indicates the `fit()`
        min_s, max_s = q1 + 0.001, 2.186
        scaler2 = MinMaxScaler(feature_range=(min_s, max_s))
        ### min_s and max_s are required to be fit by scaler for consistency between train and test sets
        scaler2.fit(np.array(second + [min_s, max_s]).reshape(-1, 1))
        second = scaler2.transform(np.array(second).reshape(-1, 1)).reshape(
            -1) if second else []
    else:
        second = scaler2.transform(np.array(second).reshape(-1, 1)).reshape(
            -1) if second else []
    ### restore original order
    return np.concatenate([first, second])[
        np.argsort(first_ind + second_ind)], scaler1, scaler2


defeuclidean(v1, v2, c=False):
"""
    Find the euclidean distance between two vectors
    or between a vector and a collection of vectors.

    Parameters
    ----------
    v1 : list, first 3D vector or collection of vectors
    v2 : list, second 3D vector
    c : bool (default=False), if True, it means the v1 is a list of vectors.

    Returns
    -------
    list of list of euclidean distances if c==True.
    Otherwise float: the euclidean distance
    """
    if c:
        res = []
        for _v in v1:
            res.append([np.sqrt((_v[0] - v2[0]) ** 2 +
                                (_v[1] - v2[1]) ** 2 +
                                (_v[2] - v2[2]) ** 2)])
        return np.asarray(res)
    return np.sqrt((v1[0] - v2[0]) ** 2 +
                   (v1[1] - v2[1]) ** 2 +
                   (v1[2] - v2[2]) ** 2)


defrod_3D(x, gm=None, median=None, scaler1=None, scaler2=None):
"""
    Find ROD scores for 3D Data.
    note that gm, scaler1 and scaler2 will be returned "as they are"
    and without being changed if the model has been fit already

    Parameters
    ----------
    x : array-like, 3D data points.
    gm: list (default=None), the geometric median
    median: float (default=None), MAD median
    scaler1: obj (default=None), MinMaxScaler of Angles group 1
    scaler2: obj (default=None), MinMaxScaler of Angles group 2

    Returns
    -------
    decision_scores, gm, scaler1, scaler2
    """
    ### find the geometric median if it is not already fit
    gm = geometric_median(x) if gm is None else gm
    ### find its norm and center data around it
    norm_ = np.linalg.norm(gm)
    _x = x - gm
    ### calculate the scaled angles between the geometric median and each data point vector
    v_norm = np.linalg.norm(_x, axis=1)
    gammas, scaler1, scaler2 = scale_angles(
        np.arccos(np.clip(np.dot(_x, gm) / (v_norm * norm_), -1, 1)),
        scaler1=scaler1, scaler2=scaler2)
    ### apply the ROD main equation to find the rotation costs
    costs = np.power(v_norm, 3) * np.cos(gammas) * np.square(np.sin(gammas))
    ### apply MAD to calculate the decision scores
    decision_scores, median = mad(costs, median=median)
    return decision_scores, list(gm), median, scaler1, scaler2


@numba.njit
defsigmoid(x):
"""
    Implementation of Sigmoid function

    Parameters
    ----------
    x : array-like, decision scores

    Returns
    -------
    array-like, x after applying sigmoid
    """
    return 1 / (1 + np.exp(-x))


defprocess_sub(subspace, gm, median, scaler1, scaler2):
"""
    Apply ROD on a 3D subSpace then process it with sigmoid
    to compare apples to apples

    Parameters
    ----------
    subspace : array-like, 3D subspace of the data
    gm: list, the geometric median
    median: float, MAD median
    scaler1: obj, MinMaxScaler of Angles group 1
    scaler2: obj, MinMaxScaler of Angles group 2

    Returns
    -------
    ROD decision scores with sigmoid applied, gm, scaler1, scaler2
    """
    mad_subspace, gm, median, scaler1, scaler2 = rod_3D(subspace, gm=gm,
                                                        median=median,
                                                        scaler1=scaler1,
                                                        scaler2=scaler2)
    return sigmoid(
        np.nan_to_num(np.array(mad_subspace))), gm, median, scaler1, scaler2


defrod_nD(X, parallel, gm=None, median=None, data_scaler=None,
           angles_scalers1=None, angles_scalers2=None):
"""
    Find ROD overall scores when Data is higher than 3D:
      ### scale dataset using Robust Scaler
      ### decompose the full space into a combinations of 3D subspaces,
      ### Apply ROD on each combination,
      ### squish scores per subspace, so we compare apples to apples,
      ### calculate average of ROD scores of all subspaces per observation.
    Note that if gm, data_scaler, angles_scalers1, angles_scalers2 are None,
    that means it is a `fit()` process and they will be calculated and returned
    to the class to be saved for future prediction. Otherwise, if they are not None,
    then it is a prediction process.

    Parameters
    ----------
    X : array-like, data points
    parallel: bool, True runs the algorithm in parallel
    gm: list (default=None), the geometric median
    median: list (default=None), MAD medians
    data_scaler: obj (default=None), RobustScaler of data
    angles_scalers1: list (default=None), MinMaxScalers of Angles group 1
    angles_scalers2: list (default=None), MinMaxScalers of Angles group 2

    Returns
    -------
    ROD decision scores, gm, median, data_scaler, angles_scalers1, angles_scalers2
    """
    if data_scaler is None:  ### for fitting
        data_scaler = RobustScaler()
        X = data_scaler.fit_transform(X)
    else:  ### for prediction
        X = data_scaler.transform(X)
    dim = X.shape[1]
    all_subspaces = [X[:, _com] for _com in com(range(dim), 3)]
    all_gms = [None] * len(all_subspaces) if gm is None else gm
    all_meds = [None] * len(all_subspaces) if median is None else median
    all_angles_scalers1 = [None] * len(
        all_subspaces) if angles_scalers1 is None else angles_scalers1
    all_angles_scalers2 = [None] * len(
        all_subspaces) if angles_scalers2 is None else angles_scalers2
    if parallel:
        p = Pool(multiprocessing.cpu_count())
        args = [[a, b, c, d, e] for a, b, c, d, e in
                zip(all_subspaces, all_gms, all_meds,
                    all_angles_scalers1, all_angles_scalers2)]
        results = p.starmap(process_sub, args)
        subspaces_scores, gm, median, angles_scalers1, angles_scalers2 = [], [], [], [], []
        for res in results:
            subspaces_scores.append(list(res[0]))
            gm.append(res[1])
            median.append(res[2])
            angles_scalers1.append(res[3])
            angles_scalers2.append(res[4])
        scores = np.average(np.array(subspaces_scores).T, axis=1).reshape(-1)
        p.close()
        p.join()
        return scores, gm, median, data_scaler, angles_scalers1, angles_scalers2
    subspaces_scores, gm, median, angles_scalers1, angles_scalers2 = [], [], [], [], []
    for subspace, _gm, med, ang_s1, ang_s2 in zip(all_subspaces, all_gms,
                                                  all_meds,
                                                  all_angles_scalers1,
                                                  all_angles_scalers2):
        scores_, gm_, med_, ang_s1_, ang_s2_ = process_sub(subspace=subspace,
                                                           gm=_gm, median=med,
                                                           scaler1=ang_s1,
                                                           scaler2=ang_s2)
        subspaces_scores.append(scores_)
        gm.append(gm_)
        median.append(med_)
        angles_scalers1.append(ang_s1_)
        angles_scalers2.append(ang_s2_)
    scores = np.average(np.array(subspaces_scores).T, axis=1).reshape(-1)
    return scores, gm, median, data_scaler, angles_scalers1, angles_scalers2




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD)
classROD(BaseDetector):
"""Rotation-based Outlier Detection (ROD), is a robust and parameter-free
    algorithm that requires no statistical distribution assumptions and
    works intuitively in three-dimensional space, where the 3D-vectors,
    representing the data points, are rotated about the geometric median
    two times counterclockwise using Rodrigues rotation formula.
    The results of the rotation are parallelepipeds where their volumes are
    mathematically analyzed as cost functions and used to calculate the
    Median Absolute Deviations to obtain the outlying score.
    For high dimensions > 3, the overall score is calculated by taking the
    average of the overall 3D-subspaces scores, that were resulted from
    decomposing the original data space.
    See :cite:`almardeny2020novel` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    parallel_execution: bool, optional (default=False).
        If set to True, the algorithm will run in parallel,
        for a better execution time. It is recommended to set
        this parameter to True ONLY for high dimensional data > 10,
        and if a proper hardware is available.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, parallel_execution=False):
        super(ROD, self).__init__(contamination=contamination)
        if not isinstance(parallel_execution, bool):
            raise TypeError("parallel_execution should be bool. "
                            "Got {}".format(type(parallel_execution)))
        self.parallel_execution = parallel_execution



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        ### reset learning parameters after each fit
        self.gm_ = None  ### geometric median(s)
        self.median_ = None  ### MAD median(s)
        self.data_scaler_ = None  ### data scaler (in case of d>3)
        self.angles_scaler1_ = None  ### scaler(s) of Angles Group 1
        self.angles_scaler2_ = None  ### scaler(s) of Angles Group 2
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.rod.ROD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
                         The anomaly score of the input samples.
        """
        X = check_array(X)
        if X.shape[1] < 3:
            X = np.hstack((X, np.zeros(shape=(X.shape[0], 3 - X.shape[1]))))

        if X.shape[1] == 3:
            scores, self.gm_, self.median_, self.angles_scaler1_, \
                self.angles_scaler2_ = rod_3D(x=X, gm=self.gm_,
                                              median=self.median_,
                                              scaler1=self.angles_scaler1_,
                                              scaler2=self.angles_scaler2_)
            return scores

        scores, self.gm_, self.median_, self.data_scaler_, \
            self.angles_scaler1_, self.angles_scaler2_ = rod_nD(X=X,
                                                                parallel=self.parallel_execution,
                                                                gm=self.gm_,
                                                                median=self.median_,
                                                                data_scaler=self.data_scaler_,
                                                                angles_scalers1=self.angles_scaler1_,
                                                                angles_scalers2=self.angles_scaler2_)
        return scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 54. pyod.models.sampling - pyod 2.0.5 documentation {#54-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:17:03

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sampling.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.sampling
```
### -*- coding: utf-8 -*-
"""Outlier detection based on Sampling (SP)
"""
### Author: Akira Tamamori <tamamori5917@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromsklearn.utilsimport check_array, check_random_state
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector
from..utils.utilityimport _get_sklearn_version

sklearn_version = _get_sklearn_version()
if sklearn_version[:3] >= '1.3':
    fromsklearn.metricsimport DistanceMetric
else:
    fromsklearn.neighborsimport DistanceMetric




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling)
classSampling(BaseDetector):
"""Sampling class for outlier detection.

    Sugiyama, M., Borgwardt, K. M.: Rapid Distance-Based Outlier Detection via
    Sampling, Advances in Neural Information Processing Systems (NIPS 2013),
    467-475, 2013.

    See :cite:`sugiyama2013rapid` for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    subset_size : float in (0., 1.0) or int (0, n_samples), optional (default=20)
        The size of subset of the data set.
        Sampling subset from the data set is performed only once.

    metric : string or callable, default 'minkowski'
        metric to use for distance computation. Any metric from scikit-learn
        or scipy.spatial.distance can be used.

        If metric is a callable function, it is called on each
        pair of instances (rows) and the resulting value recorded. The callable
        should take two arrays as input and return one value indicating the
        distance between them. This works for Scipy's metrics, but is less
        efficient than passing the metric name as a string.

        Distance matrices are not supported.

        Valid values for metric are:

        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
          'manhattan']

        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',
          'sqeuclidean', 'yule']

        See the documentation for scipy.spatial.distance for details on these
        metrics.

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    random_state : int, RandomState instance or None, optional (default None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self,
                 contamination=0.1,
                 subset_size=20,
                 metric="minkowski",
                 metric_params=None,
                 random_state=None,
                 ):
        super().__init__(contamination=contamination)
        self.subset_size = subset_size
        self.metric = metric
        self.metric_params = metric_params
        self.random_state = check_random_state(random_state)
        self.dist = None
        self.subset = None
        self.decision_scores_ = None



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        n_samples, _ = X.shape
        if (isinstance(self.subset_size, int) is True) and (
                not 0 < self.subset_size <= n_samples):
            raise ValueError(
                "subset_size=%r must be between 0 and n_samples=%r."
                % (self.subset_size, n_samples)
            )
        if isinstance(self.subset_size, float) is True:
            if 0.0 < self.subset_size <= 1.0:
                self.subset_size = int(self.subset_size * n_samples)
            else:
                raise ValueError("subset_size=%r must be between 0.0 and 1.0")

        random_indices = self.random_state.choice(
            n_samples,
            size=self.subset_size,
            replace=False,
        )
        self.subset = X[random_indices, :]

        if self.metric_params is None:
            self.dist = DistanceMetric.get_metric(self.metric)
        else:
            self.dist = DistanceMetric.get_metric(self.metric,
                                                  **self.metric_params)

        pair_dist = self.dist.pairwise(X, self.subset)
        anomaly_scores = np.min(pair_dist, axis=1)

        self.decision_scores_ = anomaly_scores
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sampling.Sampling.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The test input samples.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])

        X = check_array(X)

        pair_dist = self.dist.pairwise(X, self.subset)
        anomaly_scores = np.min(pair_dist, axis=1)

        return anomaly_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 55. pyod.models.so_gaal - pyod 2.0.5 documentation {#55-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:27

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/so_gaal.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.so_gaal
```
"""Single-Objective Generative Adversarial Active Learning.
Part of the codes are adapted from
https://github.com/leibinghe/GAAL-based-outlier-detection
"""
### Author: Sihan Chen <schen976@usc.edu>
### License: BSD 2 clause

importmath
fromcollectionsimport defaultdict

try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
importtorch.nnasnn
importtorch.nn.functionalasF
importtorch.optimasoptim

fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted
fromtorch.utils.dataimport DataLoader, TensorDataset

from.baseimport BaseDetector


classGenerator(nn.Module):
    def__init__(self, latent_size):
        super(Generator, self).__init__()
        self.layer1 = nn.Linear(latent_size, latent_size)
        self.layer2 = nn.Linear(latent_size, latent_size)
        nn.init.eye_(self.layer1.weight)
        nn.init.eye_(self.layer2.weight)

    defforward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return x


classDiscriminator(nn.Module):
    def__init__(self, latent_size, data_size):
        super(Discriminator, self).__init__()
        self.layer1 = nn.Linear(latent_size, math.ceil(math.sqrt(data_size)))
        self.layer2 = nn.Linear(math.ceil(math.sqrt(data_size)), 1)
        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in',
                                nonlinearity='relu')
        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in',
                                nonlinearity='sigmoid')

    defforward(self, x):
        x = F.relu(self.layer1(x))
        x = torch.sigmoid(self.layer2(x))
        return x




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL)
classSO_GAAL(BaseDetector):
"""Single-Objective Generative Adversarial Active Learning.

    SO-GAAL directly generates informative potential outliers to assist the
    classifier in describing a boundary that can separate outliers from normal
    data effectively. Moreover, to prevent the generator from falling into the
    mode collapsing problem, the network structure of SO-GAAL is expanded from
    a single generator (SO-GAAL) to multiple generators with different
    objectives (MO-GAAL) to generate a reasonable reference distribution for
    the whole dataset.
    Read more in the :cite:`liu2019generative`.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    stop_epochs : int, optional (default=20)
        The number of epochs of training. The number of total epochs equals to
         three times of stop_epochs.

    lr_d : float, optional (default=0.01)
        The learn rate of the discriminator.

    lr_g : float, optional (default=0.0001)
        The learn rate of the generator.

    momentum : float, optional (default=0.9)
        The momentum parameter for SGD.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9,
                 contamination=0.1):
        super(SO_GAAL, self).__init__(contamination=contamination)
        self.stop_epochs = stop_epochs
        self.lr_d = lr_d
        self.lr_g = lr_g
        self.momentum = momentum



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        latent_size = X.shape[1]
        data_size = X.shape[0]
        stop = 0
        epochs = self.stop_epochs * 3
        self.train_history = defaultdict(list)

        self.discriminator = Discriminator(latent_size, data_size)
        self.generator = Generator(latent_size)

        optimizer_d = optim.SGD(self.discriminator.parameters(), lr=self.lr_d,
                                momentum=self.momentum)
        optimizer_g = optim.SGD(self.generator.parameters(), lr=self.lr_g,
                                momentum=self.momentum)
        criterion = nn.BCELoss()

        dataloader = DataLoader(
            TensorDataset(torch.tensor(X, dtype=torch.float32)),
            batch_size=min(500, data_size),
            shuffle=True)

        for epoch in range(epochs):
            print('Epoch {} of {}'.format(epoch + 1, epochs))

            for data_batch in dataloader:
                data_batch = data_batch[0]
                batch_size = data_batch.size(0)

                ### Train Discriminator
                noise = torch.rand(batch_size, latent_size)
                generated_data = self.generator(noise)

                real_labels = torch.ones(batch_size, 1)
                fake_labels = torch.zeros(batch_size, 1)

                outputs_real = self.discriminator(data_batch)
                outputs_fake = self.discriminator(generated_data)

                d_loss_real = criterion(outputs_real, real_labels)
                d_loss_fake = criterion(outputs_fake, fake_labels)

                d_loss = d_loss_real + d_loss_fake

                optimizer_d.zero_grad()
                d_loss.backward()
                optimizer_d.step()

                self.train_history['discriminator_loss'].append(d_loss.item())
                
                trick_labels = torch.ones(batch_size, 1)

                if stop == 0:
                    ### Train Generator
                    g_loss = criterion(
                        self.discriminator(self.generator(noise)),
                        trick_labels)

                    optimizer_g.zero_grad()
                    g_loss.backward()
                    optimizer_g.step()

                    self.train_history['generator_loss'].append(g_loss.item())
                else:
                    g_loss = criterion(
                        self.discriminator(self.generator(noise)),
                        trick_labels)
                    self.train_history['generator_loss'].append(g_loss.item())


            if epoch + 1 > self.stop_epochs:
                stop = 1

        self.decision_scores_ = self.discriminator(
            torch.tensor(X, dtype=torch.float32)).detach().numpy().ravel()
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.so_gaal.SO_GAAL.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['discriminator'])
        X = check_array(X)
        pred_scores = self.discriminator(
            torch.tensor(X, dtype=torch.float32)).detach().numpy().ravel()
        return pred_scores






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 56. pyod.models.sod - pyod 2.0.5 documentation {#56-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:46

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.sod
```
### -*- coding: utf-8 -*-
"""Subspace Outlier Detection (SOD)
"""
### Author: Yahya Almardeny <almardeny@gmail.com>
### License: BSD 2 clause

importnumbaasnb
importnumpyasnp
fromsklearn.neighborsimport NearestNeighbors
fromsklearn.utilsimport check_array

from.baseimport BaseDetector
from..utils.utilityimport check_parameter


@nb.njit(parallel=True)
def_snn_imp(ind, ref_set_):
"""Internal function for fast snn calculation

    Parameters
    ----------
    ind : int
        Indices return by kNN.

    ref_set_ : int, optional (default=10)
        specifies the number of shared nearest neighbors to create the
        reference set. Note that ref_set must be smaller than n_neighbors.

    """
    n = ind.shape[0]
    _count = np.zeros(shape=(n, ref_set_), dtype=np.uint32)
    for i in nb.prange(n):
        temp = np.empty(n, dtype=np.uint32)
        test_element_set = set(ind[i])
        for j in nb.prange(n):
            temp[j] = len(set(ind[j]).intersection(test_element_set))
        temp[i] = np.iinfo(np.uint32).max
        _count[i] = np.argsort(temp)[::-1][1:ref_set_ + 1]

    return _count




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD)
classSOD(BaseDetector):
"""Subspace outlier detection (SOD) schema aims to detect outlier in
    varying subspaces of a high dimensional feature space. For each data
    object, SOD explores the axis-parallel subspace spanned by the data
    object's neighbors and determines how much the object deviates from the
    neighbors in this subspace.

    See :cite:`kriegel2009outlier` for details.

    Parameters
    ----------
    n_neighbors : int, optional (default=20)
        Number of neighbors to use by default for k neighbors queries.

    ref_set: int, optional (default=10)
        specifies the number of shared nearest neighbors to create the
        reference set. Note that ref_set must be smaller than n_neighbors.

    alpha: float in (0., 1.), optional (default=0.8)
           specifies the lower limit for selecting subspace.
           0.8 is set as default as suggested in the original paper.

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, n_neighbors=20, ref_set=10,
                 alpha=0.8):
        super(SOD, self).__init__(contamination=contamination)
        if isinstance(n_neighbors, int):
            check_parameter(n_neighbors, low=1, param_name='n_neighbors')
        else:
            raise ValueError(
                "n_neighbors should be int. Got %s" % type(n_neighbors))

        if isinstance(ref_set, int):
            check_parameter(ref_set, low=1, high=n_neighbors,
                            param_name='ref_set')
        else:
            raise ValueError("ref_set should be int. Got %s" % type(ref_set))

        if isinstance(alpha, float):
            check_parameter(alpha, low=0.0, high=1.0, param_name='alpha')
        else:
            raise ValueError("alpha should be float. Got %s" % type(alpha))

        self.n_neighbors = n_neighbors
        self.ref_set = ref_set
        self.alpha = alpha



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)
        self.decision_scores_ = self.decision_function(X)
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sod.SOD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.
        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        return self._sod(X)




    def_snn(self, X):
"""This function is called internally to calculate the shared nearest
        neighbors (SNN). SNN is reported to be more robust than k nearest
        neighbors.

        Returns
        -------
        snn_indices : numpy array of shape (n_shared_nearest_neighbors,)
            The indices of top k shared nearest neighbors for each observation.
        """
        knn = NearestNeighbors(n_neighbors=self.n_neighbors)
        knn.fit(X)
        ### Get the knn index
        ind = knn.kneighbors(return_distance=False)
        return _snn_imp(ind, self.ref_set)

    def_sod(self, X):
"""This function is called internally to perform subspace outlier 
        detection algorithm.
        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        ref_inds = self._snn(X)
        anomaly_scores = np.zeros(shape=(X.shape[0],))
        for i in range(X.shape[0]):
            obs = X[i]
            ref = X[ref_inds[i,],]
            means = np.mean(ref, axis=0)  ### mean of each column
            ### average squared distance of the reference to the mean
            var_total = np.sum(np.sum(np.square(ref - means))) / self.ref_set
            var_expect = self.alpha * var_total / X.shape[1]
            var_actual = np.var(ref, axis=0)  ### variance of each attribute
            var_inds = [1 if (j < var_expect) else 0 for j in var_actual]
            rel_dim = np.sum(var_inds)
            if rel_dim != 0:
                anomaly_scores[i] = np.sqrt(
                    np.dot(var_inds, np.square(obs - means)) / rel_dim)

        return anomaly_scores




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 57. pyod.models.sos - pyod 2.0.5 documentation {#57-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:13

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/sos.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.sos
```
### -*- coding: utf-8 -*-
"""Stochastic Outlier Selection (SOS).
Part of the codes are adapted from https://github.com/jeroenjanssens/scikit-sos
"""
### Author: Winston Li <jk_zhengli@hotmail.com>
### License: BSD 2 clause


importnumpyasnp
fromnumbaimport njit
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

from.baseimport BaseDetector


@njit
def_get_perplexity(D, beta):
"""Compute the perplexity and the A-row for a specific value of the
    precision of a Gaussian distribution.

    Parameters
    ----------
    D : array, shape (n_samples, )
        The dissimilarity matrix of the training samples.
    """

    A = np.exp(-D * beta)
    sumA = np.sum(A)
    H = np.log(sumA) + beta * np.sum(D * A) / sumA
    return H, A




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS)
classSOS(BaseDetector):
"""Stochastic Outlier Selection.
    SOS employs the concept of affinity to quantify
    the relationship from one data point to another data point. Affinity is 
    proportional to the similarity between two data points. So, a data point 
    has little affinity with a dissimilar data point. A data point is 
    selected as an outlier when all the other data points have insufficient
    affinity with it.
    Read more in the :cite:`janssens2012stochastic`.
    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1) 
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.
    perplexity : float, optional (default=4.5)
        A smooth measure of the effective number of neighbours. The perplexity
        parameter is similar to the parameter `k` in kNN algorithm (the number
        of nearest neighbors). The range of perplexity can be any real number
        between 1 and n-1, where `n` is the number of samples.

    metric: str, default 'euclidean'
        Metric used for the distance computation. Any metric from
        scipy.spatial.distance can be used.

        Valid values for metric are:

        - 'euclidean'
        - from scipy.spatial.distance: ['braycurtis', 'canberra',
          'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard',
          'kulsinski', 'mahalanobis', 'matching', 'minkowski',
          'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener',
          'sokalsneath', 'sqeuclidean', 'yule']

        See the documentation for scipy.spatial.distance for details on these
        metrics:
        http://docs.scipy.org/doc/scipy/reference/spatial.distance.html

    eps : float, optional (default = 1e-5)
        Tolerance threshold for floating point errors.
    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.
    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.
    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    Examples
    --------
    >>> from pyod.models.sos import SOS
    >>> from pyod.utils.data import generate_data
    >>> n_train = 50
    >>> n_test = 50
    >>> contamination = 0.1
    >>> X_train, y_train, X_test, y_test = generate_data(
    ...     n_train=n_train, n_test=n_test,
    ...     contamination=contamination, random_state=42)
    >>>
    >>> clf = SOS()
    >>> clf.fit(X_train)
    SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5)
    """

    def__init__(self, contamination=0.1, perplexity=4.5, metric='euclidean',
                 eps=1e-5):
        super(SOS, self).__init__(contamination=contamination)
        self.perplexity = perplexity
        self.metric = metric
        self.eps = eps

    def_x2d(self, X):
"""Computes the dissimilarity matrix of a given dataset.
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The query sample or samples to compute the dissimilarity matrix
            w.r.t. to the training samples.
        Returns
        -------
        D : array, shape (n_samples, )
            Returns the dissimilarity matrix.       
        """

        (n, d) = X.shape
        metric = self.metric.lower()
        if metric == 'none':
            if n != d:
                raise ValueError(
                    "If you specify 'none' as the metric, the data set "
                    "should be a square dissimilarity matrix")
            else:
                D = X
        elif metric == 'euclidean':
            sumX = np.sum(np.square(X), 1)

            ### np.abs protects against extremely small negative values
            ### that may arise due to floating point arithmetic errors
            D = np.sqrt(
                np.abs(np.add(np.add(-2 * np.dot(X, X.T), sumX).T, sumX)))
        else:
            try:
                fromscipy.spatialimport distance
            except ImportError as e:
                raise ImportError(
                    "Please install scipy if you wish to use a metric "
                    "other than 'euclidean' or 'none'")
            else:
                D = distance.squareform(distance.pdist(X, metric))
        return D

    def_d2a(self, D):
"""Performs a binary search to get affinities in such a way that each
        conditional Gaussian has the same perplexity. Then returns the
        affinities matrix.
        Parameters
        ----------
        D : array, shape (n_samples, )
            The dissimilarity matrix of the training samples.
        Returns
        -------
        A : array, shape (n_samples, )
            Returns the affinity matrix.       
        """

        (n, _) = D.shape
        A = np.zeros((n, n))
        beta = np.ones((n, 1))
        logU = np.log(self.perplexity)

        for i in range(n):
            ### Compute the Gaussian kernel and entropy for the current precision
            betamin = -np.inf
            betamax = np.inf
            Di = D[i, np.concatenate((np.r_[0:i], np.r_[i + 1:n]))]
            (H, thisA) = _get_perplexity(Di, beta[i])

            ### Evaluate whether the perplexity is within tolerance
            Hdiff = H - logU
            tries = 0
            while (np.isnan(Hdiff) or np.abs(
                    Hdiff) > self.eps) and tries < 5000:
                if np.isnan(Hdiff):
                    beta[i] = beta[i] / 10.0
                ### If not, increase or decrease precision
                elif Hdiff > 0:
                    betamin = beta[i].copy()
                    if betamax == np.inf or betamax == -np.inf:
                        beta[i] = beta[i] * 2.0
                    else:
                        beta[i] = (beta[i] + betamax) / 2.0
                else:
                    betamax = beta[i].copy()
                    if betamin == np.inf or betamin == -np.inf:
                        beta[i] = beta[i] / 2.0
                    else:
                        beta[i] = (beta[i] + betamin) / 2.0
                ### Recompute the values
                (H, thisA) = _get_perplexity(Di, beta[i])
                Hdiff = H - logU
                tries += 1

            ### Set the final row of A
            A[i, np.concatenate((np.r_[0:i], np.r_[i + 1:n]))] = thisA

        return A

    def_a2b(self, A):
"""Computes the binding probabilities of a given affinity
        matrix.
        Parameters
        ----------
        A : array, shape (n_samples, )
            The affinities matrix.
        Returns
        -------
        B : array, shape (n_samples, )
            Returns the matrix of binding probabilities.       
        """

        B = A / A.sum(axis=1)[:, np.newaxis]
        return B

    def_b2o(self, B):
"""Computes the binding probabilities of a given affinity
        matrix.
        Parameters
        ----------
        A : array, shape (n_samples, )
            The affinities matrix.
        Returns
        -------
        B : array, shape (n_samples, )
            Returns the matrix of binding probabilities.       
        """
        O = np.prod(1 - B, 0)
        return O



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        X = check_array(X)
        self._set_n_classes(y)
        D = self._x2d(X)
        A = self._d2a(D)
        B = self._a2b(A)
        O = self._b2o(B)
        ### Invert decision_scores_. Outliers comes with higher outlier scores
        self.decision_scores_ = O
        self._process_decision_scores()
        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.sos.SOS.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])
        X = check_array(X)
        D = self._x2d(X)
        A = self._d2a(D)
        B = self._a2b(A)
        O = self._b2o(B)
        return O






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 58. pyod.models.suod - pyod 2.0.5 documentation {#58-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:42

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/suod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.suod
```
### -*- coding: utf-8 -*-
"""SUOD
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_is_fitted

try:
    importsuod
except ImportError:
    print('please install suod first for SUOD by `pip install suod`')
fromsuod.models.baseimport SUOD as SUOD_model

from.baseimport BaseDetector
from.lofimport LOF
from.hbosimport HBOS
from.iforestimport IForest
from.copodimport COPOD
from.combinationimport average, maximization
from..utils.utilityimport standardizer




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD)
classSUOD(BaseDetector):
    ### noinspection PyPep8
"""SUOD (Scalable Unsupervised Outlier Detection) is an acceleration
    framework for large scale unsupervised outlier detector training and
    prediction. See :cite:`zhao2021suod` for details.

    Parameters
    ----------
    base_estimators : list, length must be greater than 1
        A list of base estimators. Certain methods must be present, e.g.,
        `fit` and `predict`.

    combination : str, optional (default='average')
        Decide how to aggregate the results from multiple models:

        - "average" : average the results from all base detectors
        - "maximization" : output the max value across all base detectors

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    n_jobs : optional (default=1)
        The number of jobs to run in parallel for both `fit` and
        `predict`. If -1, then the number of jobs is set to the
        the number of jobs that can actually run in parallel.

    rp_clf_list : list, optional (default=None)
        The list of outlier detection models to use random projection. The
        detector name should be consistent with PyOD.

    rp_ng_clf_list : list, optional (default=None)
        The list of outlier detection models NOT to use random projection. The
        detector name should be consistent with PyOD.

    rp_flag_global : bool, optional (default=True)
        If set to False, random projection is turned off for all base models.

    target_dim_frac : float in (0., 1), optional (default=0.5)
        The target compression ratio.

    jl_method : string, optional (default = 'basic')
        The JL projection method:

        - "basic": each component of the transformation matrix is taken at
          random in N(0,1).
        - "discrete", each component of the transformation matrix is taken at
          random in {-1,1}.
        - "circulant": the first row of the transformation matrix is taken at
          random in N(0,1), and each row is obtained from the previous one
          by a one-left shift.
        - "toeplitz": the first row and column of the transformation matrix
          is taken at random in N(0,1), and each diagonal has a constant value
          taken from these first vector.

    bps_flag : bool, optional (default=True)
        If set to False, balanced parallel scheduling is turned off.

    approx_clf_list : list, optional (default=None)
        The list of outlier detection models to use pseudo-supervised
        approximation. The detector name should be consistent with PyOD.

    approx_ng_clf_list : list, optional (default=None)
        The list of outlier detection models NOT to use pseudo-supervised
        approximation. The detector name should be consistent with PyOD.

    approx_flag_global : bool, optional (default=True)
        If set to False, pseudo-supervised approximation is turned off.

    approx_clf : object, optional (default: sklearn RandomForestRegressor)
        The supervised model used to approximate unsupervised models.

    cost_forecast_loc_fit : str, optional
        The location of the pretrained cost prediction forecast for training.

    cost_forecast_loc_pred : str, optional
        The location of the pretrained cost prediction forecast for prediction.

    verbose : int, optional (default=0)
        Controls the verbosity of the building process.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, base_estimators=None, contamination=0.1,
                 combination='average', n_jobs=None,
                 rp_clf_list=None, rp_ng_clf_list=None, rp_flag_global=True,
                 target_dim_frac=0.5, jl_method='basic', bps_flag=True,
                 approx_clf_list=None, approx_ng_clf_list=None,
                 approx_flag_global=True, approx_clf=None,
                 verbose=False):
        super(SUOD, self).__init__(contamination=contamination)
        self.base_estimators = base_estimators
        self.contamination = contamination
        self.combination = combination
        self.n_jobs = n_jobs
        self.rp_clf_list = rp_clf_list
        self.rp_ng_clf_list = rp_ng_clf_list
        self.rp_flag_global = rp_flag_global
        self.target_dim_frac = target_dim_frac
        self.jl_method = jl_method
        self.bps_flag = bps_flag
        self.approx_clf_list = approx_clf_list
        self.approx_ng_clf_list = approx_ng_clf_list
        self.approx_flag_global = approx_flag_global
        self.approx_clf = approx_clf
        self.verbose = verbose

        ### by default we will provide a group of performing models
        if self.base_estimators is None:
            self.base_estimators = [LOF(n_neighbors=15), LOF(n_neighbors=20),
                                    HBOS(n_bins=10), HBOS(n_bins=20),
                                    COPOD(), IForest(n_estimators=50),
                                    IForest(n_estimators=100),
                                    IForest(n_estimators=150)]

        self.n_estimators = len(self.base_estimators)

        ### pass in the arguments for SUOD model
        self.model_ = SUOD_model(
            base_estimators=self.base_estimators,
            contamination=self.contamination,
            n_jobs=self.n_jobs,
            rp_clf_list=self.rp_clf_list,
            rp_ng_clf_list=self.rp_ng_clf_list,
            rp_flag_global=self.rp_flag_global,
            target_dim_frac=self.target_dim_frac,
            jl_method=self.jl_method,
            approx_clf_list=self.approx_clf_list,
            approx_ng_clf_list=self.approx_ng_clf_list,
            approx_flag_global=self.approx_flag_global,
            approx_clf=self.approx_clf,
            bps_flag=self.bps_flag,
            verbose=self.verbose,
        )



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.fit)
    deffit(self, X, y=None):
"""Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

        ### validate inputs X and y (optional)
        X = check_array(X)
        n_samples, n_features = X.shape[0], X.shape[1]
        self._set_n_classes(y)

        ### fit the model and then approximate it
        self.model_.fit(X)
        self.model_.approximate(X)

        ### get the decision scores from each base estimators
        decision_score_mat = np.zeros([n_samples, self.n_estimators])
        for i in range(self.n_estimators):
            decision_score_mat[:, i] = self.model_.base_estimators[
                i].decision_scores_

        ### the scores must be standardized before combination
        decision_score_mat, self.score_scalar_ = standardizer(
            decision_score_mat, keep_scalar=True)

        ### todo: may support other combination
        if self.combination == 'average':
            decision_score = average(decision_score_mat)
        else:
            decision_score = maximization(decision_score_mat)

        assert (len(decision_score) == n_samples)

        self.decision_scores_ = decision_score.ravel()
        self._process_decision_scores()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.suod.SUOD.decision_function)
    defdecision_function(self, X):
"""Predict raw anomaly score of X using the fitted detectors.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['model_', 'decision_scores_',
                               'threshold_', 'labels_'])

        X = check_array(X)

        ### initialize the output score
        predicted_scores = self.model_.decision_function(X)

        ### standardize the score and combine
        predicted_scores = self.score_scalar_.transform(predicted_scores)

        ### todo: may support other combination
        if self.combination == 'average':
            decision_score = average(predicted_scores)
        else:
            decision_score = maximization(predicted_scores)

        assert (len(decision_score) == X.shape[0])

        return decision_score.ravel()






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 59. pyod.models.thresholds - pyod 2.0.5 documentation {#59-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:53

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/thresholds.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.thresholds
```


[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.AUCP)
defAUCP(**kwargs):
"""AUCP class for Area Under Curve Precentage thresholder.

       Use the area under the curve to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond where the auc of the kde is less
       than the (mean + abs(mean-median)) percent of the total kde auc.
    """

    frompythresh.thresholds.aucpimport AUCP as AUCP_thres
    return AUCP_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.BOOT)
defBOOT(**kwargs):
"""BOOT class for Bootstrapping thresholder.

       Use a boostrapping based method to find a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the mean of the confidence intervals.
       Parameters
       ----------
       random_state : int, optional (default=1234)
            Random seed for bootstrapping a confidence interval. Can also be set to None.

    """

    frompythresh.thresholds.bootimport BOOT as BOOT_thres
    return BOOT_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CHAU)
defCHAU(**kwargs):
"""CHAU class for Chauvenet's criterion thresholder.

       Use the Chauvenet's criterion to evaluate a non-parametric
       means to threshold scores generated by the decision_scores
       where outliers are set to any value below the Chauvenet's
       criterion.

       Parameters
       ----------

       method : {'mean', 'median', 'gmean'}, optional (default='mean')
            Calculate the area normal to distance using a scaler
            - 'mean':  Construct a scaler with the mean of the scores
            - 'median: Construct a scaler with the median of the scores
            - 'gmean': Construct a scaler with the geometric mean of the scores
    """

    frompythresh.thresholds.chauimport CHAU as CHAU_thres
    return CHAU_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLF)
defCLF(**kwargs):
"""CLF class for Trained Classifier thresholder.

       Use the trained linear classifier to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond 0.

       Parameters
       ----------

       method : {'simple', 'complex'}, optional (default='complex')
            Type of linear model

            - 'simple':  Uses only the scores
            - 'complex': Uses the scores, log of the scores, and the scores' PDF
    """

    frompythresh.thresholds.clfimport CLF as CLF_thres
    return CLF_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CLUST)
defCLUST(**kwargs):
"""CLUST class for clustering type thresholders.

       Use the clustering methods to evaluate a non-parametric means to
       threshold scores generated by the decision_scores where outliers
       are set to any value not labelled as part of the main cluster.

       Parameters
       ----------
       method : {'agg', 'birch', 'bang', 'bgm', 'bsas', 'dbscan', 'ema', 'kmeans', 'mbsas', 'mshift', 'optics', 'somsc', 'spec', 'xmeans'}, optional (default='spec')
            Clustering method
            - 'agg':    Agglomerative
            - 'birch':  Balanced Iterative Reducing and Clustering using Hierarchies
            - 'bang':   BANG
            - 'bgm':    Bayesian Gaussian Mixture
            - 'bsas':   Basic Sequential Algorithmic Scheme
            - 'dbscan': Density-based spatial clustering of applications with noise
            - 'ema':    Expectation-Maximization clustering algorithm for Gaussian Mixture Model
            - 'kmeans': K-means
            - 'mbsas':  Modified Basic Sequential Algorithmic Scheme
            - 'mshift': Mean shift
            - 'optics': Ordering Points To Identify Clustering Structure
            - 'somsc':  Self-organized feature map 
            - 'spec':   Clustering to a projection of the normalized Laplacian 
            - 'xmeans': X-means
       random_state : int, optional (default=1234)
            Random seed for the BayesianGaussianMixture clustering (method='bgm'). Can
            also be set to None.
    """

    frompythresh.thresholds.clustimport CLUST as CLUST_thres
    return CLUST_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.CPD)
defCPD(**kwargs):
"""CPD class for Change Point Detection thresholder.

       Use change point detection to find a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the detected change point.

       Parameters
       ----------

       method : {'Dynp', 'KernelCPD', 'Binseg', 'BottomUp'}, optional (default='Dynp')
            Method for change point detection

            - 'Dynp':      Dynamic programming (optimal minimum sum of errors per partition)
            - 'KernelCPD': RBF kernel function (optimal minimum sum of errors per partition)
            - 'Binseg':    Binary segmentation
            - 'BottomUp':  Bottom-up segmentation

       transform : {'cdf', 'kde'}, optional (default='cdf')
            Data transformation method prior to fit

            - 'cdf': Use the cumulative distribution function
            - 'kde': Use the kernel density estimation
    """

    frompythresh.thresholds.cpdimport CPD as CPD_thres
    return CPD_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DECOMP)
defDECOMP(**kwargs):
"""DECOMP class for Decomposition based thresholders.

       Use decomposition to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the maximum of the decomposed
       matrix that results from decomposing the cumulative distribution
       function of the decision scores.
       Parameters
       ----------

       method : {'NMF', 'PCA', 'GRP', 'SRP'}, optional (default='PCA')
            Method to use for decomposition

            - 'NMF':  Non-Negative Matrix Factorization
            - 'PCA':  Principal Component Analysis
            - 'GRP':  Gaussian Random Projection
            - 'SRP':  Sparse Random Projection
       random_state : int, optional (default=1234)
            Random seed for the decomposition algorithm. Can also be set to None.

    """

    frompythresh.thresholds.decompimport DECOMP as DECOMP_thres
    return DECOMP_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.DSN)
defDSN(**kwargs):
"""DSN class for Distance Shift from Normal thresholder.

       Use the distance shift from normal to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the distance calculated by the selected
       metric.
       Parameters
       ----------

       metric : {'JS', 'WS', 'ENG', 'BHT', 'HLL', 'HI', 'LK', 'LP', 'MAH', 'TMT', 'RES', 'KS', 'INT', 'MMD'}, optional (default='MAH')
            Metric to use for distance computation
            - 'JS':  Jensen-Shannon distance
            - 'WS':  Wasserstein or Earth Movers distance
            - 'ENG': Energy distance
            - 'BHT': Bhattacharyya distance
            - 'HLL': Hellinger distance
            - 'HI':  Histogram intersection distance
            - 'LK':  Lukaszyk-Karmowski metric for normal distributions
            - 'LP':  Levy-Prokhorov metric
            - 'MAH': Mahalanobis distance
            - 'TMT': Tanimoto distance
            - 'RES': Studentized residual distance
            - 'KS':  Kolmogorov-Smirnov distance
            - 'INT': Weighted spline interpolated distance
            - 'MMD': Maximum Mean Discrepancy distance
       random_state : int, optional (default=1234)
            Random seed for the normal distribution. Can also be set to None.

    """

    frompythresh.thresholds.dsnimport DSN as DSN_thres
    return DSN_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.EB)
defEB(**kwargs):
"""EB class for Elliptical Boundary thresholder.

       Use pseudo-random elliptical boundaries to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond a pseudo-random elliptical boundary set
       between inliers and outliers.
    """

    frompythresh.thresholds.ebimport EB as EB_thres
    return EB_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FGD)
defFGD(**kwargs):
"""FGD class for Fixed Gradient Descent thresholder.

       Use the fixed gradient descent to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond where the first derivative of the kde
       with respect to the decision scores passes the mean of the first 
       and second inflection points.
    """

    frompythresh.thresholds.fgdimport FGD as FGD_thres
    return FGD_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FILTER)
defFILTER(**kwargs):
"""FILTER class for Filtering based thresholders.

       Use the filtering based methods to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the maximum filter value.
       See :cite:`hashemi2019filter` for details.
       Parameters
       ----------

       method : {'gaussian', 'savgol', 'hilbert', 'wiener', 'medfilt', 'decimate','detrend', 'resample'}, optional (default='savgol')
            Method to filter the scores
            - 'gaussian': use a gaussian based filter
            - 'savgol':   use the savgol based filter
            - 'hilbert':  use the hilbert based filter
            - 'wiener':   use the wiener based filter
            - 'medfilt:   use a median based filter
            - 'decimate': use a decimate based filter
            - 'detrend':  use a detrend based filter
            - 'resample': use a resampling based filter
       sigma : int, optional (default='auto') 
            Variable specific to each filter type, default sets sigma to len(scores)*np.std(scores)
            - 'gaussian': standard deviation for Gaussian kernel
            - 'savgol':   savgol filter window size
            - 'hilbert':  number of Fourier components
            - 'medfilt:   kernel size
            - 'decimate': downsampling factor
            - 'detrend':  number of break points
            - 'resample': resampling window size
    """

    frompythresh.thresholds.filterimport FILTER as FILTER_thres
    return FILTER_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.FWFM)
defFWFM(**kwargs):
"""FWFM class for Full Width at Full Minimum thresholder.

       Use the full width at full minimum (aka base width) to evaluate
       a non-parametric means to threshold scores generated by the
       decision_scores where outliers are set to any value beyond the base
       width.
    """

    frompythresh.thresholds.fwfmimport FWFM as FWFM_thres
    return FWFM_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.GESD)
defGESD(**kwargs):
"""GESD class for Generalized Extreme Studentized Deviate thresholder.

       Use the generalized extreme studentized deviate to evaluate a
       non-parametric means to threshold scores generated by the decision_scores
       where outliers are set to any less than the smallest detected outlier.
       Parameters
       ----------

       max_outliers : int, optional (default='auto')
            mamiximum number of outliers that the dataset may have. Default sets 
            max_outliers to be half the size of the dataset

       alpha : float, optional (default=0.05)
            significance level
    """

    frompythresh.thresholds.gesdimport GESD as GESD_thres
    return GESD_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.HIST)
defHIST(**kwargs):
"""HIST class for Histogram based thresholders.

       Use histograms methods as described in scikit-image.filters to
       evaluate a non-parametric means to threshold scores generated by
       the decision_scores where outliers are set by histogram generated
       thresholds depending on the selected methods. 
       Parameters
       ----------
       nbins : int, optional (default='auto')
            Number of bins to use in the hostogram, default set to int(len(scores)**0.7)
       method : {'otsu', 'yen', 'isodata', 'li', 'minimum', 'triangle'}, optional (default='triangle')
            Histogram filtering based method
            - 'otsu':     OTSU's method for filtering
            - 'yen':      Yen's method for filtering
            - 'isodata':  Ridler-Calvard or inter-means method for filtering
            - 'li':       Li's iterative Minimum Cross Entropy method for filtering
            - 'minimum':  Minimum between two maxima via smoothing method for filtering
            - 'triangle': Triangle algorithm method for filtering
    """

    frompythresh.thresholds.histimport HIST as HIST_thres
    return HIST_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.IQR)
defIQR(**kwargs):
"""IQR class for Inter-Qaurtile Region thresholder.

       Use the inter-quartile region to evaluate a non-parametric
       means to threshold scores generated by the decision_scores
       where outliers are set to any value beyond the third quartile
       plus 1.5 times the inter-quartile region.
    """

    frompythresh.thresholds.iqrimport IQR as IQR_thres
    return IQR_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.KARCH)
defKARCH(**kwargs):
"""KARCH class for Riemannian Center of Mass thresholder.

       Use the Karcher mean (Riemannian Center of Mass) to evaluate a
       non-parametric means to threshold scores generated by the
       decision_scores where outliers are set to any value beyond the
       Karcher mean + one standard deviation of the decision_scores.
       Parameters
       ----------

       ndim : int, optional (default=2)
            Number of dimensions to construct the Euclidean manifold

       method : {'simple', 'complex'}, optional (default='complex')
            Method for computing the Karcher mean
            - 'simple':  Compute the Karcher mean using the 1D array of scores
            - 'complex': Compute the Karcher mean between a 2D array dot product of the scores and the sorted scores arrays
    """

    frompythresh.thresholds.karchimport KARCH as KARCH_thres
    return KARCH_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MAD)
defMAD(**kwargs):
"""MAD class for Median Absolute Deviation thresholder.

       Use the median absolute deviation to evaluate a non-parametric
       means to threshold scores generated by the decision_scores
       where outliers are set to any value beyond the mean plus the 
       median absolute deviation over the standard deviation.
    """

    frompythresh.thresholds.madimport MAD as MAD_thres
    return MAD_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MCST)
defMCST(**kwargs):
"""MCST class for Monte Carlo Shapiro Tests thresholder.

       Use uniform random sampling and statstical testing to evaluate a
       non-parametric means to threshold scores generated by the decision_scores
       where outliers are set to any value beyond the minimum value left after
       iterative Shapiro-Wilk tests have occured. Note** accuracy decreases with
       array size. For good results the should be array<1000. However still this
       threshold method may fail at any array size.
       Parameters
       ----------

       random_state : int, optional (default=1234)
            Random seed for the uniform distribution. Can also be set to None.

    """

    frompythresh.thresholds.mcstimport MCST as MCST_thres
    return MCST_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.META)
defMETA(**kwargs):
"""META class for Meta-modelling thresholder.

       Use a trained meta-model to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set based on the trained meta-model classifier. 
       Parameters
       ----------

       method : {'LIN', 'GNB', 'GNBC', 'GNBM'}, optional (default='GNBM')
           select

           - 'LIN':  RidgeCV trained linear classifier meta-model on true labels
           - 'GNB':  Gaussian Naive Bayes trained classifier meta-model on true labels
           - 'GNBC': Gaussian Naive Bayes trained classifier meta-model on best contamination
           - 'GNBM': Gaussian Naive Bayes multivariate trained classifier meta-model

    """

    frompythresh.thresholds.metaimport META as META_thres
    return META_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MOLL)
defMOLL(**kwargs):
"""MOLL class for Friedrichs' mollifier thresholder.

       Use the Friedrichs' mollifier to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond one minus the  maximum of the smoothed
       dataset via convolution.
    """

    frompythresh.thresholds.mollimport MOLL as MOLL_thres
    return MOLL_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.MTT)
defMTT(**kwargs):
"""MTT class for Modified Thompson Tau test thresholder.

       Use the modified Thompson Tau test to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the smallest outlier detected by the test.
       Parameters
       ----------

       strictness : [1,2,3,4,5], optional (default=4)
            Level of strictness corresponding to the t-Student distribution map to sample
    """
    frompythresh.thresholds.mttimport MTT as MTT_thres
    return MTT_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.OCSVM)
defOCSVM(**kwargs):
"""OCSVM class for One-Class Support Vector Machine thresholder.

       Use a one-class svm to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are determined by the one-class svm using a polynomial kernel
       with the polynomial degree either set or determined by regression
       internally.
       Parameters
       ----------
       model : {'poly', 'sgd'}, optional (default='sgd')
           OCSVM model to apply

           - 'poly':  Use a polynomial kernel with a regular OCSVM
           - 'sgd':   Used the Additive Chi2 kernel approximation with a SGDOneClassSVM

       degree : int, optional (default='auto')
           Polynomial degree to use for the one-class svm.
           Default 'auto' finds the optimal degree with linear regression

       gamma : float, optional (default='auto')
           Kernel coefficient for polynomial fit for the one-class svm.
           Default 'auto' uses 1 / n_features

       criterion : {'aic', 'bic'}, optional (default='bic')
           regression performance metric. AIC is the Akaike Information Criterion,
           and BIC is the Bayesian Information Criterion. This only applies
           when degree is set to 'auto'

       nu : float, optional (default='auto')
           An upper bound on the fraction of training errors and a lower bound
           of the fraction of support vectors. Default 'auto' sets nu as the ratio
           between the any point that is less than or equal to the median plus
           the absolute difference between the mean and geometric mean over the
           the number of points in the entire dataset 

       tol : float, optional (default=1e-3)
           The stopping criterion for the one-class svm
       random_state : int, optional (default=1234)
           Random seed for the SVM's data sampling. Can also be set to None.

    """

    frompythresh.thresholds.ocsvmimport OCSVM as OCSVM_thres
    return OCSVM_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.QMCD)
defQMCD(**kwargs):
"""QMCD class for Quasi-Monte Carlo Discreprancy thresholder.

       Use the quasi-Monte Carlo discreprancy to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond and percentile or quantile of one minus the
       descreperancy (Note** A discrepancy quantifies the distance between the
       continuous uniform distribution on a hypercube and the discrete uniform
       distribution on distinct sample points).
       Parameters
       ----------

       method : {'CD', 'WD', 'MD', 'L2-star'}, optional (default='WD')
            Type of discrepancy
            - 'CD':      Centered Discrepancy
            - 'WD':      Wrap-around Discrepancy
            - 'MD':      Mix between CD/WD
            - 'L2-star': L2-star discrepancy

       lim : {'Q', 'P'}, optional (default='P')
            Filtering method to threshold scores using 1 - discrepancy
            - 'Q': Use quntile limiting
            - 'P': Use percentile limiting
    """

    frompythresh.thresholds.qmcdimport QMCD as QMCD_thres
    return QMCD_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.REGR)
defREGR(**kwargs):
"""REGR class for Regression based thresholder.

       Use the regression to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the y-intercept value of the linear fit.
       Parameters
       ----------

       method : {'siegel', 'theil'}, optional (default='siegel')
            Regression based method to calculate the y-intercept
            - 'siegel': implements a method for robust linear regression using repeated medians
            - 'theil':  implements a method for robust linear regression using paired values
       random_state : int, optional (default=1234)
            random seed for the normal distribution. Can also be set to None

    """

    frompythresh.thresholds.regrimport REGR as REGR_thres
    return REGR_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.VAE)
defVAE(**kwargs):
"""VAE class for Variational AutoEncoder thresholder.

       Use a VAE to evaluate a non-parametric means
       to threshold scores generated by the decision_scores where outliers
       are set to any value beyond the maximum minus the minimum of the
       reconstructed distribution probabilities after encoding.

       Parameters
       ----------

       verbose : bool, optional (default=False)
            display training progress

       device : str, optional (default='cpu')
            device for pytorch

       latent_dims : int, optional (default='auto')
            number of latent dimensions the encoder will map the scores to.
            Default 'auto' applies automatic dimensionality selection using
            a profile likelihood.

       random_state : int, optional (default=1234)
            random seed for the normal distribution. Can also be set to None

       epochs : int, optional (default=100)
            number of epochs to train the VAE

       batch_size : int, optional (default=64)
            batch size for the dataloader during training

       loss : str, optional (default='kl')
            Loss function during training

            - 'kl' : use the combined negative log likelihood and Kullback-Leibler divergence
            - 'mmd': use the combined negative log likelihood and maximum mean discrepancy

       Attributes
       ----------

       thresh_ : threshold value that separates inliers from outliers
    """

    frompythresh.thresholds.vaeimport VAE as VAE_thres
    return VAE_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.WIND)
defWIND(**kwargs):
"""WIND class for topological Winding number thresholder.

       Use the topological winding number (with respect to the origin) to
       evaluate a non-parametric means to threshold scores generated by
       the decision_scores where outliers are set to any value beyond the
       mean intersection point calculated from the winding number.
       Parameters
       ----------

       random_state : int, optional (default=1234)
            Random seed for the normal distribution. Can also be set to None.

    """

    frompythresh.thresholds.windimport WIND as WIND_thres
    return WIND_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.YJ)
defYJ(**kwargs):
"""YJ class for Yeo-Johnson transformation thresholder.

       Use the Yeo-Johnson transformation to evaluate
       a non-parametric means to threshold scores generated by the
       decision_scores where outliers are set to any value beyond the
       max value in the YJ transformed data.
    """

    frompythresh.thresholds.yjimport YJ as YJ_thres
    return YJ_thres(**kwargs)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.thresholds.ZSCORE)
defZSCORE(**kwargs):
"""ZSCORE class for ZSCORE thresholder.

       Use the zscore to evaluate a non-parametric means to threshold
       scores generated by the decision_scores where outliers are set
       to any value beyond a zscore of one.
    """

    frompythresh.thresholds.zscoreimport ZSCORE as ZSCORE_thres
    return ZSCORE_thres(**kwargs)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 60. pyod.models.vae - pyod 2.0.5 documentation {#60-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:38

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/vae.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.vae
```
### -*- coding: utf-8 -*-

"""Variational Auto Encoder (VAE)
and beta-VAE for Unsupervised Outlier Detection

Reference:
        :cite:`kingma2013auto` Kingma, Diederik, Welling
        'Auto-Encodeing Variational Bayes'
        https://arxiv.org/abs/1312.6114
        :cite:`burgess2018understanding` Burges et al
        'Understanding disentangling in beta-VAE'
        https://arxiv.org/pdf/1804.03599.pdf
"""

### Author: Tiankai Yang <tiankaiy@usc.edu>
### License: BSD 2 clause


try:
    importtorch
except ImportError:
    print('please install torch first')

importtorch
fromtorchimport nn

from.base_dlimport BaseDeepLearningDetector
from..utils.stat_modelsimport pairwise_distances_no_broadcast
from..utils.torch_utilityimport LinearBlock, get_criterion_by_name


defvae_loss(x, x_recon, z_mu, z_logvar, beta=1.0, capacity=0.0):
"""Compute the loss of VAE

    Parameters
    ----------
    x : torch.Tensor, shape (n_samples, n_features)
        The input data.

    x_recon : torch.Tensor, shape (n_samples, n_features)
        The reconstructed data.

    z_mu : torch.Tensor, shape (n_samples, latent_dim)
        The mean of the latent distribution.

    z_logvar : torch.Tensor, shape (n_samples, latent_dim)
        The log variance of the latent distribution.

    beta : float, optional (default=1.0)
        The weight of KL divergence.

    capacity : float, optional (default=0.0)
        The maximum capacity of a loss bottleneck.

    Returns
    -------
    loss : torch.Tensor, shape (n_samples,)
        The loss of VAE.
    """
    ### Reconstruction loss
    recon_loss = get_criterion_by_name('mse')(x_recon, x)

    ### KL divergence
    kl_loss = torch.mean(
        -0.5 * torch.sum(1 + z_logvar - z_mu ** 2 - torch.exp(z_logvar),
                         dim=1), dim=0)
    kl_loss = torch.clamp(kl_loss, min=0, max=capacity)

    return recon_loss + beta * kl_loss




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE)
classVAE(BaseDeepLearningDetector):
""" Variational auto encoder
    Encoder maps X onto a latent space Z
    Decoder samples Z from N(0,1)
    VAE_loss = Reconstruction_loss + KL_loss

    Reference
    See :cite:`kingma2013auto` Kingma, Diederik, Welling
    'Auto-Encodeing Variational Bayes'
    https://arxiv.org/abs/1312.6114 for details.

    beta VAE
    In Loss, the emphasis is on KL_loss
    and capacity of a bottleneck:
    VAE_loss = Reconstruction_loss + beta * KL_loss

    Reference
    See :cite:`burgess2018understanding` Burges et al
    'Understanding disentangling in beta-VAE'
    https://arxiv.org/pdf/1804.03599.pdf for details.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, 
        i.e. the proportion of outliers in the data set. 
        Used when fitting to define the threshold on the decision function.

    preprocessing : bool, optional (default=True)
        If True, apply the preprocessing procedure before training models.

    lr : float, optional (default=1e-3)
        The initial learning rate for the optimizer.

    epoch_num : int, optional (default=30)
        The number of epochs for training.

    batch_size : int, optional (default=32)
        The batch size for training.

    optimizer_name : str, optional (default='adam')
        The name of theoptimizer used to train the model.

    device : str, optional (default=None)
        The device to use for the model. If None, it will be decided
        automatically. If you want to use MPS, set it to 'mps'.

    random_state : int, optional (default=42)
        The random seed for reproducibility.

    use_compile : bool, optional (default=False)
        Whether to compile the model.
        If True, the model will be compiled before training.
        This is only available for
        PyTorch version >= 2.0.0. and Python < 3.12.

    compile_mode : str, optional (default='default')
        The mode to compile the model.
        Can be either ‚Äúdefault‚Äù, ‚Äúreduce-overhead‚Äù,
        ‚Äúmax-autotune‚Äù or ‚Äúmax-autotune-no-cudagraphs‚Äù.
        See https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile for details.

    verbose : int, optional (default=1)
        Verbosity mode.
        - 0 = silent
        - 1 = progress bar
        - 2 = one line per epoch.

    optimizer_params : dict, optional (default={'weight_decay': 1e-5})
        Additional parameters for the optimizer.
        For example, `optimizer_params={'weight_decay': 1e-5}`.

    beta : float, optional (default=1.0)
        Coefficient of beta VAE. The weight of KL divergence.
        Default is regular VAE.

    capacity : float, optional (default=0.0)
        The maximum capacity of a loss bottleneck.

    encoder_neuron_list : list, optional (default=[128, 64, 32])
        The number of neurons per hidden layers in encoder.
        So the encoder has the structure as [feature_size, 128, 64, 32, latent_dim].

    decoder_neuron_list : list, optional (default=[32, 64, 128])
        The number of neurons per hidden layers in decoder.
        So the decoder has the structure as [latent_dim, 32, 64, 128, feature_size].

    latent_dim : int, optional (default=2)
        The dimension of latent space.

    hidden_activation_name : str, optional (default='relu')
        The activation function used in hidden layers.

    output_activation_name : str, optional (default='sigmoid')
        The activation function used in output layer.

    batch_norm : boolean, optional (default=False)
        Whether to apply Batch Normalization,
        See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    Attributes
    ----------
    model : torch.nn.Module
        The underlying VAE model.

    optimizer : torch.optim
        The optimizer used to train the model.

    criterion : python function
        The loss function used to train the model.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def__init__(self, contamination=0.1, preprocessing=True,
                 lr=1e-3, epoch_num=30, batch_size=32,
                 optimizer_name='adam',
                 device=None, random_state=42,
                 use_compile=False, compile_mode='default',
                 verbose=1,
                 optimizer_params: dict = {'weight_decay': 1e-5},
                 beta=1.0, capacity=0.0,
                 encoder_neuron_list=[128, 64, 32],
                 decoder_neuron_list=[32, 64, 128],
                 latent_dim=2,
                 hidden_activation_name='relu',
                 output_activation_name='sigmoid',
                 batch_norm=False, dropout_rate=0.2):
        super(VAE, self).__init__(contamination=contamination,
                                  preprocessing=preprocessing,
                                  lr=lr, epoch_num=epoch_num,
                                  batch_size=batch_size,
                                  optimizer_name=optimizer_name,
                                  loss_func=vae_loss,
                                  device=device, random_state=random_state,
                                  use_compile=use_compile,
                                  compile_mode=compile_mode,
                                  verbose=verbose,
                                  optimizer_params=optimizer_params)
        self.beta = beta
        self.capacity = capacity
        self.encoder_neuron_list = encoder_neuron_list
        self.decoder_neuron_list = decoder_neuron_list
        self.latent_dim = latent_dim
        self.hidden_activation_name = hidden_activation_name
        self.output_activation_name = output_activation_name
        self.batch_norm = batch_norm
        self.dropout_rate = dropout_rate



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.build_model)
    defbuild_model(self):
        self.model = VAEModel(self.feature_size,
                              encoder_neuron_list=self.encoder_neuron_list,
                              decoder_neuron_list=self.decoder_neuron_list,
                              latent_dim=self.latent_dim,
                              hidden_activation_name=self.hidden_activation_name,
                              output_activation_name=self.output_activation_name,
                              batch_norm=self.batch_norm,
                              dropout_rate=self.dropout_rate)






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.training_forward)
    deftraining_forward(self, batch_data):
        x = batch_data
        x = x.to(self.device)
        self.optimizer.zero_grad()
        x_recon, z_mu, z_logvar = self.model(x)
        loss = self.criterion(x, x_recon, z_mu, z_logvar,
                              beta=self.beta, capacity=self.capacity)
        loss.backward()
        self.optimizer.step()
        return loss.item()






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.vae.VAE.evaluating_forward)
    defevaluating_forward(self, batch_data):
        x = batch_data
        x_gpu = x.to(self.device)
        x_recon, _, _ = self.model(x_gpu)
        score = pairwise_distances_no_broadcast(x.numpy(),
                                                x_recon.cpu().numpy())
        return score







classVAEModel(nn.Module):
    def__init__(self,
                 feature_size,
                 encoder_neuron_list=[128, 64, 32],
                 decoder_neuron_list=[32, 64, 128],
                 latent_dim=2,
                 hidden_activation_name='relu',
                 output_activation_name='sigmoid',
                 batch_norm=False, dropout_rate=0.2):
        super(VAEModel, self).__init__()

        self.feature_size = feature_size
        self.encoder_neuron_list = encoder_neuron_list
        self.decoder_neuron_list = decoder_neuron_list
        self.latent_dim = latent_dim
        self.hidden_activation_name = hidden_activation_name
        self.output_activation_name = output_activation_name
        self.batch_norm = batch_norm
        self.dropout_rate = dropout_rate

        self.encoder = self._build_encoder()
        self.decoder = self._build_decoder()

        self.encoder_mu = nn.Linear(encoder_neuron_list[-1], latent_dim)
        self.encoder_logvar = nn.Linear(encoder_neuron_list[-1], latent_dim)

    def_build_encoder(self):
        encoder_layers = []
        last_neuron_size = self.feature_size
        for neuron_size in self.encoder_neuron_list:
            encoder_layers.append(LinearBlock(last_neuron_size, neuron_size,
                                              activation_name=self.hidden_activation_name,
                                              batch_norm=self.batch_norm,
                                              dropout_rate=self.dropout_rate))
            last_neuron_size = neuron_size
        return nn.Sequential(*encoder_layers)

    def_build_decoder(self):
        decoder_layers = []
        last_neuron_size = self.latent_dim
        for neuron_size in self.decoder_neuron_list:
            decoder_layers.append(LinearBlock(last_neuron_size, neuron_size,
                                              activation_name=self.hidden_activation_name,
                                              batch_norm=self.batch_norm,
                                              dropout_rate=self.dropout_rate))
            last_neuron_size = neuron_size
        decoder_layers.append(LinearBlock(last_neuron_size, self.feature_size,
                                          activation_name=self.output_activation_name,
                                          batch_norm=False,
                                          dropout_rate=0))
        return nn.Sequential(*decoder_layers)

    defforward(self, x):
        z_mu, z_logvar = self.encode(x)
        z = self.reparameterize(z_mu, z_logvar)
        x_recon = self.decode(z)
        return x_recon, z_mu, z_logvar

    defreparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std).to(std.device)
        return mu + eps * std

    defencode(self, x):
        h = self.encoder(x)
        z_mu = self.encoder_mu(h)
        z_logvar = self.encoder_logvar(h)
        return z_mu, z_logvar

    defdecode(self, z):
        return self.decoder(z)

```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 61. pyod.models.xgbod - pyod 2.0.5 documentation {#61-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:24

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/xgbod.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.models.xgbod
```
### -*- coding: utf-8 -*-
"""XGBOD: Improving Supervised Outlier Detection with Unsupervised
Representation Learning. A semi-supervised outlier detection framework.
"""
### Author: Yue Zhao <yzhao062@gmail.com>
### License: BSD 2 clause


importnumpyasnp
fromsklearn.metricsimport roc_auc_score
fromsklearn.utilsimport check_array
fromsklearn.utils.validationimport check_X_y
fromsklearn.utils.validationimport check_is_fitted

try:
    importxgboost
except ImportError:
    print('please install xgboost first for running XGBOD')

fromxgboost.sklearnimport XGBClassifier

from.baseimport BaseDetector
from.knnimport KNN
from.lofimport LOF
from.iforestimport IForest
from.hbosimport HBOS
from.ocsvmimport OCSVM

from..utils.utilityimport check_parameter
from..utils.utilityimport check_detector
from..utils.utilityimport standardizer
from..utils.utilityimport precision_n_scores




[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD)
classXGBOD(BaseDetector):
r"""XGBOD class for outlier detection.
    It first uses the passed in unsupervised outlier detectors to extract
    richer representation of the data and then concatenates the newly
    generated features to the original feature for constructing the augmented
    feature space. An XGBoost classifier is then applied on this augmented
    feature space. Read more in the :cite:`zhao2018xgbod`.

    Parameters
    ----------
    estimator_list : list, optional (default=None)
        The list of pyod detectors passed in for unsupervised learning

    standardization_flag_list : list, optional (default=None)
        The list of boolean flags for indicating whether to perform
        standardization for each detector.

    max_depth : int
        Maximum tree depth for base learners.

    learning_rate : float
        Boosting learning rate (xgb's "eta")

    n_estimators : int
        Number of boosted trees to fit.

    silent : bool
        Whether to print messages while running boosting.

    objective : string or callable
        Specify the learning task and the corresponding learning objective or
        a custom objective function to be used (see note below).

    booster : string
        Specify which booster to use: gbtree, gblinear or dart.

    n_jobs : int
        Number of parallel threads used to run xgboost.  (replaces ``nthread``)

    gamma : float
        Minimum loss reduction required to make a further partition on a leaf
        node of the tree.

    min_child_weight : int
        Minimum sum of instance weight(hessian) needed in a child.

    max_delta_step : int
        Maximum delta step we allow each tree's weight estimation to be.

    subsample : float
        Subsample ratio of the training instance.

    colsample_bytree : float
        Subsample ratio of columns when constructing each tree.

    colsample_bylevel : float
        Subsample ratio of columns for each split, in each level.

    reg_alpha : float (xgb's alpha)
        L1 regularization term on weights.

    reg_lambda : float (xgb's lambda)
        L2 regularization term on weights.

    scale_pos_weight : float
        Balancing of positive and negative weights.

    base_score:
        The initial prediction score of all instances, global bias.

    random_state : int
        Random number seed.  (replaces seed)

    ### missing : float, optional
    ###     Value in the data which needs to be present as a missing value. If
    ###     None, defaults to np.nan.

    importance_type: string, default "gain"
        The feature importance type for the ``feature_importances_``
        property: either "gain",
        "weight", "cover", "total_gain" or "total_cover".

    \*\*kwargs : dict, optional
        Keyword arguments for XGBoost Booster object.  Full documentation of
        parameters can be found here:
        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.
        Attempting to set a parameter via the constructor args and \*\*kwargs
        dict simultaneously will result in a TypeError.

        Note: \*\*kwargs is unsupported by scikit-learn. We do not
        guarantee that parameters passed via this argument will interact
        properly with scikit-learn.

    Attributes
    ----------
    n_detector_ : int
        The number of unsupervised of detectors used.

    clf_ : object
        The XGBoost classifier.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    """

    def__init__(self, estimator_list=None, standardization_flag_list=None,
                 max_depth=3, learning_rate=0.1,
                 n_estimators=100, silent=True,
                 objective="binary:logistic", booster='gbtree',
                 n_jobs=1, nthread=None, gamma=0, min_child_weight=1,
                 max_delta_step=0, subsample=1, colsample_bytree=1,
                 colsample_bylevel=1,
                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
                 base_score=0.5, random_state=0,
                 ### missing=None,
                 **kwargs):
        super(XGBOD, self).__init__()
        self.estimator_list = estimator_list
        self.standardization_flag_list = standardization_flag_list
        self.max_depth = max_depth
        self.learning_rate = learning_rate
        self.n_estimators = n_estimators
        self.silent = silent
        self.objective = objective
        self.booster = booster
        self.n_jobs = n_jobs
        self.nthread = nthread
        self.gamma = gamma
        self.min_child_weight = min_child_weight
        self.max_delta_step = max_delta_step
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.colsample_bylevel = colsample_bylevel
        self.reg_alpha = reg_alpha
        self.reg_lambda = reg_lambda
        self.scale_pos_weight = scale_pos_weight
        self.base_score = base_score
        self.random_state = random_state
        ### self.missing = missing
        self.kwargs = kwargs

    def_init_detectors(self, X):
"""initialize unsupervised detectors if no predefined detectors is
        provided.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The train data

        Returns
        -------
        estimator_list : list of object
            The initialized list of detectors

        standardization_flag_list : list of boolean
            The list of bool flag to indicate whether standardization is needed

        """
        estimator_list = []
        standardization_flag_list = []

        ### predefined range of n_neighbors for KNN, AvgKNN, and LOF
        k_range = [1, 3, 5, 10, 20, 30, 40, 50]

        ### validate the value of k
        k_range = [k for k in k_range if k < X.shape[0]]

        for k in k_range:
            estimator_list.append(KNN(n_neighbors=k, method='largest'))
            ### estimator_list.append(KNN(n_neighbors=k, method='mean'))
            estimator_list.append(LOF(n_neighbors=k))
            ### standardization_flag_list.append(True)
            standardization_flag_list.append(True)
            standardization_flag_list.append(True)

        n_bins_range = [5, 10, 15, 20, 25, 30, 50]
        for n_bins in n_bins_range:
            estimator_list.append(HBOS(n_bins=n_bins))
            standardization_flag_list.append(False)

        ### predefined range of nu for one-class svm
        nu_range = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]
        for nu in nu_range:
            estimator_list.append(OCSVM(nu=nu))
            standardization_flag_list.append(True)

        ### predefined range for number of estimators in isolation forests
        n_range = [10, 20, 50, 70, 100, 150, 200]
        for n in n_range:
            estimator_list.append(
                IForest(n_estimators=n, random_state=self.random_state))
            standardization_flag_list.append(False)

        ### ### predefined range for number of estimators in LODA
        ### n_bins_range = [3, 5, 10, 15, 20, 25, 30, 50]
        ### for n_bins in n_bins_range:
        ###     estimator_list.append(LODA(n_bins=n_bins))
        ###     standardization_flag_list.append(False)

        return estimator_list, standardization_flag_list

    def_validate_estimator(self, X):
        if self.estimator_list is None:
            self.estimator_list, \
                self.standardization_flag_list = self._init_detectors(X)

        ### perform standardization for all detectors by default
        if self.standardization_flag_list is None:
            self.standardization_flag_list = [True] * len(self.estimator_list)

        ### validate two lists length
        if len(self.estimator_list) != len(self.standardization_flag_list):
            raise ValueError(
                "estimator_list length ({0}) is not equal "
                "to standardization_flag_list length ({1})".format(
                    len(self.estimator_list),
                    len(self.standardization_flag_list)))

        ### validate the estimator list is not empty
        check_parameter(len(self.estimator_list), low=1,
                        param_name='number of estimators',
                        include_left=True, include_right=True)

        for estimator in self.estimator_list:
            check_detector(estimator)

        return len(self.estimator_list)

    def_generate_new_features(self, X):
        X_add = np.zeros([X.shape[0], self.n_detector_])

        ### keep the standardization scalar for test conversion
        X_norm = self._scalar.transform(X)

        for ind, estimator in enumerate(self.estimator_list):
            if self.standardization_flag_list[ind]:
                X_add[:, ind] = estimator.decision_function(X_norm)

            else:
                X_add[:, ind] = estimator.decision_function(X)
        return X_add



[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit)
    deffit(self, X, y):
"""Fit the model using X and y as training data.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            Training data.

        y : numpy array of shape (n_samples,)
            The ground truth (binary label)

            - 0 : inliers
            - 1 : outliers

        Returns
        -------
        self : object
        """

        ### Validate inputs X and y
        X, y = check_X_y(X, y)
        X = check_array(X)
        self._set_n_classes(y)
        self.n_detector_ = self._validate_estimator(X)
        self.X_train_add_ = np.zeros([X.shape[0], self.n_detector_])

        ### keep the standardization scalar for test conversion
        X_norm, self._scalar = standardizer(X, keep_scalar=True)

        for ind, estimator in enumerate(self.estimator_list):
            if self.standardization_flag_list[ind]:
                estimator.fit(X_norm)
                self.X_train_add_[:, ind] = estimator.decision_scores_

            else:
                estimator.fit(X)
                self.X_train_add_[:, ind] = estimator.decision_scores_

        ### construct the new feature space
        self.X_train_new_ = np.concatenate((X, self.X_train_add_), axis=1)

        ### initialize, train, and predict on XGBoost
        self.clf_ = clf = XGBClassifier(max_depth=self.max_depth,
                                        learning_rate=self.learning_rate,
                                        n_estimators=self.n_estimators,
                                        silent=self.silent,
                                        objective=self.objective,
                                        booster=self.booster,
                                        n_jobs=self.n_jobs,
                                        nthread=self.nthread,
                                        gamma=self.gamma,
                                        min_child_weight=self.min_child_weight,
                                        max_delta_step=self.max_delta_step,
                                        subsample=self.subsample,
                                        colsample_bytree=self.colsample_bytree,
                                        colsample_bylevel=self.colsample_bylevel,
                                        reg_alpha=self.reg_alpha,
                                        reg_lambda=self.reg_lambda,
                                        scale_pos_weight=self.scale_pos_weight,
                                        base_score=self.base_score,
                                        random_state=self.random_state,
                                        ### missing=self.missing,
                                        **self.kwargs)
        self.clf_.fit(self.X_train_new_, y)
        self.decision_scores_ = self.clf_.predict_proba(
            self.X_train_new_)[:, 1]
        self.labels_ = self.clf_.predict(self.X_train_new_).ravel()

        return self






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.decision_function)
    defdecision_function(self, X):

        check_is_fitted(self, ['clf_', 'decision_scores_',
                               'labels_', '_scalar'])

        X = check_array(X)

        ### construct the new feature space
        X_add = self._generate_new_features(X)
        X_new = np.concatenate((X, X_add), axis=1)

        pred_scores = self.clf_.predict_proba(X_new)[:, 1]
        return pred_scores.ravel()






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict)
    defpredict(self, X):
"""Predict if a particular sample is an outlier or not.
        Calling xgboost `predict` function.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether or not
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.
        """

        check_is_fitted(self, ['clf_', 'decision_scores_',
                               'labels_', '_scalar'])

        X = check_array(X)

        ### construct the new feature space
        X_add = self._generate_new_features(X)
        X_new = np.concatenate((X, X_add), axis=1)

        pred_scores = self.clf_.predict(X_new)
        return pred_scores.ravel()






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.predict_proba)
    defpredict_proba(self, X):
"""Predict the probability of a sample being outlier.
        Calling xgboost `predict_proba` function.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.


        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether or not
            it should be considered as an outlier according to the
            fitted model. Return the outlier probability, ranging
            in [0,1].
        """
        return self.decision_function(X)






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict)
    deffit_predict(self, X, y):
        self.fit(X, y)
        return self.labels_






[docs][](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.xgbod.XGBOD.fit_predict_score)
    deffit_predict_score(self, X, y, scoring='roc_auc_score'):
"""Fit the detector, predict on samples, and evaluate the model by
        predefined metrics, e.g., ROC.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        scoring : str, optional (default='roc_auc_score')
            Evaluation metric:

            - 'roc_auc_score': ROC score
            - 'prc_n_score': Precision @ rank n score

        Returns
        -------
        score : float
        """

        self.fit(X, y)

        if scoring == 'roc_auc_score':
            score = roc_auc_score(y, self.decision_scores_)
        elif scoring == 'prc_n_score':
            score = precision_n_scores(y, self.decision_scores_)
        else:
            raise NotImplementedError('PyOD built-in scoring only supports '
                                      'ROC and Precision @ rank n')

        print("{metric}: {score}".format(metric=scoring, score=score))

        return score






```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 62. pyod.utils.data - pyod 2.0.5 documentation {#62-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:17

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/data.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.utils.data
```
### -*- coding: utf-8 -*-
"""Utility functions for manipulating data
"""
### Author: Yue Zhao <zhaoy@cmu.edu>
### Author: Yahya Almardeny <almardeny@gmail.com>
### License: BSD 2 clause

from__future__import division
from__future__import print_function

fromwarningsimport warn

importnumpyasnp
fromsklearn.datasetsimport make_blobs
fromsklearn.metricsimport roc_auc_score
fromsklearn.model_selectionimport train_test_split
fromsklearn.utilsimport check_X_y
fromsklearn.utilsimport check_consistent_length
fromsklearn.utilsimport check_random_state
fromsklearn.utilsimport column_or_1d

from.utilityimport check_parameter
from.utilityimport precision_n_scores

MAX_INT = np.iinfo(np.int32).max


def_generate_data(n_inliers, n_outliers, n_features, coef, offset,
                   random_state, n_nan=0, n_inf=0):
"""Internal function to generate data samples.

    Parameters
    ----------
    n_inliers : int
        The number of inliers.

    n_outliers : int
        The number of outliers.

    n_features : int
        The number of features (dimensions).

    coef : float in range [0,1)+0.001
        The coefficient of data generation.

    offset : int
        Adjust the value range of Gaussian and Uniform.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    n_nan : int
        The number of values that are missing (np.nan). Defaults to zero.

    n_inf : int
        The number of values that are infinite. (np.inf). Defaults to zero.

    Returns
    -------
    X : numpy array of shape (n_train, n_features)
        Data.

    y : numpy array of shape (n_train,)
        Ground truth.
    """

    inliers = coef * random_state.randn(n_inliers, n_features) + offset
    outliers = random_state.uniform(low=-1 * offset, high=offset,
                                    size=(n_outliers, n_features))
    X = np.r_[inliers, outliers]

    y = np.r_[np.zeros((n_inliers,)), np.ones((n_outliers,))]

    if n_nan > 0:
        X = np.r_[X, np.full((n_nan, n_features), np.nan)]
        y = np.r_[y, np.full((n_nan), np.nan)]

    if n_inf > 0:
        X = np.r_[X, np.full((n_inf, n_features), np.inf)]
        y = np.r_[y, np.full((n_inf), np.inf)]

    return X, y




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.get_outliers_inliers)
defget_outliers_inliers(X, y):
"""Internal method to separate inliers from outliers.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The input samples

    y : list or array of shape (n_samples,)
        The ground truth of input samples.

    Returns
    -------
    X_outliers : numpy array of shape (n_samples, n_features)
        Outliers.

    X_inliers : numpy array of shape (n_samples, n_features)
        Inliers.

    """
    X_outliers = X[np.where(y == 1)]
    X_inliers = X[np.where(y == 0)]
    return X_outliers, X_inliers







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data)
defgenerate_data(n_train=1000, n_test=500, n_features=2, contamination=0.1,
                  train_only=False, offset=10, behaviour='new',
                  random_state=None, n_nan=0, n_inf=0):
"""Utility function to generate synthesized data.
    Normal data is generated by a multivariate Gaussian distribution and
    outliers are generated by a uniform distribution.
    "X_train, X_test, y_train, y_test" are returned.

    Parameters
    ----------
    n_train : int, (default=1000)
        The number of training points to generate.

    n_test : int, (default=500)
        The number of test points to generate.

    n_features : int, optional (default=2)
        The number of features (dimensions).

    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    train_only : bool, optional (default=False)
        If true, generate train data only.

    offset : int, optional (default=10)
        Adjust the value range of Gaussian and Uniform.

    behaviour : str, default='new'
        Behaviour of the returned datasets which can be either 'old' or
        'new'. Passing ``behaviour='new'`` returns
        "X_train, X_test, y_train, y_test", while passing ``behaviour='old'``
        returns "X_train, y_train, X_test, y_test".

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    n_nan : int
        The number of values that are missing (np.nan). Defaults to zero.

    n_inf : int
        The number of values that are infinite. (np.inf). Defaults to zero.

    Returns
    -------
    X_train : numpy array of shape (n_train, n_features)
        Training data.

    X_test : numpy array of shape (n_test, n_features)
        Test data.

    y_train : numpy array of shape (n_train,)
        Training ground truth.

    y_test : numpy array of shape (n_test,)
        Test ground truth.

    """

    ### initialize a random state and seeds for the instance
    random_state = check_random_state(random_state)
    offset_ = random_state.randint(low=offset)
    coef_ = random_state.random_sample() + 0.001  ### in case of underflow

    if isinstance(contamination, (float, int)):
        n_outliers_train = int(n_train * contamination)
    else:
        contamination = 0.1
        n_outliers_train = int(n_train * contamination)

    n_inliers_train = int(n_train - n_outliers_train)

    X_train, y_train = _generate_data(n_inliers_train, n_outliers_train,
                                      n_features, coef_, offset_, random_state,
                                      n_nan, n_inf)

    if train_only:
        return X_train, y_train

    n_outliers_test = int(n_test * contamination)
    n_inliers_test = int(n_test - n_outliers_test)

    X_test, y_test = _generate_data(n_inliers_test, n_outliers_test,
                                    n_features, coef_, offset_, random_state,
                                    n_nan, n_inf)

    if behaviour == 'old':
        warn('behaviour="old" is deprecated and will be removed '
             'in version 0.9.0. Please use behaviour="new", which '
             'makes the returned datasets in the order of '
             'X_train, X_test, y_train, y_test.',
             FutureWarning)
        return X_train, y_train, X_test, y_test

    else:
        return X_train, X_test, y_train, y_test







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.check_consistent_shape)
defcheck_consistent_shape(X_train, y_train, X_test, y_test, y_train_pred,
                           y_test_pred):
"""Internal shape to check input data shapes are consistent.

    Parameters
    ----------
    X_train : numpy array of shape (n_samples, n_features)
        The training samples.

    y_train : list or array of shape (n_samples,)
        The ground truth of training samples.

    X_test : numpy array of shape (n_samples, n_features)
        The test samples.

    y_test : list or array of shape (n_samples,)
        The ground truth of test samples.

    y_train_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the training samples.

    y_test_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the test samples.

    Returns
    -------
    X_train : numpy array of shape (n_samples, n_features)
        The training samples.

    y_train : list or array of shape (n_samples,)
        The ground truth of training samples.

    X_test : numpy array of shape (n_samples, n_features)
        The test samples.

    y_test : list or array of shape (n_samples,)
        The ground truth of test samples.

    y_train_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the training samples.

    y_test_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the test samples.
    """

    ### check input data shapes are consistent
    X_train, y_train = check_X_y(X_train, y_train)
    X_test, y_test = check_X_y(X_test, y_test)

    y_test_pred = column_or_1d(y_test_pred)
    y_train_pred = column_or_1d(y_train_pred)

    check_consistent_length(y_train, y_train_pred)
    check_consistent_length(y_test, y_test_pred)

    if X_train.shape[1] != X_test.shape[1]:
        raise ValueError("X_train {0} and X_test {1} have different number "
                         "of features.".format(X_train.shape, X_test.shape))

    return X_train, y_train, X_test, y_test, y_train_pred, y_test_pred







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.evaluate_print)
defevaluate_print(clf_name, y, y_pred):
"""Utility function for evaluating and printing the results for examples.
    Default metrics include ROC and Precision @ n

    Parameters
    ----------
    clf_name : str
        The name of the detector.

    y : list or numpy array of shape (n_samples,)
        The ground truth. Binary (0: inliers, 1: outliers).

    y_pred : list or numpy array of shape (n_samples,)
        The raw outlier scores as returned by a fitted model.

    """

    y = column_or_1d(y)
    y_pred = column_or_1d(y_pred)
    check_consistent_length(y, y_pred)

    print('{clf_name} ROC:{roc}, precision @ rank n:{prn}'.format(
        clf_name=clf_name,
        roc=np.round(roc_auc_score(y, y_pred), decimals=4),
        prn=np.round(precision_n_scores(y, y_pred), decimals=4)))







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters)
defgenerate_data_clusters(n_train=1000, n_test=500, n_clusters=2,
                           n_features=2, contamination=0.1, size='same',
                           density='same', dist=0.25, random_state=None,
                           return_in_clusters=False):
"""Utility function to generate synthesized data in clusters.
       Generated data can involve the low density pattern problem and global
       outliers which are considered as difficult tasks for outliers detection
       algorithms.

    Parameters
    ----------
    n_train : int, (default=1000)
        The number of training points to generate.

    n_test : int, (default=500)
        The number of test points to generate.

    n_clusters : int, optional (default=2)
       The number of centers (i.e. clusters) to generate.

    n_features : int, optional (default=2)
       The number of features for each sample.

    contamination : float in (0., 0.5), optional (default=0.1)
       The amount of contamination of the data set, i.e.
       the proportion of outliers in the data set.

    size : str, optional (default='same')
       Size of each cluster: 'same' generates clusters with same size,
       'different' generate clusters with different sizes.

    density : str, optional (default='same')
       Density of each cluster: 'same' generates clusters with same density,
       'different' generate clusters with different densities.

    dist: float, optional (default=0.25)
       Distance between clusters. Should be between 0. and 1.0
       It is used to avoid clusters overlapping as much as possible.
       However, if number of samples and number of clusters are too high,
       it is unlikely to separate them fully even if ``dist`` set to 1.0

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    return_in_clusters : bool, optional (default=False)
        If True, the function returns x_train, y_train, x_test, y_test each as
        a list of numpy arrays where each index represents a cluster.
        If False, it returns x_train, y_train, x_test, y_test each as numpy
        array after joining the sequence of clusters arrays,

    Returns
    -------
    X_train : numpy array of shape (n_train, n_features)
        Training data.

    y_train : numpy array of shape (n_train,)
        Training ground truth.

    X_test : numpy array of shape (n_test, n_features)
        Test data.

    y_test : numpy array of shape (n_test,)
        Test ground truth.
    """
    ### initialize a random state and seeds for the instance
    random_state = check_random_state(random_state)

    if isinstance(n_clusters, int):
        check_parameter(n_clusters, low=1, param_name='n_clusters')
    else:
        raise ValueError("n_clusters should be int, got %s" % n_clusters)

    if isinstance(n_features, int):
        check_parameter(n_features, low=1, param_name='n_features')
    else:
        raise ValueError("n_features should be int, got %s" % n_features)

    if isinstance(contamination, (float, int)):
        check_parameter(contamination, low=0, high=0.5,
                        param_name='contamination')
    else:
        raise ValueError(
            "contamination should be float, got %s" % contamination)

    if isinstance(dist, float):
        check_parameter(dist, low=0, high=1.0, param_name='dist')
    else:
        raise ValueError("dist should be float, got %s" % dist)

    if not isinstance(return_in_clusters, bool):
        raise ValueError("return_in_clusters should be of type bool, "
                         "got %s" % return_in_clusters)

    ### find the required number of outliers and inliers
    n_samples = n_train + n_test
    n_outliers = int(n_samples * contamination)
    n_inliers = n_samples - n_outliers

    if size == 'same':
        a_ = [int(n_inliers / n_clusters)] * (n_clusters - 1)
        clusters_size = a_ + [int(n_inliers - sum(a_))]
    elif size == 'different':
        if (n_clusters * 10) > n_samples:
            raise ValueError('number of samples should be at least 10 times of'
                             'the number of clusters')
        if (n_clusters * 10) > n_inliers:
            raise ValueError('contamination ratio is too high, try to increase'
                             ' number of samples or decrease the contamination')
        _r = 1. / n_clusters
        _offset = random_state.uniform(_r * 0.2, _r * 0.4,
                                       size=(int(n_clusters / 2),)).tolist()
        _offset += [i * -1. for i in _offset]
        clusters_size = np.round(
            np.multiply(n_inliers, np.add(_r, _offset))).astype(int)
        if n_clusters % 2 == 0:  ### if it is even number
            clusters_size[n_clusters - 1] += n_inliers - sum(clusters_size)
        else:
            clusters_size = np.append(clusters_size,
                                      n_inliers - sum(clusters_size))
    else:
        raise ValueError(
            'size should be a string of value \'same\' or \'different\'')

    ### check for clusters densities and apply split accordingly
    if density == 'same':
        clusters_density = random_state.uniform(low=0.1, high=0.5, size=(
            1,)).tolist() * n_clusters
    elif density == 'different':
        clusters_density = random_state.uniform(low=0.1, high=0.5,
                                                size=(n_clusters,))
    else:
        raise ValueError(
            'density should be a string of value \'same\' or \'different\'')

    ### calculate number of outliers for every cluster
    n_outliers_ = []
    for i in range(n_clusters):
        n_outliers_.append(int(round(clusters_size[i] * contamination)))
    _diff = int((n_outliers - sum(n_outliers_)) / n_clusters)
    for i in range(n_clusters - 1):
        n_outliers_[i] += _diff
    n_outliers_[n_clusters - 1] += n_outliers - sum(n_outliers_)
    random_state.shuffle(n_outliers_)

    ### generate data
    X_clusters, y_clusters = [], []
    X, y = np.zeros([n_samples, n_features]), np.zeros([n_samples, ])

    center_box = list(filter(lambda a: a != 0, np.linspace(
        -np.power(n_samples * n_clusters, dist),
        np.power(n_samples * n_clusters, dist),
        n_clusters + 2)))

    ### index tracker for value assignment
    tracker_idx = 0

    for i in range(n_clusters):
        inliers, outliers = [], []
        _blob, _y = make_blobs(n_samples=clusters_size[i], centers=1,
                               cluster_std=clusters_density[i],
                               center_box=(center_box[i], center_box[i + 1]),
                               n_features=n_features,
                               random_state=random_state)

        inliers.append(_blob)

        center_box_l = center_box[i] * (1.2 + dist + clusters_density[i])
        center_box_r = center_box[i + 1] * (1.2 + dist + clusters_density[i])

        outliers.append(make_blobs(n_samples=n_outliers_[i], centers=1,
                                   cluster_std=random_state.uniform(
                                       clusters_density[i] * 3.5,
                                       clusters_density[i] * 4.,
                                       size=(1,)[0]),
                                   center_box=(center_box_l, center_box_r),
                                   n_features=n_features,
                                   random_state=random_state)[0])
        _y = np.append(_y, [1] * int(n_outliers_[i]))

        ### generate X
        if np.array(outliers).ravel().shape[0] > 0:
            stacked_X_temp = np.vstack(
                (np.concatenate(inliers), np.concatenate(outliers)))
            X_clusters.append(stacked_X_temp)
            tracker_idx_new = tracker_idx + stacked_X_temp.shape[0]
            X[tracker_idx:tracker_idx_new, :] = stacked_X_temp
        else:
            X_clusters.append(np.concatenate(inliers))

        ### generate Y
        y_clusters.append(_y)
        y[tracker_idx:tracker_idx_new, ] = _y

        tracker_idx = tracker_idx_new

    if return_in_clusters:
        return X_clusters, y_clusters

    ### return X_train, X_test, y_train, y_test
    else:
        return train_test_split(X, y, test_size=n_test,
                                random_state=random_state)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_categorical)
defgenerate_data_categorical(n_train=1000, n_test=500, n_features=2,
                              n_informative=2, n_category_in=2,
                              n_category_out=2, contamination=0.1,
                              shuffle=True, random_state=None):
"""Utility function to generate synthesized categorical data.

    Parameters
    ----------
    n_train : int, (default=1000)
        The number of training points to generate.

    n_test : int, (default=500)
        The number of test points to generate.

    n_features : int, optional (default=2)
       The number of features for each sample.

    n_informative : int in (1, n_features), optional (default=2)
       The number of informative features in the outlier points.
       The higher the easier the outlier detection should be.
       Note that n_informative should not be less than or
       equal n_features.

    n_category_in : int in (1, n_inliers), optional (default=2)
       The number of categories in the inlier points.

    n_category_out : int in (1, n_outliers), optional (default=2)
       The number of categories in the outlier points.

    contamination : float in (0., 0.5), optional (default=0.1)
       The amount of contamination of the data set, i.e.
       the proportion of outliers in the data set.

    shuffle: bool, optional(default=True)
        If True, inliers will be shuffled which makes more noisy distribution.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.


    Returns
    -------
    X_train : numpy array of shape (n_train, n_features)
        Training data.

    y_train : numpy array of shape (n_train,)
        Training ground truth.

    X_test : numpy array of shape (n_test, n_features)
        Test data.

    y_test : numpy array of shape (n_test,)
        Test ground truth.
    """

    ### initialize a random state and seeds for the instance
    random_state = check_random_state(random_state)

    if isinstance(n_train, int):
        check_parameter(n_train, low=1, param_name='n_train')
    else:
        raise ValueError("n_train should be int, got %s" % n_train)

    if isinstance(n_test, int):
        check_parameter(n_test, low=0, param_name='n_test')
    else:
        raise ValueError("n_test should be int, got %s" % n_test)

    if isinstance(n_features, int):
        check_parameter(n_features, low=0, param_name='n_features')
    else:
        raise ValueError("n_features should be int, got %s" % n_features)

    if isinstance(n_informative, int):
        check_parameter(n_informative, low=0, high=n_features + 1, param_name='n_informative')
    else:
        raise ValueError("n_informative should be int, got %s" % n_informative)

    if isinstance(contamination, (float, int)):
        check_parameter(contamination, low=0, high=0.5,
                        param_name='contamination')
    else:
        raise ValueError("contamination should be float, got %s" % contamination)

    if not isinstance(shuffle, bool):
        raise ValueError("shuffle should be bool, got %s" % shuffle)

    ### find the required number of outliers and inliers
    n_samples = n_train + n_test
    n_outliers = int(n_samples * contamination)
    n_inliers = n_samples - n_outliers

    if isinstance(n_category_in, int):
        check_parameter(n_category_in, low=0, high=n_inliers + 1, param_name='n_category_in')
    else:
        raise ValueError("n_category_in should be int, got %s" % n_category_in)

    if isinstance(n_category_out, int):
        check_parameter(n_category_out, low=0, high=n_outliers + 1, param_name='n_category_out')
    else:
        raise ValueError("n_category_out should be int, got %s" % n_category_out)

    ### Encapsulated functions to generate features
    def__f(f):
        quot, rem = divmod(f - 1, 26)
        return __f(quot) + chr(rem + ord('A')) if f != 0 else ''

    ### generate pool of features to be the base for naming the data points
    features = []
    for i in range(1, n_features + 1):
        features.append(__f(i))

    ### find the required distributions of categories over inliers and outliers
    temp_ = [int(n_inliers / n_category_in)] * (n_category_in - 1)
    dist_in = temp_ + [int(n_inliers - sum(temp_))]
    temp_ = [int(n_outliers / n_category_out)] * (n_category_out - 1)
    dist_out = temp_ + [int(n_outliers - sum(temp_))]

    ### generate categorical data
    X = []
    count = 0
    for f in features:
        inliers = np.hstack([[f + str(i)] * dist_in[i] for i in range(n_category_in)])
        if shuffle:
            random_state.shuffle(inliers)
        if count < n_informative:
            outliers = list(np.hstack(
                [[f + str((n_category_in * 2) + i)] * dist_out[i] for i in range(n_category_out)]))
        else:
            outliers = list(inliers[random_state.randint(0, len(inliers), size=n_outliers)])
        count += 1

        X.append(list(inliers) + outliers)

    return train_test_split(np.array(X).T,
                            np.array(([0] * n_inliers) + ([1] * n_outliers)),
                            test_size=n_test,
                            random_state=random_state)




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 63. pyod.utils.example - pyod 2.0.5 documentation {#63-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/example.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:39

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/example.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/example.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.utils.example
```
### -*- coding: utf-8 -*-
"""Utility functions for running examples
"""
### Author: Yue Zhao <zhaoy@cmu.edu>
### License: BSD 2 clause


from__future__import division
from__future__import print_function

importmatplotlib.pyplotasplt

from.dataimport check_consistent_shape
from.dataimport get_outliers_inliers




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.visualize)
defvisualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,
              y_test_pred, show_figure=True,
              save_figure=False):  ### pragma: no cover
"""Utility function for visualizing the results in examples.
    Internal use only.

    Parameters
    ----------
    clf_name : str
        The name of the detector.

    X_train : numpy array of shape (n_samples, n_features)
        The training samples.

    y_train : list or array of shape (n_samples,)
        The ground truth of training samples.

    X_test : numpy array of shape (n_samples, n_features)
        The test samples.

    y_test : list or array of shape (n_samples,)
        The ground truth of test samples.

    y_train_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the training samples.

    y_test_pred : numpy array of shape (n_samples, n_features)
        The predicted binary labels of the test samples.

    show_figure : bool, optional (default=True)
        If set to True, show the figure.

    save_figure : bool, optional (default=False)
        If set to True, save the figure to the local.

    """

    def_add_sub_plot(X_inliers, X_outliers, sub_plot_title,
                      inlier_color='blue', outlier_color='orange'):
"""Internal method to add subplot of inliers and outliers.

        Parameters
        ----------
        X_inliers : numpy array of shape (n_samples, n_features)
            Outliers.

        X_outliers : numpy array of shape (n_samples, n_features)
            Inliers.

        sub_plot_title : str
            Subplot title.

        inlier_color : str, optional (default='blue')
            The color of inliers.

        outlier_color : str, optional (default='orange')
            The color of outliers.

        """
        plt.axis("equal")
        plt.scatter(X_inliers[:, 0], X_inliers[:, 1], label='inliers',
                    color=inlier_color, s=40)
        plt.scatter(X_outliers[:, 0], X_outliers[:, 1],
                    label='outliers', color=outlier_color, s=50, marker='^')
        plt.title(sub_plot_title, fontsize=15)
        plt.xticks([])
        plt.yticks([])
        plt.legend(loc=3, prop={'size': 10})

    ### check input data shapes are consistent
    X_train, y_train, X_test, y_test, y_train_pred, y_test_pred = \
        check_consistent_shape(X_train, y_train, X_test, y_test, y_train_pred,
                               y_test_pred)

    if X_train.shape[1] != 2:
        raise ValueError("Input data has to be 2-d for visualization. The "
                         "input data has {shape}.".format(shape=X_train.shape))

    X_train_outliers, X_train_inliers = get_outliers_inliers(X_train, y_train)
    X_train_outliers_pred, X_train_inliers_pred = get_outliers_inliers(
        X_train, y_train_pred)

    X_test_outliers, X_test_inliers = get_outliers_inliers(X_test, y_test)
    X_test_outliers_pred, X_test_inliers_pred = get_outliers_inliers(
        X_test, y_test_pred)

    ### plot ground truth vs. predicted results
    fig = plt.figure(figsize=(12, 10))
    plt.suptitle("Demo of {clf_name} Detector".format(clf_name=clf_name),
                 fontsize=15)

    fig.add_subplot(221)
    _add_sub_plot(X_train_inliers, X_train_outliers, 'Train Set Ground Truth',
                  inlier_color='blue', outlier_color='orange')

    fig.add_subplot(222)
    _add_sub_plot(X_train_inliers_pred, X_train_outliers_pred,
                  'Train Set Prediction', inlier_color='blue',
                  outlier_color='orange')

    fig.add_subplot(223)
    _add_sub_plot(X_test_inliers, X_test_outliers, 'Test Set Ground Truth',
                  inlier_color='green', outlier_color='red')

    fig.add_subplot(224)
    _add_sub_plot(X_test_inliers_pred, X_test_outliers_pred,
                  'Test Set Prediction', inlier_color='green',
                  outlier_color='red')

    if save_figure:
        plt.savefig('{clf_name}.png'.format(clf_name=clf_name), dpi=300)

    if show_figure:
        plt.show()







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.example.data_visualize)
defdata_visualize(X_train, y_train, show_figure=True,
                   save_figure=False):  ### pragma: no cover
"""Utility function for visualizing the synthetic samples generated by
    generate_data_cluster function.

    Parameters
    ----------
    X_train : numpy array of shape (n_samples, n_features)
        The training samples.

    y_train : list or array of shape (n_samples,)
        The ground truth of training samples.

    show_figure : bool, optional (default=True)
        If set to True, show the figure.

    save_figure : bool, optional (default=False)
        If set to True, save the figure to the local.

    """

    def_plot(X_inliers, X_outliers, inlier_color='blue',
              outlier_color='orange'):
"""Internal method to add subplot of inliers and outliers.

        Parameters
        ----------
        X_inliers : numpy array of shape (n_samples, n_features)
            Outliers.

        X_outliers : numpy array of shape (n_samples, n_features)
            Inliers.

        sub_plot_title : str
            Subplot title.

        inlier_color : str, optional (default='blue')
            The color of inliers.

        outlier_color : str, optional (default='orange')
            The color of outliers.

        """
        plt.axis("equal")
        plt.scatter(X_inliers[:, 0], X_inliers[:, 1], label='inliers',
                    color=inlier_color, s=40)
        plt.scatter(X_outliers[:, 0], X_outliers[:, 1],
                    label='outliers', color=outlier_color, s=50, marker='^')
        plt.xticks([])
        plt.yticks([])
        plt.legend(loc='best', prop={'size': 10})

    assert len(X_train) <= 5
    in_colors = ['blue', 'green', 'purple', 'brown', 'black']
    out_colors = ['red', 'orange', 'grey', 'violet', 'pink']
    plt.figure(figsize=(13, 10))
    plt.suptitle("Demo of Generating Data in Clusters", fontsize=15)
    for i, cluster in enumerate(X_train):
        X_train_outliers, X_train_inliers = get_outliers_inliers(cluster,
                                                                 y_train[i])
        _plot(X_train_inliers, X_train_outliers,
              inlier_color=in_colors[i],
              outlier_color=out_colors[i])

    if save_figure:
        plt.savefig()

    if show_figure:
        plt.show()




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 64. pyod.utils.stat_models - pyod 2.0.5 documentation {#64-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:13

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/stat_models.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.utils.stat_models
```
### -*- coding: utf-8 -*-
""" A collection of statistical models
"""
### Author: Yue Zhao <zhaoy@cmu.edu>
### License: BSD 2 clause

from__future__import division
from__future__import print_function

importnumpyasnp
fromnumbaimport njit
fromscipy.statsimport pearsonr
fromsklearn.utils.validationimport check_array
### noinspection PyProtectedMember
fromsklearn.utils.validationimport check_consistent_length


### TODO: disable p value calculation due to python 2.7 break
### from scipy.special import betainc




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pairwise_distances_no_broadcast)
defpairwise_distances_no_broadcast(X, Y):
"""Utility function to calculate row-wise euclidean distance of two matrix.
    Different from pair-wise calculation, this function would not broadcast.

    For instance, X and Y are both (4,3) matrices, the function would return
    a distance vector with shape (4,), instead of (4,4).

    Parameters
    ----------
    X : array of shape (n_samples, n_features)
        First input samples

    Y : array of shape (n_samples, n_features)
        Second input samples

    Returns
    -------
    distance : array of shape (n_samples,)
        Row-wise euclidean distance of X and Y
    """
    X = check_array(X)
    Y = check_array(Y)

    if X.shape[0] != Y.shape[0] or X.shape[1] != Y.shape[1]:
        raise ValueError("pairwise_distances_no_broadcast function receive"
                         "matrix with different shapes {0} and {1}".format(
            X.shape, Y.shape))
    return _pairwise_distances_no_broadcast_helper(X, Y)





@njit
def_pairwise_distances_no_broadcast_helper(X, Y):  ### pragma: no cover
"""Internal function for calculating the distance with numba. Do not use.

    Parameters
    ----------
    X : array of shape (n_samples, n_features)
        First input samples

    Y : array of shape (n_samples, n_features)
        Second input samples

    Returns
    -------
    distance : array of shape (n_samples,)
        Intermediate results. Do not use.

    """
    euclidean_sq = np.square(Y - X)
    return np.sqrt(np.sum(euclidean_sq, axis=1)).ravel()




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.wpearsonr)
defwpearsonr(x, y, w=None):
"""Utility function to calculate the weighted Pearson correlation of two
    samples.

    See https://stats.stackexchange.com/questions/221246/such-thing-as-a-weighted-correlation
    for more information

    Parameters
    ----------
    x : array, shape (n,)
        Input x.

    y : array, shape (n,)
        Input y.

    w : array, shape (n,)
        Weights w.

    Returns
    -------
    scores : float in range of [-1,1]
        Weighted Pearson Correlation between x and y.

    """

    ### unweighted version
    ### note the return is different
    ### TODO: fix output differences
    if w is None:
        return pearsonr(x, y)

    x = np.asarray(x)
    y = np.asarray(y)
    w = np.asarray(w)

    check_consistent_length([x, y, w])
    ### n = len(x)

    w_sum = w.sum()
    mx = np.sum(x * w) / w_sum
    my = np.sum(y * w) / w_sum

    xm, ym = (x - mx), (y - my)

    r_num = np.sum(xm * ym * w) / w_sum

    xm2 = np.sum(xm * xm * w) / w_sum
    ym2 = np.sum(ym * ym * w) / w_sum

    r_den = np.sqrt(xm2 * ym2)
    r = r_num / r_den

    r = max(min(r, 1.0), -1.0)

    ### TODO: disable p value calculation due to python 2.7 break
    ###    df = n_train_ - 2
    #
    ###    if abs(r) == 1.0:
    ###        prob = 0.0
    ###    else:
    ###        t_squared = r ** 2 * (df / ((1.0 - r) * (1.0 + r)))
    ###        prob = _betai(0.5 * df, 0.5, df / (df + t_squared))
    return r  ### , prob





#####################################
###      PROBABILITY CALCULATIONS     #
#####################################

### TODO: disable p value calculation due to python 2.7 break
### def _betai(a, b, x):
###     x = np.asarray(x)
###     x = np.where(x < 1.0, x, 1.0)  ### if x > 1 then return 1.0
###     return betainc(a, b, x)




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.pearsonr_mat)
defpearsonr_mat(mat, w=None):
"""Utility function to calculate pearson matrix (row-wise).

    Parameters
    ----------
    mat : numpy array of shape (n_samples, n_features)
        Input matrix.

    w : numpy array of shape (n_features,)
        Weights.

    Returns
    -------
    pear_mat : numpy array of shape (n_samples, n_samples)
        Row-wise pearson score matrix.

    """
    mat = check_array(mat)
    n_row = mat.shape[0]
    n_col = mat.shape[1]
    pear_mat = np.full([n_row, n_row], 1).astype(float)

    if w is not None:
        for cx in range(n_row):
            for cy in range(cx + 1, n_row):
                curr_pear = wpearsonr(mat[cx, :], mat[cy, :], w)
                pear_mat[cx, cy] = curr_pear
                pear_mat[cy, cx] = curr_pear
    else:
        for cx in range(n_col):
            for cy in range(cx + 1, n_row):
                curr_pear = pearsonr(mat[cx, :], mat[cy, :])[0]
                pear_mat[cx, cy] = curr_pear
                pear_mat[cy, cx] = curr_pear

    return pear_mat







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.column_ecdf)
defcolumn_ecdf(matrix: np.ndarray) -> np.ndarray:
"""
    Utility function to compute the column wise empirical cumulative distribution of a 2D feature matrix,
    where the rows are samples and the columns are features per sample. The accumulation is done in the positive
    direction of the sample axis.

    E.G.
    p(1) = 0.2, p(0) = 0.3, p(2) = 0.1, p(6) = 0.4
    ECDF E(5) = p(x <= 5)
    ECDF E would be E(-1) = 0, E(0) = 0.3, E(1) = 0.5, E(2) = 0.6, E(3) = 0.6, E(4) = 0.6, E(5) = 0.6, E(6) = 1

    Similar to and tested against:
    https://www.statsmodels.org/stable/generated/statsmodels.distributions.empirical_distribution.ECDF.html

    Returns
    -------

    """
    ### check the matrix dimensions
    assert len(matrix.shape) == 2, 'Matrix needs to be two dimensional for the ECDF computation.'

    ### create a probability array the same shape as the feature matrix which we will reorder to build
    ### the ecdf
    probabilities = np.linspace(np.ones(matrix.shape[1]) / matrix.shape[0], np.ones(matrix.shape[1]), matrix.shape[0])

    ### get the sorting indices for a numpy array
    sort_idx = np.argsort(matrix, axis=0)

    ### sort the numpy array, as we need to look for duplicates in the feature values (that would have different
    ### probabilities if we would just resort the probabilities array)
    matrix = np.take_along_axis(matrix, sort_idx, axis=0)

    ### deal with equal values
    ecdf_terminate_equals_inplace(matrix, probabilities)

    ### return the resorted accumulated probabilities (by reverting the sorting of the input matrix)
    ### looks a little complicated but is faster this way
    reordered_probabilities = np.ones_like(probabilities)
    np.put_along_axis(reordered_probabilities, sort_idx, probabilities, axis=0)
    return reordered_probabilities







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.stat_models.ecdf_terminate_equals_inplace)
@njit
defecdf_terminate_equals_inplace(matrix: np.ndarray, probabilities: np.ndarray):
"""
    This is a helper function for computing the ecdf of an array. It has been outsourced from the original
    function in order to be able to use the njit compiler of numpy for increased speeds, as it unfortunately
    needs a loop over all rows and columns of a matrix. It acts in place on the probabilities' matrix.

    Parameters
    ----------
    matrix : a feature matrix where the rows are samples and each column is a feature !(expected to be sorted)!

    probabilities : a probability matrix that will be used building the ecdf. It has values between 0 and 1 and
                    is also sorted.

    Returns
    -------

    """
    for cx in range(probabilities.shape[1]):
        for rx in range(probabilities.shape[0] - 2, -1, -1):
            if matrix[rx, cx] == matrix[rx + 1, cx]:
                probabilities[rx, cx] = probabilities[rx + 1, cx]




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## 65. pyod.utils.utility - pyod 2.0.5 documentation {#65-2}

**URL:** https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html
**Ê∑±Â∫¶:** 2
**Áà¨ÂèñÊó∂Èó¥:** 2025-09-09 13:16:35

Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
[Skip to content](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html#furo-main-content)
Toggle site navigation sidebar
[pyod 2.0.5 documentation](https://pyod.readthedocs.io/en/latest/index.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
[ pyod 2.0.5 documentation ](https://pyod.readthedocs.io/en/latest/index.html)
Getting Started
  * [Installation](https://pyod.readthedocs.io/en/latest/install.html)
  * [Model Save & Load](https://pyod.readthedocs.io/en/latest/model_persistence.html)
  * [Fast Train with SUOD](https://pyod.readthedocs.io/en/latest/fast_train.html)
  * [Examples](https://pyod.readthedocs.io/en/latest/example.html)
  * [Benchmarks](https://pyod.readthedocs.io/en/latest/benchmark.html)


Documentation
  * [API CheatSheet](https://pyod.readthedocs.io/en/latest/api_cc.html)
  * [API Reference](https://pyod.readthedocs.io/en/latest/pyod.html)
Toggle navigation of API Reference
    * [All Models](https://pyod.readthedocs.io/en/latest/pyod.models.html)
    * [Utility Functions](https://pyod.readthedocs.io/en/latest/pyod.utils.html)


Additional Information
  * [Known Issues & Warnings](https://pyod.readthedocs.io/en/latest/issues.html)
  * [Outlier Detection 101](https://pyod.readthedocs.io/en/latest/relevant_knowledge.html)
  * [Citations & Achievements](https://pyod.readthedocs.io/en/latest/pubs.html)
  * [Frequently Asked Questions](https://pyod.readthedocs.io/en/latest/faq.html)
  * [About us](https://pyod.readthedocs.io/en/latest/about.html)


[ Back to top ](https://pyod.readthedocs.io/en/latest/_modules/pyod/utils/utility.html)
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
### Source code for pyod.utils.utility
```
### -*- coding: utf-8 -*-
"""A set of utility functions to support outlier detection.
"""
### Author: Yue Zhao <zhaoy@cmu.edu>
### License: BSD 2 clause

from__future__import division
from__future__import print_function

importnumbers

importnumpyasnp
importsklearn
fromnumpyimport percentile
fromsklearn.metricsimport precision_score
fromsklearn.preprocessingimport StandardScaler
fromsklearn.utilsimport check_array
fromsklearn.utilsimport check_consistent_length
fromsklearn.utilsimport check_random_state
fromsklearn.utilsimport column_or_1d
fromsklearn.utils.randomimport sample_without_replacement

MAX_INT = np.iinfo(np.int32).max
MIN_INT = -1 * MAX_INT




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_parameter)
defcheck_parameter(param, low=MIN_INT, high=MAX_INT, param_name='',
                    include_left=False, include_right=False):
"""Check if an input is within the defined range.

    Parameters
    ----------
    param : int, float
        The input parameter to check.

    low : int, float
        The lower bound of the range.

    high : int, float
        The higher bound of the range.

    param_name : str, optional (default='')
        The name of the parameter.

    include_left : bool, optional (default=False)
        Whether includes the lower bound (lower bound <=).

    include_right : bool, optional (default=False)
        Whether includes the higher bound (<= higher bound).

    Returns
    -------
    within_range : bool or raise errors
        Whether the parameter is within the range of (low, high)

    """

    ### param, low and high should all be numerical
    if not isinstance(param, (numbers.Integral, np.integer, float)):
        raise TypeError('{param_name} is set to {param} Not numerical'.format(
            param=param, param_name=param_name))

    if not isinstance(low, (numbers.Integral, np.integer, float)):
        raise TypeError('low is set to {low}. Not numerical'.format(low=low))

    if not isinstance(high, (numbers.Integral, np.integer, float)):
        raise TypeError('high is set to {high}. Not numerical'.format(
            high=high))

    ### at least one of the bounds should be specified
    if low is MIN_INT and high is MAX_INT:
        raise ValueError('Neither low nor high bounds is undefined')

    ### if wrong bound values are used
    if low > high:
        raise ValueError(
            'Lower bound > Higher bound')

    ### value check under different bound conditions
    if (include_left and include_right) and (param < low or param > high):
        raise ValueError(
            '{param_name} is set to {param}. '
            'Not in the range of [{low}, {high}].'.format(
                param=param, low=low, high=high, param_name=param_name))

    elif (include_left and not include_right) and (
            param < low or param >= high):
        raise ValueError(
            '{param_name} is set to {param}. '
            'Not in the range of [{low}, {high}).'.format(
                param=param, low=low, high=high, param_name=param_name))

    elif (not include_left and include_right) and (
            param <= low or param > high):
        raise ValueError(
            '{param_name} is set to {param}. '
            'Not in the range of ({low}, {high}].'.format(
                param=param, low=low, high=high, param_name=param_name))

    elif (not include_left and not include_right) and (
            param <= low or param >= high):
        raise ValueError(
            '{param_name} is set to {param}. '
            'Not in the range of ({low}, {high}).'.format(
                param=param, low=low, high=high, param_name=param_name))
    else:
        return True







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.check_detector)
defcheck_detector(detector):
"""Checks if fit and decision_function methods exist for given detector

    Parameters
    ----------
    detector : pyod.models
        Detector instance for which the check is performed.

    """

    if not hasattr(detector, 'fit') or not hasattr(detector,
                                                   'decision_function'):
        raise AttributeError("%s is not a detector instance." % (detector))







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.standardizer)
defstandardizer(X, X_t=None, keep_scalar=False):
"""Conduct Z-normalization on data to turn input samples become zero-mean
    and unit variance.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The training samples

    X_t : numpy array of shape (n_samples_new, n_features), optional (default=None)
        The data to be converted

    keep_scalar : bool, optional (default=False)
        The flag to indicate whether to return the scalar

    Returns
    -------
    X_norm : numpy array of shape (n_samples, n_features)
        X after the Z-score normalization

    X_t_norm : numpy array of shape (n_samples, n_features)
        X_t after the Z-score normalization

    scalar : sklearn scalar object
        The scalar used in conversion

    """
    X = check_array(X)
    scaler = StandardScaler().fit(X)

    if X_t is None:
        if keep_scalar:
            return scaler.transform(X), scaler
        else:
            return scaler.transform(X)
    else:
        X_t = check_array(X_t)
        if X.shape[1] != X_t.shape[1]:
            raise ValueError(
                "The number of input data feature should be consistent"
                "X has {0} features and X_t has {1} features.".format(
                    X.shape[1], X_t.shape[1]))
        if keep_scalar:
            return scaler.transform(X), scaler.transform(X_t), scaler
        else:
            return scaler.transform(X), scaler.transform(X_t)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.score_to_label)
defscore_to_label(pred_scores, outliers_fraction=0.1):
"""Turn raw outlier outlier scores to binary labels (0 or 1).

    Parameters
    ----------
    pred_scores : list or numpy array of shape (n_samples,)
        Raw outlier scores. Outliers are assumed have larger values.

    outliers_fraction : float in (0,1)
        Percentage of outliers.

    Returns
    -------
    outlier_labels : numpy array of shape (n_samples,)
        For each observation, tells whether or not
        it should be considered as an outlier according to the
        fitted model. Return the outlier probability, ranging
        in [0,1].
    """
    ### check input values
    pred_scores = column_or_1d(pred_scores)
    check_parameter(outliers_fraction, 0, 1)

    threshold = percentile(pred_scores, 100 * (1 - outliers_fraction))
    pred_labels = (pred_scores > threshold).astype('int')
    return pred_labels







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.precision_n_scores)
defprecision_n_scores(y, y_pred, n=None):
"""Utility function to calculate precision @ rank n.

    Parameters
    ----------
    y : list or numpy array of shape (n_samples,)
        The ground truth. Binary (0: inliers, 1: outliers).

    y_pred : list or numpy array of shape (n_samples,)
        The raw outlier scores as returned by a fitted model.

    n : int, optional (default=None)
        The number of outliers. if not defined, infer using ground truth.

    Returns
    -------
    precision_at_rank_n : float
        Precision at rank n score.

    """

    ### turn raw prediction decision scores into binary labels
    y_pred = get_label_n(y, y_pred, n)

    ### enforce formats of y and labels_
    y = column_or_1d(y)
    y_pred = column_or_1d(y_pred)

    return precision_score(y, y_pred)







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_label_n)
defget_label_n(y, y_pred, n=None):
"""Function to turn raw outlier scores into binary labels by assign 1
    to top n outlier scores.

    Parameters
    ----------
    y : list or numpy array of shape (n_samples,)
        The ground truth. Binary (0: inliers, 1: outliers).

    y_pred : list or numpy array of shape (n_samples,)
        The raw outlier scores as returned by a fitted model.

    n : int, optional (default=None)
        The number of outliers. if not defined, infer using ground truth.

    Returns
    -------
    labels : numpy array of shape (n_samples,)
        binary labels 0: normal points and 1: outliers

    Examples
    --------
    >>> from pyod.utils.utility import get_label_n
    >>> y = [0, 1, 1, 0, 0]
    >>> y_pred = [0.1, 0.5, 0.3, 0.2, 0.7]
    >>> get_label_n(y, y_pred)
    array([0, 1, 0, 0, 1])

    """

    ### enforce formats of inputs
    y = column_or_1d(y)
    y_pred = column_or_1d(y_pred)

    check_consistent_length(y, y_pred)
    y_len = len(y)  ### the length of targets

    ### calculate the percentage of outliers
    if n is not None:
        outliers_fraction = n / y_len
    else:
        outliers_fraction = np.count_nonzero(y) / y_len

    threshold = percentile(y_pred, 100 * (1 - outliers_fraction))
    y_pred = (y_pred > threshold).astype('int')

    return y_pred







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_intersection)
defget_intersection(lst1, lst2):
"""get the overlapping between two lists

    Parameters
    ----------
    li1 : list or numpy array
        Input list 1.

    li2 : list or numpy array
        Input list 2.

    Returns
    -------
    difference : list
        The overlapping between li1 and li2.
    """
    return list(set(lst1) & set(lst2))







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_list_diff)
defget_list_diff(li1, li2):
"""get the elements in li1 but not li2. li1-li2

    Parameters
    ----------
    li1 : list or numpy array
        Input list 1.

    li2 : list or numpy array
        Input list 2.

    Returns
    -------
    difference : list
        The difference between li1 and li2.
    """
    ### if isinstance(li1, (np.ndarray, np.generic)):
    ###     li1 = li1.tolist()
    ### if isinstance(li2, (np.ndarray, np.generic)):
    ###     li1 = li1.tolist()

    return (list(set(li1) - set(li2)))







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_diff_elements)
defget_diff_elements(li1, li2):
"""get the elements in li1 but not li2, and vice versa

    Parameters
    ----------
    li1 : list or numpy array
        Input list 1.

    li2 : list or numpy array
        Input list 2.

    Returns
    -------
    difference : list
        The difference between li1 and li2.
    """
    ### if isinstance(li1, (np.ndarray, np.generic)):
    ###     li1 = li1.tolist()
    ### if isinstance(li2, (np.ndarray, np.generic)):
    ###     li1 = li1.tolist()

    return (list(set(li1) - set(li2)) + list(set(li2) - set(li1)))







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.argmaxn)
defargmaxn(value_list, n, order='desc'):
"""Return the index of top n elements in the list
    if order is set to 'desc', otherwise return the index of n smallest ones.

    Parameters
    ----------
    value_list : list, array, numpy array of shape (n_samples,)
        A list containing all values.

    n : int
        The number of elements to select.

    order : str, optional (default='desc')
        The order to sort {'desc', 'asc'}:

        - 'desc': descending
        - 'asc': ascending

    Returns
    -------
    index_list : numpy array of shape (n,)
        The index of the top n elements.
    """

    value_list = column_or_1d(value_list)
    length = len(value_list)

    ### validate the choice of n
    check_parameter(n, 1, length, include_left=True, include_right=True,
                    param_name='n')

    ### for the smallest n, flip the value
    if order != 'desc':
        n = length - n

    value_sorted = np.partition(value_list, length - n)
    threshold = value_sorted[int(length - n)]

    if order == 'desc':
        return np.where(np.greater_equal(value_list, threshold))[0]
    else:  ### return the index of n smallest elements
        return np.where(np.less(value_list, threshold))[0]







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.invert_order)
definvert_order(scores, method='multiplication'):
""" Invert the order of a list of values. The smallest value becomes
    the largest in the inverted list. This is useful while combining
    multiple detectors since their score order could be different.

    Parameters
    ----------
    scores : list, array or numpy array with shape (n_samples,)
        The list of values to be inverted

    method : str, optional (default='multiplication')
        Methods used for order inversion. Valid methods are:

        - 'multiplication': multiply by -1
        - 'subtraction': max(scores) - scores

    Returns
    -------
    inverted_scores : numpy array of shape (n_samples,)
        The inverted list

    Examples
    --------
    >>> scores1 = [0.1, 0.3, 0.5, 0.7, 0.2, 0.1]
    >>> invert_order(scores1)
    array([-0.1, -0.3, -0.5, -0.7, -0.2, -0.1])
    >>> invert_order(scores1, method='subtraction')
    array([0.6, 0.4, 0.2, 0. , 0.5, 0.6])
    """

    scores = column_or_1d(scores)

    if method == 'multiplication':
        return scores.ravel() * -1

    if method == 'subtraction':
        return (scores.max() - scores).ravel()





def_get_sklearn_version():  ### pragma: no cover
""" Utility function to decide the version of sklearn.
    PyOD will result in different behaviors with different sklearn version

    Returns
    -------
    sk_learn version : int

    """

    sklearn_version = str(sklearn.__version__)
    ### print(sklearn_version)
    ### if int(sklearn_version.split(".")[1]) < 19 or int(
    ###         sklearn_version.split(".")[1]) > 24:
    ###     raise ValueError("Sklearn version error")
    ### print(sklearn_version)

    return sklearn_version

    ### return int(sklearn_version.split(".")[1])


### def _sklearn_version_21():  ### pragma: no cover
###     """ Utility function to decide the version of sklearn
###     In sklearn 21.0, LOF is changed. Specifically, _decision_function
###     is replaced by _score_samples
#
###     Returns
###     -------
###     sklearn_21_flag : bool
###         True if sklearn.__version__ is newer than 0.21.0
#
###     """
###     sklearn_version = str(sklearn.__version__)
###     if int(sklearn_version.split(".")[1]) > 20:
###         return True
###     else:
###         return False




[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_bagging_indices)
defgenerate_bagging_indices(random_state, bootstrap_features, n_features,
                             min_features, max_features):
""" Randomly draw feature indices. Internal use only.

    Modified from sklearn/ensemble/bagging.py

    Parameters
    ----------
    random_state : RandomState
        A random number generator instance to define the state of the random
        permutations generator.

    bootstrap_features : bool
        Specifies whether to bootstrap indice generation

    n_features : int
        Specifies the population size when generating indices

    min_features : int
        Lower limit for number of features to randomly sample

    max_features : int
        Upper limit for number of features to randomly sample

    Returns
    -------
    feature_indices : numpy array, shape (n_samples,)
        Indices for features to bag

    """

    ### Get valid random state
    random_state = check_random_state(random_state)

    ### decide number of features to draw
    random_n_features = random_state.randint(min_features, max_features)

    ### Draw indices
    feature_indices = generate_indices(random_state, bootstrap_features,
                                       n_features, random_n_features)

    return feature_indices







[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.generate_indices)
defgenerate_indices(random_state, bootstrap, n_population, n_samples):
""" Draw randomly sampled indices. Internal use only.

    See sklearn/ensemble/bagging.py

    Parameters
    ----------
    random_state : RandomState
        A random number generator instance to define the state of the random
        permutations generator.

    bootstrap :  bool
        Specifies whether to bootstrap indice generation

    n_population : int
        Specifies the population size when generating indices

    n_samples : int
        Specifies number of samples to draw

    Returns
    -------
    indices : numpy array, shape (n_samples,)
        randomly drawn indices
    """

    ### Draw sample indices
    if bootstrap:
        indices = random_state.randint(0, n_population, n_samples)
    else:
        indices = sample_without_replacement(n_population, n_samples,
                                             random_state=random_state)

    return indices





### todo: add a test for it in test_utility.py


[docs][](https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.utility.get_optimal_n_bins)
defget_optimal_n_bins(X, upper_bound=None, epsilon=1):
""" Determine optimal number of bins for a histogram using the Birge 
    Rozenblac method (see :cite:`birge2006many` for details.)
    See  https://doi.org/10.1051/ps:2006001 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        The samples to determine the optimal number of bins for. 
    upper_bound :  int, default=None 
        The maximum value of n_bins to be considered. 
        If set to None, np.sqrt(X.shape[0]) will be used as upper bound. 
    epsilon : float, default = 1 
        A stabilizing term added to the logarithm to prevent division by zero. 
    Returns 
    ------- 
    optimal_n_bins : int 
        The optimal value of n_bins according to the Birge Rozenblac method 
    """
    if upper_bound is None:
        upper_bound = int(np.sqrt(X.shape[0]))

    n = X.shape[0]
    maximum_likelihood = np.zeros((upper_bound - 1, 1))

    for i, b in enumerate(range(1, upper_bound)):
        histogram, _ = np.histogram(X, bins=b)

        maximum_likelihood[i] = np.sum(
            histogram * np.log(b * histogram / n + epsilon) - (
                    b - 1 + np.power(np.log(b), 2.5)))

    return np.argmax(maximum_likelihood) + 1




```

Copyright ¬© 2022, Yue Zhao 
Made with [Sphinx](https://www.sphinx-doc.org/) and [@pradyunsg](https://pradyunsg.me)'s [Furo](https://github.com/pradyunsg/furo)

---

## üìä Áà¨ÂèñÁªüËÆ°

- ÊÄªÈ°µÈù¢Êï∞: 65
- ÊÄªÂ≠óÁ¨¶Êï∞: 1,850,843
- ÂêÑÊ∑±Â∫¶È°µÈù¢ÂàÜÂ∏É:
  - Ê∑±Â∫¶ 0: 1 È°µÈù¢
  - Ê∑±Â∫¶ 1: 15 È°µÈù¢
  - Ê∑±Â∫¶ 2: 49 È°µÈù¢
