# 🌐 深度网站爬取器项目

基于 crawl4ai 开发的智能深度网站爬取工具，可以递归爬取任意网站的所有子页面并生成完整的 markdown 文档项目。

## 📁 项目文件结构

```
crawl4ai/
├── 🔥 deep_website_crawler.py        # 核心爬取器（主程序）
├── 🚀 run_deep_crawler.py            # 用户友好的运行脚本  
├── 🧪 test_deep_crawler.py           # 功能测试脚本
├── 📖 深度爬取器使用指南.md          # 详细使用说明
├── 📋 深度爬取器项目说明.md          # 本文件（项目总结）
├── example_usage.py                   # crawl4ai基础示例
├── interactive_crawl4ai.py           # 交互式爬取程序
├── test_crawl4ai.py                  # crawl4ai测试脚本
└── crawl4ai_results/                 # 已有爬取结果
    ├── html/                         # HTML文件
    ├── json/                         # 元数据文件
    └── markdown/                     # 单页markdown文件
```

## 🎯 核心功能

### ✨ 主要特性
- **🔄 递归深度爬取**: 自动发现并爬取网站的所有子页面
- **🎛️ 智能过滤**: 自动过滤无效链接、文件下载、搜索页面等
- **⚡ 并发处理**: 支持多线程并发爬取，提高效率
- **🛡️ 防封机制**: 请求延时、域名限制、User-Agent轮换
- **📊 实时监控**: 显示爬取进度、成功率、错误统计
- **💾 结果管理**: 自动保存HTML、Markdown、元数据
- **📚 文档合并**: 生成完整的markdown项目文档
- **🔧 配置灵活**: 深度限制、页面数控制、内容过滤等

### 🎨 输出格式
- **📄 合并markdown文档**: 包含所有页面的完整内容，带目录结构
- **🌐 单页HTML文件**: 保留原始页面结构和样式
- **📝 单页markdown文件**: 清洁的文本内容，适合阅读
- **📊 详细元数据**: JSON格式的爬取统计和链接信息

## 🚀 快速开始

### 1️⃣ 安装依赖
```bash
pip install crawl4ai aiohttp beautifulsoup4
```

### 2️⃣ 运行测试
```bash
python test_deep_crawler.py
```

### 3️⃣ 开始爬取
```bash
python run_deep_crawler.py
```

### 4️⃣ 查看结果
爬取完成后，查看输出目录中的 `merged_website_content.md` 文件。

## 📋 预配置的爬取示例

### 🔥 PyOD官方文档（推荐）
- **网站**: https://pyod.readthedocs.io/en/latest/index.html
- **特点**: 完整的Python异常检测库文档
- **配置**: 3层深度，最多100页面，智能过滤
- **输出**: 完整的PyOD学习文档

### 🌟 自定义网站爬取
- 修改 `run_deep_crawler.py` 中的配置
- 设置目标URL、深度、过滤规则等
- 支持包含/排除模式的正则表达式

## ⚙️ 核心配置参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `start_url` | 必填 | 起始爬取URL |
| `max_depth` | 3 | 最大爬取深度 |
| `max_pages` | 100 | 最大页面数量 |
| `concurrent_limit` | 3 | 并发线程数 |
| `delay` | 1.5 | 请求间隔（秒） |
| `same_domain_only` | True | 是否限制同域名 |
| `merge_markdown` | True | 是否生成合并文档 |

## 🎯 适用场景

### ✅ 推荐使用场景
- 📚 **技术文档网站**: API文档、官方指南、教程网站
- 📰 **内容网站**: 博客、新闻网站、知识库
- 🏢 **企业网站**: 产品介绍、公司信息
- 🔬 **学术资源**: 研究论文、学术网站
- 📖 **在线书籍**: 在线教材、文档集合

### ⚠️ 注意事项
- 始终遵守网站的 robots.txt 和使用条款
- 设置合理的请求频率，避免对服务器造成压力
- 仅用于合法的学习、研究或个人用途
- 对于大型网站，建议分批进行爬取

## 📊 性能指标

### 🚀 爬取效率
- **小型网站** (≤50页): 2-5分钟
- **中型网站** (50-200页): 5-20分钟  
- **大型网站** (≥200页): 20分钟以上

### 💾 资源占用
- **内存使用**: 50-200MB（取决于页面大小）
- **磁盘空间**: 每页面约1-10MB（包含HTML+Markdown）
- **网络带宽**: 根据页面大小和并发数而定

## 🔧 自定义和扩展

### 添加新的爬取配置
在 `run_deep_crawler.py` 的 `CRAWL_CONFIGS` 中添加:

```python
"my_site": CrawlConfig(
    start_url="https://my-target-site.com",
    max_depth=2,
    max_pages=50,
    include_patterns=[r'/docs/', r'/api/'],
    exclude_patterns=[r'/admin/', r'\.pdf$']
)
```

### 高级过滤规则
```python
# 只爬取特定路径
include_patterns=[r'/blog/', r'/post/', r'/article/']

# 排除特定内容
exclude_patterns=[
    r'\.pdf$', r'\.doc$',           # 文档文件
    r'/search\?', r'/filter\?',     # 搜索和过滤页面
    r'/admin/', r'/login/',         # 管理和登录页面
    r'/api/', r'/ajax/',            # API接口
    r'#', r'javascript:'           # 锚点和JS链接
]
```

## 📈 未来改进计划

### 🎯 计划中的功能
- [ ] **代理支持**: 支持HTTP/HTTPS代理和代理池
- [ ] **内容分析**: 基于AI的内容质量评估和分类
- [ ] **增量更新**: 支持增量爬取，只爬取更新的内容
- [ ] **多格式输出**: 支持PDF、Word、HTML等多种输出格式
- [ ] **可视化界面**: Web界面配置和监控爬取过程
- [ ] **分布式爬取**: 支持多机器协同爬取大型网站

### 🔧 技术优化
- [ ] **内存优化**: 优化大型网站的内存使用
- [ ] **速度优化**: 更智能的并发控制和缓存机制  
- [ ] **错误处理**: 更健壮的错误恢复和重试机制
- [ ] **内容提取**: 更精准的正文提取和噪音过滤

## 🆘 常见问题

### Q: 爬取速度太慢怎么办？
A: 可以适当增加 `concurrent_limit` 和减少 `delay`，但要注意不要被网站封禁。

### Q: 被网站阻止了怎么办？
A: 增加 `delay` 延时，减少 `concurrent_limit` 并发数，或考虑使用代理。

### Q: 爬取到太多无用页面？
A: 完善 `exclude_patterns` 排除规则，或使用 `include_patterns` 只包含特定页面。

### Q: 内存占用太高？
A: 减少 `max_pages` 和 `concurrent_limit`，或分批进行爬取。

## 📞 技术支持

### 🐛 问题反馈
如果遇到bug或有改进建议:
1. 检查错误日志和控制台输出
2. 尝试调整配置参数
3. 运行测试脚本验证环境

### 💡 使用技巧
- **首次使用**: 先运行 `test_deep_crawler.py` 验证环境
- **小规模测试**: 设置较小的 `max_pages` 和 `max_depth` 进行测试
- **监控进度**: 观察控制台输出了解爬取进展
- **结果检查**: 查看生成的 `merged_website_content.md` 文件

## 📄 许可和免责声明

本工具基于开源协议发布，仅供学习和研究使用。使用者需自行承担使用本工具的法律责任，并确保遵守相关法律法规和网站使用条款。

**请合法、合理、负责任地使用本工具！** 🙏

---

## 🎉 开始您的深度爬取之旅！

现在您已经拥有了一个功能完整的深度网站爬取工具。快去尝试爬取您感兴趣的网站，构建您的个人知识库吧！

**祝您使用愉快！** 🚀✨

