#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Crawl4AI äº¤äº’å¼ç¨‹åº
åŠŸèƒ½å®Œæ•´çš„ç½‘é¡µçˆ¬å–å·¥å…·ï¼Œé›†æˆå®˜æ–¹æ–‡æ¡£çš„æ‰€æœ‰ä¸»è¦åŠŸèƒ½

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0
åŸºäº: Crawl4AI v0.7.4 å®˜æ–¹æ–‡æ¡£
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
    from crawl4ai.extraction_strategy import LLMExtractionStrategy, CosineStrategy, JsonCssExtractionStrategy
    from crawl4ai.chunking_strategy import RegexChunking, NlpSentenceChunking
    from crawl4ai.content_filter_strategy import BM25ContentFilter
    # å¯¼å…¥åˆ†é¡µç›¸å…³çš„é…ç½®ç±»
    try:
        from crawl4ai.config import VirtualScrollConfig
    except ImportError:
        # å¦‚æœæ²¡æœ‰VirtualScrollConfigï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å­—å…¸é…ç½®
        VirtualScrollConfig = None
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£… crawl4ai: pip install crawl4ai")
    sys.exit(1)

class InteractiveCrawl4AI:
    """äº¤äº’å¼ Crawl4AI ä¸»ç±»"""
    
    def __init__(self):
        self.session_data = {}
        self.results_history = []
        self.config_file = "crawl4ai_config.json"
        self.results_dir = "crawl4ai_results"
        self.profiles_dir = "browser_profiles"
        self.auto_save_enabled = True
        self.load_config()
        self.setup_results_directory()
        self.setup_profiles_directory()
    
    def setup_results_directory(self):
        """åˆ›å»ºç»“æœä¿å­˜ç›®å½•"""
        try:
            os.makedirs(self.results_dir, exist_ok=True)
            # åˆ›å»ºå­ç›®å½•
            subdirs = ['html', 'markdown', 'json', 'screenshots', 'extracted']
            for subdir in subdirs:
                os.makedirs(os.path.join(self.results_dir, subdir), exist_ok=True)
            print(f"âœ… ç»“æœä¿å­˜ç›®å½•å·²å‡†å¤‡: {self.results_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºç»“æœç›®å½•å¤±è´¥: {e}")
    
    def setup_profiles_directory(self):
        """åˆ›å»ºæµè§ˆå™¨é…ç½®æ–‡ä»¶ç›®å½•"""
        try:
            os.makedirs(self.profiles_dir, exist_ok=True)
            print(f"âœ… æµè§ˆå™¨é…ç½®æ–‡ä»¶ç›®å½•å·²å‡†å¤‡: {self.profiles_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºé…ç½®æ–‡ä»¶ç›®å½•å¤±è´¥: {e}")
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.session_data = json.load(f)
                self.auto_save_enabled = self.session_data.get('auto_save_results', True)
                print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                self.session_data = {}
    
    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            self.session_data['auto_save_results'] = self.auto_save_enabled
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.session_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é…ç½®å¤±è´¥: {e}")
    
    def save_crawl_result(self, result, url, crawl_type="basic", extra_data=None):
        """å®æ—¶ä¿å­˜çˆ¬å–ç»“æœ"""
        if not self.auto_save_enabled:
            return None
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            domain = url.split('/')[2].replace(':', '_').replace('.', '_')
            base_filename = f"{timestamp}_{domain}_{crawl_type}"
            
            saved_files = []
            
            # ä¿å­˜HTML
            if hasattr(result, 'html') and result.html:
                html_file = os.path.join(self.results_dir, 'html', f"{base_filename}.html")
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(result.html)
                saved_files.append(html_file)
            
            # ä¿å­˜Markdown
            if hasattr(result, 'markdown') and result.markdown:
                md_file = os.path.join(self.results_dir, 'markdown', f"{base_filename}.md")
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(result.markdown)
                saved_files.append(md_file)
            
            # ä¿å­˜æå–çš„å†…å®¹
            if hasattr(result, 'extracted_content') and result.extracted_content:
                extracted_file = os.path.join(self.results_dir, 'extracted', f"{base_filename}_extracted.json")
                with open(extracted_file, 'w', encoding='utf-8') as f:
                    if isinstance(result.extracted_content, str):
                        json.dump({"content": result.extracted_content}, f, ensure_ascii=False, indent=2)
                    else:
                        json.dump(result.extracted_content, f, ensure_ascii=False, indent=2)
                saved_files.append(extracted_file)
            
            # ä¿å­˜æˆªå›¾
            if hasattr(result, 'screenshot') and result.screenshot:
                screenshot_file = os.path.join(self.results_dir, 'screenshots', f"{base_filename}.png")
                with open(screenshot_file, 'wb') as f:
                    f.write(result.screenshot)
                saved_files.append(screenshot_file)
            
            # ä¿å­˜å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
            metadata = {
                'timestamp': datetime.now().isoformat(),
                'url': url,
                'crawl_type': crawl_type,
                'success': result.success if hasattr(result, 'success') else True,
                'status_code': result.status_code if hasattr(result, 'status_code') else None,
                'title': result.metadata.get('title', 'N/A') if hasattr(result, 'metadata') else 'N/A',
                'html_length': len(result.html) if hasattr(result, 'html') else 0,
                'markdown_length': len(result.markdown) if hasattr(result, 'markdown') else 0,
                'links_count': {
                    'internal': len(result.links.get('internal', [])) if hasattr(result, 'links') else 0,
                    'external': len(result.links.get('external', [])) if hasattr(result, 'links') else 0
                },
                'media_count': {
                    'images': len(result.media.get('images', [])) if hasattr(result, 'media') else 0,
                    'videos': len(result.media.get('videos', [])) if hasattr(result, 'media') else 0
                },
                'saved_files': saved_files
            }
            
            if extra_data:
                metadata.update(extra_data)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_file = os.path.join(self.results_dir, 'json', f"{base_filename}_metadata.json")
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            saved_files.append(metadata_file)
            
            print(f"ğŸ’¾ ç»“æœå·²è‡ªåŠ¨ä¿å­˜ ({len(saved_files)} ä¸ªæ–‡ä»¶):")
            for file_path in saved_files:
                print(f"  ğŸ“ {os.path.basename(file_path)}")
            
            return {
                'base_filename': base_filename,
                'saved_files': saved_files,
                'metadata_file': metadata_file
            }
            
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨ä¿å­˜å¤±è´¥: {e}")
            return None
    
    def create_browser_profile(self, profile_name, description=""):
        """åˆ›å»ºæ–°çš„æµè§ˆå™¨é…ç½®æ–‡ä»¶"""
        try:
            profile_path = os.path.join(self.profiles_dir, profile_name)
            os.makedirs(profile_path, exist_ok=True)
            
            # ä¿å­˜é…ç½®æ–‡ä»¶å…ƒæ•°æ®
            metadata = {
                'name': profile_name,
                'description': description,
                'created': datetime.now().isoformat(),
                'last_used': None,
                'usage_count': 0,
                'websites': []
            }
            
            metadata_file = os.path.join(profile_path, 'profile_metadata.json')
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æµè§ˆå™¨é…ç½®æ–‡ä»¶å·²åˆ›å»º: {profile_name}")
            return profile_path
            
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def list_browser_profiles(self):
        """åˆ—å‡ºæ‰€æœ‰æµè§ˆå™¨é…ç½®æ–‡ä»¶"""
        try:
            profiles = []
            if os.path.exists(self.profiles_dir):
                for item in os.listdir(self.profiles_dir):
                    profile_path = os.path.join(self.profiles_dir, item)
                    if os.path.isdir(profile_path):
                        metadata_file = os.path.join(profile_path, 'profile_metadata.json')
                        if os.path.exists(metadata_file):
                            with open(metadata_file, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                                profiles.append(metadata)
            return profiles
        except Exception as e:
            print(f"âš ï¸ è·å–é…ç½®æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def update_profile_usage(self, profile_name, website_url):
        """æ›´æ–°é…ç½®æ–‡ä»¶ä½¿ç”¨è®°å½•"""
        try:
            profile_path = os.path.join(self.profiles_dir, profile_name)
            metadata_file = os.path.join(profile_path, 'profile_metadata.json')
            
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                metadata['last_used'] = datetime.now().isoformat()
                metadata['usage_count'] = metadata.get('usage_count', 0) + 1
                
                # è®°å½•è®¿é—®çš„ç½‘ç«™
                if website_url not in metadata.get('websites', []):
                    if 'websites' not in metadata:
                        metadata['websites'] = []
                    metadata['websites'].append(website_url)
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°é…ç½®æ–‡ä»¶ä½¿ç”¨è®°å½•å¤±è´¥: {e}")
    
    def get_browser_config(self, profile_name=None, **kwargs):
        """è·å–æµè§ˆå™¨é…ç½®"""
        config_params = {
            'headless': kwargs.get('headless', False),  # é»˜è®¤éæ— å¤´æ¨¡å¼ä»¥ä¾¿ç™»å½•
            'browser_type': kwargs.get('browser_type', 'chromium'),
            'use_persistent_context': True,
            'ignore_https_errors': True,
            'java_script_enabled': True,
            'viewport_width': kwargs.get('viewport_width', 1920),
            'viewport_height': kwargs.get('viewport_height', 1080)
        }
        
        if profile_name:
            profile_path = os.path.join(self.profiles_dir, profile_name)
            config_params['user_data_dir'] = profile_path
        
        # æ·»åŠ å…¶ä»–è‡ªå®šä¹‰å‚æ•°
        for key, value in kwargs.items():
            if key not in config_params:
                config_params[key] = value
        
        return BrowserConfig(**config_params)
    
    def display_banner(self):
        """æ˜¾ç¤ºç¨‹åºæ¨ªå¹…"""
        print("=" * 80)
        print("ğŸš€ Crawl4AI äº¤äº’å¼ç½‘é¡µçˆ¬å–å·¥å…·")
        print("ğŸ“š é›†æˆå®˜æ–¹æ–‡æ¡£çš„å®Œæ•´åŠŸèƒ½ | ç‰ˆæœ¬ 1.0")
        print("ğŸ”— åŸºäº: https://docs.crawl4ai.com/")
        print("=" * 80)
    
    def display_main_menu(self):
        """æ˜¾ç¤ºä¸»èœå•"""
        auto_save_status = "âœ…å¼€å¯" if self.auto_save_enabled else "âŒå…³é—­"
        print(f"\nğŸ“‹ ä¸»èœå• (è‡ªåŠ¨ä¿å­˜: {auto_save_status}):")
        print("1. åŸºç¡€ç½‘é¡µçˆ¬å–")
        print("2. é«˜çº§çˆ¬å–é…ç½®")
        print("3. ç»“æ„åŒ–æ•°æ®æå–")
        print("4. LLM æ™ºèƒ½æå–")
        print("5. æ‰¹é‡URLå¤„ç†")
        print("6. JavaScriptæ¸²æŸ“")
        print("7. å†…å®¹è¿‡æ»¤å’Œæ¸…ç†")
        print("8. è‡ªåŠ¨ç¿»é¡µçˆ¬å–")
        print("9. å¯¼å‡ºå’Œä¿å­˜")
        print("10. ä¼šè¯ç®¡ç†")
        print("11. æœç´¢å†å²è®°å½•")
        print("12. æµè§ˆå™¨é…ç½®æ–‡ä»¶ç®¡ç†")
        print("13. ç®¡ç†è‡ªåŠ¨ä¿å­˜")
        print("14. è®¾ç½®å’Œé…ç½®")
        print("15. å¸®åŠ©å’Œæ–‡æ¡£")
        print("0. é€€å‡ºç¨‹åº")
        print("-" * 50)
    
    async def basic_crawl(self):
        """åŸºç¡€ç½‘é¡µçˆ¬å–åŠŸèƒ½"""
        print("\nğŸš€ åŸºç¡€ç½‘é¡µçˆ¬å–")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        # å…¨é¡µæ»šåŠ¨é€‰é¡¹
        print("\næ˜¯å¦å¯ç”¨å…¨é¡µæ»šåŠ¨ (é€‚ç”¨äºåŠ¨æ€åŠ è½½å†…å®¹çš„ç½‘ç«™):")
        print("1. å¦ (æ™®é€šçˆ¬å–)")
        print("2. æ˜¯ (æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹)")
        
        scroll_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        scan_full_page = scroll_choice == "2"
        
        scroll_delay = 0.2  # é»˜è®¤æ»šåŠ¨å»¶è¿Ÿ
        if scan_full_page:
            delay_input = input("æ»šåŠ¨å»¶è¿Ÿæ—¶é—´ (ç§’ï¼Œé»˜è®¤0.2): ").strip()
            if delay_input and delay_input.replace('.', '').isdigit():
                scroll_delay = float(delay_input)
        
        # åŸºç¡€é€‰é¡¹
        print("\né€‰æ‹©è¾“å‡ºæ ¼å¼:")
        print("1. HTML (åŸå§‹)")
        print("2. æ¸…æ´æ–‡æœ¬")
        print("3. Markdown")
        print("4. å…¨éƒ¨")
        
        format_choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        try:
            async with AsyncWebCrawler(verbose=True) as crawler:
                # æ„å»ºçˆ¬å–å‚æ•°
                crawl_params = {'url': url}
                if scan_full_page:
                    crawl_params['scan_full_page'] = True
                    crawl_params['scroll_delay'] = scroll_delay
                    print(f"ğŸ”„ å¯ç”¨å…¨é¡µæ»šåŠ¨ï¼Œå»¶è¿Ÿ: {scroll_delay}ç§’")
                
                result = await crawler.arun(**crawl_params)
                
                if result.success:
                    print(f"\nâœ… çˆ¬å–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸŒ URL: {result.url}")
                    print(f"ğŸ“Š çŠ¶æ€ç : {result.status_code}")
                    print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.metadata.get('processing_time', 'N/A')}")
                    
                    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå†…å®¹
                    if format_choice == "1" or format_choice == "4":
                        print(f"\nğŸ“ HTMLå†…å®¹ ({len(result.html)} å­—ç¬¦):")
                        print("-" * 30)
                        print(result.html[:500] + "..." if len(result.html) > 500 else result.html)
                    
                    if format_choice == "2" or format_choice == "4":
                        print(f"\nğŸ§¹ æ¸…æ´æ–‡æœ¬ ({len(result.cleaned_html)} å­—ç¬¦):")
                        print("-" * 30)
                        print(result.cleaned_html[:500] + "..." if len(result.cleaned_html) > 500 else result.cleaned_html)
                    
                    if format_choice == "3" or format_choice == "4":
                        print(f"\nğŸ“‹ Markdown ({len(result.markdown)} å­—ç¬¦):")
                        print("-" * 30)
                        print(result.markdown[:500] + "..." if len(result.markdown) > 500 else result.markdown)
                    
                    # æ˜¾ç¤ºé“¾æ¥å’Œåª’ä½“ä¿¡æ¯
                    print(f"\nğŸ”— é“¾æ¥ç»Ÿè®¡:")
                    print(f"  å†…éƒ¨é“¾æ¥: {len(result.links.get('internal', []))}")
                    print(f"  å¤–éƒ¨é“¾æ¥: {len(result.links.get('external', []))}")
                    print(f"ğŸ–¼ï¸ åª’ä½“ç»Ÿè®¡:")
                    print(f"  å›¾ç‰‡: {len(result.media.get('images', []))}")
                    print(f"  è§†é¢‘: {len(result.media.get('videos', []))}")
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    save_info = self.save_crawl_result(result, url, 'basic_crawl')
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'basic_crawl',
                        'success': True,
                        'title': result.metadata.get('title', 'N/A')
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def advanced_crawl(self):
        """é«˜çº§çˆ¬å–é…ç½®"""
        print("\nâš™ï¸ é«˜çº§çˆ¬å–é…ç½®")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        # é«˜çº§é…ç½®é€‰é¡¹
        print("\nğŸ”§ é…ç½®é€‰é¡¹:")
        
        # ç”¨æˆ·ä»£ç†
        custom_ua = input("è‡ªå®šä¹‰User-Agent (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        
        # ç­‰å¾…æ—¶é—´
        wait_time = input("é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´/ç§’ (é»˜è®¤3): ").strip()
        wait_time = int(wait_time) if wait_time.isdigit() else 3
        
        # å»¶è¿Ÿæ—¶é—´
        delay = input("è¯·æ±‚å»¶è¿Ÿæ—¶é—´/ç§’ (é»˜è®¤1): ").strip()
        delay = float(delay) if delay.replace('.', '').isdigit() else 1.0
        
        # ç¼“å­˜é€‰é¡¹
        bypass_cache = input("ç»•è¿‡ç¼“å­˜? (y/N): ").strip().lower() == 'y'
        
        # å¤„ç†iframe
        process_iframes = input("å¤„ç†iframe? (y/N): ").strip().lower() == 'y'
        
        # ç§»é™¤è¦†ç›–å…ƒç´ 
        remove_overlay = input("ç§»é™¤è¦†ç›–å…ƒç´ ? (y/N): ").strip().lower() == 'y'
        
        # æˆªå›¾
        take_screenshot = input("æˆªå–å±å¹•æˆªå›¾? (y/N): ").strip().lower() == 'y'
        
        try:
            crawler_kwargs = {
                'verbose': True
            }
            
            if custom_ua:
                crawler_kwargs['user_agent'] = custom_ua
            
            arun_kwargs = {
                'url': url,
                'wait_for': f'sleep:{wait_time}',
                'delay_before_return_html': delay,
                'bypass_cache': bypass_cache,
                'process_iframes': process_iframes,
                'remove_overlay_elements': remove_overlay,
                'screenshot': take_screenshot
            }
            
            async with AsyncWebCrawler(**crawler_kwargs) as crawler:
                result = await crawler.arun(**arun_kwargs)
                
                if result.success:
                    print(f"\nâœ… é«˜çº§çˆ¬å–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸ“Š HTMLé•¿åº¦: {len(result.html)}")
                    print(f"ğŸ“ æ¸…æ´æ–‡æœ¬é•¿åº¦: {len(result.cleaned_html)}")
                    print(f"ğŸ“‹ Markdowné•¿åº¦: {len(result.markdown)}")
                    
                    if take_screenshot and hasattr(result, 'screenshot'):
                        print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜")
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {
                        'config': {
                            'wait_time': wait_time,
                            'delay': delay,
                            'bypass_cache': bypass_cache,
                            'process_iframes': process_iframes,
                            'remove_overlay': remove_overlay,
                            'screenshot': take_screenshot
                        }
                    }
                    save_info = self.save_crawl_result(result, url, 'advanced_crawl', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'advanced_crawl',
                        'success': True,
                        'config': extra_data['config']
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def structured_extraction(self):
        """ç»“æ„åŒ–æ•°æ®æå–"""
        print("\nğŸ¯ ç»“æ„åŒ–æ•°æ®æå–")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        print("\né€‰æ‹©æå–ç­–ç•¥:")
        print("1. CSSé€‰æ‹©å™¨æå–")
        print("2. JSON+CSSæå–")
        print("3. ä½™å¼¦ç›¸ä¼¼åº¦æå–")
        
        strategy_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        try:
            async with AsyncWebCrawler(verbose=True) as crawler:
                extraction_strategy = None
                
                if strategy_choice == "1":
                    # CSSé€‰æ‹©å™¨æå–
                    selector = input("è¯·è¾“å…¥CSSé€‰æ‹©å™¨: ").strip()
                    if selector:
                        result = await crawler.arun(
                            url=url,
                            css_selector=selector
                        )
                        
                elif strategy_choice == "2":
                    # JSON+CSSæå–
                    schema = {}
                    print("å®šä¹‰æå–æ¨¡å¼ (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                    while True:
                        key = input("å­—æ®µå: ").strip()
                        if not key:
                            break
                        selector = input(f"{key}çš„CSSé€‰æ‹©å™¨: ").strip()
                        if selector:
                            schema[key] = selector
                    
                    if schema:
                        extraction_strategy = JsonCssExtractionStrategy(schema)
                        result = await crawler.arun(
                            url=url,
                            extraction_strategy=extraction_strategy
                        )
                
                elif strategy_choice == "3":
                    # ä½™å¼¦ç›¸ä¼¼åº¦æå–
                    query = input("è¯·è¾“å…¥æŸ¥è¯¢å…³é”®è¯: ").strip()
                    if query:
                        extraction_strategy = CosineStrategy(
                            semantic_filter=query,
                            word_count_threshold=10,
                            max_dist=0.2,
                            linkage_method='ward',
                            top_k=3
                        )
                        result = await crawler.arun(
                            url=url,
                            extraction_strategy=extraction_strategy
                        )
                
                else:
                    # é»˜è®¤åŸºç¡€æå–
                    result = await crawler.arun(url=url)
                
                if result.success:
                    print(f"\nâœ… ç»“æ„åŒ–æå–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    
                    if result.extracted_content:
                        print(f"ğŸ“Š æå–çš„å†…å®¹:")
                        print("-" * 30)
                        if isinstance(result.extracted_content, str):
                            print(result.extracted_content[:1000] + "..." if len(result.extracted_content) > 1000 else result.extracted_content)
                        else:
                            print(json.dumps(result.extracted_content, ensure_ascii=False, indent=2)[:1000])
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {'strategy': strategy_choice}
                    save_info = self.save_crawl_result(result, url, 'structured_extraction', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'structured_extraction',
                        'strategy': strategy_choice,
                        'success': True
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ æå–å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def llm_extraction(self):
        """LLM æ™ºèƒ½æå–"""
        print("\nğŸ¤– LLM æ™ºèƒ½æå–")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        # LLMé…ç½®
        print("\nğŸ§  LLMé…ç½®:")
        api_token = input("è¯·è¾“å…¥API Token (OpenAI/å…¶ä»–): ").strip()
        if not api_token:
            print("âŒ éœ€è¦API Tokenæ‰èƒ½ä½¿ç”¨LLMåŠŸèƒ½")
            return
        
        model_name = input("æ¨¡å‹åç§° (é»˜è®¤gpt-3.5-turbo): ").strip()
        if not model_name:
            model_name = "gpt-3.5-turbo"
        
        instruction = input("æå–æŒ‡ä»¤ (ä¾‹å¦‚: æå–æ‰€æœ‰äº§å“ä¿¡æ¯): ").strip()
        if not instruction:
            instruction = "æå–é¡µé¢çš„ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯"
        
        try:
            extraction_strategy = LLMExtractionStrategy(
                provider="openai",
                api_token=api_token,
                model=model_name,
                instruction=instruction
            )
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                result = await crawler.arun(
                    url=url,
                    extraction_strategy=extraction_strategy
                )
                
                if result.success:
                    print(f"\nâœ… LLMæå–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    
                    if result.extracted_content:
                        print(f"ğŸ¤– LLMæå–ç»“æœ:")
                        print("-" * 30)
                        print(result.extracted_content)
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {
                        'model': model_name,
                        'instruction': instruction
                    }
                    save_info = self.save_crawl_result(result, url, 'llm_extraction', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'llm_extraction',
                        'model': model_name,
                        'instruction': instruction,
                        'success': True
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ LLMæå–å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def batch_crawl(self):
        """æ‰¹é‡URLå¤„ç†"""
        print("\nğŸ“Š æ‰¹é‡URLå¤„ç†")
        print("=" * 40)
        
        print("è¾“å…¥URLåˆ—è¡¨ (æ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ):")
        urls = []
        while True:
            url = input().strip()
            if not url:
                break
            urls.append(url)
        
        if not urls:
            print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•URL")
            return
        
        print(f"ğŸ“‹ å°†å¤„ç† {len(urls)} ä¸ªURL")
        
        # å¹¶å‘è®¾ç½®
        max_concurrent = input("æœ€å¤§å¹¶å‘æ•° (é»˜è®¤3): ").strip()
        max_concurrent = int(max_concurrent) if max_concurrent.isdigit() else 3
        
        try:
            async with AsyncWebCrawler(verbose=True) as crawler:
                results = []
                semaphore = asyncio.Semaphore(max_concurrent)
                
                async def crawl_single(url):
                    async with semaphore:
                        try:
                            result = await crawler.arun(url=url)
                            return result
                        except Exception as e:
                            print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                            return None
                
                # å¹¶å‘å¤„ç†
                tasks = [crawl_single(url) for url in urls]
                results = await asyncio.gather(*tasks)
                
                # ç»Ÿè®¡ç»“æœ
                success_count = sum(1 for r in results if r and r.success)
                
                print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
                print(f"ğŸ“Š æˆåŠŸ: {success_count}/{len(urls)}")
                
                for i, result in enumerate(results):
                    if result and result.success:
                        print(f"  âœ… {urls[i]} - æˆåŠŸ (æ ‡é¢˜: {result.metadata.get('title', 'N/A')[:50]})")
                    else:
                        print(f"  âŒ {urls[i]} - å¤±è´¥")
                
                # æ‰¹é‡ä¿å­˜ç»“æœ
                for i, result in enumerate(results):
                    if result and result.success:
                        self.save_crawl_result(result, urls[i], 'batch_crawl')
                
                # ä¿å­˜åˆ°å†å²
                self.results_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'type': 'batch_crawl',
                    'urls': urls,
                    'success_count': success_count,
                    'total_count': len(urls)
                })
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def javascript_rendering(self):
        """JavaScriptæ¸²æŸ“"""
        print("\nğŸŒ JavaScriptæ¸²æŸ“")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        # å…¨é¡µæ»šåŠ¨é€‰é¡¹
        print("\næ˜¯å¦å¯ç”¨å…¨é¡µæ»šåŠ¨ (é€‚ç”¨äºåŠ¨æ€åŠ è½½å†…å®¹çš„ç½‘ç«™):")
        print("1. å¦ (ä½¿ç”¨ä¸‹é¢çš„JavaScriptæ“ä½œ)")
        print("2. æ˜¯ (æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹)")
        
        scroll_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        scan_full_page = scroll_choice == "2"
        
        scroll_delay = 0.2  # é»˜è®¤æ»šåŠ¨å»¶è¿Ÿ
        if scan_full_page:
            delay_input = input("æ»šåŠ¨å»¶è¿Ÿæ—¶é—´ (ç§’ï¼Œé»˜è®¤0.2): ").strip()
            if delay_input and delay_input.replace('.', '').isdigit():
                scroll_delay = float(delay_input)
        
        js_choice = None
        if not scan_full_page:
            print("\né€‰æ‹©JavaScriptæ“ä½œ:")
            print("1. ç­‰å¾…å…ƒç´ åŠ è½½")
            print("2. æ‰§è¡Œè‡ªå®šä¹‰JSä»£ç ")
            print("3. ç‚¹å‡»å…ƒç´ ")
            print("4. æ»šåŠ¨é¡µé¢")
            
            js_choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        try:
            arun_kwargs = {'url': url}
            
            # æ·»åŠ å…¨é¡µæ»šåŠ¨å‚æ•°
            if scan_full_page:
                arun_kwargs['scan_full_page'] = True
                arun_kwargs['scroll_delay'] = scroll_delay
                print(f"ğŸ”„ å¯ç”¨å…¨é¡µæ»šåŠ¨ï¼Œå»¶è¿Ÿ: {scroll_delay}ç§’")
                print("ğŸ’¡ æ³¨æ„: æ»šåŠ¨è¿‡ç¨‹ä¸­æµè§ˆå™¨çª—å£ä¼šè‡ªåŠ¨å‘ä¸‹æ»šåŠ¨ä»¥åŠ è½½åŠ¨æ€å†…å®¹")
            
            if js_choice == "1":
                # ç­‰å¾…å…ƒç´ 
                selector = input("ç­‰å¾…çš„å…ƒç´ CSSé€‰æ‹©å™¨: ").strip()
                timeout = input("è¶…æ—¶æ—¶é—´/ç§’ (é»˜è®¤10): ").strip()
                timeout = int(timeout) if timeout.isdigit() else 10
                
                if selector:
                    arun_kwargs['wait_for'] = f"css:{selector}"
                    arun_kwargs['page_timeout'] = timeout * 1000
                    
            elif js_choice == "2":
                # æ‰§è¡ŒJSä»£ç 
                js_code = input("è¯·è¾“å…¥JavaScriptä»£ç : ").strip()
                if js_code:
                    arun_kwargs['js_code'] = [js_code]
                    
            elif js_choice == "3":
                # ç‚¹å‡»å…ƒç´ 
                selector = input("è¦ç‚¹å‡»çš„å…ƒç´ CSSé€‰æ‹©å™¨: ").strip()
                if selector:
                    js_code = f"document.querySelector('{selector}').click();"
                    arun_kwargs['js_code'] = [js_code]
                    arun_kwargs['wait_for'] = 'sleep:2'
                    
            elif js_choice == "4":
                # æ»šåŠ¨é¡µé¢
                scroll_type = input("æ»šåŠ¨ç±»å‹ (bottom/top): ").strip().lower()
                if scroll_type == "bottom":
                    js_code = "window.scrollTo(0, document.body.scrollHeight);"
                else:
                    js_code = "window.scrollTo(0, 0);"
                arun_kwargs['js_code'] = [js_code]
                arun_kwargs['wait_for'] = 'sleep:2'
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                result = await crawler.arun(**arun_kwargs)
                
                if result.success:
                    print(f"\nâœ… JavaScriptæ¸²æŸ“æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸ“Š HTMLé•¿åº¦: {len(result.html)}")
                    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(result.cleaned_html)}")
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {'js_operation': js_choice}
                    save_info = self.save_crawl_result(result, url, 'javascript_rendering', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'javascript_rendering',
                        'js_operation': js_choice,
                        'success': True
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ æ¸²æŸ“å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def content_filtering(self):
        """å†…å®¹è¿‡æ»¤å’Œæ¸…ç†"""
        print("\nğŸ” å†…å®¹è¿‡æ»¤å’Œæ¸…ç†")
        print("=" * 40)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        print("\né€‰æ‹©è¿‡æ»¤é€‰é¡¹:")
        print("1. åŸºç¡€æ¸…ç†")
        print("2. BM25å†…å®¹è¿‡æ»¤")
        print("3. è‡ªå®šä¹‰æ ‡ç­¾è¿‡æ»¤")
        
        filter_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        try:
            arun_kwargs = {'url': url}
            
            if filter_choice == "2":
                # BM25è¿‡æ»¤
                query = input("BM25æŸ¥è¯¢å…³é”®è¯: ").strip()
                if query:
                    content_filter = BM25ContentFilter(query)
                    arun_kwargs['content_filter'] = content_filter
                    
            elif filter_choice == "3":
                # è‡ªå®šä¹‰æ ‡ç­¾è¿‡æ»¤
                excluded_tags = input("è¦æ’é™¤çš„HTMLæ ‡ç­¾ (é€—å·åˆ†éš”): ").strip()
                if excluded_tags:
                    tags = [tag.strip() for tag in excluded_tags.split(',')]
                    arun_kwargs['excluded_tags'] = tags
            
            # å…¶ä»–æ¸…ç†é€‰é¡¹
            remove_overlay = input("ç§»é™¤è¦†ç›–å…ƒç´ ? (y/N): ").strip().lower() == 'y'
            if remove_overlay:
                arun_kwargs['remove_overlay_elements'] = True
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                result = await crawler.arun(**arun_kwargs)
                
                if result.success:
                    print(f"\nâœ… å†…å®¹è¿‡æ»¤æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸ“Š åŸå§‹HTMLé•¿åº¦: {len(result.html)}")
                    print(f"ğŸ§¹ æ¸…æ´æ–‡æœ¬é•¿åº¦: {len(result.cleaned_html)}")
                    print(f"ğŸ“‹ Markdowné•¿åº¦: {len(result.markdown)}")
                    
                    # æ˜¾ç¤ºæ¸…ç†åçš„å†…å®¹é¢„è§ˆ
                    print(f"\nğŸ“ æ¸…æ´å†…å®¹é¢„è§ˆ:")
                    print("-" * 30)
                    preview = result.cleaned_html[:500] + "..." if len(result.cleaned_html) > 500 else result.cleaned_html
                    print(preview)
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {'filter_type': filter_choice}
                    save_info = self.save_crawl_result(result, url, 'content_filtering', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'content_filtering',
                        'filter_type': filter_choice,
                        'success': True
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                else:
                    print(f"âŒ è¿‡æ»¤å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    async def pagination_crawl(self):
        """è‡ªåŠ¨ç¿»é¡µçˆ¬å–åŠŸèƒ½"""
        print("\nğŸ“„ è‡ªåŠ¨ç¿»é¡µçˆ¬å–")
        print("=" * 40)
        print("æ”¯æŒå¤šç§ç¿»é¡µç­–ç•¥ï¼Œé€‚ç”¨äºå„ç§åŠ¨æ€å†…å®¹ç½‘ç«™")
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        print("\né€‰æ‹©ç¿»é¡µç­–ç•¥:")
        print("1. è™šæ‹Ÿæ»šåŠ¨ (æ— é™æ»šåŠ¨ç½‘ç«™ï¼Œå¦‚ç¤¾äº¤åª’ä½“)")
        print("2. ç‚¹å‡»æŒ‰é’®ç¿»é¡µ (ä¼ ç»Ÿç¿»é¡µæŒ‰é’®)")
        print("3. JavaScriptæ³¨å…¥ç¿»é¡µ (è‡ªå®šä¹‰è„šæœ¬)")
        print("4. åˆ†é¡µURLæ¨¡å¼ (URLå‚æ•°ç¿»é¡µ)")
        print("5. æ™ºèƒ½æ··åˆæ¨¡å¼ (è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç­–ç•¥)")
        
        strategy_choice = input("è¯·é€‰æ‹©ç­–ç•¥ (1-5): ").strip()
        
        if strategy_choice == "1":
            await self._virtual_scroll_crawl(url)
        elif strategy_choice == "2":
            await self._button_click_crawl(url)
        elif strategy_choice == "3":
            await self._javascript_injection_crawl(url)
        elif strategy_choice == "4":
            await self._url_pattern_crawl(url)
        elif strategy_choice == "5":
            await self._smart_hybrid_crawl(url)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    async def _virtual_scroll_crawl(self, url: str):
        """è™šæ‹Ÿæ»šåŠ¨ç¿»é¡µç­–ç•¥"""
        print("\nğŸ”„ è™šæ‹Ÿæ»šåŠ¨ç¿»é¡µé…ç½®")
        print("-" * 30)
        
        # é…ç½®å‚æ•°
        max_pages = input("æœ€å¤§æ»šåŠ¨æ¬¡æ•° (é»˜è®¤10): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 10
        
        scroll_delay = input("æ»šåŠ¨é—´éš”æ—¶é—´/ç§’ (é»˜è®¤2): ").strip()
        scroll_delay = float(scroll_delay) if scroll_delay.replace('.', '').isdigit() else 2.0
        
        scroll_distance = input("å•æ¬¡æ»šåŠ¨è·ç¦»/åƒç´  (é»˜è®¤1000): ").strip()
        scroll_distance = int(scroll_distance) if scroll_distance.isdigit() else 1000
        
        # åœæ­¢æ¡ä»¶
        print("\nåœæ­¢æ¡ä»¶è®¾ç½®:")
        print("1. å›ºå®šæ»šåŠ¨æ¬¡æ•°")
        print("2. æ£€æµ‹åˆ°åº•éƒ¨")
        print("3. å†…å®¹ä¸å†å˜åŒ–")
        print("4. æŒ‡å®šå…³é”®è¯å‡ºç°")
        
        stop_condition = input("é€‰æ‹©åœæ­¢æ¡ä»¶ (1-4): ").strip()
        
        target_keyword = ""
        if stop_condition == "4":
            target_keyword = input("è¾“å…¥ç›®æ ‡å…³é”®è¯: ").strip()
        
        try:
            # æ„å»ºè™šæ‹Ÿæ»šåŠ¨é…ç½®
            if VirtualScrollConfig:
                # ä½¿ç”¨å®˜æ–¹é…ç½®ç±»
                scroll_config = VirtualScrollConfig(
                    pages=max_pages,
                    delay=scroll_delay,
                    scroll_distance=scroll_distance
                )
                crawler_config = {'virtual_scroll_config': scroll_config}
            else:
                # ä½¿ç”¨å­—å…¸é…ç½®
                crawler_config = {
                    'virtual_scroll': {
                        'enabled': True,
                        'pages': max_pages,
                        'delay': scroll_delay,
                        'scroll_distance': scroll_distance,
                        'stop_condition': stop_condition,
                        'target_keyword': target_keyword
                    }
                }
            
            print(f"\nğŸš€ å¼€å§‹è™šæ‹Ÿæ»šåŠ¨çˆ¬å–...")
            print(f"ğŸ“Š é…ç½®: {max_pages}æ¬¡æ»šåŠ¨, é—´éš”{scroll_delay}ç§’, è·ç¦»{scroll_distance}px")
            
            all_content = []
            page_count = 0
            previous_content_hash = ""
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                for page in range(max_pages):
                    print(f"\nğŸ”„ ç¬¬ {page + 1}/{max_pages} æ¬¡æ»šåŠ¨...")
                    
                    # æ„å»ºJavaScriptæ»šåŠ¨å‘½ä»¤
                    js_scroll_command = f"""
                    // æ»šåŠ¨åˆ°æŒ‡å®šä½ç½®
                    window.scrollBy(0, {scroll_distance});
                    
                    // ç­‰å¾…å†…å®¹åŠ è½½
                    await new Promise(resolve => setTimeout(resolve, {scroll_delay * 1000}));
                    
                    // æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨
                    const isAtBottom = (window.innerHeight + window.scrollY) >= document.body.offsetHeight - 100;
                    
                    // è¿”å›é¡µé¢ä¿¡æ¯
                    return {{
                        scrolled: true,
                        isAtBottom: isAtBottom,
                        currentScroll: window.scrollY,
                        bodyHeight: document.body.offsetHeight,
                        newContent: document.body.innerText.length
                    }};
                    """
                    
                    # æ‰§è¡Œçˆ¬å–
                    result = await crawler.arun(
                        url=url,
                        js_code=js_scroll_command,
                        wait_for=f"sleep:{scroll_delay}",
                        bypass_cache=True
                    )
                    
                    if result.success:
                        # å†…å®¹å»é‡æ£€æŸ¥
                        current_content_hash = hash(result.cleaned_html)
                        if current_content_hash == previous_content_hash:
                            print("ğŸ›‘ å†…å®¹æœªå˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨")
                            break
                        
                        previous_content_hash = current_content_hash
                        page_count += 1
                        
                        # ä¿å­˜å½“å‰é¡µé¢å†…å®¹
                        page_data = {
                            'page': page + 1,
                            'url': result.url,
                            'title': result.metadata.get('title', 'N/A'),
                            'content_length': len(result.cleaned_html),
                            'markdown_length': len(result.markdown),
                            'links_count': len(result.links.get('internal', [])) + len(result.links.get('external', [])),
                            'images_count': len(result.media.get('images', [])),
                            'html': result.html,
                            'cleaned_html': result.cleaned_html,
                            'markdown': result.markdown,
                            'links': result.links,
                            'media': result.media,
                            'metadata': result.metadata
                        }
                        
                        all_content.append(page_data)
                        
                        print(f"âœ… ç¬¬{page + 1}é¡µçˆ¬å–æˆåŠŸ")
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                        print(f"ğŸ”— é“¾æ¥æ•°: {page_data['links_count']}")
                        print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°: {page_data['images_count']}")
                        
                        # æ£€æŸ¥åœæ­¢æ¡ä»¶
                        if stop_condition == "2":
                            # æ£€æµ‹æ˜¯å¦åˆ°è¾¾åº•éƒ¨ï¼ˆé€šè¿‡JavaScriptè¿”å›å€¼ï¼‰
                            if hasattr(result, 'js_result') and result.js_result.get('isAtBottom'):
                                print("ğŸ›‘ å·²åˆ°è¾¾é¡µé¢åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                        
                        elif stop_condition == "4" and target_keyword:
                            if target_keyword.lower() in result.cleaned_html.lower():
                                print(f"ğŸ¯ å‘ç°ç›®æ ‡å…³é”®è¯ '{target_keyword}'ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                        
                        # æš‚åœä»¥é¿å…è¿‡å¿«è¯·æ±‚
                        await asyncio.sleep(scroll_delay)
                        
                    else:
                        print(f"âŒ ç¬¬{page + 1}é¡µçˆ¬å–å¤±è´¥: {result.error_message}")
                        break
            
            # åˆå¹¶ç»“æœå¹¶ä¿å­˜
            if all_content:
                await self._save_pagination_results(url, all_content, "virtual_scroll", {
                    'strategy': 'virtual_scroll',
                    'max_pages': max_pages,
                    'scroll_delay': scroll_delay,
                    'scroll_distance': scroll_distance,
                    'stop_condition': stop_condition,
                    'actual_pages': page_count
                })
                
                print(f"\nğŸ‰ è™šæ‹Ÿæ»šåŠ¨çˆ¬å–å®Œæˆ!")
                print(f"ğŸ“Š æ€»å…±çˆ¬å–äº† {page_count} é¡µ")
                print(f"ğŸ“„ æ€»å†…å®¹é•¿åº¦: {sum(len(p['cleaned_html']) for p in all_content)}")
                print(f"ğŸ”— æ€»é“¾æ¥æ•°: {sum(p['links_count'] for p in all_content)}")
                print(f"ğŸ–¼ï¸ æ€»å›¾ç‰‡æ•°: {sum(p['images_count'] for p in all_content)}")
            else:
                print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•å†…å®¹")
                
        except Exception as e:
            print(f"âŒ è™šæ‹Ÿæ»šåŠ¨çˆ¬å–é”™è¯¯: {e}")
    
    async def _button_click_crawl(self, url: str):
        """ç‚¹å‡»æŒ‰é’®ç¿»é¡µç­–ç•¥"""
        print("\nğŸ–±ï¸ ç‚¹å‡»æŒ‰é’®ç¿»é¡µé…ç½®")
        print("-" * 30)
        
        # æŒ‰é’®é€‰æ‹©å™¨é…ç½®
        print("è¯·é…ç½®ç¿»é¡µæŒ‰é’®:")
        next_button_selector = input("ä¸‹ä¸€é¡µæŒ‰é’®CSSé€‰æ‹©å™¨ (å¦‚: .next-page, #next-btn): ").strip()
        if not next_button_selector:
            print("âŒ å¿…é¡»æä¾›ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨")
            return
        
        max_pages = input("æœ€å¤§ç¿»é¡µæ•° (é»˜è®¤10): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 10
        
        click_delay = input("ç‚¹å‡»é—´éš”æ—¶é—´/ç§’ (é»˜è®¤3): ").strip()
        click_delay = float(click_delay) if click_delay.replace('.', '').isdigit() else 3.0
        
        wait_for_load = input("é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´/ç§’ (é»˜è®¤5): ").strip()
        wait_for_load = int(wait_for_load) if wait_for_load.isdigit() else 5
        
        # å¯é€‰çš„åœæ­¢æ¡ä»¶
        stop_selector = input("åœæ­¢æ¡ä»¶é€‰æ‹©å™¨ (å¯é€‰ï¼Œå¦‚æŒ‰é’®ç¦ç”¨ç±»): ").strip()
        
        try:
            print(f"\nğŸš€ å¼€å§‹æŒ‰é’®ç‚¹å‡»ç¿»é¡µ...")
            print(f"ğŸ”˜ æŒ‰é’®é€‰æ‹©å™¨: {next_button_selector}")
            print(f"ğŸ“Š æœ€å¤§ç¿»é¡µ: {max_pages}, é—´éš”: {click_delay}ç§’")
            
            all_content = []
            page_count = 0
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                # çˆ¬å–ç¬¬ä¸€é¡µ
                print("ğŸ“„ çˆ¬å–ç¬¬1é¡µ (åˆå§‹é¡µé¢)...")
                result = await crawler.arun(
                    url=url,
                    wait_for=f"sleep:{wait_for_load}"
                )
                
                if result.success:
                    page_data = self._extract_page_data(result, 1)
                    all_content.append(page_data)
                    page_count = 1
                    print(f"âœ… ç¬¬1é¡µçˆ¬å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                else:
                    print(f"âŒ ç¬¬1é¡µçˆ¬å–å¤±è´¥: {result.error_message}")
                    return
                
                # ç¿»é¡µçˆ¬å–
                for page in range(2, max_pages + 1):
                    print(f"\nğŸ”„ ç‚¹å‡»ç¿»é¡µåˆ°ç¬¬{page}é¡µ...")
                    
                    # æ„å»ºç‚¹å‡»ç¿»é¡µçš„JavaScript
                    click_js = f"""
                    // æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                    const nextButton = document.querySelector('{next_button_selector}');
                    
                    if (!nextButton) {{
                        return {{ error: 'Next button not found', selector: '{next_button_selector}' }};
                    }}
                    
                    // æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                    const isDisabled = nextButton.disabled || 
                                     nextButton.classList.contains('disabled') ||
                                     nextButton.getAttribute('aria-disabled') === 'true';
                    
                    if (isDisabled) {{
                        return {{ error: 'Next button is disabled', disabled: true }};
                    }}
                    
                    // æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                    nextButton.scrollIntoView({{ behavior: 'smooth', block: 'center' }});
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    
                    // ç‚¹å‡»æŒ‰é’®
                    nextButton.click();
                    
                    // ç­‰å¾…é¡µé¢æ›´æ–°
                    await new Promise(resolve => setTimeout(resolve, {click_delay * 1000}));
                    
                    return {{ 
                        clicked: true, 
                        page: {page},
                        buttonText: nextButton.textContent.trim(),
                        currentUrl: window.location.href
                    }};
                    """
                    
                    # æ‰§è¡Œç‚¹å‡»å’Œçˆ¬å–
                    result = await crawler.arun(
                        url=url if page == 2 else result.url,  # ç¬¬äºŒé¡µç”¨åŸURLï¼Œä¹‹åç”¨å½“å‰URL
                        js_code=click_js,
                        wait_for=f"sleep:{wait_for_load}",
                        bypass_cache=True
                    )
                    
                    if result.success:
                        # æ£€æŸ¥JavaScriptæ‰§è¡Œç»“æœ
                        if hasattr(result, 'js_result'):
                            js_result = result.js_result
                            if isinstance(js_result, dict) and js_result.get('error'):
                                print(f"ğŸ›‘ {js_result['error']}")
                                break
                        
                        page_data = self._extract_page_data(result, page)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹ï¼ˆé¿å…é‡å¤ï¼‰
                        if any(p['content_hash'] == page_data['content_hash'] for p in all_content):
                            print("ğŸ›‘ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                            break
                        
                        all_content.append(page_data)
                        page_count = page
                        
                        print(f"âœ… ç¬¬{page}é¡µçˆ¬å–æˆåŠŸ")
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                        print(f"ğŸ”— é“¾æ¥æ•°: {page_data['links_count']}")
                        
                        # æ£€æŸ¥åœæ­¢æ¡ä»¶
                        if stop_selector:
                            stop_element_js = f"document.querySelector('{stop_selector}') !== null"
                            stop_result = await crawler.arun(
                                url=result.url,
                                js_code=f"return {stop_element_js}",
                                wait_for="sleep:1"
                            )
                            if stop_result.success and hasattr(stop_result, 'js_result') and stop_result.js_result:
                                print("ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢æ¡ä»¶ï¼Œç»“æŸç¿»é¡µ")
                                break
                        
                        # æš‚åœä»¥é¿å…è¿‡å¿«è¯·æ±‚
                        await asyncio.sleep(click_delay)
                        
                    else:
                        print(f"âŒ ç¬¬{page}é¡µçˆ¬å–å¤±è´¥: {result.error_message}")
                        break
            
            # ä¿å­˜ç»“æœ
            if all_content:
                await self._save_pagination_results(url, all_content, "button_click", {
                    'strategy': 'button_click',
                    'next_button_selector': next_button_selector,
                    'max_pages': max_pages,
                    'click_delay': click_delay,
                    'actual_pages': page_count
                })
                
                print(f"\nğŸ‰ æŒ‰é’®ç‚¹å‡»ç¿»é¡µå®Œæˆ!")
                print(f"ğŸ“Š æˆåŠŸçˆ¬å– {page_count} é¡µ")
                print(f"ğŸ“„ æ€»å†…å®¹é•¿åº¦: {sum(len(p['cleaned_html']) for p in all_content)}")
            else:
                print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•å†…å®¹")
                
        except Exception as e:
            print(f"âŒ æŒ‰é’®ç‚¹å‡»ç¿»é¡µé”™è¯¯: {e}")
    
    async def _javascript_injection_crawl(self, url: str):
        """JavaScriptæ³¨å…¥ç¿»é¡µç­–ç•¥"""
        print("\nâš¡ JavaScriptæ³¨å…¥ç¿»é¡µé…ç½®")
        print("-" * 30)
        print("å¯ä»¥è‡ªå®šä¹‰JavaScriptä»£ç æ¥å®ç°å¤æ‚çš„ç¿»é¡µé€»è¾‘")
        
        max_pages = input("æœ€å¤§ç¿»é¡µæ•° (é»˜è®¤5): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 5
        
        page_delay = input("ç¿»é¡µé—´éš”æ—¶é—´/ç§’ (é»˜è®¤3): ").strip()
        page_delay = float(page_delay) if page_delay.replace('.', '').isdigit() else 3.0
        
        print("\né€‰æ‹©é¢„è®¾JavaScriptæ¨¡æ¿:")
        print("1. è‡ªåŠ¨æ£€æµ‹ç¿»é¡µæŒ‰é’®")
        print("2. æ»šåŠ¨è§¦å‘åŠ è½½æ›´å¤š")
        print("3. AJAXç¿»é¡µè¯·æ±‚")
        print("4. è‡ªå®šä¹‰JavaScriptä»£ç ")
        
        template_choice = input("é€‰æ‹©æ¨¡æ¿ (1-4): ").strip()
        
        if template_choice == "1":
            js_template = """
            // è‡ªåŠ¨æ£€æµ‹ç¿»é¡µæŒ‰é’®
            const selectors = [
                'a[rel="next"]', '.next', '.page-next', '#next',
                'button:contains("ä¸‹ä¸€é¡µ")', 'button:contains("Next")',
                'a:contains("ä¸‹ä¸€é¡µ")', 'a:contains("Next")',
                '.pagination .next', '.pager .next'
            ];
            
            let nextButton = null;
            for (const selector of selectors) {
                try {
                    nextButton = document.querySelector(selector);
                    if (nextButton && nextButton.offsetParent !== null) break;
                } catch(e) {}
            }
            
            if (!nextButton) {
                return { error: 'No next button found' };
            }
            
            nextButton.scrollIntoView({ behavior: 'smooth' });
            await new Promise(resolve => setTimeout(resolve, 1000));
            nextButton.click();
            
            return { clicked: true, buttonFound: nextButton.tagName + '.' + nextButton.className };
            """
            
        elif template_choice == "2":
            js_template = """
            // æ»šåŠ¨è§¦å‘åŠ è½½æ›´å¤š
            const loadMoreSelectors = [
                '.load-more', '#load-more', 'button:contains("åŠ è½½æ›´å¤š")',
                'button:contains("Load More")', '.show-more'
            ];
            
            // å…ˆå°è¯•æ»šåŠ¨åˆ°åº•éƒ¨
            window.scrollTo(0, document.body.scrollHeight);
            await new Promise(resolve => setTimeout(resolve, 2000));
            
            // æŸ¥æ‰¾åŠ è½½æ›´å¤šæŒ‰é’®
            let loadButton = null;
            for (const selector of loadMoreSelectors) {
                try {
                    loadButton = document.querySelector(selector);
                    if (loadButton && loadButton.offsetParent !== null) break;
                } catch(e) {}
            }
            
            if (loadButton) {
                loadButton.click();
                await new Promise(resolve => setTimeout(resolve, 3000));
                return { scrolled: true, clicked: true };
            }
            
            return { scrolled: true, clicked: false };
            """
            
        elif template_choice == "3":
            js_template = """
            // AJAXç¿»é¡µè¯·æ±‚ï¼ˆéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™è°ƒæ•´ï¼‰
            const currentPage = parseInt(document.querySelector('.current-page')?.textContent || '1');
            const nextPage = currentPage + 1;
            
            // è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç½‘ç«™çš„AJAXè¯·æ±‚è¿›è¡Œè°ƒæ•´
            try {
                const response = await fetch(`/api/content?page=${nextPage}`, {
                    method: 'GET',
                    headers: { 'Content-Type': 'application/json' }
                });
                
                if (response.ok) {
                    const data = await response.json();
                    // å°†æ–°å†…å®¹æ’å…¥åˆ°é¡µé¢ä¸­
                    const container = document.querySelector('.content-container');
                    if (container && data.html) {
                        container.innerHTML += data.html;
                    }
                    return { ajax: true, page: nextPage, success: true };
                }
            } catch(e) {
                return { ajax: true, error: e.message };
            }
            
            return { ajax: true, error: 'AJAX request failed' };
            """
            
        elif template_choice == "4":
            print("\nè¯·è¾“å…¥è‡ªå®šä¹‰JavaScriptä»£ç  (è¾“å…¥'END'ç»“æŸ):")
            js_lines = []
            while True:
                line = input("JS> ")
                if line.strip() == "END":
                    break
                js_lines.append(line)
            js_template = "\n".join(js_lines)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
        
        try:
            print(f"\nğŸš€ å¼€å§‹JavaScriptæ³¨å…¥ç¿»é¡µ...")
            print(f"ğŸ“Š æœ€å¤§ç¿»é¡µ: {max_pages}, é—´éš”: {page_delay}ç§’")
            
            all_content = []
            page_count = 0
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                current_url = url
                
                for page in range(1, max_pages + 1):
                    print(f"\nğŸ“„ å¤„ç†ç¬¬{page}é¡µ...")
                    
                    if page == 1:
                        # ç¬¬ä¸€é¡µï¼Œæ­£å¸¸çˆ¬å–
                        result = await crawler.arun(
                            url=current_url,
                            wait_for=f"sleep:{page_delay}"
                        )
                    else:
                        # åç»­é¡µé¢ï¼Œæ‰§è¡ŒJavaScriptç¿»é¡µ
                        result = await crawler.arun(
                            url=current_url,
                            js_code=js_template,
                            wait_for=f"sleep:{page_delay}",
                            bypass_cache=True
                        )
                    
                    if result.success:
                        page_data = self._extract_page_data(result, page)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹
                        if page > 1 and any(p['content_hash'] == page_data['content_hash'] for p in all_content):
                            print("ğŸ›‘ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        all_content.append(page_data)
                        page_count = page
                        current_url = result.url  # æ›´æ–°å½“å‰URL
                        
                        print(f"âœ… ç¬¬{page}é¡µå¤„ç†æˆåŠŸ")
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                        
                        # æ£€æŸ¥JavaScriptæ‰§è¡Œç»“æœ
                        if page > 1 and hasattr(result, 'js_result'):
                            js_result = result.js_result
                            if isinstance(js_result, dict):
                                if js_result.get('error'):
                                    print(f"ğŸ›‘ JavaScriptæ‰§è¡Œé”™è¯¯: {js_result['error']}")
                                    break
                                elif js_result.get('clicked'):
                                    print("âœ… æˆåŠŸæ‰§è¡Œç¿»é¡µæ“ä½œ")
                        
                        await asyncio.sleep(page_delay)
                        
                    else:
                        print(f"âŒ ç¬¬{page}é¡µå¤„ç†å¤±è´¥: {result.error_message}")
                        break
            
            # ä¿å­˜ç»“æœ
            if all_content:
                await self._save_pagination_results(url, all_content, "javascript_injection", {
                    'strategy': 'javascript_injection',
                    'template_choice': template_choice,
                    'max_pages': max_pages,
                    'page_delay': page_delay,
                    'actual_pages': page_count,
                    'custom_js': js_template[:200] + "..." if len(js_template) > 200 else js_template
                })
                
                print(f"\nğŸ‰ JavaScriptæ³¨å…¥ç¿»é¡µå®Œæˆ!")
                print(f"ğŸ“Š æˆåŠŸå¤„ç† {page_count} é¡µ")
                print(f"ğŸ“„ æ€»å†…å®¹é•¿åº¦: {sum(len(p['cleaned_html']) for p in all_content)}")
            else:
                print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å†…å®¹")
                
        except Exception as e:
            print(f"âŒ JavaScriptæ³¨å…¥ç¿»é¡µé”™è¯¯: {e}")
    
    async def _url_pattern_crawl(self, url: str):
        """åˆ†é¡µURLæ¨¡å¼ç¿»é¡µç­–ç•¥"""
        print("\nğŸ”— åˆ†é¡µURLæ¨¡å¼ç¿»é¡µé…ç½®")
        print("-" * 30)
        print("é€‚ç”¨äºURLä¸­åŒ…å«é¡µç å‚æ•°çš„ç½‘ç«™")
        
        print("\nåˆ†æURLæ¨¡å¼:")
        print(f"å½“å‰URL: {url}")
        
        # æ£€æµ‹URLä¸­å¯èƒ½çš„é¡µç å‚æ•°
        import re
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
        
        parsed_url = urlparse(url)
        params = parse_qs(parsed_url.query)
        
        # å¸¸è§çš„é¡µç å‚æ•°å
        page_param_names = ['page', 'p', 'pagenum', 'pageno', 'offset', 'start', 'skip']
        detected_params = []
        
        for param_name in page_param_names:
            if param_name in params:
                detected_params.append(param_name)
        
        if detected_params:
            print(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„é¡µç å‚æ•°: {', '.join(detected_params)}")
            suggested_param = detected_params[0]
        else:
            # æ£€æŸ¥URLè·¯å¾„ä¸­çš„æ•°å­—
            path_numbers = re.findall(r'/(\d+)/', parsed_url.path)
            if path_numbers:
                print(f"ğŸ” æ£€æµ‹åˆ°è·¯å¾„ä¸­çš„æ•°å­—: {', '.join(path_numbers)}")
                suggested_param = "path_number"
            else:
                suggested_param = "page"
        
        print("\né€‰æ‹©URLæ¨¡å¼:")
        print("1. æŸ¥è¯¢å‚æ•°æ¨¡å¼ (å¦‚: ?page=1, ?p=2)")
        print("2. è·¯å¾„å‚æ•°æ¨¡å¼ (å¦‚: /page/1/, /2/)")
        print("3. ç‰‡æ®µæ¨¡å¼ (å¦‚: #page=1)")
        print("4. è‡ªå®šä¹‰æ¨¡å¼")
        
        pattern_choice = input("é€‰æ‹©æ¨¡å¼ (1-4): ").strip()
        
        if pattern_choice == "1":
            param_name = input(f"é¡µç å‚æ•°å (å»ºè®®: {suggested_param}): ").strip() or suggested_param
            start_page = input("èµ·å§‹é¡µç  (é»˜è®¤1): ").strip()
            start_page = int(start_page) if start_page.isdigit() else 1
            
            def generate_url(base_url, page_num):
                if '?' in base_url:
                    # æ›¿æ¢ç°æœ‰å‚æ•°æˆ–æ·»åŠ æ–°å‚æ•°
                    parsed = urlparse(base_url)
                    params = parse_qs(parsed.query)
                    params[param_name] = [str(page_num)]
                    
                    new_query = urlencode(params, doseq=True)
                    return urlunparse(parsed._replace(query=new_query))
                else:
                    separator = '&' if '?' in base_url else '?'
                    return f"{base_url}{separator}{param_name}={page_num}"
            
        elif pattern_choice == "2":
            path_pattern = input("è·¯å¾„æ¨¡å¼ (å¦‚: /page/{page}/ æˆ– /{page}/): ").strip()
            if not path_pattern:
                path_pattern = "/page/{page}/"
            
            start_page = input("èµ·å§‹é¡µç  (é»˜è®¤1): ").strip()
            start_page = int(start_page) if start_page.isdigit() else 1
            
            def generate_url(base_url, page_num):
                parsed = urlparse(base_url)
                new_path = path_pattern.format(page=page_num)
                return urlunparse(parsed._replace(path=new_path))
            
        elif pattern_choice == "3":
            fragment_param = input("ç‰‡æ®µå‚æ•°å (é»˜è®¤page): ").strip() or "page"
            start_page = input("èµ·å§‹é¡µç  (é»˜è®¤1): ").strip()
            start_page = int(start_page) if start_page.isdigit() else 1
            
            def generate_url(base_url, page_num):
                parsed = urlparse(base_url)
                new_fragment = f"{fragment_param}={page_num}"
                return urlunparse(parsed._replace(fragment=new_fragment))
            
        elif pattern_choice == "4":
            print("è¯·è¾“å…¥URLæ¨¡æ¿ï¼Œä½¿ç”¨ {page} ä½œä¸ºé¡µç å ä½ç¬¦")
            print("ä¾‹å¦‚: https://example.com/news?page={page}")
            url_template = input("URLæ¨¡æ¿: ").strip()
            
            start_page = input("èµ·å§‹é¡µç  (é»˜è®¤1): ").strip()
            start_page = int(start_page) if start_page.isdigit() else 1
            
            def generate_url(base_url, page_num):
                return url_template.format(page=page_num)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
        
        max_pages = input(f"æœ€å¤§é¡µæ•° (é»˜è®¤10): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 10
        
        page_delay = input("é¡µé¢é—´éš”æ—¶é—´/ç§’ (é»˜è®¤2): ").strip()
        page_delay = float(page_delay) if page_delay.replace('.', '').isdigit() else 2.0
        
        # åœæ­¢æ¡ä»¶
        check_404 = input("æ£€æµ‹404é¡µé¢è‡ªåŠ¨åœæ­¢? (Y/n): ").strip().lower() != 'n'
        min_content_length = input("æœ€å°å†…å®¹é•¿åº¦ (é»˜è®¤100): ").strip()
        min_content_length = int(min_content_length) if min_content_length.isdigit() else 100
        
        try:
            print(f"\nğŸš€ å¼€å§‹URLæ¨¡å¼ç¿»é¡µçˆ¬å–...")
            print(f"ğŸ“Š é¡µç èŒƒå›´: {start_page} - {start_page + max_pages - 1}")
            print(f"â±ï¸ é—´éš”æ—¶é—´: {page_delay}ç§’")
            
            all_content = []
            page_count = 0
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                for page_offset in range(max_pages):
                    current_page = start_page + page_offset
                    current_url = generate_url(url, current_page)
                    
                    print(f"\nğŸ“„ çˆ¬å–ç¬¬{current_page}é¡µ...")
                    print(f"ğŸ”— URL: {current_url}")
                    
                    result = await crawler.arun(
                        url=current_url,
                        wait_for=f"sleep:{page_delay}",
                        bypass_cache=True
                    )
                    
                    if result.success:
                        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•ˆ
                        if check_404 and result.status_code == 404:
                            print("ğŸ›‘ æ£€æµ‹åˆ°404é¡µé¢ï¼Œåœæ­¢çˆ¬å–")
                            break
                        
                        if len(result.cleaned_html) < min_content_length:
                            print(f"ğŸ›‘ é¡µé¢å†…å®¹è¿‡çŸ­ ({len(result.cleaned_html)} < {min_content_length})ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                            break
                        
                        page_data = self._extract_page_data(result, current_page)
                        
                        # æ£€æŸ¥é‡å¤å†…å®¹
                        if any(p['content_hash'] == page_data['content_hash'] for p in all_content):
                            print("ğŸ›‘ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œåœæ­¢çˆ¬å–")
                            break
                        
                        all_content.append(page_data)
                        page_count += 1
                        
                        print(f"âœ… ç¬¬{current_page}é¡µçˆ¬å–æˆåŠŸ")
                        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                        print(f"ğŸ“Š çŠ¶æ€ç : {result.status_code}")
                        
                        await asyncio.sleep(page_delay)
                        
                    else:
                        print(f"âŒ ç¬¬{current_page}é¡µçˆ¬å–å¤±è´¥: {result.error_message}")
                        if check_404 and "404" in str(result.error_message):
                            print("ğŸ›‘ æ£€æµ‹åˆ°404é”™è¯¯ï¼Œåœæ­¢çˆ¬å–")
                            break
            
            # ä¿å­˜ç»“æœ
            if all_content:
                await self._save_pagination_results(url, all_content, "url_pattern", {
                    'strategy': 'url_pattern',
                    'pattern_choice': pattern_choice,
                    'start_page': start_page,
                    'max_pages': max_pages,
                    'page_delay': page_delay,
                    'actual_pages': page_count,
                    'url_pattern': generate_url(url, 999)  # ç¤ºä¾‹URL
                })
                
                print(f"\nğŸ‰ URLæ¨¡å¼ç¿»é¡µå®Œæˆ!")
                print(f"ğŸ“Š æˆåŠŸçˆ¬å– {page_count} é¡µ")
                print(f"ğŸ“„ æ€»å†…å®¹é•¿åº¦: {sum(len(p['cleaned_html']) for p in all_content)}")
            else:
                print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•å†…å®¹")
                
        except Exception as e:
            print(f"âŒ URLæ¨¡å¼ç¿»é¡µé”™è¯¯: {e}")
    
    async def _smart_hybrid_crawl(self, url: str):
        """æ™ºèƒ½æ··åˆæ¨¡å¼ç¿»é¡µç­–ç•¥"""
        print("\nğŸ§  æ™ºèƒ½æ··åˆæ¨¡å¼ç¿»é¡µ")
        print("-" * 30)
        print("è‡ªåŠ¨æ£€æµ‹ç½‘ç«™ç±»å‹å¹¶é€‰æ‹©æœ€ä½³ç¿»é¡µç­–ç•¥")
        
        max_pages = input("æœ€å¤§é¡µæ•° (é»˜è®¤10): ").strip()
        max_pages = int(max_pages) if max_pages.isdigit() else 10
        
        try:
            print(f"\nğŸ” æ­£åœ¨åˆ†æç½‘ç«™ç»“æ„...")
            
            async with AsyncWebCrawler(verbose=True) as crawler:
                # ç¬¬ä¸€æ­¥ï¼šåˆ†æé¦–é¡µ
                result = await crawler.arun(url=url, wait_for="sleep:3")
                
                if not result.success:
                    print(f"âŒ æ— æ³•åŠ è½½é¦–é¡µ: {result.error_message}")
                    return
                
                print("âœ… é¦–é¡µåŠ è½½æˆåŠŸï¼Œåˆ†æç¿»é¡µç‰¹å¾...")
                
                # åˆ†æé¡µé¢ç‰¹å¾çš„JavaScript
                analysis_js = """
                const features = {
                    hasNextButton: false,
                    hasPageNumbers: false,
                    hasLoadMore: false,
                    hasInfiniteScroll: false,
                    urlHasPageParam: false,
                    pageElements: []
                };
                
                // æ£€æµ‹ç¿»é¡µæŒ‰é’®
                const nextSelectors = [
                    'a[rel="next"]', '.next', '.page-next', '#next',
                    'button:contains("ä¸‹ä¸€é¡µ")', 'button:contains("Next")',
                    'a:contains("ä¸‹ä¸€é¡µ")', 'a:contains("Next")',
                    '.pagination .next', '.pager .next'
                ];
                
                for (const selector of nextSelectors) {
                    try {
                        const element = document.querySelector(selector);
                        if (element && element.offsetParent !== null) {
                            features.hasNextButton = true;
                            features.pageElements.push({
                                type: 'next_button',
                                selector: selector,
                                text: element.textContent.trim()
                            });
                            break;
                        }
                    } catch(e) {}
                }
                
                // æ£€æµ‹é¡µç 
                const pageSelectors = [
                    '.pagination a', '.pager a', '.page-numbers a',
                    '[class*="page"] a', '[id*="page"] a'
                ];
                
                for (const selector of pageSelectors) {
                    try {
                        const elements = document.querySelectorAll(selector);
                        if (elements.length > 1) {
                            features.hasPageNumbers = true;
                            features.pageElements.push({
                                type: 'page_numbers',
                                selector: selector,
                                count: elements.length
                            });
                            break;
                        }
                    } catch(e) {}
                }
                
                // æ£€æµ‹åŠ è½½æ›´å¤šæŒ‰é’®
                const loadMoreSelectors = [
                    '.load-more', '#load-more', '.show-more',
                    'button:contains("åŠ è½½æ›´å¤š")', 'button:contains("Load More")',
                    'button:contains("æŸ¥çœ‹æ›´å¤š")', 'button:contains("Show More")'
                ];
                
                for (const selector of loadMoreSelectors) {
                    try {
                        const element = document.querySelector(selector);
                        if (element && element.offsetParent !== null) {
                            features.hasLoadMore = true;
                            features.pageElements.push({
                                type: 'load_more',
                                selector: selector,
                                text: element.textContent.trim()
                            });
                            break;
                        }
                    } catch(e) {}
                }
                
                // æ£€æµ‹æ— é™æ»šåŠ¨ç‰¹å¾
                const scrollTriggers = [
                    '[data-infinite]', '[class*="infinite"]', '[id*="infinite"]',
                    '[class*="scroll"]', '[data-scroll]'
                ];
                
                for (const selector of scrollTriggers) {
                    try {
                        if (document.querySelector(selector)) {
                            features.hasInfiniteScroll = true;
                            features.pageElements.push({
                                type: 'infinite_scroll',
                                selector: selector
                            });
                            break;
                        }
                    } catch(e) {}
                }
                
                // æ£€æµ‹URLå‚æ•°
                const urlParams = new URLSearchParams(window.location.search);
                const pageParams = ['page', 'p', 'pagenum', 'pageno', 'offset', 'start'];
                for (const param of pageParams) {
                    if (urlParams.has(param)) {
                        features.urlHasPageParam = true;
                        features.pageElements.push({
                            type: 'url_param',
                            param: param,
                            value: urlParams.get(param)
                        });
                        break;
                    }
                }
                
                return features;
                """
                
                # æ‰§è¡Œåˆ†æ
                analysis_result = await crawler.arun(
                    url=url,
                    js_code=analysis_js,
                    wait_for="sleep:2"
                )
                
                if analysis_result.success and hasattr(analysis_result, 'js_result'):
                    features = analysis_result.js_result
                    
                    print("\nğŸ“Š ç½‘ç«™ç‰¹å¾åˆ†æç»“æœ:")
                    print(f"ğŸ”˜ ç¿»é¡µæŒ‰é’®: {'âœ…' if features.get('hasNextButton') else 'âŒ'}")
                    print(f"ğŸ”¢ é¡µç å¯¼èˆª: {'âœ…' if features.get('hasPageNumbers') else 'âŒ'}")
                    print(f"ğŸ“„ åŠ è½½æ›´å¤š: {'âœ…' if features.get('hasLoadMore') else 'âŒ'}")
                    print(f"â™¾ï¸ æ— é™æ»šåŠ¨: {'âœ…' if features.get('hasInfiniteScroll') else 'âŒ'}")
                    print(f"ğŸ”— URLé¡µç : {'âœ…' if features.get('urlHasPageParam') else 'âŒ'}")
                    
                    # æ™ºèƒ½é€‰æ‹©ç­–ç•¥
                    selected_strategy = None
                    strategy_reason = ""
                    
                    if features.get('hasInfiniteScroll') or features.get('hasLoadMore'):
                        selected_strategy = "virtual_scroll"
                        strategy_reason = "æ£€æµ‹åˆ°æ— é™æ»šåŠ¨æˆ–åŠ è½½æ›´å¤šæŒ‰é’®"
                    elif features.get('hasNextButton'):
                        selected_strategy = "button_click"
                        strategy_reason = "æ£€æµ‹åˆ°æ˜ç¡®çš„ç¿»é¡µæŒ‰é’®"
                    elif features.get('urlHasPageParam'):
                        selected_strategy = "url_pattern"
                        strategy_reason = "æ£€æµ‹åˆ°URLä¸­çš„é¡µç å‚æ•°"
                    elif features.get('hasPageNumbers'):
                        selected_strategy = "button_click"
                        strategy_reason = "æ£€æµ‹åˆ°é¡µç å¯¼èˆª"
                    else:
                        selected_strategy = "virtual_scroll"
                        strategy_reason = "æœªæ£€æµ‹åˆ°æ˜ç¡®ç¿»é¡µç‰¹å¾ï¼Œå°è¯•è™šæ‹Ÿæ»šåŠ¨"
                    
                    print(f"\nğŸ¯ æ¨èç­–ç•¥: {selected_strategy}")
                    print(f"ğŸ’¡ åŸå› : {strategy_reason}")
                    
                    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨æ¨èç­–ç•¥
                    use_recommended = input(f"\næ˜¯å¦ä½¿ç”¨æ¨èç­–ç•¥ '{selected_strategy}'? (Y/n): ").strip().lower() != 'n'
                    
                    if use_recommended:
                        print(f"\nğŸš€ ä½¿ç”¨æ¨èç­–ç•¥: {selected_strategy}")
                        
                        if selected_strategy == "virtual_scroll":
                            # ä½¿ç”¨æ£€æµ‹åˆ°çš„å…ƒç´ é…ç½®è™šæ‹Ÿæ»šåŠ¨
                            await self._execute_smart_virtual_scroll(url, max_pages, features)
                        elif selected_strategy == "button_click":
                            # ä½¿ç”¨æ£€æµ‹åˆ°çš„æŒ‰é’®é…ç½®ç‚¹å‡»ç¿»é¡µ
                            await self._execute_smart_button_click(url, max_pages, features)
                        elif selected_strategy == "url_pattern":
                            # ä½¿ç”¨æ£€æµ‹åˆ°çš„URLå‚æ•°é…ç½®ç¿»é¡µ
                            await self._execute_smart_url_pattern(url, max_pages, features)
                    else:
                        print("è¯·æ‰‹åŠ¨é€‰æ‹©å…¶ä»–ç¿»é¡µç­–ç•¥")
                        return
                else:
                    print("âŒ æ— æ³•åˆ†æç½‘ç«™ç‰¹å¾ï¼Œä½¿ç”¨é»˜è®¤è™šæ‹Ÿæ»šåŠ¨ç­–ç•¥")
                    await self._virtual_scroll_crawl(url)
                    
        except Exception as e:
            print(f"âŒ æ™ºèƒ½æ··åˆæ¨¡å¼é”™è¯¯: {e}")
    
    async def _execute_smart_virtual_scroll(self, url: str, max_pages: int, features: dict):
        """æ‰§è¡Œæ™ºèƒ½è™šæ‹Ÿæ»šåŠ¨"""
        print("ğŸ“± æ‰§è¡Œæ™ºèƒ½è™šæ‹Ÿæ»šåŠ¨ç­–ç•¥...")
        
        # ä»ç‰¹å¾ä¸­æå–é…ç½®
        load_more_elements = [elem for elem in features.get('pageElements', []) if elem['type'] == 'load_more']
        
        all_content = []
        page_count = 0
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            for page in range(max_pages):
                print(f"\nğŸ”„ ç¬¬ {page + 1}/{max_pages} æ¬¡å¤„ç†...")
                
                if load_more_elements:
                    # æœ‰åŠ è½½æ›´å¤šæŒ‰é’®ï¼Œä½¿ç”¨ç‚¹å‡»ç­–ç•¥
                    selector = load_more_elements[0]['selector']
                    js_code = f"""
                    // æ»šåŠ¨åˆ°åº•éƒ¨
                    window.scrollTo(0, document.body.scrollHeight);
                    await new Promise(resolve => setTimeout(resolve, 2000));
                    
                    // ç‚¹å‡»åŠ è½½æ›´å¤š
                    const loadBtn = document.querySelector('{selector}');
                    if (loadBtn && loadBtn.offsetParent !== null) {{
                        loadBtn.click();
                        await new Promise(resolve => setTimeout(resolve, 3000));
                        return {{ clicked: true, hasMore: true }};
                    }}
                    return {{ clicked: false, hasMore: false }};
                    """
                else:
                    # ä½¿ç”¨çº¯æ»šåŠ¨ç­–ç•¥
                    js_code = """
                    const originalHeight = document.body.scrollHeight;
                    window.scrollTo(0, document.body.scrollHeight);
                    await new Promise(resolve => setTimeout(resolve, 3000));
                    const newHeight = document.body.scrollHeight;
                    return { scrolled: true, heightChanged: newHeight > originalHeight };
                    """
                
                result = await crawler.arun(
                    url=url,
                    js_code=js_code,
                    wait_for="sleep:3",
                    bypass_cache=True
                )
                
                if result.success:
                    page_data = self._extract_page_data(result, page + 1)
                    
                    # æ£€æŸ¥é‡å¤å†…å®¹
                    if any(p['content_hash'] == page_data['content_hash'] for p in all_content):
                        print("ğŸ›‘ å†…å®¹æœªå˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨")
                        break
                    
                    all_content.append(page_data)
                    page_count += 1
                    
                    print(f"âœ… ç¬¬{page + 1}æ¬¡å¤„ç†æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(result.cleaned_html)}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå†…å®¹
                    if hasattr(result, 'js_result'):
                        js_result = result.js_result
                        if isinstance(js_result, dict):
                            if load_more_elements and not js_result.get('hasMore'):
                                print("ğŸ›‘ æ²¡æœ‰æ›´å¤šå†…å®¹å¯åŠ è½½")
                                break
                            elif not load_more_elements and not js_result.get('heightChanged'):
                                print("ğŸ›‘ é¡µé¢é«˜åº¦æœªå˜åŒ–ï¼Œå·²åˆ°åº•éƒ¨")
                                break
                else:
                    print(f"âŒ ç¬¬{page + 1}æ¬¡å¤„ç†å¤±è´¥: {result.error_message}")
                    break
        
        # ä¿å­˜ç»“æœ
        if all_content:
            await self._save_pagination_results(url, all_content, "smart_virtual_scroll", {
                'strategy': 'smart_virtual_scroll',
                'detected_features': features,
                'max_pages': max_pages,
                'actual_pages': page_count
            })
            
            print(f"\nğŸ‰ æ™ºèƒ½è™šæ‹Ÿæ»šåŠ¨å®Œæˆ!")
            print(f"ğŸ“Š æˆåŠŸå¤„ç† {page_count} æ¬¡")
            print(f"ğŸ“„ æ€»å†…å®¹é•¿åº¦: {sum(len(p['cleaned_html']) for p in all_content)}")
    
    async def _execute_smart_button_click(self, url: str, max_pages: int, features: dict):
        """æ‰§è¡Œæ™ºèƒ½æŒ‰é’®ç‚¹å‡»"""
        print("ğŸ–±ï¸ æ‰§è¡Œæ™ºèƒ½æŒ‰é’®ç‚¹å‡»ç­–ç•¥...")
        
        # ä»ç‰¹å¾ä¸­æå–æŒ‰é’®é€‰æ‹©å™¨
        next_button_elements = [elem for elem in features.get('pageElements', []) if elem['type'] == 'next_button']
        
        if not next_button_elements:
            print("âŒ æœªæ‰¾åˆ°ç¿»é¡µæŒ‰é’®")
            return
        
        selector = next_button_elements[0]['selector']
        await self._button_click_crawl(url)  # ä½¿ç”¨ç°æœ‰çš„æŒ‰é’®ç‚¹å‡»é€»è¾‘
    
    async def _execute_smart_url_pattern(self, url: str, max_pages: int, features: dict):
        """æ‰§è¡Œæ™ºèƒ½URLæ¨¡å¼"""
        print("ğŸ”— æ‰§è¡Œæ™ºèƒ½URLæ¨¡å¼ç­–ç•¥...")
        
        # ä»ç‰¹å¾ä¸­æå–URLå‚æ•°
        url_param_elements = [elem for elem in features.get('pageElements', []) if elem['type'] == 'url_param']
        
        if not url_param_elements:
            print("âŒ æœªæ‰¾åˆ°URLé¡µç å‚æ•°")
            return
        
        param_name = url_param_elements[0]['param']
        print(f"ğŸ” ä½¿ç”¨æ£€æµ‹åˆ°çš„å‚æ•°: {param_name}")
        
        await self._url_pattern_crawl(url)  # ä½¿ç”¨ç°æœ‰çš„URLæ¨¡å¼é€»è¾‘
    
    def _extract_page_data(self, result, page_num: int) -> dict:
        """æå–é¡µé¢æ•°æ®çš„é€šç”¨æ–¹æ³•"""
        content_hash = hash(result.cleaned_html)
        
        return {
            'page': page_num,
            'url': result.url,
            'title': result.metadata.get('title', 'N/A'),
            'content_length': len(result.cleaned_html),
            'markdown_length': len(result.markdown),
            'links_count': len(result.links.get('internal', [])) + len(result.links.get('external', [])),
            'images_count': len(result.media.get('images', [])),
            'content_hash': content_hash,
            'html': result.html,
            'cleaned_html': result.cleaned_html,
            'markdown': result.markdown,
            'links': result.links,
            'media': result.media,
            'metadata': result.metadata,
            'status_code': result.status_code
        }
    
    async def _save_pagination_results(self, original_url: str, all_content: list, strategy: str, config: dict):
        """ä¿å­˜ç¿»é¡µçˆ¬å–ç»“æœ"""
        try:
            # åˆå¹¶æ‰€æœ‰é¡µé¢çš„å†…å®¹
            merged_html = "\n".join([page['html'] for page in all_content])
            merged_cleaned = "\n".join([page['cleaned_html'] for page in all_content])
            merged_markdown = "\n".join([page['markdown'] for page in all_content])
            
            # åˆå¹¶é“¾æ¥å’Œåª’ä½“
            all_links = {'internal': [], 'external': []}
            all_media = {'images': [], 'videos': []}
            
            for page in all_content:
                all_links['internal'].extend(page['links'].get('internal', []))
                all_links['external'].extend(page['links'].get('external', []))
                all_media['images'].extend(page['media'].get('images', []))
                all_media['videos'].extend(page['media'].get('videos', []))
            
            # å»é‡
            all_links['internal'] = list(set(all_links['internal']))
            all_links['external'] = list(set(all_links['external']))
            all_media['images'] = list(set(all_media['images']))
            all_media['videos'] = list(set(all_media['videos']))
            
            # åˆ›å»ºåˆå¹¶çš„ç»“æœå¯¹è±¡ï¼ˆæ¨¡æ‹ŸCrawlResultï¼‰
            class MockResult:
                def __init__(self):
                    self.success = True
                    self.url = original_url
                    self.html = merged_html
                    self.cleaned_html = merged_cleaned
                    self.markdown = merged_markdown
                    self.links = all_links
                    self.media = all_media
                    self.metadata = {
                        'title': all_content[0]['title'] if all_content else 'N/A',
                        'strategy': strategy,
                        'total_pages': len(all_content),
                        'config': config,
                        'pages_info': [
                            {
                                'page': p['page'],
                                'url': p['url'],
                                'title': p['title'],
                                'content_length': p['content_length']
                            }
                            for p in all_content
                        ]
                    }
                    self.status_code = 200
            
            merged_result = MockResult()
            
            # ä¿å­˜åˆå¹¶çš„ç»“æœ
            extra_data = {
                'strategy': strategy,
                'config': config,
                'total_pages': len(all_content),
                'individual_pages': all_content
            }
            
            save_info = self.save_crawl_result(
                merged_result, 
                original_url, 
                f'pagination_{strategy}', 
                extra_data
            )
            
            # ä¿å­˜åˆ°å†å²
            history_entry = {
                'timestamp': datetime.now().isoformat(),
                'url': original_url,
                'type': f'pagination_{strategy}',
                'success': True,
                'strategy': strategy,
                'total_pages': len(all_content),
                'config': config
            }
            if save_info:
                history_entry['saved_files'] = save_info['saved_files']
                history_entry['base_filename'] = save_info['base_filename']
            
            self.results_history.append(history_entry)
            
            print(f"\nğŸ’¾ ç¿»é¡µç»“æœå·²ä¿å­˜")
            if save_info:
                print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_info['base_filename']}")
                print(f"ğŸ“‹ ä¿å­˜æ–‡ä»¶: {', '.join(save_info['saved_files'])}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¿»é¡µç»“æœæ—¶å‡ºé”™: {e}")
    
    def export_and_save(self):
        """å¯¼å‡ºå’Œä¿å­˜åŠŸèƒ½"""
        print("\nğŸ’¾ å¯¼å‡ºå’Œä¿å­˜")
        print("=" * 40)
        
        if not self.results_history:
            print("âŒ æ²¡æœ‰å¯å¯¼å‡ºçš„å†å²è®°å½•")
            return
        
        print("é€‰æ‹©å¯¼å‡ºæ ¼å¼:")
        print("1. JSONæ ¼å¼")
        print("2. CSVæ ¼å¼")
        print("3. æ–‡æœ¬æ ¼å¼")
        
        export_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            if export_choice == "1":
                # JSONå¯¼å‡º
                filename = f"crawl4ai_history_{timestamp}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(self.results_history, f, ensure_ascii=False, indent=2)
                print(f"âœ… å·²å¯¼å‡ºåˆ°: {filename}")
                
            elif export_choice == "2":
                # CSVå¯¼å‡º
                filename = f"crawl4ai_history_{timestamp}.csv"
                import csv
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    if self.results_history:
                        writer = csv.DictWriter(f, fieldnames=self.results_history[0].keys())
                        writer.writeheader()
                        writer.writerows(self.results_history)
                print(f"âœ… å·²å¯¼å‡ºåˆ°: {filename}")
                
            elif export_choice == "3":
                # æ–‡æœ¬å¯¼å‡º
                filename = f"crawl4ai_history_{timestamp}.txt"
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write("Crawl4AI å†å²è®°å½•\n")
                    f.write("=" * 50 + "\n\n")
                    for i, record in enumerate(self.results_history, 1):
                        f.write(f"{i}. {record.get('timestamp', 'N/A')}\n")
                        f.write(f"   ç±»å‹: {record.get('type', 'N/A')}\n")
                        f.write(f"   URL: {record.get('url', 'N/A')}\n")
                        f.write(f"   çŠ¶æ€: {'æˆåŠŸ' if record.get('success', False) else 'å¤±è´¥'}\n")
                        f.write("-" * 30 + "\n")
                print(f"âœ… å·²å¯¼å‡ºåˆ°: {filename}")
                
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
    
    def session_management(self):
        """ä¼šè¯ç®¡ç†"""
        print("\nâš™ï¸ ä¼šè¯ç®¡ç†")
        print("=" * 40)
        
        print("é€‰æ‹©æ“ä½œ:")
        print("1. æŸ¥çœ‹å½“å‰ä¼šè¯ä¿¡æ¯")
        print("2. ä¿å­˜ä¼šè¯")
        print("3. æ¸…é™¤å†å²è®°å½•")
        print("4. è®¾ç½®ä¼šè¯å‚æ•°")
        
        session_choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if session_choice == "1":
            # æŸ¥çœ‹ä¼šè¯ä¿¡æ¯
            print(f"\nğŸ“Š ä¼šè¯ç»Ÿè®¡:")
            print(f"å†å²è®°å½•æ•°: {len(self.results_history)}")
            print(f"é…ç½®æ–‡ä»¶: {self.config_file}")
            
            if self.results_history:
                success_count = sum(1 for r in self.results_history if r.get('success', False))
                print(f"æˆåŠŸç‡: {success_count}/{len(self.results_history)} ({success_count/len(self.results_history)*100:.1f}%)")
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                type_counts = {}
                for record in self.results_history:
                    record_type = record.get('type', 'unknown')
                    type_counts[record_type] = type_counts.get(record_type, 0) + 1
                
                print("\nğŸ“ˆ æ“ä½œç±»å‹ç»Ÿè®¡:")
                for op_type, count in type_counts.items():
                    print(f"  {op_type}: {count}")
        
        elif session_choice == "2":
            # ä¿å­˜ä¼šè¯
            self.save_config()
            
        elif session_choice == "3":
            # æ¸…é™¤å†å²
            confirm = input("ç¡®è®¤æ¸…é™¤æ‰€æœ‰å†å²è®°å½•? (y/N): ").strip().lower()
            if confirm == 'y':
                self.results_history.clear()
                print("âœ… å†å²è®°å½•å·²æ¸…é™¤")
            
        elif session_choice == "4":
            # è®¾ç½®ä¼šè¯å‚æ•°
            print("\nè®¾ç½®ä¼šè¯å‚æ•°:")
            max_history = input(f"æœ€å¤§å†å²è®°å½•æ•° (å½“å‰: {self.session_data.get('max_history', 100)}): ").strip()
            if max_history.isdigit():
                self.session_data['max_history'] = int(max_history)
            
            auto_save_config = input("è‡ªåŠ¨ä¿å­˜é…ç½®? (y/N): ").strip().lower()
            self.session_data['auto_save'] = auto_save_config == 'y'
            
            auto_save_results = input(f"è‡ªåŠ¨ä¿å­˜çˆ¬å–ç»“æœ? (å½“å‰: {'y' if self.auto_save_enabled else 'n'}): ").strip().lower()
            if auto_save_results in ['y', 'n']:
                self.auto_save_enabled = auto_save_results == 'y'
            
            print("âœ… ä¼šè¯å‚æ•°å·²æ›´æ–°")
    
    def search_history(self):
        """æœç´¢å†å²è®°å½•"""
        print("\nğŸ” æœç´¢å†å²è®°å½•")
        print("=" * 40)
        
        if not self.results_history:
            print("âŒ æ²¡æœ‰å†å²è®°å½•")
            return
        
        search_term = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ (URL/æ ‡é¢˜/ç±»å‹): ").strip().lower()
        if not search_term:
            return
        
        matches = []
        for i, record in enumerate(self.results_history):
            # æœç´¢URLã€æ ‡é¢˜ã€ç±»å‹
            searchable_text = " ".join([
                record.get('url', ''),
                record.get('title', ''),
                record.get('type', '')
            ]).lower()
            
            if search_term in searchable_text:
                matches.append((i, record))
        
        if matches:
            print(f"\nğŸ¯ æ‰¾åˆ° {len(matches)} æ¡åŒ¹é…è®°å½•:")
            print("-" * 50)
            
            for i, (index, record) in enumerate(matches[:10], 1):  # æœ€å¤šæ˜¾ç¤º10æ¡
                print(f"{i}. [{record.get('timestamp', 'N/A')}]")
                print(f"   ç±»å‹: {record.get('type', 'N/A')}")
                print(f"   URL: {record.get('url', 'N/A')}")
                print(f"   çŠ¶æ€: {'âœ…æˆåŠŸ' if record.get('success', False) else 'âŒå¤±è´¥'}")
                if record.get('title'):
                    print(f"   æ ‡é¢˜: {record['title'][:50]}...")
                print("-" * 30)
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®°å½•")
    
    async def manage_browser_profiles(self):
        """æµè§ˆå™¨é…ç½®æ–‡ä»¶ç®¡ç†"""
        print("\nğŸ‘¤ æµè§ˆå™¨é…ç½®æ–‡ä»¶ç®¡ç†")
        print("=" * 40)
        
        while True:
            profiles = self.list_browser_profiles()
            
            print("\né€‰æ‹©æ“ä½œ:")
            print("1. åˆ›å»ºæ–°é…ç½®æ–‡ä»¶")
            print("2. åˆ—å‡ºæ‰€æœ‰é…ç½®æ–‡ä»¶")
            print("3. ä½¿ç”¨é…ç½®æ–‡ä»¶çˆ¬å–")
            print("4. åˆ é™¤é…ç½®æ–‡ä»¶")
            print("5. è¿”å›ä¸»èœå•")
            
            choice = input("è¯·é€‰æ‹© (1-5): ").strip()
            
            if choice == "1":
                # åˆ›å»ºæ–°é…ç½®æ–‡ä»¶
                print("\nğŸ“ åˆ›å»ºæ–°çš„æµè§ˆå™¨é…ç½®æ–‡ä»¶")
                profile_name = input("é…ç½®æ–‡ä»¶åç§°: ").strip()
                if not profile_name:
                    print("âŒ é…ç½®æ–‡ä»¶åç§°ä¸èƒ½ä¸ºç©º")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if any(p['name'] == profile_name for p in profiles):
                    print("âŒ é…ç½®æ–‡ä»¶å·²å­˜åœ¨")
                    continue
                
                description = input("æè¿° (å¯é€‰): ").strip()
                
                profile_path = self.create_browser_profile(profile_name, description)
                if profile_path:
                    print(f"âœ… é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼")
                    print(f"ğŸ“ ä¿å­˜è·¯å¾„: {profile_path}")
                    print("\nğŸ’¡ æç¤º: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨æ­¤é…ç½®æ–‡ä»¶è¿›è¡Œçˆ¬å–ï¼Œç™»å½•çŠ¶æ€å°†è‡ªåŠ¨ä¿å­˜")
            
            elif choice == "2":
                # åˆ—å‡ºé…ç½®æ–‡ä»¶
                print(f"\nğŸ“‹ æµè§ˆå™¨é…ç½®æ–‡ä»¶åˆ—è¡¨ (å…± {len(profiles)} ä¸ª):")
                if not profiles:
                    print("âŒ æš‚æ— é…ç½®æ–‡ä»¶")
                else:
                    print("-" * 70)
                    for i, profile in enumerate(profiles, 1):
                        print(f"{i}. {profile['name']}")
                        if profile.get('description'):
                            print(f"   æè¿°: {profile['description']}")
                        print(f"   åˆ›å»ºæ—¶é—´: {profile.get('created', 'N/A')}")
                        print(f"   ä½¿ç”¨æ¬¡æ•°: {profile.get('usage_count', 0)}")
                        if profile.get('last_used'):
                            print(f"   æœ€åä½¿ç”¨: {profile['last_used']}")
                        if profile.get('websites'):
                            print(f"   å·²ç™»å½•ç½‘ç«™: {len(profile['websites'])} ä¸ª")
                            for site in profile['websites'][:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                                domain = site.split('/')[2] if '/' in site else site
                                print(f"     â€¢ {domain}")
                            if len(profile['websites']) > 3:
                                print(f"     ... è¿˜æœ‰ {len(profile['websites']) - 3} ä¸ª")
                        print("-" * 70)
            
            elif choice == "3":
                # ä½¿ç”¨é…ç½®æ–‡ä»¶çˆ¬å–
                if not profiles:
                    print("âŒ æš‚æ— é…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»º")
                    continue
                
                print("\nğŸš€ ä½¿ç”¨é…ç½®æ–‡ä»¶çˆ¬å–")
                print("é€‰æ‹©é…ç½®æ–‡ä»¶:")
                for i, profile in enumerate(profiles, 1):
                    print(f"{i}. {profile['name']} (ä½¿ç”¨ {profile.get('usage_count', 0)} æ¬¡)")
                
                try:
                    profile_idx = int(input("è¯·é€‰æ‹©é…ç½®æ–‡ä»¶ (è¾“å…¥æ•°å­—): ").strip()) - 1
                    if 0 <= profile_idx < len(profiles):
                        selected_profile = profiles[profile_idx]
                        await self.crawl_with_profile(selected_profile['name'])
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
            
            elif choice == "4":
                # åˆ é™¤é…ç½®æ–‡ä»¶
                if not profiles:
                    print("âŒ æš‚æ— é…ç½®æ–‡ä»¶")
                    continue
                
                print("\nğŸ—‘ï¸ åˆ é™¤é…ç½®æ–‡ä»¶")
                print("é€‰æ‹©è¦åˆ é™¤çš„é…ç½®æ–‡ä»¶:")
                for i, profile in enumerate(profiles, 1):
                    print(f"{i}. {profile['name']}")
                
                try:
                    profile_idx = int(input("è¯·é€‰æ‹©é…ç½®æ–‡ä»¶ (è¾“å…¥æ•°å­—): ").strip()) - 1
                    if 0 <= profile_idx < len(profiles):
                        selected_profile = profiles[profile_idx]
                        confirm = input(f"ç¡®è®¤åˆ é™¤é…ç½®æ–‡ä»¶ '{selected_profile['name']}'? (y/N): ").strip().lower()
                        if confirm == 'y':
                            import shutil
                            profile_path = os.path.join(self.profiles_dir, selected_profile['name'])
                            shutil.rmtree(profile_path)
                            print(f"âœ… é…ç½®æ–‡ä»¶ '{selected_profile['name']}' å·²åˆ é™¤")
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
            
            elif choice == "5":
                break
            
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
    
    async def crawl_with_profile(self, profile_name):
        """ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶è¿›è¡Œçˆ¬å–"""
        print(f"\nğŸš€ ä½¿ç”¨é…ç½®æ–‡ä»¶ '{profile_name}' è¿›è¡Œçˆ¬å–")
        print("=" * 50)
        
        url = input("è¯·è¾“å…¥URL: ").strip()
        if not url:
            print("âŒ URLä¸èƒ½ä¸ºç©º")
            return
        
        # å…¨é¡µæ»šåŠ¨é€‰é¡¹
        print("\næ˜¯å¦å¯ç”¨å…¨é¡µæ»šåŠ¨ (é€‚ç”¨äºåŠ¨æ€åŠ è½½å†…å®¹çš„ç½‘ç«™):")
        print("1. å¦ (æ™®é€šçˆ¬å–)")
        print("2. æ˜¯ (æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹)")
        
        scroll_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        scan_full_page = scroll_choice == "2"
        
        scroll_delay = 0.2  # é»˜è®¤æ»šåŠ¨å»¶è¿Ÿ
        if scan_full_page:
            delay_input = input("æ»šåŠ¨å»¶è¿Ÿæ—¶é—´ (ç§’ï¼Œé»˜è®¤0.2): ").strip()
            if delay_input and delay_input.replace('.', '').isdigit():
                scroll_delay = float(delay_input)
        
        # é€‰æ‹©çˆ¬å–æ¨¡å¼
        print("\né€‰æ‹©çˆ¬å–æ¨¡å¼:")
        print("1. åŸºç¡€çˆ¬å–")
        print("2. JavaScriptæ¸²æŸ“çˆ¬å–")
        print("3. é«˜çº§é…ç½®çˆ¬å–")
        
        mode_choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        try:
            # è·å–æµè§ˆå™¨é…ç½®
            browser_config = self.get_browser_config(profile_name)
            
            # æ ¹æ®æ¨¡å¼è®¾ç½®ä¸åŒå‚æ•°
            crawler_kwargs = {'config': browser_config, 'verbose': True}
            arun_kwargs = {'url': url}
            
            # æ·»åŠ å…¨é¡µæ»šåŠ¨å‚æ•°
            if scan_full_page:
                arun_kwargs['scan_full_page'] = True
                arun_kwargs['scroll_delay'] = scroll_delay
                print(f"ğŸ”„ å¯ç”¨å…¨é¡µæ»šåŠ¨ï¼Œå»¶è¿Ÿ: {scroll_delay}ç§’")
                print("ğŸ’¡ æ³¨æ„: æ»šåŠ¨è¿‡ç¨‹ä¸­æµè§ˆå™¨çª—å£ä¼šè‡ªåŠ¨å‘ä¸‹æ»šåŠ¨ä»¥åŠ è½½åŠ¨æ€å†…å®¹")
            
            if mode_choice == "2":
                # JavaScriptæ¸²æŸ“æ¨¡å¼
                wait_time = input("ç­‰å¾…æ—¶é—´ (ç§’, é»˜è®¤3): ").strip()
                wait_time = int(wait_time) if wait_time.isdigit() else 3
                arun_kwargs['wait_for'] = f'sleep:{wait_time}'
                # æ³¨æ„ï¼šå¦‚æœå¯ç”¨äº†scan_full_pageï¼Œå°±ä¸éœ€è¦æ‰‹åŠ¨æ»šåŠ¨äº†
                if not scan_full_page:
                    arun_kwargs['js_code'] = "window.scrollTo(0, document.body.scrollHeight);"
                
            elif mode_choice == "3":
                # é«˜çº§é…ç½®æ¨¡å¼
                wait_time = input("ç­‰å¾…æ—¶é—´ (ç§’, é»˜è®¤3): ").strip()
                wait_time = int(wait_time) if wait_time.isdigit() else 3
                arun_kwargs['wait_for'] = f'sleep:{wait_time}'
                
                screenshot = input("æˆªå›¾? (y/N): ").strip().lower() == 'y'
                arun_kwargs['screenshot'] = screenshot
                
                bypass_cache = input("ç»•è¿‡ç¼“å­˜? (y/N): ").strip().lower() == 'y'
                arun_kwargs['bypass_cache'] = bypass_cache
            
            print(f"\nğŸŒ å¼€å§‹ä½¿ç”¨é…ç½®æ–‡ä»¶ '{profile_name}' çˆ¬å–...")
            print("ğŸ’¡ æç¤º: æµè§ˆå™¨å°†ä»¥éæ— å¤´æ¨¡å¼å¯åŠ¨ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨ç™»å½•ç½‘ç«™")
            
            async with AsyncWebCrawler(**crawler_kwargs) as crawler:
                result = await crawler.arun(**arun_kwargs)
                
                if result.success:
                    print(f"\nâœ… çˆ¬å–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸŒ URL: {url}")
                    print(f"ğŸ“Š çŠ¶æ€ç : {result.status_code}")
                    print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
                    if scan_full_page:
                        print(f"ğŸ”„ å…¨é¡µæ»šåŠ¨å·²å®Œæˆï¼Œå†…å®¹å¯èƒ½åŒ…å«åŠ¨æ€åŠ è½½çš„éƒ¨åˆ†")
                    
                    # æ›´æ–°é…ç½®æ–‡ä»¶ä½¿ç”¨è®°å½•
                    self.update_profile_usage(profile_name, url)
                    
                    # å®æ—¶ä¿å­˜ç»“æœ
                    extra_data = {'profile_used': profile_name}
                    save_info = self.save_crawl_result(result, url, 'profile_crawl', extra_data)
                    
                    # ä¿å­˜åˆ°å†å²
                    history_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'url': url,
                        'type': 'profile_crawl',
                        'profile_used': profile_name,
                        'success': True,
                        'title': result.metadata.get('title', 'N/A')
                    }
                    if save_info:
                        history_entry['saved_files'] = save_info['saved_files']
                        history_entry['base_filename'] = save_info['base_filename']
                    self.results_history.append(history_entry)
                    
                    # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
                    if len(result.markdown) > 500:
                        print(f"\nğŸ“‹ å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
                        print("-" * 30)
                        print(result.markdown[:500] + "...")
                    else:
                        print(f"\nğŸ“‹ å®Œæ•´å†…å®¹:")
                        print("-" * 30)
                        print(result.markdown)
                    
                    print(f"\nâœ… é…ç½®æ–‡ä»¶ '{profile_name}' çš„ç™»å½•çŠ¶æ€å·²è‡ªåŠ¨ä¿å­˜")
                    
                else:
                    print(f"\nâŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def show_settings(self):
        """æ˜¾ç¤ºè®¾ç½®å’Œé…ç½®"""
        print("\nâš™ï¸ è®¾ç½®å’Œé…ç½®")
        print("=" * 40)
        
        print("å½“å‰é…ç½®:")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {self.config_file}")
        print(f"ğŸ“Š å†å²è®°å½•: {len(self.results_history)} æ¡")
        print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜é…ç½®: {'æ˜¯' if self.session_data.get('auto_save', False) else 'å¦'}")
        print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜ç»“æœ: {'æ˜¯' if self.auto_save_enabled else 'å¦'}")
        print(f"ğŸ“ˆ æœ€å¤§å†å²: {self.session_data.get('max_history', 100)} æ¡")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        
        print("\nå¯ç”¨åŠŸèƒ½:")
        features = [
            "âœ… åŸºç¡€ç½‘é¡µçˆ¬å–",
            "âœ… é«˜çº§é…ç½®é€‰é¡¹",
            "âœ… ç»“æ„åŒ–æ•°æ®æå–",
            "âœ… LLMæ™ºèƒ½æå–",
            "âœ… æ‰¹é‡URLå¤„ç†",
            "âœ… JavaScriptæ¸²æŸ“",
            "âœ… å†…å®¹è¿‡æ»¤æ¸…ç†",
            "âœ… å¤šæ ¼å¼å¯¼å‡º",
            "âœ… ä¼šè¯ç®¡ç†",
            "âœ… å†å²è®°å½•æœç´¢"
        ]
        
        for feature in features:
            print(f"  {feature}")
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nâ“ å¸®åŠ©å’Œæ–‡æ¡£")
        print("=" * 40)
        
        print("ğŸš€ Crawl4AI äº¤äº’å¼å·¥å…·ä½¿ç”¨æŒ‡å—")
        print("\nğŸ“š ä¸»è¦åŠŸèƒ½:")
        
        help_info = [
            ("1ï¸âƒ£ åŸºç¡€çˆ¬å–", "ç®€å•çš„ç½‘é¡µå†…å®¹æŠ“å–ï¼Œæ”¯æŒHTMLã€æ–‡æœ¬ã€Markdownæ ¼å¼"),
            ("2ï¸âƒ£ é«˜çº§é…ç½®", "è‡ªå®šä¹‰User-Agentã€ç­‰å¾…æ—¶é—´ã€ç¼“å­˜ã€iframeå¤„ç†ç­‰"),
            ("3ï¸âƒ£ ç»“æ„åŒ–æå–", "ä½¿ç”¨CSSé€‰æ‹©å™¨ã€JSONæ¨¡å¼ã€ä½™å¼¦ç›¸ä¼¼åº¦æå–ç‰¹å®šå†…å®¹"),
            ("4ï¸âƒ£ LLMæ™ºèƒ½æå–", "é›†æˆOpenAIç­‰LLMæ¨¡å‹è¿›è¡Œæ™ºèƒ½å†…å®¹æå–"),
            ("5ï¸âƒ£ æ‰¹é‡å¤„ç†", "åŒæ—¶å¤„ç†å¤šä¸ªURLï¼Œæ”¯æŒå¹¶å‘æ§åˆ¶"),
            ("6ï¸âƒ£ JavaScriptæ¸²æŸ“", "å¤„ç†åŠ¨æ€å†…å®¹ï¼Œæ‰§è¡ŒJSä»£ç ï¼Œç­‰å¾…å…ƒç´ åŠ è½½"),
            ("7ï¸âƒ£ å†…å®¹è¿‡æ»¤", "ä½¿ç”¨BM25ç®—æ³•ã€æ ‡ç­¾è¿‡æ»¤ç­‰æ¸…ç†å’Œè¿‡æ»¤å†…å®¹"),
            ("8ï¸âƒ£ å¯¼å‡ºä¿å­˜", "æ”¯æŒJSONã€CSVã€TXTå¤šç§æ ¼å¼å¯¼å‡º"),
        ]
        
        for title, desc in help_info:
            print(f"\n{title}")
            print(f"  {desc}")
        
        print(f"\nğŸ”— å®˜æ–¹èµ„æº:")
        print("  ğŸ“– å®˜æ–¹æ–‡æ¡£: https://docs.crawl4ai.com/")
        print("  ğŸ™ GitHub: https://github.com/unclecode/crawl4ai")
        print("  ğŸ“¦ PyPI: https://pypi.org/project/crawl4ai/")
        
        print(f"\nğŸ’¡ ä½¿ç”¨æŠ€å·§:")
        tips = [
            "ä½¿ç”¨é«˜çº§é…ç½®å¯ä»¥å¤„ç†å¤æ‚çš„åŠ¨æ€ç½‘é¡µ",
            "LLMæå–åŠŸèƒ½éœ€è¦æœ‰æ•ˆçš„API Token",
            "æ‰¹é‡å¤„ç†æ—¶å»ºè®®æ§åˆ¶å¹¶å‘æ•°é¿å…è¢«é™åˆ¶",
            "JavaScriptæ¸²æŸ“é€‚ç”¨äºSPAåº”ç”¨å’ŒåŠ¨æ€åŠ è½½å†…å®¹",
            "å®šæœŸä¿å­˜ä¼šè¯ä»¥é¿å…ä¸¢å¤±å†å²è®°å½•"
        ]
        
        for i, tip in enumerate(tips, 1):
            print(f"  {i}. {tip}")
    
    async def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        self.display_banner()
        
        while True:
            try:
                self.display_main_menu()
                choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (è¾“å…¥æ•°å­—): ").strip()
                
                if choice == "1":
                    await self.basic_crawl()
                elif choice == "2":
                    await self.advanced_crawl()
                elif choice == "3":
                    await self.structured_extraction()
                elif choice == "4":
                    await self.llm_extraction()
                elif choice == "5":
                    await self.batch_crawl()
                elif choice == "6":
                    await self.javascript_rendering()
                elif choice == "7":
                    await self.content_filtering()
                elif choice == "8":
                    await self.pagination_crawl()
                elif choice == "9":
                    self.export_and_save()
                elif choice == "10":
                    self.session_management()
                elif choice == "11":
                    self.search_history()
                elif choice == "12":
                    await self.manage_browser_profiles()
                elif choice == "13":
                    self.manage_auto_save()
                elif choice == "14":
                    self.show_settings()
                elif choice == "15":
                    self.show_help()
                elif choice == "0":
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ Crawl4AI äº¤äº’å¼å·¥å…·!")
                    if self.session_data.get('auto_save', False):
                        self.save_config()
                        print("âœ… ä¼šè¯å·²è‡ªåŠ¨ä¿å­˜")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥æœ‰æ•ˆæ•°å­— (0-15)")
                
                # é™åˆ¶å†å²è®°å½•æ•°é‡
                max_history = self.session_data.get('max_history', 100)
                if len(self.results_history) > max_history:
                    self.results_history = self.results_history[-max_history:]
                
                input("\næŒ‰å›è½¦é”®ç»§ç»­...")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§!")
                break
            except Exception as e:
                print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
                print("ç¨‹åºå°†ç»§ç»­è¿è¡Œ...")
                input("æŒ‰å›è½¦é”®ç»§ç»­...")

def main():
    """ç¨‹åºå…¥å£"""
    try:
        app = InteractiveCrawl4AI()
        asyncio.run(app.run())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å†è§!")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
