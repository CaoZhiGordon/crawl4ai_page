#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦çˆ¬å–å™¨æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•æ·±åº¦çˆ¬å–å™¨çš„åŸºæœ¬åŠŸèƒ½ï¼Œç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½æ­£ç¡®å®‰è£…
å¹¶ä¸”çˆ¬å–å™¨èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚

è¿è¡Œ: python test_deep_crawler.py
"""

import asyncio
import sys
import os
from datetime import datetime

def test_imports():
    """æµ‹è¯•æ‰€æœ‰å¿…éœ€çš„åŒ…å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•ä¾èµ–åŒ…å¯¼å…¥...")
    
    missing_packages = []
    
    # æµ‹è¯• crawl4ai
    try:
        import crawl4ai
        print("âœ… crawl4ai - OK")
    except ImportError:
        print("âŒ crawl4ai - ç¼ºå¤±")
        missing_packages.append("crawl4ai")
    
    # æµ‹è¯• aiohttp
    try:
        import aiohttp
        print("âœ… aiohttp - OK")
    except ImportError:
        print("âŒ aiohttp - ç¼ºå¤±") 
        missing_packages.append("aiohttp")
    
    # æµ‹è¯• beautifulsoup4
    try:
        from bs4 import BeautifulSoup
        print("âœ… beautifulsoup4 - OK")
    except ImportError:
        print("âŒ beautifulsoup4 - ç¼ºå¤±")
        missing_packages.append("beautifulsoup4")
    
    # æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å—
    try:
        from deep_website_crawler import DeepWebsiteCrawler, CrawlConfig
        print("âœ… deep_website_crawler - OK")
    except ImportError as e:
        print(f"âŒ deep_website_crawler - å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    if missing_packages:
        print(f"\nâŒ ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print(f"è¯·è¿è¡Œ: pip install {' '.join(missing_packages)}")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å¯¼å…¥æˆåŠŸ!")
    return True

async def test_basic_crawl():
    """æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½"""
    print("\nğŸš€ æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½...")
    
    try:
        from deep_website_crawler import DeepWebsiteCrawler, CrawlConfig
        
        # åˆ›å»ºä¸€ä¸ªéå¸¸ç®€å•çš„æµ‹è¯•é…ç½®
        config = CrawlConfig(
            start_url="https://httpbin.org/html",  # ç®€å•çš„æµ‹è¯•é¡µé¢
            max_depth=1,                           # åªçˆ¬å–1å±‚
            max_pages=2,                           # æœ€å¤š2ä¸ªé¡µé¢
            concurrent_limit=1,                    # å•çº¿ç¨‹
            delay=1.0,                             # 1ç§’å»¶æ—¶
            same_domain_only=True,
            output_dir="test_crawl_output",
            merge_markdown=True
        )
        
        print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   URL: {config.start_url}")
        print(f"   æœ€å¤§æ·±åº¦: {config.max_depth}")
        print(f"   æœ€å¤§é¡µé¢: {config.max_pages}")
        
        # åˆ›å»ºçˆ¬å–å™¨å¹¶è¿è¡Œ
        crawler = DeepWebsiteCrawler(config)
        results = await crawler.crawl_website()
        
        # æ£€æŸ¥ç»“æœ
        if results:
            print(f"âœ… åŸºæœ¬çˆ¬å–æµ‹è¯•æˆåŠŸ!")
            print(f"   çˆ¬å–é¡µé¢æ•°: {len(results)}")
            
            # ç”Ÿæˆåˆå¹¶æ–‡æ¡£
            if config.merge_markdown:
                await crawler.generate_merged_markdown()
                print(f"âœ… åˆå¹¶æ–‡æ¡£ç”ŸæˆæˆåŠŸ!")
            
            return True
        else:
            print(f"âŒ åŸºæœ¬çˆ¬å–æµ‹è¯•å¤±è´¥: æ²¡æœ‰çˆ¬å–åˆ°é¡µé¢")
            return False
            
    except Exception as e:
        print(f"âŒ åŸºæœ¬çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_url_validation():
    """æµ‹è¯•URLéªŒè¯åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•URLéªŒè¯åŠŸèƒ½...")
    
    try:
        from deep_website_crawler import DeepWebsiteCrawler, CrawlConfig
        
        config = CrawlConfig(
            start_url="https://example.com",
            max_depth=2,
            max_pages=10
        )
        
        crawler = DeepWebsiteCrawler(config)
        
        # æµ‹è¯•å„ç§URL
        test_cases = [
            ("https://example.com/page1", 1, True),    # åº”è¯¥é€šè¿‡
            ("https://other.com/page1", 1, False),     # ä¸åŒåŸŸåï¼Œåº”è¯¥å¤±è´¥
            ("javascript:void(0)", 1, False),          # JavaScripté“¾æ¥ï¼Œåº”è¯¥å¤±è´¥
            ("https://example.com/file.pdf", 1, False),# PDFæ–‡ä»¶ï¼Œåº”è¯¥å¤±è´¥
            ("https://example.com/page1#anchor", 1, True), # é”šç‚¹ä¼šè¢«è§„èŒƒåŒ–
        ]
        
        all_passed = True
        for url, depth, expected in test_cases:
            result = crawler.is_valid_url(url, depth)
            if result == expected:
                print(f"âœ… {url[:50]}... -> {result} (ç¬¦åˆé¢„æœŸ)")
            else:
                print(f"âŒ {url[:50]}... -> {result} (é¢„æœŸ: {expected})")
                all_passed = False
        
        if all_passed:
            print("âœ… URLéªŒè¯æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
            return True
        else:
            print("âŒ éƒ¨åˆ†URLéªŒè¯æµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ URLéªŒè¯æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_configuration():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®ç³»ç»Ÿ...")
    
    try:
        from deep_website_crawler import CrawlConfig
        
        # æµ‹è¯•é»˜è®¤é…ç½®
        config1 = CrawlConfig(start_url="https://example.com")
        print(f"âœ… é»˜è®¤é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   é»˜è®¤æ·±åº¦: {config1.max_depth}")
        print(f"   é»˜è®¤é¡µé¢æ•°: {config1.max_pages}")
        
        # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
        config2 = CrawlConfig(
            start_url="https://example.com",
            max_depth=5,
            max_pages=200,
            concurrent_limit=10,
            delay=0.5
        )
        print(f"âœ… è‡ªå®šä¹‰é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   è‡ªå®šä¹‰æ·±åº¦: {config2.max_depth}")
        print(f"   è‡ªå®šä¹‰é¡µé¢æ•°: {config2.max_pages}")
        
        # éªŒè¯æ’é™¤æ¨¡å¼
        if config1.exclude_patterns:
            print(f"âœ… é»˜è®¤æ’é™¤æ¨¡å¼å·²è®¾ç½® ({len(config1.exclude_patterns)} ä¸ªè§„åˆ™)")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    try:
        import shutil
        test_dir = "test_crawl_output"
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)
            print(f"ğŸ§¹ å·²æ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_dir}")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª" + "=" * 50 + "ğŸ§ª")
    print("        æ·±åº¦ç½‘ç«™çˆ¬å–å™¨ - åŠŸèƒ½æµ‹è¯•")
    print("ğŸ§ª" + "=" * 50 + "ğŸ§ª")
    
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    test_results = []
    
    # 1. æµ‹è¯•å¯¼å…¥
    print(f"\n{'='*20} ç¬¬1é¡¹æµ‹è¯• {'='*20}")
    result1 = test_imports()
    test_results.append(("ä¾èµ–åŒ…å¯¼å…¥", result1))
    
    if not result1:
        print(f"\nâŒ ä¾èµ–åŒ…æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å…¶ä»–æµ‹è¯•")
        return
    
    # 2. æµ‹è¯•é…ç½®ç³»ç»Ÿ
    print(f"\n{'='*20} ç¬¬2é¡¹æµ‹è¯• {'='*20}")
    result2 = test_configuration()
    test_results.append(("é…ç½®ç³»ç»Ÿ", result2))
    
    # 3. æµ‹è¯•URLéªŒè¯
    print(f"\n{'='*20} ç¬¬3é¡¹æµ‹è¯• {'='*20}")
    result3 = await test_url_validation()
    test_results.append(("URLéªŒè¯", result3))
    
    # 4. æµ‹è¯•åŸºæœ¬çˆ¬å–ï¼ˆå¯é€‰ï¼Œå› ä¸ºéœ€è¦ç½‘ç»œè¿æ¥ï¼‰
    print(f"\n{'='*20} ç¬¬4é¡¹æµ‹è¯• {'='*20}")
    print("âš ï¸ æ³¨æ„: æ­¤æµ‹è¯•éœ€è¦ç½‘ç»œè¿æ¥ï¼Œå¯èƒ½éœ€è¦30-60ç§’...")
    
    try:
        user_input = input("æ˜¯å¦è¿›è¡Œç½‘ç»œçˆ¬å–æµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
        if user_input in ['y', 'yes', 'æ˜¯']:
            result4 = await test_basic_crawl()
            test_results.append(("åŸºæœ¬çˆ¬å–", result4))
        else:
            print("â­ï¸ è·³è¿‡ç½‘ç»œçˆ¬å–æµ‹è¯•")
            test_results.append(("åŸºæœ¬çˆ¬å–", None))  # æ ‡è®°ä¸ºè·³è¿‡
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        test_results.append(("åŸºæœ¬çˆ¬å–", False))
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ€»ç»“
    print(f"\n{'ğŸ¯'*20} æµ‹è¯•ç»“æœæ€»ç»“ {'ğŸ¯'*20}")
    
    passed = 0
    failed = 0
    skipped = 0
    
    for test_name, result in test_results:
        if result is True:
            print(f"âœ… {test_name}: é€šè¿‡")
            passed += 1
        elif result is False:
            print(f"âŒ {test_name}: å¤±è´¥")
            failed += 1
        else:
            print(f"â­ï¸ {test_name}: è·³è¿‡")
            skipped += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
    print(f"   âœ… é€šè¿‡: {passed}")
    print(f"   âŒ å¤±è´¥: {failed}")
    print(f"   â­ï¸ è·³è¿‡: {skipped}")
    
    if failed == 0:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! æ·±åº¦çˆ¬å–å™¨å·²å‡†å¤‡å°±ç»ª!")
        print(f"ğŸ’¡ æ‚¨å¯ä»¥è¿è¡Œ: python run_deep_crawler.py")
    else:
        print(f"\nâš ï¸ æœ‰ {failed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    cleanup_test_files()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ æµ‹è¯•è¢«ä¸­æ–­ï¼Œå†è§!")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

